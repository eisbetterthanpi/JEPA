{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WkwnVjJTrW1"
      },
      "outputs": [],
      "source": [
        "!pip install -qq procgen faiss-cpu vector-quantize-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "import faiss\n",
        "import torch\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "db2f71de-e691-4a0d-911d-59a9e56a38df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "\n",
        "ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FuA25qQknUAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c682a0e7-0965-43a8-805e-cbff0f7aea13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        # self.enc = nn.Sequential(nn.Linear(in_dim, d_model), nn.ReLU(),)\n",
        "        self.enc = get_res(d_model)\n",
        "        self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            )\n",
        "        # self.pred = gru(emb_dim, rnn_units, num_layers)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25.0 # 25.0 # λ\n",
        "        self.std_coeff=1.0 # 25.0 # µ\n",
        "        self.cov_coeff=25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy):\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.rand((batch,self.dim_z),device=device)*2 -1)#*self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "        optim = torch.optim.SGD([z], lr=3e3)\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        num_steps = 10\n",
        "        for i in range(num_steps):\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            sy_ = self.pred(sxaz)\n",
        "            # print(\"y_, y\",y_.shape, y.shape)\n",
        "            loss = lossfn(sy_, sy)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "        if loss.item()>0.1: print(\"argm\",loss.item(), z[0].item())\n",
        "        return z#.detach()\n",
        "\n",
        "    def loss(self, x, y, a, z=None):\n",
        "        sx, sy = self.enc(x), self.enc(y)\n",
        "        z = self.argm(sx, a, sy)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "        sy_ = self.pred(sxaz)\n",
        "        repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "        # v_c_loss = self.v_creg(self.exp(sx))\n",
        "        vx, vy = self.exp(sx), self.exp(sy)\n",
        "        v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "        return repr_loss + v_c_loss\n",
        "\n",
        "    def forward(self, sx, a): # state, ctrl\n",
        "        batch=sx.size(dim=0)\n",
        "        z=torch.zeros((batch,self.dim_z),device=device)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "        sy_ = self.pred(sxaz)\n",
        "        return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCD647ZpPrGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df2eaff-0c9e-456f-86fa-2810c3a004a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# @title agent\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # loss = loss + jloss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(jloss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShHQ_ynlwoyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "f8ef0421-575e-4ae7-a059-0a46a64e334f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "import pickle\n",
        "# def save(folder=''):\n",
        "#     agent.save(folder)\n",
        "#     with open(folder+'buffer.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "# def load(folder=''):\n",
        "#     agent.load(folder)\n",
        "#     with open(folder+'buffer.pkl', 'rb') as f: return pickle.load(f)\n",
        "\n",
        "\n",
        "def save(folder, name='agent.pth'):\n",
        "    torch.save(agent.state_dict(), folder+name)\n",
        "    agent.mem.save(file=folder+name)\n",
        "    with open(folder+'buffer.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "def load(folder, name='agent.pth'):\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=torch.device(device)), strict=False)\n",
        "    # agent.load_state_dict(torch.load(folder+name, map_location=device), strict=False)\n",
        "    # torch.load(folder+name, map_location=torch.device('cpu'))\n",
        "    # agent.mem.load(file=folder+name)\n",
        "    with open(folder+'buffer.pkl', 'rb') as f: return pickle.load(f)\n",
        "\n",
        "# save(folder)\n",
        "# save(folder, name='agentres-4.pth')\n",
        "buffer = load(folder)\n",
        "# save('/content/')\n",
        "# buffer = load('/content/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubGk_uT3FL-S"
      },
      "outputs": [],
      "source": [
        "# name='agent.pth'\n",
        "# print(folder+name)\n",
        "# torch.load(folder+name, map_location='o')\n",
        "# with open(folder+'buffer_rand512.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "with open(folder+'buffer_rand512.pkl', 'rb') as f: buffer = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjm2kV3H7ZVR",
        "outputId": "d4040132-28f1-4347-8028-2e951476da85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6872065\n"
          ]
        }
      ],
      "source": [
        "print(sum(p.numel() for p in agent.parameters() if p.requires_grad))\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NVcknabHMxH6"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = self.data_process(buffer)\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    # def data_process(self, data): # str 10780437\n",
        "    #     return torch.tensor([self.stoi.get(c) for c in data]) # list of int 4570571 # stoi.get(c,UNK_IDX)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        # state = list(state)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(\"__getitem__\",state)\n",
        "        return state, action, reward\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "    def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "        lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "        with torch.no_grad():\n",
        "            imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "            data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "            # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "            data=data.flatten(start_dim=-3)\n",
        "            data=lin(data) # random projection\n",
        "            data = F.normalize(data, dim=-1)\n",
        "            idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "            sample = data[idx]\n",
        "            index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "            # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "            index.add(data)\n",
        "            D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "            priority = (2**-D).sum(-1) # L2\n",
        "            # priority = -D.sum(-1) # IP\n",
        "            topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "            index_list = idx[topk.values] # most clustered\n",
        "            for i in reversed(index_list): data.pop(i)\n",
        "        return data\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PraFUAPB3j7v"
      },
      "outputs": [],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    # out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    out = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cm6KjvBrnNO",
        "outputId": "0002ec1a-a37d-49e1-cf63-040f0ee1a6c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 #### train ####\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "         [ 7.9966e-04, -1.0531e-03, -1.1339e-03,  ...,  4.8271e-04,\n",
            "           1.6954e-04, -7.7209e-05]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2024, 0.1527, 0.1658,  ..., 0.1353, 0.1838, 0.0842],\n",
            "        [0.2031, 0.1558, 0.1695,  ..., 0.1354, 0.1850, 0.0833],\n",
            "        [0.2011, 0.1538, 0.1660,  ..., 0.1359, 0.1865, 0.0840],\n",
            "        ...,\n",
            "        [0.2000, 0.1521, 0.1653,  ..., 0.1340, 0.1830, 0.0853],\n",
            "        [0.2018, 0.1537, 0.1665,  ..., 0.1347, 0.1836, 0.0840],\n",
            "        [0.2042, 0.1543, 0.1666,  ..., 0.1385, 0.1873, 0.0842]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 5.2623e-03, -1.1290e-02, -2.5074e-03,  ..., -6.3948e-04,\n",
            "           6.6883e-03, -1.0155e-02],\n",
            "         [-2.1471e-04,  4.6065e-04,  1.0230e-04,  ...,  2.6092e-05,\n",
            "          -2.7289e-04,  4.1433e-04],\n",
            "         [-4.9658e-03,  1.0654e-02,  2.3661e-03,  ...,  6.0345e-04,\n",
            "          -6.3114e-03,  9.5826e-03],\n",
            "         ...,\n",
            "         [-1.6194e-02,  3.4744e-02,  7.7162e-03,  ...,  1.9680e-03,\n",
            "          -2.0583e-02,  3.1250e-02],\n",
            "         [ 1.0737e-02, -2.3036e-02, -5.1161e-03,  ..., -1.3048e-03,\n",
            "           1.3647e-02, -2.0720e-02],\n",
            "         [-5.2826e-03,  1.1333e-02,  2.5170e-03,  ...,  6.4195e-04,\n",
            "          -6.7141e-03,  1.0194e-02]],\n",
            "\n",
            "        [[ 1.3771e-03, -1.4618e-03, -1.9759e-03,  ...,  7.0935e-04,\n",
            "           8.3207e-04,  1.8946e-04],\n",
            "         [-6.5367e-04,  6.9389e-04,  9.3791e-04,  ..., -3.3671e-04,\n",
            "          -3.9496e-04, -8.9933e-05],\n",
            "         [ 1.9235e-03, -2.0419e-03, -2.7600e-03,  ...,  9.9082e-04,\n",
            "           1.1622e-03,  2.6464e-04],\n",
            "         ...,\n",
            "         [ 1.3806e-03, -1.4656e-03, -1.9810e-03,  ...,  7.1117e-04,\n",
            "           8.3420e-04,  1.8995e-04],\n",
            "         [ 6.2534e-03, -6.6382e-03, -8.9727e-03,  ...,  3.2212e-03,\n",
            "           3.7784e-03,  8.6036e-04],\n",
            "         [ 6.8130e-03, -7.2322e-03, -9.7757e-03,  ...,  3.5094e-03,\n",
            "           4.1166e-03,  9.3736e-04]],\n",
            "\n",
            "        [[ 2.2795e-03, -4.5513e-03, -3.8138e-03,  ...,  1.3995e-03,\n",
            "           8.5507e-04, -5.0412e-04],\n",
            "         [-1.0776e-02,  2.1515e-02,  1.8029e-02,  ..., -6.6158e-03,\n",
            "          -4.0422e-03,  2.3831e-03],\n",
            "         [-7.5135e-04,  1.5002e-03,  1.2571e-03,  ..., -4.6130e-04,\n",
            "          -2.8184e-04,  1.6616e-04],\n",
            "         ...,\n",
            "         [ 3.4539e-03, -6.8961e-03, -5.7787e-03,  ...,  2.1205e-03,\n",
            "           1.2956e-03, -7.6384e-04],\n",
            "         [ 9.6351e-03, -1.9238e-02, -1.6120e-02,  ...,  5.9155e-03,\n",
            "           3.6143e-03, -2.1308e-03],\n",
            "         [-1.1709e-04,  2.3379e-04,  1.9591e-04,  ..., -7.1891e-05,\n",
            "          -4.3924e-05,  2.5896e-05]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-4.7797e-04,  4.6270e-04,  3.5398e-04,  ...,  9.1185e-05,\n",
            "          -5.8141e-05,  1.5434e-04],\n",
            "         [-7.4348e-03,  7.1972e-03,  5.5062e-03,  ...,  1.4184e-03,\n",
            "          -9.0437e-04,  2.4007e-03],\n",
            "         [-2.0928e-03,  2.0259e-03,  1.5499e-03,  ...,  3.9925e-04,\n",
            "          -2.5457e-04,  6.7578e-04],\n",
            "         ...,\n",
            "         [ 7.6985e-03, -7.4525e-03, -5.7014e-03,  ..., -1.4687e-03,\n",
            "           9.3644e-04, -2.4859e-03],\n",
            "         [ 1.0545e-02, -1.0208e-02, -7.8092e-03,  ..., -2.0116e-03,\n",
            "           1.2826e-03, -3.4049e-03],\n",
            "         [ 6.7141e-03, -6.4996e-03, -4.9724e-03,  ..., -1.2809e-03,\n",
            "           8.1671e-04, -2.1680e-03]],\n",
            "\n",
            "        [[ 7.7764e-03, -1.6718e-02, -7.6746e-04,  ...,  4.6986e-04,\n",
            "           5.5377e-03, -1.3173e-02],\n",
            "         [-7.9931e-03,  1.7184e-02,  7.8884e-04,  ..., -4.8295e-04,\n",
            "          -5.6919e-03,  1.3540e-02],\n",
            "         [-3.9864e-03,  8.5701e-03,  3.9343e-04,  ..., -2.4087e-04,\n",
            "          -2.8388e-03,  6.7528e-03],\n",
            "         ...,\n",
            "         [-1.5758e-02,  3.3876e-02,  1.5551e-03,  ..., -9.5209e-04,\n",
            "          -1.1221e-02,  2.6693e-02],\n",
            "         [ 1.0784e-02, -2.3183e-02, -1.0642e-03,  ...,  6.5156e-04,\n",
            "           7.6791e-03, -1.8267e-02],\n",
            "         [-5.3978e-03,  1.1604e-02,  5.3271e-04,  ..., -3.2614e-04,\n",
            "          -3.8438e-03,  9.1436e-03]],\n",
            "\n",
            "        [[ 6.5317e-03, -9.5719e-03, -9.2148e-03,  ...,  4.4585e-03,\n",
            "           8.6583e-04, -1.7933e-03],\n",
            "         [-9.5622e-03,  1.4013e-02,  1.3490e-02,  ..., -6.5271e-03,\n",
            "          -1.2676e-03,  2.6253e-03],\n",
            "         [-4.9643e-03,  7.2749e-03,  7.0035e-03,  ..., -3.3886e-03,\n",
            "          -6.5806e-04,  1.3630e-03],\n",
            "         ...,\n",
            "         [ 4.4951e-03, -6.5874e-03, -6.3416e-03,  ...,  3.0684e-03,\n",
            "           5.9587e-04, -1.2342e-03],\n",
            "         [ 9.5516e-03, -1.3997e-02, -1.3475e-02,  ...,  6.5199e-03,\n",
            "           1.2661e-03, -2.6224e-03],\n",
            "         [ 8.4826e-05, -1.2431e-04, -1.1967e-04,  ...,  5.7902e-05,\n",
            "           1.1244e-05, -2.3289e-05]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2012, 0.1530, 0.1665,  ..., 0.1351, 0.1842, 0.0844],\n",
            "        [0.2020, 0.1529, 0.1660,  ..., 0.1352, 0.1840, 0.0844],\n",
            "        [0.2035, 0.1539, 0.1662,  ..., 0.1379, 0.1868, 0.0842],\n",
            "        ...,\n",
            "        [0.2006, 0.1520, 0.1646,  ..., 0.1340, 0.1830, 0.0851],\n",
            "        [0.2062, 0.1541, 0.1689,  ..., 0.1386, 0.1871, 0.0842],\n",
            "        [0.2007, 0.1537, 0.1661,  ..., 0.1356, 0.1866, 0.0840]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 5.3489e-03, -1.2456e-02, -2.5886e-03,  ..., -7.5093e-04,\n",
            "           7.5626e-03, -1.1683e-02],\n",
            "         [-5.1886e-04,  1.2082e-03,  2.5110e-04,  ...,  7.2842e-05,\n",
            "          -7.3360e-04,  1.1333e-03],\n",
            "         [-4.6364e-03,  1.0796e-02,  2.2438e-03,  ...,  6.5090e-04,\n",
            "          -6.5552e-03,  1.0127e-02],\n",
            "         ...,\n",
            "         [-1.4823e-02,  3.4518e-02,  7.1736e-03,  ...,  2.0810e-03,\n",
            "          -2.0958e-02,  3.2377e-02],\n",
            "         [ 1.0181e-02, -2.3707e-02, -4.9269e-03,  ..., -1.4292e-03,\n",
            "           1.4394e-02, -2.2237e-02],\n",
            "         [-6.1513e-03,  1.4324e-02,  2.9769e-03,  ...,  8.6358e-04,\n",
            "          -8.6971e-03,  1.3436e-02]],\n",
            "\n",
            "        [[ 6.4289e-04, -6.3432e-04, -8.4910e-04,  ...,  2.1631e-04,\n",
            "           3.1689e-04,  1.1565e-04],\n",
            "         [ 1.2407e-03, -1.2242e-03, -1.6387e-03,  ...,  4.1746e-04,\n",
            "           6.1157e-04,  2.2319e-04],\n",
            "         [ 1.4400e-03, -1.4208e-03, -1.9019e-03,  ...,  4.8451e-04,\n",
            "           7.0980e-04,  2.5904e-04],\n",
            "         ...,\n",
            "         [ 1.7217e-03, -1.6987e-03, -2.2739e-03,  ...,  5.7928e-04,\n",
            "           8.4864e-04,  3.0971e-04],\n",
            "         [ 5.7105e-03, -5.6344e-03, -7.5421e-03,  ...,  1.9214e-03,\n",
            "           2.8148e-03,  1.0272e-03],\n",
            "         [ 6.7976e-03, -6.7070e-03, -8.9779e-03,  ...,  2.2871e-03,\n",
            "           3.3507e-03,  1.2228e-03]],\n",
            "\n",
            "        [[ 3.6289e-03, -6.2263e-03, -4.9465e-03,  ...,  1.8725e-03,\n",
            "           1.2987e-03, -1.3910e-04],\n",
            "         [-1.1966e-02,  2.0531e-02,  1.6311e-02,  ..., -6.1745e-03,\n",
            "          -4.2826e-03,  4.5867e-04],\n",
            "         [-8.3628e-04,  1.4349e-03,  1.1399e-03,  ..., -4.3152e-04,\n",
            "          -2.9929e-04,  3.2055e-05],\n",
            "         ...,\n",
            "         [ 4.2573e-03, -7.3045e-03, -5.8031e-03,  ...,  2.1967e-03,\n",
            "           1.5236e-03, -1.6318e-04],\n",
            "         [ 1.0378e-02, -1.7807e-02, -1.4147e-02,  ...,  5.3552e-03,\n",
            "           3.7142e-03, -3.9781e-04],\n",
            "         [ 8.6583e-05, -1.4856e-04, -1.1802e-04,  ...,  4.4676e-05,\n",
            "           3.0987e-05, -3.3188e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.7133e-03, -1.6700e-03, -1.2275e-03,  ..., -2.2423e-04,\n",
            "           2.6539e-04, -7.9614e-04],\n",
            "         [-7.2827e-03,  7.0985e-03,  5.2175e-03,  ...,  9.5310e-04,\n",
            "          -1.1281e-03,  3.3841e-03],\n",
            "         [-3.9640e-03,  3.8637e-03,  2.8398e-03,  ...,  5.1877e-04,\n",
            "          -6.1401e-04,  1.8420e-03],\n",
            "         ...,\n",
            "         [ 7.9063e-03, -7.7063e-03, -5.6642e-03,  ..., -1.0347e-03,\n",
            "           1.2247e-03, -3.6739e-03],\n",
            "         [ 8.2799e-03, -8.0705e-03, -5.9318e-03,  ..., -1.0836e-03,\n",
            "           1.2825e-03, -3.8475e-03],\n",
            "         [ 5.5166e-03, -5.3771e-03, -3.9522e-03,  ..., -7.2197e-04,\n",
            "           8.5451e-04, -2.5635e-03]],\n",
            "\n",
            "        [[ 7.1832e-03, -1.6515e-02, -2.3743e-03,  ...,  6.0266e-04,\n",
            "           6.3394e-03, -1.3141e-02],\n",
            "         [-6.5388e-03,  1.5033e-02,  2.1613e-03,  ..., -5.4859e-04,\n",
            "          -5.7707e-03,  1.1962e-02],\n",
            "         [-3.3328e-03,  7.6626e-03,  1.1016e-03,  ..., -2.7962e-04,\n",
            "          -2.9413e-03,  6.0971e-03],\n",
            "         ...,\n",
            "         [-1.4350e-02,  3.2993e-02,  4.7432e-03,  ..., -1.2040e-03,\n",
            "          -1.2665e-02,  2.6252e-02],\n",
            "         [ 1.1249e-02, -2.5864e-02, -3.7183e-03,  ...,  9.4381e-04,\n",
            "           9.9281e-03, -2.0580e-02],\n",
            "         [-5.1992e-03,  1.1954e-02,  1.7185e-03,  ..., -4.3621e-04,\n",
            "          -4.5885e-03,  9.5116e-03]],\n",
            "\n",
            "        [[ 6.6342e-03, -8.8157e-03, -7.7177e-03,  ...,  3.4484e-03,\n",
            "           1.2069e-03, -9.5433e-04],\n",
            "         [-9.2244e-03,  1.2258e-02,  1.0731e-02,  ..., -4.7948e-03,\n",
            "          -1.6781e-03,  1.3269e-03],\n",
            "         [-5.5189e-03,  7.3337e-03,  6.4202e-03,  ..., -2.8687e-03,\n",
            "          -1.0040e-03,  7.9389e-04],\n",
            "         ...,\n",
            "         [ 4.7747e-03, -6.3448e-03, -5.5545e-03,  ...,  2.4819e-03,\n",
            "           8.6861e-04, -6.8684e-04],\n",
            "         [ 1.0119e-02, -1.3447e-02, -1.1772e-02,  ...,  5.2601e-03,\n",
            "           1.8409e-03, -1.4557e-03],\n",
            "         [ 1.5544e-03, -2.0655e-03, -1.8082e-03,  ...,  8.0797e-04,\n",
            "           2.8277e-04, -2.2360e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2043, 0.1537, 0.1661,  ..., 0.1359, 0.1846, 0.0851],\n",
            "        [0.2061, 0.1568, 0.1694,  ..., 0.1365, 0.1856, 0.0860],\n",
            "        [0.2010, 0.1537, 0.1661,  ..., 0.1358, 0.1865, 0.0840],\n",
            "        ...,\n",
            "        [0.2003, 0.1520, 0.1663,  ..., 0.1348, 0.1850, 0.0859],\n",
            "        [0.2049, 0.1572, 0.1712,  ..., 0.1380, 0.1872, 0.0845],\n",
            "        [0.2010, 0.1538, 0.1660,  ..., 0.1358, 0.1865, 0.0840]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 5.6465e-03, -1.2760e-02, -2.0749e-03,  ..., -1.0260e-03,\n",
            "           8.5640e-03, -1.1941e-02],\n",
            "         [-5.6505e-04,  1.2769e-03,  2.0764e-04,  ...,  1.0268e-04,\n",
            "          -8.5700e-04,  1.1950e-03],\n",
            "         [-5.1794e-03,  1.1705e-02,  1.9033e-03,  ...,  9.4116e-04,\n",
            "          -7.8556e-03,  1.0954e-02],\n",
            "         ...,\n",
            "         [-1.6222e-02,  3.6660e-02,  5.9612e-03,  ...,  2.9478e-03,\n",
            "          -2.4604e-02,  3.4307e-02],\n",
            "         [ 1.1133e-02, -2.5159e-02, -4.0910e-03,  ..., -2.0230e-03,\n",
            "           1.6885e-02, -2.3544e-02],\n",
            "         [-6.9607e-03,  1.5730e-02,  2.5578e-03,  ...,  1.2649e-03,\n",
            "          -1.0557e-02,  1.4721e-02]],\n",
            "\n",
            "        [[ 1.6665e-03, -2.0245e-03, -2.2442e-03,  ...,  5.8842e-04,\n",
            "           8.4211e-04,  3.4225e-04],\n",
            "         [ 9.1326e-05, -1.1094e-04, -1.2298e-04,  ...,  3.2246e-05,\n",
            "           4.6148e-05,  1.8755e-05],\n",
            "         [ 2.8801e-04, -3.4987e-04, -3.8785e-04,  ...,  1.0169e-04,\n",
            "           1.4554e-04,  5.9149e-05],\n",
            "         ...,\n",
            "         [ 2.3051e-03, -2.8002e-03, -3.1041e-03,  ...,  8.1389e-04,\n",
            "           1.1648e-03,  4.7339e-04],\n",
            "         [ 4.7270e-03, -5.7422e-03, -6.3655e-03,  ...,  1.6690e-03,\n",
            "           2.3886e-03,  9.7076e-04],\n",
            "         [ 6.6017e-03, -8.0196e-03, -8.8901e-03,  ...,  2.3309e-03,\n",
            "           3.3359e-03,  1.3558e-03]],\n",
            "\n",
            "        [[ 3.8248e-03, -6.8213e-03, -6.5072e-03,  ...,  2.7661e-03,\n",
            "           1.8375e-03, -7.6029e-04],\n",
            "         [-1.0495e-02,  1.8717e-02,  1.7855e-02,  ..., -7.5901e-03,\n",
            "          -5.0420e-03,  2.0862e-03],\n",
            "         [-1.9459e-03,  3.4703e-03,  3.3106e-03,  ..., -1.4073e-03,\n",
            "          -9.3482e-04,  3.8680e-04],\n",
            "         ...,\n",
            "         [ 3.7677e-03, -6.7196e-03, -6.4101e-03,  ...,  2.7249e-03,\n",
            "           1.8101e-03, -7.4895e-04],\n",
            "         [ 9.6273e-03, -1.7170e-02, -1.6379e-02,  ...,  6.9626e-03,\n",
            "           4.6251e-03, -1.9137e-03],\n",
            "         [ 2.7046e-04, -4.8235e-04, -4.6014e-04,  ...,  1.9560e-04,\n",
            "           1.2993e-04, -5.3762e-05]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.1873e-03, -2.2944e-03, -1.8733e-03,  ..., -5.9882e-05,\n",
            "           5.3866e-04, -1.0030e-03],\n",
            "         [-6.1380e-03,  6.4385e-03,  5.2569e-03,  ...,  1.6804e-04,\n",
            "          -1.5116e-03,  2.8146e-03],\n",
            "         [-2.4813e-03,  2.6027e-03,  2.1251e-03,  ...,  6.7930e-05,\n",
            "          -6.1105e-04,  1.1378e-03],\n",
            "         ...,\n",
            "         [ 5.9903e-03, -6.2836e-03, -5.1304e-03,  ..., -1.6400e-04,\n",
            "           1.4752e-03, -2.7468e-03],\n",
            "         [ 8.9609e-03, -9.3995e-03, -7.6745e-03,  ..., -2.4532e-04,\n",
            "           2.2068e-03, -4.1090e-03],\n",
            "         [ 6.2232e-03, -6.5278e-03, -5.3298e-03,  ..., -1.7037e-04,\n",
            "           1.5326e-03, -2.8536e-03]],\n",
            "\n",
            "        [[ 6.4375e-03, -1.6586e-02, -1.5107e-03,  ...,  6.3375e-04,\n",
            "           6.5872e-03, -1.4907e-02],\n",
            "         [-6.1484e-03,  1.5841e-02,  1.4428e-03,  ..., -6.0529e-04,\n",
            "          -6.2913e-03,  1.4237e-02],\n",
            "         [-2.1772e-03,  5.6095e-03,  5.1092e-04,  ..., -2.1434e-04,\n",
            "          -2.2278e-03,  5.0417e-03],\n",
            "         ...,\n",
            "         [-1.2173e-02,  3.1364e-02,  2.8567e-03,  ..., -1.1984e-03,\n",
            "          -1.2456e-02,  2.8189e-02],\n",
            "         [ 9.9726e-03, -2.5694e-02, -2.3402e-03,  ...,  9.8176e-04,\n",
            "           1.0204e-02, -2.3093e-02],\n",
            "         [-3.6351e-03,  9.3656e-03,  8.5304e-04,  ..., -3.5786e-04,\n",
            "          -3.7196e-03,  8.4176e-03]],\n",
            "\n",
            "        [[ 6.5438e-03, -9.2711e-03, -8.7417e-03,  ...,  4.1141e-03,\n",
            "           1.2501e-03, -7.5873e-04],\n",
            "         [-1.0429e-02,  1.4776e-02,  1.3932e-02,  ..., -6.5567e-03,\n",
            "          -1.9923e-03,  1.2092e-03],\n",
            "         [-6.2080e-03,  8.7954e-03,  8.2932e-03,  ..., -3.9030e-03,\n",
            "          -1.1859e-03,  7.1980e-04],\n",
            "         ...,\n",
            "         [ 3.9322e-03, -5.5710e-03, -5.2529e-03,  ...,  2.4722e-03,\n",
            "           7.5117e-04, -4.5592e-04],\n",
            "         [ 1.0506e-02, -1.4885e-02, -1.4035e-02,  ...,  6.6053e-03,\n",
            "           2.0070e-03, -1.2182e-03],\n",
            "         [ 7.3320e-04, -1.0388e-03, -9.7947e-04,  ...,  4.6097e-04,\n",
            "           1.4006e-04, -8.5013e-05]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2034, 0.1534, 0.1647,  ..., 0.1342, 0.1837, 0.0853],\n",
            "        [0.2047, 0.1538, 0.1661,  ..., 0.1359, 0.1849, 0.0851],\n",
            "        [0.2028, 0.1536, 0.1658,  ..., 0.1372, 0.1865, 0.0839],\n",
            "        ...,\n",
            "        [0.2046, 0.1542, 0.1655,  ..., 0.1347, 0.1842, 0.0859],\n",
            "        [0.2015, 0.1538, 0.1667,  ..., 0.1347, 0.1837, 0.0839],\n",
            "        [0.2053, 0.1547, 0.1675,  ..., 0.1396, 0.1880, 0.0843]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 5.5484e-03, -1.3215e-02, -2.5879e-03,  ..., -1.5027e-03,\n",
            "           1.0225e-02, -1.2587e-02],\n",
            "         [-1.0865e-03,  2.5877e-03,  5.0675e-04,  ...,  2.9424e-04,\n",
            "          -2.0023e-03,  2.4648e-03],\n",
            "         [-4.7719e-03,  1.1366e-02,  2.2257e-03,  ...,  1.2924e-03,\n",
            "          -8.7943e-03,  1.0826e-02],\n",
            "         ...,\n",
            "         [-1.4677e-02,  3.4957e-02,  6.8455e-03,  ...,  3.9748e-03,\n",
            "          -2.7048e-02,  3.3296e-02],\n",
            "         [ 1.0337e-02, -2.4620e-02, -4.8213e-03,  ..., -2.7995e-03,\n",
            "           1.9050e-02, -2.3451e-02],\n",
            "         [-6.7239e-03,  1.6015e-02,  3.1362e-03,  ...,  1.8210e-03,\n",
            "          -1.2392e-02,  1.5254e-02]],\n",
            "\n",
            "        [[ 1.6656e-03, -2.0956e-03, -2.1556e-03,  ...,  3.6924e-04,\n",
            "           7.2333e-04,  4.7424e-04],\n",
            "         [-3.0556e-04,  3.8445e-04,  3.9545e-04,  ..., -6.7739e-05,\n",
            "          -1.3270e-04, -8.7003e-05],\n",
            "         [ 4.6027e-04, -5.7911e-04, -5.9567e-04,  ...,  1.0204e-04,\n",
            "           1.9989e-04,  1.3105e-04],\n",
            "         ...,\n",
            "         [ 1.1008e-03, -1.3850e-03, -1.4247e-03,  ...,  2.4404e-04,\n",
            "           4.7806e-04,  3.1344e-04],\n",
            "         [ 4.0047e-03, -5.0387e-03, -5.1828e-03,  ...,  8.8780e-04,\n",
            "           1.7392e-03,  1.1403e-03],\n",
            "         [ 5.5456e-03, -6.9774e-03, -7.1770e-03,  ...,  1.2294e-03,\n",
            "           2.4083e-03,  1.5790e-03]],\n",
            "\n",
            "        [[ 4.5221e-03, -7.4709e-03, -6.2546e-03,  ...,  2.1668e-03,\n",
            "           1.9357e-03,  1.1626e-04],\n",
            "         [-1.0803e-02,  1.7848e-02,  1.4942e-02,  ..., -5.1763e-03,\n",
            "          -4.6243e-03, -2.7775e-04],\n",
            "         [-1.4598e-03,  2.4117e-03,  2.0191e-03,  ..., -6.9946e-04,\n",
            "          -6.2486e-04, -3.7531e-05],\n",
            "         ...,\n",
            "         [ 3.2965e-03, -5.4461e-03, -4.5595e-03,  ...,  1.5795e-03,\n",
            "           1.4110e-03,  8.4753e-05],\n",
            "         [ 9.9535e-03, -1.6444e-02, -1.3767e-02,  ...,  4.7692e-03,\n",
            "           4.2606e-03,  2.5591e-04],\n",
            "         [ 6.8191e-05, -1.1266e-04, -9.4317e-05,  ...,  3.2674e-05,\n",
            "           2.9189e-05,  1.7532e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.0546e-03, -1.1267e-03, -8.6372e-04,  ..., -4.7810e-05,\n",
            "           2.0445e-04, -3.7455e-04],\n",
            "         [-4.1871e-03,  4.4733e-03,  3.4292e-03,  ...,  1.8982e-04,\n",
            "          -8.1171e-04,  1.4871e-03],\n",
            "         [-3.0909e-03,  3.3022e-03,  2.5314e-03,  ...,  1.4012e-04,\n",
            "          -5.9921e-04,  1.0978e-03],\n",
            "         ...,\n",
            "         [ 6.9670e-03, -7.4432e-03, -5.7059e-03,  ..., -3.1584e-04,\n",
            "           1.3506e-03, -2.4744e-03],\n",
            "         [ 8.2773e-03, -8.8431e-03, -6.7790e-03,  ..., -3.7524e-04,\n",
            "           1.6046e-03, -2.9398e-03],\n",
            "         [ 5.3365e-03, -5.7013e-03, -4.3706e-03,  ..., -2.4193e-04,\n",
            "           1.0345e-03, -1.8953e-03]],\n",
            "\n",
            "        [[ 6.2163e-03, -1.5079e-02, -9.1806e-04,  ...,  3.2203e-04,\n",
            "           6.6308e-03, -1.3519e-02],\n",
            "         [-5.8905e-03,  1.4289e-02,  8.6995e-04,  ..., -3.0516e-04,\n",
            "          -6.2833e-03,  1.2810e-02],\n",
            "         [-1.4417e-03,  3.4972e-03,  2.1292e-04,  ..., -7.4686e-05,\n",
            "          -1.5378e-03,  3.1352e-03],\n",
            "         ...,\n",
            "         [-1.2759e-02,  3.0950e-02,  1.8843e-03,  ..., -6.6098e-04,\n",
            "          -1.3610e-02,  2.7747e-02],\n",
            "         [ 9.4734e-03, -2.2980e-02, -1.3991e-03,  ...,  4.9077e-04,\n",
            "           1.0105e-02, -2.0602e-02],\n",
            "         [-3.5696e-03,  8.6591e-03,  5.2719e-04,  ..., -1.8492e-04,\n",
            "          -3.8077e-03,  7.7629e-03]],\n",
            "\n",
            "        [[ 7.0836e-03, -8.1449e-03, -7.8045e-03,  ...,  3.3630e-03,\n",
            "           4.6728e-04, -1.5577e-03],\n",
            "         [-1.0560e-02,  1.2142e-02,  1.1634e-02,  ..., -5.0133e-03,\n",
            "          -6.9658e-04,  2.3220e-03],\n",
            "         [-7.2222e-03,  8.3042e-03,  7.9572e-03,  ..., -3.4288e-03,\n",
            "          -4.7642e-04,  1.5882e-03],\n",
            "         ...,\n",
            "         [ 3.3866e-03, -3.8940e-03, -3.7313e-03,  ...,  1.6078e-03,\n",
            "           2.2340e-04, -7.4471e-04],\n",
            "         [ 1.1282e-02, -1.2973e-02, -1.2430e-02,  ...,  5.3564e-03,\n",
            "           7.4425e-04, -2.4810e-03],\n",
            "         [ 1.0055e-03, -1.1561e-03, -1.1078e-03,  ...,  4.7736e-04,\n",
            "           6.6328e-05, -2.2110e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2038, 0.1523, 0.1655,  ..., 0.1360, 0.1850, 0.0846],\n",
            "        [0.2036, 0.1523, 0.1655,  ..., 0.1359, 0.1847, 0.0845],\n",
            "        [0.2018, 0.1536, 0.1658,  ..., 0.1364, 0.1865, 0.0839],\n",
            "        ...,\n",
            "        [0.2056, 0.1541, 0.1664,  ..., 0.1361, 0.1854, 0.0853],\n",
            "        [0.2053, 0.1565, 0.1690,  ..., 0.1361, 0.1852, 0.0858],\n",
            "        [0.2027, 0.1536, 0.1657,  ..., 0.1371, 0.1865, 0.0839]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 5.9330e-03, -1.2411e-02, -3.1539e-03,  ..., -1.5285e-03,\n",
            "           9.7729e-03, -1.1728e-02],\n",
            "         [-4.1598e-04,  8.7016e-04,  2.2113e-04,  ...,  1.0716e-04,\n",
            "          -6.8520e-04,  8.2225e-04],\n",
            "         [-4.9679e-03,  1.0392e-02,  2.6409e-03,  ...,  1.2798e-03,\n",
            "          -8.1833e-03,  9.8200e-03],\n",
            "         ...,\n",
            "         [-1.6875e-02,  3.5301e-02,  8.9708e-03,  ...,  4.3475e-03,\n",
            "          -2.7797e-02,  3.3357e-02],\n",
            "         [ 1.1241e-02, -2.3515e-02, -5.9758e-03,  ..., -2.8960e-03,\n",
            "           1.8517e-02, -2.2220e-02],\n",
            "         [-7.5359e-03,  1.5764e-02,  4.0060e-03,  ...,  1.9414e-03,\n",
            "          -1.2413e-02,  1.4896e-02]],\n",
            "\n",
            "        [[ 1.5822e-03, -2.0378e-03, -2.2419e-03,  ...,  6.1422e-04,\n",
            "           7.6666e-04,  4.7292e-04],\n",
            "         [-4.2668e-04,  5.4953e-04,  6.0457e-04,  ..., -1.6564e-04,\n",
            "          -2.0675e-04, -1.2753e-04],\n",
            "         [ 9.6338e-05, -1.2408e-04, -1.3650e-04,  ...,  3.7399e-05,\n",
            "           4.6680e-05,  2.8795e-05],\n",
            "         ...,\n",
            "         [ 9.1562e-04, -1.1792e-03, -1.2973e-03,  ...,  3.5545e-04,\n",
            "           4.4366e-04,  2.7368e-04],\n",
            "         [ 3.3288e-03, -4.2872e-03, -4.7166e-03,  ...,  1.2922e-03,\n",
            "           1.6129e-03,  9.9495e-04],\n",
            "         [ 4.8190e-03, -6.2065e-03, -6.8280e-03,  ...,  1.8707e-03,\n",
            "           2.3350e-03,  1.4404e-03]],\n",
            "\n",
            "        [[ 4.3320e-03, -7.8182e-03, -8.2674e-03,  ...,  3.9153e-03,\n",
            "           2.9550e-03, -9.3571e-04],\n",
            "         [-1.0250e-02,  1.8499e-02,  1.9562e-02,  ..., -9.2644e-03,\n",
            "          -6.9922e-03,  2.2141e-03],\n",
            "         [-2.3314e-03,  4.2076e-03,  4.4494e-03,  ..., -2.1071e-03,\n",
            "          -1.5903e-03,  5.0358e-04],\n",
            "         ...,\n",
            "         [ 2.5867e-03, -4.6683e-03, -4.9366e-03,  ...,  2.3379e-03,\n",
            "           1.7645e-03, -5.5872e-04],\n",
            "         [ 9.4457e-03, -1.7047e-02, -1.8027e-02,  ...,  8.5371e-03,\n",
            "           6.4433e-03, -2.0403e-03],\n",
            "         [-2.2481e-04,  4.0572e-04,  4.2903e-04,  ..., -2.0318e-04,\n",
            "          -1.5335e-04,  4.8558e-05]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 3.5684e-04, -3.8916e-04, -2.5767e-04,  ..., -6.2062e-05,\n",
            "           5.5427e-05, -1.2443e-04],\n",
            "         [-7.1463e-03,  7.7937e-03,  5.1602e-03,  ...,  1.2429e-03,\n",
            "          -1.1100e-03,  2.4919e-03],\n",
            "         [-3.2158e-03,  3.5071e-03,  2.3221e-03,  ...,  5.5929e-04,\n",
            "          -4.9950e-04,  1.1213e-03],\n",
            "         ...,\n",
            "         [ 6.9802e-03, -7.6124e-03, -5.0402e-03,  ..., -1.2140e-03,\n",
            "           1.0842e-03, -2.4339e-03],\n",
            "         [ 6.5413e-03, -7.1338e-03, -4.7233e-03,  ..., -1.1377e-03,\n",
            "           1.0160e-03, -2.2809e-03],\n",
            "         [ 5.5501e-03, -6.0528e-03, -4.0076e-03,  ..., -9.6527e-04,\n",
            "           8.6208e-04, -1.9353e-03]],\n",
            "\n",
            "        [[ 6.2154e-03, -1.5323e-02, -1.9678e-03,  ..., -1.4505e-04,\n",
            "           7.9814e-03, -1.3596e-02],\n",
            "         [-4.1543e-03,  1.0242e-02,  1.3152e-03,  ...,  9.6947e-05,\n",
            "          -5.3347e-03,  9.0874e-03],\n",
            "         [-1.7923e-03,  4.4188e-03,  5.6746e-04,  ...,  4.1828e-05,\n",
            "          -2.3016e-03,  3.9208e-03],\n",
            "         ...,\n",
            "         [-1.2889e-02,  3.1777e-02,  4.0808e-03,  ...,  3.0080e-04,\n",
            "          -1.6552e-02,  2.8195e-02],\n",
            "         [ 9.5920e-03, -2.3648e-02, -3.0368e-03,  ..., -2.2385e-04,\n",
            "           1.2318e-02, -2.0982e-02],\n",
            "         [-4.0344e-03,  9.9463e-03,  1.2773e-03,  ...,  9.4151e-05,\n",
            "          -5.1808e-03,  8.8253e-03]],\n",
            "\n",
            "        [[ 7.7542e-03, -9.4706e-03, -8.8791e-03,  ...,  3.6095e-03,\n",
            "           1.2133e-03, -2.3190e-03],\n",
            "         [-9.9509e-03,  1.2154e-02,  1.1395e-02,  ..., -4.6321e-03,\n",
            "          -1.5570e-03,  2.9759e-03],\n",
            "         [-6.5127e-03,  7.9543e-03,  7.4575e-03,  ..., -3.0316e-03,\n",
            "          -1.0190e-03,  1.9477e-03],\n",
            "         ...,\n",
            "         [ 2.7830e-03, -3.3991e-03, -3.1868e-03,  ...,  1.2955e-03,\n",
            "           4.3545e-04, -8.3230e-04],\n",
            "         [ 1.1554e-02, -1.4111e-02, -1.3230e-02,  ...,  5.3781e-03,\n",
            "           1.8078e-03, -3.4552e-03],\n",
            "         [ 2.1063e-03, -2.5726e-03, -2.4119e-03,  ...,  9.8048e-04,\n",
            "           3.2957e-04, -6.2992e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2013, 0.1523, 0.1642,  ..., 0.1338, 0.1832, 0.0848],\n",
            "        [0.2046, 0.1563, 0.1688,  ..., 0.1357, 0.1849, 0.0856],\n",
            "        [0.2046, 0.1544, 0.1669,  ..., 0.1388, 0.1875, 0.0842],\n",
            "        ...,\n",
            "        [0.2006, 0.1520, 0.1646,  ..., 0.1340, 0.1830, 0.0851],\n",
            "        [0.2047, 0.1522, 0.1650,  ..., 0.1362, 0.1867, 0.0862],\n",
            "        [0.2053, 0.1547, 0.1675,  ..., 0.1396, 0.1880, 0.0843]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 5.4150e-03, -1.0089e-02, -3.4889e-03,  ..., -1.4912e-03,\n",
            "           8.5389e-03, -9.6590e-03],\n",
            "         [-5.5830e-04,  1.0402e-03,  3.5971e-04,  ...,  1.5375e-04,\n",
            "          -8.8038e-04,  9.9586e-04],\n",
            "         [-5.0944e-03,  9.4919e-03,  3.2823e-03,  ...,  1.4029e-03,\n",
            "          -8.0334e-03,  9.0872e-03],\n",
            "         ...,\n",
            "         [-1.8374e-02,  3.4233e-02,  1.1838e-02,  ...,  5.0598e-03,\n",
            "          -2.8973e-02,  3.2774e-02],\n",
            "         [ 1.2000e-02, -2.2357e-02, -7.7313e-03,  ..., -3.3045e-03,\n",
            "           1.8922e-02, -2.1404e-02],\n",
            "         [-7.5866e-03,  1.4135e-02,  4.8880e-03,  ...,  2.0892e-03,\n",
            "          -1.1963e-02,  1.3533e-02]],\n",
            "\n",
            "        [[ 1.5351e-03, -1.7250e-03, -2.0642e-03,  ...,  5.9126e-04,\n",
            "           8.2569e-04,  3.2901e-04],\n",
            "         [-4.5084e-04,  5.0661e-04,  6.0623e-04,  ..., -1.7364e-04,\n",
            "          -2.4249e-04, -9.6627e-05],\n",
            "         [ 9.2760e-04, -1.0424e-03, -1.2473e-03,  ...,  3.5727e-04,\n",
            "           4.9893e-04,  1.9881e-04],\n",
            "         ...,\n",
            "         [ 9.1697e-04, -1.0304e-03, -1.2330e-03,  ...,  3.5318e-04,\n",
            "           4.9321e-04,  1.9653e-04],\n",
            "         [ 4.5385e-03, -5.1000e-03, -6.1028e-03,  ...,  1.7481e-03,\n",
            "           2.4411e-03,  9.7273e-04],\n",
            "         [ 5.8001e-03, -6.5176e-03, -7.7992e-03,  ...,  2.2340e-03,\n",
            "           3.1197e-03,  1.2431e-03]],\n",
            "\n",
            "        [[ 3.6951e-03, -7.7057e-03, -7.2520e-03,  ...,  3.3851e-03,\n",
            "           2.5881e-03, -3.2294e-04],\n",
            "         [-1.0148e-02,  2.1161e-02,  1.9915e-02,  ..., -9.2962e-03,\n",
            "          -7.1074e-03,  8.8685e-04],\n",
            "         [-2.5922e-03,  5.4058e-03,  5.0875e-03,  ..., -2.3747e-03,\n",
            "          -1.8156e-03,  2.2655e-04],\n",
            "         ...,\n",
            "         [ 2.4312e-03, -5.0699e-03, -4.7714e-03,  ...,  2.2272e-03,\n",
            "           1.7028e-03, -2.1247e-04],\n",
            "         [ 8.6104e-03, -1.7956e-02, -1.6899e-02,  ...,  7.8880e-03,\n",
            "           6.0308e-03, -7.5251e-04],\n",
            "         [ 3.9691e-05, -8.2770e-05, -7.7897e-05,  ...,  3.6361e-05,\n",
            "           2.7800e-05, -3.4688e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.9011e-05,  1.9389e-05,  1.1523e-05,  ...,  4.0490e-06,\n",
            "          -1.4487e-06,  8.3984e-06],\n",
            "         [-6.5953e-03,  6.7264e-03,  3.9975e-03,  ...,  1.4047e-03,\n",
            "          -5.0258e-04,  2.9136e-03],\n",
            "         [-2.4500e-03,  2.4987e-03,  1.4849e-03,  ...,  5.2180e-04,\n",
            "          -1.8669e-04,  1.0823e-03],\n",
            "         ...,\n",
            "         [ 5.2898e-03, -5.3949e-03, -3.2062e-03,  ..., -1.1266e-03,\n",
            "           4.0309e-04, -2.3369e-03],\n",
            "         [ 5.3462e-03, -5.4525e-03, -3.2404e-03,  ..., -1.1387e-03,\n",
            "           4.0739e-04, -2.3618e-03],\n",
            "         [ 7.8272e-03, -7.9827e-03, -4.7441e-03,  ..., -1.6671e-03,\n",
            "           5.9644e-04, -3.4578e-03]],\n",
            "\n",
            "        [[ 6.3939e-03, -1.5138e-02, -1.6278e-03,  ..., -1.0045e-03,\n",
            "           8.2097e-03, -1.4387e-02],\n",
            "         [-4.6955e-03,  1.1117e-02,  1.1954e-03,  ...,  7.3769e-04,\n",
            "          -6.0291e-03,  1.0566e-02],\n",
            "         [-1.4908e-03,  3.5294e-03,  3.7953e-04,  ...,  2.3421e-04,\n",
            "          -1.9141e-03,  3.3544e-03],\n",
            "         ...,\n",
            "         [-1.3165e-02,  3.1168e-02,  3.3516e-03,  ...,  2.0682e-03,\n",
            "          -1.6904e-02,  2.9622e-02],\n",
            "         [ 9.8105e-03, -2.3227e-02, -2.4977e-03,  ..., -1.5413e-03,\n",
            "           1.2597e-02, -2.2075e-02],\n",
            "         [-4.1707e-03,  9.8742e-03,  1.0618e-03,  ...,  6.5523e-04,\n",
            "          -5.3551e-03,  9.3845e-03]],\n",
            "\n",
            "        [[ 6.1344e-03, -8.0497e-03, -7.9597e-03,  ...,  3.3524e-03,\n",
            "           1.5066e-03, -9.8198e-04],\n",
            "         [-8.4171e-03,  1.1045e-02,  1.0922e-02,  ..., -4.5999e-03,\n",
            "          -2.0673e-03,  1.3474e-03],\n",
            "         [-4.7151e-03,  6.1873e-03,  6.1181e-03,  ..., -2.5768e-03,\n",
            "          -1.1581e-03,  7.5479e-04],\n",
            "         ...,\n",
            "         [ 3.6707e-04, -4.8168e-04, -4.7629e-04,  ...,  2.0060e-04,\n",
            "           9.0154e-05, -5.8760e-05],\n",
            "         [ 9.5125e-03, -1.2482e-02, -1.2343e-02,  ...,  5.1985e-03,\n",
            "           2.3363e-03, -1.5227e-03],\n",
            "         [ 2.7789e-03, -3.6465e-03, -3.6057e-03,  ...,  1.5186e-03,\n",
            "           6.8251e-04, -4.4484e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2007, 0.1530, 0.1667,  ..., 0.1352, 0.1844, 0.0844],\n",
            "        [0.2037, 0.1561, 0.1687,  ..., 0.1348, 0.1847, 0.0854],\n",
            "        [0.2000, 0.1536, 0.1661,  ..., 0.1353, 0.1865, 0.0842],\n",
            "        ...,\n",
            "        [0.2056, 0.1519, 0.1661,  ..., 0.1369, 0.1882, 0.0851],\n",
            "        [0.2059, 0.1542, 0.1666,  ..., 0.1362, 0.1855, 0.0853],\n",
            "        [0.2033, 0.1515, 0.1647,  ..., 0.1359, 0.1853, 0.0860]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 5.2036e-03, -9.5146e-03, -3.1683e-03,  ..., -1.6365e-03,\n",
            "           9.0109e-03, -9.3929e-03],\n",
            "         [-1.3364e-03,  2.4436e-03,  8.1370e-04,  ...,  4.2030e-04,\n",
            "          -2.3142e-03,  2.4123e-03],\n",
            "         [-5.2057e-03,  9.5185e-03,  3.1696e-03,  ...,  1.6372e-03,\n",
            "          -9.0147e-03,  9.3968e-03],\n",
            "         ...,\n",
            "         [-1.6911e-02,  3.0922e-02,  1.0297e-02,  ...,  5.3185e-03,\n",
            "          -2.9285e-02,  3.0526e-02],\n",
            "         [ 1.1256e-02, -2.0582e-02, -6.8536e-03,  ..., -3.5401e-03,\n",
            "           1.9492e-02, -2.0319e-02],\n",
            "         [-7.7389e-03,  1.4150e-02,  4.7120e-03,  ...,  2.4339e-03,\n",
            "          -1.3401e-02,  1.3969e-02]],\n",
            "\n",
            "        [[ 1.9942e-03, -2.2823e-03, -3.1749e-03,  ...,  1.2109e-03,\n",
            "           1.5482e-03,  5.4371e-04],\n",
            "         [-5.8833e-04,  6.7333e-04,  9.3665e-04,  ..., -3.5725e-04,\n",
            "          -4.5674e-04, -1.6040e-04],\n",
            "         [ 1.2102e-03, -1.3851e-03, -1.9268e-03,  ...,  7.3488e-04,\n",
            "           9.3954e-04,  3.2997e-04],\n",
            "         ...,\n",
            "         [ 1.6975e-04, -1.9427e-04, -2.7025e-04,  ...,  1.0307e-04,\n",
            "           1.3178e-04,  4.6281e-05],\n",
            "         [ 3.7985e-03, -4.3473e-03, -6.0474e-03,  ...,  2.3065e-03,\n",
            "           2.9489e-03,  1.0356e-03],\n",
            "         [ 4.4708e-03, -5.1167e-03, -7.1178e-03,  ...,  2.7148e-03,\n",
            "           3.4708e-03,  1.2189e-03]],\n",
            "\n",
            "        [[ 4.0429e-03, -7.8513e-03, -7.7422e-03,  ...,  3.9481e-03,\n",
            "           2.4111e-03, -7.4050e-04],\n",
            "         [-1.0529e-02,  2.0447e-02,  2.0163e-02,  ..., -1.0282e-02,\n",
            "          -6.2792e-03,  1.9285e-03],\n",
            "         [-2.7835e-03,  5.4055e-03,  5.3304e-03,  ..., -2.7182e-03,\n",
            "          -1.6600e-03,  5.0982e-04],\n",
            "         ...,\n",
            "         [ 2.7140e-03, -5.2705e-03, -5.1973e-03,  ...,  2.6504e-03,\n",
            "           1.6185e-03, -4.9709e-04],\n",
            "         [ 9.3842e-03, -1.8224e-02, -1.7971e-02,  ...,  9.1642e-03,\n",
            "           5.5964e-03, -1.7188e-03],\n",
            "         [ 5.1119e-05, -9.9272e-05, -9.7893e-05,  ...,  4.9920e-05,\n",
            "           3.0485e-05, -9.3629e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.0743e-04, -2.1154e-04, -1.3720e-04,  ..., -6.2182e-05,\n",
            "           2.6238e-06, -8.7874e-05],\n",
            "         [-4.2503e-03,  4.3345e-03,  2.8114e-03,  ...,  1.2742e-03,\n",
            "          -5.3764e-05,  1.8006e-03],\n",
            "         [-3.1977e-03,  3.2611e-03,  2.1151e-03,  ...,  9.5861e-04,\n",
            "          -4.0449e-05,  1.3547e-03],\n",
            "         ...,\n",
            "         [ 4.1328e-03, -4.2148e-03, -2.7337e-03,  ..., -1.2390e-03,\n",
            "           5.2279e-05, -1.7508e-03],\n",
            "         [ 3.8378e-03, -3.9139e-03, -2.5385e-03,  ..., -1.1505e-03,\n",
            "           4.8546e-05, -1.6258e-03],\n",
            "         [ 8.1217e-03, -8.2827e-03, -5.3721e-03,  ..., -2.4347e-03,\n",
            "           1.0274e-04, -3.4407e-03]],\n",
            "\n",
            "        [[ 7.1464e-03, -1.5269e-02, -1.9824e-03,  ..., -1.4761e-03,\n",
            "           8.9783e-03, -1.2648e-02],\n",
            "         [-5.0658e-03,  1.0824e-02,  1.4052e-03,  ...,  1.0464e-03,\n",
            "          -6.3643e-03,  8.9656e-03],\n",
            "         [-2.2773e-03,  4.8657e-03,  6.3171e-04,  ...,  4.7038e-04,\n",
            "          -2.8610e-03,  4.0304e-03],\n",
            "         ...,\n",
            "         [-1.4766e-02,  3.1550e-02,  4.0961e-03,  ...,  3.0500e-03,\n",
            "          -1.8551e-02,  2.6134e-02],\n",
            "         [ 1.1891e-02, -2.5407e-02, -3.2986e-03,  ..., -2.4562e-03,\n",
            "           1.4939e-02, -2.1046e-02],\n",
            "         [-4.8697e-03,  1.0405e-02,  1.3508e-03,  ...,  1.0059e-03,\n",
            "          -6.1179e-03,  8.6186e-03]],\n",
            "\n",
            "        [[ 5.1488e-03, -7.9418e-03, -7.4071e-03,  ...,  2.6682e-03,\n",
            "           1.9403e-03, -8.8418e-04],\n",
            "         [-7.6741e-03,  1.1837e-02,  1.1040e-02,  ..., -3.9768e-03,\n",
            "          -2.8919e-03,  1.3178e-03],\n",
            "         [-5.0513e-03,  7.7913e-03,  7.2668e-03,  ..., -2.6176e-03,\n",
            "          -1.9035e-03,  8.6743e-04],\n",
            "         ...,\n",
            "         [ 1.0435e-03, -1.6096e-03, -1.5012e-03,  ...,  5.4076e-04,\n",
            "           3.9323e-04, -1.7920e-04],\n",
            "         [ 9.3869e-03, -1.4479e-02, -1.3504e-02,  ...,  4.8644e-03,\n",
            "           3.5373e-03, -1.6120e-03],\n",
            "         [ 2.8878e-03, -4.4543e-03, -4.1544e-03,  ...,  1.4965e-03,\n",
            "           1.0882e-03, -4.9591e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2022, 0.1527, 0.1643,  ..., 0.1338, 0.1834, 0.0848],\n",
            "        [0.2029, 0.1535, 0.1659,  ..., 0.1353, 0.1837, 0.0844],\n",
            "        [0.2007, 0.1537, 0.1661,  ..., 0.1356, 0.1866, 0.0840],\n",
            "        ...,\n",
            "        [0.2052, 0.1521, 0.1659,  ..., 0.1367, 0.1876, 0.0850],\n",
            "        [0.2019, 0.1537, 0.1664,  ..., 0.1348, 0.1835, 0.0840],\n",
            "        [0.2046, 0.1522, 0.1650,  ..., 0.1362, 0.1866, 0.0862]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 5.6250e-03, -9.4140e-03, -3.0855e-03,  ..., -1.5941e-03,\n",
            "           8.2043e-03, -8.9928e-03],\n",
            "         [-1.6562e-03,  2.7719e-03,  9.0850e-04,  ...,  4.6937e-04,\n",
            "          -2.4157e-03,  2.6479e-03],\n",
            "         [-5.9285e-03,  9.9219e-03,  3.2520e-03,  ...,  1.6801e-03,\n",
            "          -8.6470e-03,  9.4780e-03],\n",
            "         ...,\n",
            "         [-1.7876e-02,  2.9918e-02,  9.8058e-03,  ...,  5.0661e-03,\n",
            "          -2.6074e-02,  2.8580e-02],\n",
            "         [ 1.1615e-02, -1.9439e-02, -6.3713e-03,  ..., -3.2917e-03,\n",
            "           1.6941e-02, -1.8569e-02],\n",
            "         [-6.2060e-03,  1.0386e-02,  3.4042e-03,  ...,  1.7588e-03,\n",
            "          -9.0518e-03,  9.9217e-03]],\n",
            "\n",
            "        [[ 3.0212e-03, -2.9385e-03, -3.9691e-03,  ...,  1.5793e-03,\n",
            "           2.0739e-03,  7.1230e-04],\n",
            "         [-1.4646e-03,  1.4245e-03,  1.9242e-03,  ..., -7.6564e-04,\n",
            "          -1.0054e-03, -3.4531e-04],\n",
            "         [ 8.7620e-04, -8.5220e-04, -1.1511e-03,  ...,  4.5803e-04,\n",
            "           6.0146e-04,  2.0658e-04],\n",
            "         ...,\n",
            "         [-6.4510e-06,  6.2743e-06,  8.4750e-06,  ..., -3.3723e-06,\n",
            "          -4.4282e-06, -1.5209e-06],\n",
            "         [ 5.3738e-03, -5.2266e-03, -7.0598e-03,  ...,  2.8091e-03,\n",
            "           3.6887e-03,  1.2669e-03],\n",
            "         [ 5.8379e-03, -5.6779e-03, -7.6695e-03,  ...,  3.0517e-03,\n",
            "           4.0073e-03,  1.3764e-03]],\n",
            "\n",
            "        [[ 2.7439e-03, -7.0825e-03, -6.8712e-03,  ...,  3.5827e-03,\n",
            "           2.0780e-03, -2.1857e-04],\n",
            "         [-8.1477e-03,  2.1031e-02,  2.0403e-02,  ..., -1.0638e-02,\n",
            "          -6.1703e-03,  6.4900e-04],\n",
            "         [-2.2966e-03,  5.9279e-03,  5.7510e-03,  ..., -2.9986e-03,\n",
            "          -1.7392e-03,  1.8293e-04],\n",
            "         ...,\n",
            "         [ 1.5706e-03, -4.0539e-03, -3.9330e-03,  ...,  2.0507e-03,\n",
            "           1.1894e-03, -1.2510e-04],\n",
            "         [ 7.7441e-03, -1.9989e-02, -1.9392e-02,  ...,  1.0111e-02,\n",
            "           5.8646e-03, -6.1684e-04],\n",
            "         [-3.1301e-04,  8.0793e-04,  7.8382e-04,  ..., -4.0869e-04,\n",
            "          -2.3704e-04,  2.4932e-05]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 9.2980e-05, -9.2621e-05, -6.2344e-05,  ..., -2.9184e-05,\n",
            "           1.1519e-05, -3.2726e-05],\n",
            "         [-5.8012e-03,  5.7788e-03,  3.8898e-03,  ...,  1.8209e-03,\n",
            "          -7.1868e-04,  2.0418e-03],\n",
            "         [-4.6267e-03,  4.6089e-03,  3.1023e-03,  ...,  1.4522e-03,\n",
            "          -5.7319e-04,  1.6285e-03],\n",
            "         ...,\n",
            "         [ 4.0098e-03, -3.9943e-03, -2.6886e-03,  ..., -1.2586e-03,\n",
            "           4.9676e-04, -1.4113e-03],\n",
            "         [ 7.3645e-03, -7.3361e-03, -4.9380e-03,  ..., -2.3116e-03,\n",
            "           9.1236e-04, -2.5920e-03],\n",
            "         [ 8.4539e-03, -8.4213e-03, -5.6685e-03,  ..., -2.6535e-03,\n",
            "           1.0473e-03, -2.9755e-03]],\n",
            "\n",
            "        [[ 6.9745e-03, -1.4217e-02, -1.6360e-03,  ..., -2.2443e-03,\n",
            "           7.3518e-03, -1.1762e-02],\n",
            "         [-7.0136e-03,  1.4297e-02,  1.6452e-03,  ...,  2.2569e-03,\n",
            "          -7.3930e-03,  1.1828e-02],\n",
            "         [-1.9758e-03,  4.0276e-03,  4.6348e-04,  ...,  6.3580e-04,\n",
            "          -2.0827e-03,  3.3321e-03],\n",
            "         ...,\n",
            "         [-1.5264e-02,  3.1115e-02,  3.5806e-03,  ...,  4.9118e-03,\n",
            "          -1.6090e-02,  2.5742e-02],\n",
            "         [ 1.3727e-02, -2.7981e-02, -3.2199e-03,  ..., -4.4171e-03,\n",
            "           1.4469e-02, -2.3149e-02],\n",
            "         [-3.4728e-03,  7.0790e-03,  8.1463e-04,  ...,  1.1175e-03,\n",
            "          -3.6606e-03,  5.8567e-03]],\n",
            "\n",
            "        [[ 5.1060e-03, -7.6036e-03, -6.9551e-03,  ...,  3.1534e-03,\n",
            "           1.7223e-03, -7.4892e-04],\n",
            "         [-6.9940e-03,  1.0415e-02,  9.5267e-03,  ..., -4.3193e-03,\n",
            "          -2.3592e-03,  1.0258e-03],\n",
            "         [-4.8409e-03,  7.2087e-03,  6.5939e-03,  ..., -2.9896e-03,\n",
            "          -1.6329e-03,  7.1002e-04],\n",
            "         ...,\n",
            "         [ 1.2323e-03, -1.8351e-03, -1.6786e-03,  ...,  7.6105e-04,\n",
            "           4.1568e-04, -1.8075e-04],\n",
            "         [ 8.3207e-03, -1.2391e-02, -1.1334e-02,  ...,  5.1387e-03,\n",
            "           2.8067e-03, -1.2204e-03],\n",
            "         [ 2.8138e-03, -4.1901e-03, -3.8328e-03,  ...,  1.7377e-03,\n",
            "           9.4913e-04, -4.1271e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2032, 0.1535, 0.1659,  ..., 0.1355, 0.1839, 0.0847],\n",
            "        [0.2048, 0.1527, 0.1682,  ..., 0.1373, 0.1873, 0.0840],\n",
            "        [0.2043, 0.1543, 0.1667,  ..., 0.1386, 0.1873, 0.0842],\n",
            "        ...,\n",
            "        [0.2025, 0.1514, 0.1649,  ..., 0.1357, 0.1847, 0.0860],\n",
            "        [0.2028, 0.1557, 0.1699,  ..., 0.1331, 0.1857, 0.0840],\n",
            "        [0.2030, 0.1525, 0.1655,  ..., 0.1355, 0.1841, 0.0843]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 5.8807e-03, -8.4993e-03, -2.8006e-03,  ..., -7.7451e-04,\n",
            "           7.0069e-03, -8.7404e-03],\n",
            "         [-1.8547e-03,  2.6806e-03,  8.8327e-04,  ...,  2.4428e-04,\n",
            "          -2.2099e-03,  2.7566e-03],\n",
            "         [-7.6531e-03,  1.1061e-02,  3.6446e-03,  ...,  1.0079e-03,\n",
            "          -9.1186e-03,  1.1375e-02],\n",
            "         ...,\n",
            "         [-1.9595e-02,  2.8320e-02,  9.3316e-03,  ...,  2.5807e-03,\n",
            "          -2.3347e-02,  2.9123e-02],\n",
            "         [ 1.3593e-02, -1.9646e-02, -6.4733e-03,  ..., -1.7902e-03,\n",
            "           1.6196e-02, -2.0203e-02],\n",
            "         [-7.4223e-03,  1.0727e-02,  3.5347e-03,  ...,  9.7754e-04,\n",
            "          -8.8436e-03,  1.1032e-02]],\n",
            "\n",
            "        [[ 1.9844e-03, -2.0975e-03, -2.9633e-03,  ...,  1.1453e-03,\n",
            "           1.3428e-03,  2.7581e-04],\n",
            "         [-6.8187e-04,  7.2072e-04,  1.0182e-03,  ..., -3.9356e-04,\n",
            "          -4.6142e-04, -9.4773e-05],\n",
            "         [-8.6233e-04,  9.1147e-04,  1.2877e-03,  ..., -4.9771e-04,\n",
            "          -5.8354e-04, -1.1986e-04],\n",
            "         ...,\n",
            "         [ 6.9079e-04, -7.3014e-04, -1.0316e-03,  ...,  3.9870e-04,\n",
            "           4.6746e-04,  9.6012e-05],\n",
            "         [ 5.3346e-03, -5.6386e-03, -7.9662e-03,  ...,  3.0790e-03,\n",
            "           3.6099e-03,  7.4146e-04],\n",
            "         [ 4.4047e-03, -4.6557e-03, -6.5776e-03,  ...,  2.5423e-03,\n",
            "           2.9807e-03,  6.1221e-04]],\n",
            "\n",
            "        [[ 2.5438e-03, -7.7422e-03, -8.0574e-03,  ...,  4.3588e-03,\n",
            "           2.9217e-03, -1.2830e-04],\n",
            "         [-6.7721e-03,  2.0611e-02,  2.1450e-02,  ..., -1.1604e-02,\n",
            "          -7.7780e-03,  3.4156e-04],\n",
            "         [-1.9963e-03,  6.0757e-03,  6.3230e-03,  ..., -3.4206e-03,\n",
            "          -2.2928e-03,  1.0068e-04],\n",
            "         ...,\n",
            "         [ 1.5050e-03, -4.5806e-03, -4.7671e-03,  ...,  2.5788e-03,\n",
            "           1.7286e-03, -7.5908e-05],\n",
            "         [ 6.5231e-03, -1.9853e-02, -2.0662e-02,  ...,  1.1177e-02,\n",
            "           7.4921e-03, -3.2900e-04],\n",
            "         [ 3.6302e-04, -1.1049e-03, -1.1499e-03,  ...,  6.2203e-04,\n",
            "           4.1695e-04, -1.8310e-05]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.0443e-03, -1.9532e-03, -1.3683e-03,  ..., -7.3098e-04,\n",
            "           1.1024e-04, -5.3395e-04],\n",
            "         [-6.6106e-03,  6.3160e-03,  4.4247e-03,  ...,  2.3637e-03,\n",
            "          -3.5646e-04,  1.7266e-03],\n",
            "         [-4.4450e-03,  4.2469e-03,  2.9751e-03,  ...,  1.5894e-03,\n",
            "          -2.3968e-04,  1.1610e-03],\n",
            "         ...,\n",
            "         [ 5.7199e-03, -5.4650e-03, -3.8285e-03,  ..., -2.0452e-03,\n",
            "           3.0843e-04, -1.4940e-03],\n",
            "         [ 7.6097e-03, -7.2705e-03, -5.0934e-03,  ..., -2.7209e-03,\n",
            "           4.1033e-04, -1.9875e-03],\n",
            "         [ 6.4814e-03, -6.1925e-03, -4.3382e-03,  ..., -2.3175e-03,\n",
            "           3.4949e-04, -1.6928e-03]],\n",
            "\n",
            "        [[ 7.1288e-03, -1.3621e-02, -2.4012e-03,  ..., -2.5717e-03,\n",
            "           7.8017e-03, -1.0923e-02],\n",
            "         [-7.6412e-03,  1.4600e-02,  2.5738e-03,  ...,  2.7566e-03,\n",
            "          -8.3624e-03,  1.1708e-02],\n",
            "         [-2.9645e-03,  5.6644e-03,  9.9851e-04,  ...,  1.0694e-03,\n",
            "          -3.2443e-03,  4.5423e-03],\n",
            "         ...,\n",
            "         [-1.6089e-02,  3.0741e-02,  5.4191e-03,  ...,  5.8040e-03,\n",
            "          -1.7607e-02,  2.4652e-02],\n",
            "         [ 1.4596e-02, -2.7890e-02, -4.9164e-03,  ..., -5.2656e-03,\n",
            "           1.5974e-02, -2.2365e-02],\n",
            "         [-3.9894e-03,  7.6228e-03,  1.3437e-03,  ...,  1.4392e-03,\n",
            "          -4.3659e-03,  6.1128e-03]],\n",
            "\n",
            "        [[ 4.1863e-03, -7.0253e-03, -6.2162e-03,  ...,  2.5670e-03,\n",
            "           1.3538e-03, -9.5685e-04],\n",
            "         [-8.4172e-03,  1.4126e-02,  1.2499e-02,  ..., -5.1613e-03,\n",
            "          -2.7221e-03,  1.9239e-03],\n",
            "         [-4.9668e-03,  8.3352e-03,  7.3752e-03,  ..., -3.0456e-03,\n",
            "          -1.6062e-03,  1.1352e-03],\n",
            "         ...,\n",
            "         [ 1.7740e-03, -2.9771e-03, -2.6342e-03,  ...,  1.0878e-03,\n",
            "           5.7370e-04, -4.0548e-04],\n",
            "         [ 9.0313e-03, -1.5156e-02, -1.3411e-02,  ...,  5.5378e-03,\n",
            "           2.9207e-03, -2.0643e-03],\n",
            "         [ 2.7978e-03, -4.6953e-03, -4.1545e-03,  ...,  1.7156e-03,\n",
            "           9.0481e-04, -6.3950e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2029, 0.1536, 0.1659,  ..., 0.1352, 0.1837, 0.0844],\n",
            "        [0.2058, 0.1542, 0.1665,  ..., 0.1362, 0.1855, 0.0853],\n",
            "        [0.2018, 0.1536, 0.1658,  ..., 0.1364, 0.1865, 0.0839],\n",
            "        ...,\n",
            "        [0.2056, 0.1523, 0.1657,  ..., 0.1365, 0.1875, 0.0864],\n",
            "        [0.2027, 0.1530, 0.1645,  ..., 0.1340, 0.1835, 0.0850],\n",
            "        [0.2055, 0.1548, 0.1677,  ..., 0.1398, 0.1882, 0.0844]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 5.7837e-03, -7.3494e-03, -2.6905e-03,  ..., -1.0545e-03,\n",
            "           5.3882e-03, -6.6907e-03],\n",
            "         [-7.9601e-04,  1.0115e-03,  3.7029e-04,  ...,  1.4513e-04,\n",
            "          -7.4158e-04,  9.2085e-04],\n",
            "         [-8.5170e-03,  1.0823e-02,  3.9619e-03,  ...,  1.5528e-03,\n",
            "          -7.9346e-03,  9.8527e-03],\n",
            "         ...,\n",
            "         [-2.2931e-02,  2.9139e-02,  1.0667e-02,  ...,  4.1807e-03,\n",
            "          -2.1363e-02,  2.6527e-02],\n",
            "         [ 1.5804e-02, -2.0082e-02, -7.3516e-03,  ..., -2.8812e-03,\n",
            "           1.4723e-02, -1.8282e-02],\n",
            "         [-8.9152e-03,  1.1329e-02,  4.1472e-03,  ...,  1.6254e-03,\n",
            "          -8.3056e-03,  1.0313e-02]],\n",
            "\n",
            "        [[ 2.0754e-03, -2.0993e-03, -2.9203e-03,  ...,  9.9324e-04,\n",
            "           1.4919e-03,  4.8903e-04],\n",
            "         [-9.1839e-04,  9.2899e-04,  1.2923e-03,  ..., -4.3953e-04,\n",
            "          -6.6017e-04, -2.1640e-04],\n",
            "         [-2.0655e-04,  2.0894e-04,  2.9065e-04,  ..., -9.8853e-05,\n",
            "          -1.4848e-04, -4.8671e-05],\n",
            "         ...,\n",
            "         [ 2.1287e-04, -2.1533e-04, -2.9954e-04,  ...,  1.0188e-04,\n",
            "           1.5302e-04,  5.0160e-05],\n",
            "         [ 5.1218e-03, -5.1810e-03, -7.2071e-03,  ...,  2.4512e-03,\n",
            "           3.6818e-03,  1.2069e-03],\n",
            "         [ 4.3175e-03, -4.3674e-03, -6.0753e-03,  ...,  2.0663e-03,\n",
            "           3.1036e-03,  1.0174e-03]],\n",
            "\n",
            "        [[ 2.4622e-03, -7.6977e-03, -7.1904e-03,  ...,  3.8358e-03,\n",
            "           2.9239e-03,  4.9511e-05],\n",
            "         [-7.8124e-03,  2.4424e-02,  2.2814e-02,  ..., -1.2171e-02,\n",
            "          -9.2773e-03, -1.5709e-04],\n",
            "         [-1.6016e-03,  5.0073e-03,  4.6772e-03,  ..., -2.4951e-03,\n",
            "          -1.9020e-03, -3.2206e-05],\n",
            "         ...,\n",
            "         [ 2.1038e-03, -6.5771e-03, -6.1436e-03,  ...,  3.2774e-03,\n",
            "           2.4983e-03,  4.2303e-05],\n",
            "         [ 6.9216e-03, -2.1639e-02, -2.0213e-02,  ...,  1.0783e-02,\n",
            "           8.2194e-03,  1.3918e-04],\n",
            "         [ 3.2198e-04, -1.0066e-03, -9.4028e-04,  ...,  5.0160e-04,\n",
            "           3.8236e-04,  6.4745e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.8782e-03, -1.5449e-03, -1.3283e-03,  ..., -7.0200e-04,\n",
            "           2.8938e-04, -6.8481e-04],\n",
            "         [-5.4788e-03,  4.5067e-03,  3.8747e-03,  ...,  2.0478e-03,\n",
            "          -8.4416e-04,  1.9976e-03],\n",
            "         [-5.3411e-03,  4.3934e-03,  3.7772e-03,  ...,  1.9963e-03,\n",
            "          -8.2293e-04,  1.9474e-03],\n",
            "         ...,\n",
            "         [ 4.2754e-03, -3.5168e-03, -3.0236e-03,  ..., -1.5980e-03,\n",
            "           6.5874e-04, -1.5589e-03],\n",
            "         [ 8.1881e-03, -6.7353e-03, -5.7907e-03,  ..., -3.0604e-03,\n",
            "           1.2616e-03, -2.9855e-03],\n",
            "         [ 3.9581e-03, -3.2558e-03, -2.7992e-03,  ..., -1.4794e-03,\n",
            "           6.0985e-04, -1.4432e-03]],\n",
            "\n",
            "        [[ 7.2334e-03, -1.3758e-02, -2.4124e-03,  ..., -1.6099e-03,\n",
            "           7.2901e-03, -1.1538e-02],\n",
            "         [-5.9180e-03,  1.1256e-02,  1.9737e-03,  ...,  1.3171e-03,\n",
            "          -5.9644e-03,  9.4401e-03],\n",
            "         [-3.7722e-03,  7.1750e-03,  1.2581e-03,  ...,  8.3954e-04,\n",
            "          -3.8017e-03,  6.0172e-03],\n",
            "         ...,\n",
            "         [-1.6336e-02,  3.1073e-02,  5.4484e-03,  ...,  3.6359e-03,\n",
            "          -1.6464e-02,  2.6059e-02],\n",
            "         [ 1.3981e-02, -2.6592e-02, -4.6628e-03,  ..., -3.1116e-03,\n",
            "           1.4090e-02, -2.2301e-02],\n",
            "         [-4.6487e-03,  8.8421e-03,  1.5504e-03,  ...,  1.0346e-03,\n",
            "          -4.6851e-03,  7.4153e-03]],\n",
            "\n",
            "        [[ 4.5288e-03, -6.2742e-03, -6.1535e-03,  ...,  2.7033e-03,\n",
            "           2.7758e-03, -3.9466e-04],\n",
            "         [-8.6005e-03,  1.1915e-02,  1.1686e-02,  ..., -5.1337e-03,\n",
            "          -5.2714e-03,  7.4948e-04],\n",
            "         [-3.9765e-03,  5.5089e-03,  5.4030e-03,  ..., -2.3736e-03,\n",
            "          -2.4373e-03,  3.4652e-04],\n",
            "         ...,\n",
            "         [ 1.1497e-03, -1.5928e-03, -1.5621e-03,  ...,  6.8626e-04,\n",
            "           7.0467e-04, -1.0019e-04],\n",
            "         [ 9.9464e-03, -1.3780e-02, -1.3515e-02,  ...,  5.9371e-03,\n",
            "           6.0964e-03, -8.6677e-04],\n",
            "         [ 4.2982e-03, -5.9547e-03, -5.8402e-03,  ...,  2.5656e-03,\n",
            "           2.6345e-03, -3.7456e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2020, 0.1529, 0.1660,  ..., 0.1352, 0.1840, 0.0844],\n",
            "        [0.2052, 0.1545, 0.1690,  ..., 0.1379, 0.1859, 0.0838],\n",
            "        [0.2031, 0.1537, 0.1660,  ..., 0.1376, 0.1865, 0.0840],\n",
            "        ...,\n",
            "        [0.2016, 0.1530, 0.1663,  ..., 0.1351, 0.1841, 0.0845],\n",
            "        [0.2030, 0.1560, 0.1691,  ..., 0.1338, 0.1851, 0.0848],\n",
            "        [0.2041, 0.1519, 0.1648,  ..., 0.1361, 0.1861, 0.0861]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 6.6068e-03, -8.1795e-03, -3.0212e-03,  ..., -1.3414e-03,\n",
            "           6.2382e-03, -7.4983e-03],\n",
            "         [-8.4967e-04,  1.0519e-03,  3.8854e-04,  ...,  1.7252e-04,\n",
            "          -8.0227e-04,  9.6433e-04],\n",
            "         [-9.1757e-03,  1.1360e-02,  4.1959e-03,  ...,  1.8630e-03,\n",
            "          -8.6638e-03,  1.0414e-02],\n",
            "         ...,\n",
            "         [-2.1549e-02,  2.6679e-02,  9.8539e-03,  ...,  4.3753e-03,\n",
            "          -2.0347e-02,  2.4457e-02],\n",
            "         [ 1.4963e-02, -1.8525e-02, -6.8425e-03,  ..., -3.0382e-03,\n",
            "           1.4129e-02, -1.6983e-02],\n",
            "         [-7.8642e-03,  9.7363e-03,  3.5962e-03,  ...,  1.5968e-03,\n",
            "          -7.4255e-03,  8.9254e-03]],\n",
            "\n",
            "        [[ 3.2218e-03, -3.4596e-03, -4.0927e-03,  ...,  1.8065e-03,\n",
            "           1.6580e-03,  1.4411e-04],\n",
            "         [-2.5035e-03,  2.6883e-03,  3.1803e-03,  ..., -1.4038e-03,\n",
            "          -1.2884e-03, -1.1198e-04],\n",
            "         [-1.7679e-03,  1.8984e-03,  2.2458e-03,  ..., -9.9133e-04,\n",
            "          -9.0982e-04, -7.9079e-05],\n",
            "         ...,\n",
            "         [ 9.4675e-05, -1.0166e-04, -1.2027e-04,  ...,  5.3087e-05,\n",
            "           4.8722e-05,  4.2348e-06],\n",
            "         [ 6.0919e-03, -6.5415e-03, -7.7386e-03,  ...,  3.4159e-03,\n",
            "           3.1350e-03,  2.7249e-04],\n",
            "         [ 5.1854e-03, -5.5681e-03, -6.5871e-03,  ...,  2.9076e-03,\n",
            "           2.6685e-03,  2.3194e-04]],\n",
            "\n",
            "        [[ 2.6230e-03, -7.0875e-03, -7.1179e-03,  ...,  3.4177e-03,\n",
            "           2.6703e-03, -5.3672e-04],\n",
            "         [-7.4269e-03,  2.0068e-02,  2.0154e-02,  ..., -9.6773e-03,\n",
            "          -7.5609e-03,  1.5197e-03],\n",
            "         [-1.9518e-03,  5.2741e-03,  5.2967e-03,  ..., -2.5433e-03,\n",
            "          -1.9870e-03,  3.9939e-04],\n",
            "         ...,\n",
            "         [ 1.7519e-03, -4.7337e-03, -4.7540e-03,  ...,  2.2827e-03,\n",
            "           1.7835e-03, -3.5847e-04],\n",
            "         [ 7.9211e-03, -2.1404e-02, -2.1495e-02,  ...,  1.0321e-02,\n",
            "           8.0639e-03, -1.6208e-03],\n",
            "         [ 6.3201e-04, -1.7078e-03, -1.7151e-03,  ...,  8.2352e-04,\n",
            "           6.4341e-04, -1.2932e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.8502e-03, -1.5581e-03, -1.5428e-03,  ..., -7.5480e-04,\n",
            "           3.9764e-04, -6.3962e-04],\n",
            "         [-3.3006e-03,  2.7794e-03,  2.7522e-03,  ...,  1.3465e-03,\n",
            "          -7.0934e-04,  1.1410e-03],\n",
            "         [-3.2942e-03,  2.7740e-03,  2.7468e-03,  ...,  1.3439e-03,\n",
            "          -7.0796e-04,  1.1388e-03],\n",
            "         ...,\n",
            "         [ 3.4692e-03, -2.9214e-03, -2.8928e-03,  ..., -1.4153e-03,\n",
            "           7.4558e-04, -1.1993e-03],\n",
            "         [ 7.9884e-03, -6.7269e-03, -6.6612e-03,  ..., -3.2589e-03,\n",
            "           1.7168e-03, -2.7616e-03],\n",
            "         [ 3.3931e-03, -2.8573e-03, -2.8293e-03,  ..., -1.3842e-03,\n",
            "           7.2922e-04, -1.1730e-03]],\n",
            "\n",
            "        [[ 8.3473e-03, -1.2325e-02, -4.0793e-03,  ..., -1.6216e-03,\n",
            "           8.0452e-03, -1.1607e-02],\n",
            "         [-4.3192e-03,  6.3775e-03,  2.1108e-03,  ...,  8.3907e-04,\n",
            "          -4.1629e-03,  6.0061e-03],\n",
            "         [-4.0396e-03,  5.9647e-03,  1.9742e-03,  ...,  7.8476e-04,\n",
            "          -3.8935e-03,  5.6174e-03],\n",
            "         ...,\n",
            "         [-1.9206e-02,  2.8359e-02,  9.3860e-03,  ...,  3.7310e-03,\n",
            "          -1.8511e-02,  2.6707e-02],\n",
            "         [ 1.5076e-02, -2.2260e-02, -7.3675e-03,  ..., -2.9286e-03,\n",
            "           1.4530e-02, -2.0964e-02],\n",
            "         [-5.9422e-03,  8.7739e-03,  2.9040e-03,  ...,  1.1544e-03,\n",
            "          -5.7272e-03,  8.2630e-03]],\n",
            "\n",
            "        [[ 4.8135e-03, -6.6740e-03, -5.7338e-03,  ...,  3.5011e-03,\n",
            "           4.0451e-03, -1.4935e-03],\n",
            "         [-8.3362e-03,  1.1558e-02,  9.9300e-03,  ..., -6.0633e-03,\n",
            "          -7.0054e-03,  2.5864e-03],\n",
            "         [-3.7687e-03,  5.2253e-03,  4.4892e-03,  ..., -2.7411e-03,\n",
            "          -3.1671e-03,  1.1693e-03],\n",
            "         ...,\n",
            "         [ 2.0975e-03, -2.9083e-03, -2.4986e-03,  ...,  1.5256e-03,\n",
            "           1.7627e-03, -6.5079e-04],\n",
            "         [ 9.1297e-03, -1.2659e-02, -1.0875e-02,  ...,  6.6405e-03,\n",
            "           7.6723e-03, -2.8326e-03],\n",
            "         [ 3.7582e-03, -5.2109e-03, -4.4768e-03,  ...,  2.7335e-03,\n",
            "           3.1583e-03, -1.1660e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2055, 0.1541, 0.1664,  ..., 0.1361, 0.1853, 0.0853],\n",
            "        [0.2007, 0.1539, 0.1674,  ..., 0.1345, 0.1842, 0.0839],\n",
            "        [0.2042, 0.1542, 0.1666,  ..., 0.1385, 0.1872, 0.0842],\n",
            "        ...,\n",
            "        [0.2029, 0.1535, 0.1659,  ..., 0.1353, 0.1837, 0.0844],\n",
            "        [0.2032, 0.1535, 0.1659,  ..., 0.1355, 0.1839, 0.0847],\n",
            "        [0.1998, 0.1523, 0.1656,  ..., 0.1340, 0.1831, 0.0854]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 6.2443e-03, -7.6538e-03, -2.6814e-03,  ..., -8.1606e-04,\n",
            "           5.6068e-03, -6.8880e-03],\n",
            "         [-9.3620e-04,  1.1475e-03,  4.0202e-04,  ...,  1.2235e-04,\n",
            "          -8.4062e-04,  1.0327e-03],\n",
            "         [-1.0066e-02,  1.2338e-02,  4.3223e-03,  ...,  1.3154e-03,\n",
            "          -9.0378e-03,  1.1103e-02],\n",
            "         ...,\n",
            "         [-2.2714e-02,  2.7840e-02,  9.7536e-03,  ...,  2.9684e-03,\n",
            "          -2.0394e-02,  2.5055e-02],\n",
            "         [ 1.6017e-02, -1.9632e-02, -6.8778e-03,  ..., -2.0932e-03,\n",
            "           1.4381e-02, -1.7668e-02],\n",
            "         [-8.1290e-03,  9.9639e-03,  3.4907e-03,  ...,  1.0624e-03,\n",
            "          -7.2990e-03,  8.9670e-03]],\n",
            "\n",
            "        [[ 3.0205e-03, -3.1846e-03, -3.8682e-03,  ...,  1.6430e-03,\n",
            "           1.7271e-03,  4.0908e-04],\n",
            "         [-2.1375e-03,  2.2537e-03,  2.7374e-03,  ..., -1.1627e-03,\n",
            "          -1.2222e-03, -2.8949e-04],\n",
            "         [-1.4700e-03,  1.5499e-03,  1.8825e-03,  ..., -7.9958e-04,\n",
            "          -8.4055e-04, -1.9909e-04],\n",
            "         ...,\n",
            "         [ 6.2466e-04, -6.5861e-04, -7.9998e-04,  ...,  3.3978e-04,\n",
            "           3.5719e-04,  8.4602e-05],\n",
            "         [ 6.5192e-03, -6.8736e-03, -8.3490e-03,  ...,  3.5461e-03,\n",
            "           3.7278e-03,  8.8295e-04],\n",
            "         [ 5.4271e-03, -5.7220e-03, -6.9503e-03,  ...,  2.9520e-03,\n",
            "           3.1033e-03,  7.3503e-04]],\n",
            "\n",
            "        [[ 2.1749e-03, -6.5899e-03, -6.9652e-03,  ...,  3.5540e-03,\n",
            "           2.7928e-03, -2.5339e-04],\n",
            "         [-7.0865e-03,  2.1472e-02,  2.2695e-02,  ..., -1.1580e-02,\n",
            "          -9.0996e-03,  8.2561e-04],\n",
            "         [-1.4087e-03,  4.2684e-03,  4.5115e-03,  ..., -2.3020e-03,\n",
            "          -1.8089e-03,  1.6412e-04],\n",
            "         ...,\n",
            "         [ 1.8785e-03, -5.6917e-03, -6.0159e-03,  ...,  3.0696e-03,\n",
            "           2.4121e-03, -2.1885e-04],\n",
            "         [ 7.2245e-03, -2.1890e-02, -2.3137e-02,  ...,  1.1805e-02,\n",
            "           9.2767e-03, -8.4168e-04],\n",
            "         [ 6.6805e-04, -2.0241e-03, -2.1394e-03,  ...,  1.0916e-03,\n",
            "           8.5782e-04, -7.7830e-05]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.7270e-03, -2.0167e-03, -2.1984e-03,  ..., -9.6397e-04,\n",
            "           6.0615e-04, -8.6301e-04],\n",
            "         [-7.3421e-03,  5.4299e-03,  5.9190e-03,  ...,  2.5954e-03,\n",
            "          -1.6320e-03,  2.3236e-03],\n",
            "         [-1.8652e-03,  1.3794e-03,  1.5037e-03,  ...,  6.5936e-04,\n",
            "          -4.1461e-04,  5.9030e-04],\n",
            "         ...,\n",
            "         [ 3.6986e-03, -2.7354e-03, -2.9817e-03,  ..., -1.3075e-03,\n",
            "           8.2213e-04, -1.1705e-03],\n",
            "         [ 1.0713e-02, -7.9232e-03, -8.6368e-03,  ..., -3.7872e-03,\n",
            "           2.3814e-03, -3.3905e-03],\n",
            "         [ 3.8419e-03, -2.8413e-03, -3.0972e-03,  ..., -1.3581e-03,\n",
            "           8.5398e-04, -1.2159e-03]],\n",
            "\n",
            "        [[ 7.4569e-03, -1.1220e-02, -3.7181e-03,  ..., -1.7635e-03,\n",
            "           6.6704e-03, -9.5648e-03],\n",
            "         [-4.1740e-03,  6.2802e-03,  2.0812e-03,  ...,  9.8711e-04,\n",
            "          -3.7338e-03,  5.3540e-03],\n",
            "         [-4.6608e-03,  7.0125e-03,  2.3239e-03,  ...,  1.1022e-03,\n",
            "          -4.1692e-03,  5.9783e-03],\n",
            "         ...,\n",
            "         [-2.0023e-02,  3.0127e-02,  9.9839e-03,  ...,  4.7353e-03,\n",
            "          -1.7911e-02,  2.5684e-02],\n",
            "         [ 1.6568e-02, -2.4929e-02, -8.2613e-03,  ..., -3.9183e-03,\n",
            "           1.4821e-02, -2.1252e-02],\n",
            "         [-6.2033e-03,  9.3335e-03,  3.0931e-03,  ...,  1.4670e-03,\n",
            "          -5.5491e-03,  7.9570e-03]],\n",
            "\n",
            "        [[ 4.7062e-03, -6.2925e-03, -5.1691e-03,  ...,  2.1488e-03,\n",
            "           3.0149e-03, -9.7841e-04],\n",
            "         [-8.7212e-03,  1.1661e-02,  9.5790e-03,  ..., -3.9820e-03,\n",
            "          -5.5870e-03,  1.8131e-03],\n",
            "         [-3.3726e-03,  4.5094e-03,  3.7043e-03,  ..., -1.5399e-03,\n",
            "          -2.1606e-03,  7.0116e-04],\n",
            "         ...,\n",
            "         [ 1.1926e-03, -1.5945e-03, -1.3099e-03,  ...,  5.4451e-04,\n",
            "           7.6398e-04, -2.4793e-04],\n",
            "         [ 9.1200e-03, -1.2194e-02, -1.0017e-02,  ...,  4.1641e-03,\n",
            "           5.8425e-03, -1.8960e-03],\n",
            "         [ 4.8348e-03, -6.4644e-03, -5.3104e-03,  ...,  2.2076e-03,\n",
            "           3.0973e-03, -1.0052e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2036, 0.1536, 0.1660,  ..., 0.1357, 0.1842, 0.0848],\n",
            "        [0.2041, 0.1567, 0.1714,  ..., 0.1371, 0.1870, 0.0845],\n",
            "        [0.2058, 0.1549, 0.1681,  ..., 0.1401, 0.1886, 0.0845],\n",
            "        ...,\n",
            "        [0.2006, 0.1530, 0.1667,  ..., 0.1352, 0.1844, 0.0843],\n",
            "        [0.2061, 0.1568, 0.1693,  ..., 0.1365, 0.1856, 0.0860],\n",
            "        [0.2008, 0.1519, 0.1661,  ..., 0.1349, 0.1848, 0.0858]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 5.5563e-03, -6.3927e-03, -2.8919e-03,  ..., -4.9319e-04,\n",
            "           4.7000e-03, -6.1440e-03],\n",
            "         [-2.5253e-03,  2.9055e-03,  1.3144e-03,  ...,  2.2416e-04,\n",
            "          -2.1362e-03,  2.7925e-03],\n",
            "         [-1.0007e-02,  1.1513e-02,  5.2084e-03,  ...,  8.8824e-04,\n",
            "          -8.4649e-03,  1.1065e-02],\n",
            "         ...,\n",
            "         [-2.2428e-02,  2.5804e-02,  1.1673e-02,  ...,  1.9908e-03,\n",
            "          -1.8972e-02,  2.4800e-02],\n",
            "         [ 1.6016e-02, -1.8427e-02, -8.3361e-03,  ..., -1.4216e-03,\n",
            "           1.3548e-02, -1.7710e-02],\n",
            "         [-8.1949e-03,  9.4285e-03,  4.2653e-03,  ...,  7.2740e-04,\n",
            "          -6.9320e-03,  9.0617e-03]],\n",
            "\n",
            "        [[ 3.4540e-03, -3.4098e-03, -4.0304e-03,  ...,  1.7953e-03,\n",
            "           1.6492e-03,  5.0620e-04],\n",
            "         [-1.9777e-03,  1.9523e-03,  2.3077e-03,  ..., -1.0280e-03,\n",
            "          -9.4427e-04, -2.8984e-04],\n",
            "         [-5.2896e-04,  5.2218e-04,  6.1722e-04,  ..., -2.7494e-04,\n",
            "          -2.5256e-04, -7.7521e-05],\n",
            "         ...,\n",
            "         [ 6.8900e-04, -6.8017e-04, -8.0396e-04,  ...,  3.5813e-04,\n",
            "           3.2897e-04,  1.0098e-04],\n",
            "         [ 8.0182e-03, -7.9155e-03, -9.3561e-03,  ...,  4.1677e-03,\n",
            "           3.8284e-03,  1.1751e-03],\n",
            "         [ 5.6032e-03, -5.5314e-03, -6.5381e-03,  ...,  2.9124e-03,\n",
            "           2.6753e-03,  8.2117e-04]],\n",
            "\n",
            "        [[ 2.9857e-03, -6.9882e-03, -8.5620e-03,  ...,  4.5304e-03,\n",
            "           3.0994e-03, -1.0004e-03],\n",
            "         [-8.1561e-03,  1.9090e-02,  2.3389e-02,  ..., -1.2376e-02,\n",
            "          -8.4666e-03,  2.7327e-03],\n",
            "         [-2.1876e-03,  5.1203e-03,  6.2734e-03,  ..., -3.3195e-03,\n",
            "          -2.2709e-03,  7.3297e-04],\n",
            "         ...,\n",
            "         [ 2.1572e-03, -5.0491e-03, -6.1861e-03,  ...,  3.2733e-03,\n",
            "           2.2393e-03, -7.2277e-04],\n",
            "         [ 8.9010e-03, -2.0833e-02, -2.5525e-02,  ...,  1.3506e-02,\n",
            "           9.2399e-03, -2.9823e-03],\n",
            "         [ 6.9593e-04, -1.6289e-03, -1.9957e-03,  ...,  1.0560e-03,\n",
            "           7.2242e-04, -2.3317e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.7174e-03, -1.4083e-03, -1.4116e-03,  ..., -4.3769e-04,\n",
            "           1.5211e-04, -5.1618e-04],\n",
            "         [-5.2592e-03,  4.3125e-03,  4.3228e-03,  ...,  1.3403e-03,\n",
            "          -4.6579e-04,  1.5807e-03],\n",
            "         [-2.8216e-03,  2.3137e-03,  2.3192e-03,  ...,  7.1909e-04,\n",
            "          -2.4990e-04,  8.4804e-04],\n",
            "         ...,\n",
            "         [ 4.3606e-03, -3.5757e-03, -3.5842e-03,  ..., -1.1113e-03,\n",
            "           3.8621e-04, -1.3106e-03],\n",
            "         [ 9.3067e-03, -7.6314e-03, -7.6496e-03,  ..., -2.3718e-03,\n",
            "           8.2427e-04, -2.7972e-03],\n",
            "         [ 2.9620e-03, -2.4288e-03, -2.4346e-03,  ..., -7.5487e-04,\n",
            "           2.6234e-04, -8.9024e-04]],\n",
            "\n",
            "        [[ 8.5824e-03, -1.1143e-02, -3.1369e-03,  ..., -1.3333e-03,\n",
            "           6.5642e-03, -1.0513e-02],\n",
            "         [-5.3906e-03,  6.9986e-03,  1.9703e-03,  ...,  8.3747e-04,\n",
            "          -4.1230e-03,  6.6030e-03],\n",
            "         [-4.3089e-03,  5.5943e-03,  1.5749e-03,  ...,  6.6942e-04,\n",
            "          -3.2957e-03,  5.2780e-03],\n",
            "         ...,\n",
            "         [-2.2801e-02,  2.9603e-02,  8.3339e-03,  ...,  3.5423e-03,\n",
            "          -1.7439e-02,  2.7929e-02],\n",
            "         [ 1.7338e-02, -2.2510e-02, -6.3371e-03,  ..., -2.6936e-03,\n",
            "           1.3261e-02, -2.1237e-02],\n",
            "         [-5.0255e-03,  6.5246e-03,  1.8369e-03,  ...,  7.8075e-04,\n",
            "          -3.8438e-03,  6.1558e-03]],\n",
            "\n",
            "        [[ 4.6195e-03, -6.0750e-03, -5.0158e-03,  ...,  1.6041e-03,\n",
            "           3.3917e-03, -2.0420e-03],\n",
            "         [-7.8931e-03,  1.0380e-02,  8.5703e-03,  ..., -2.7408e-03,\n",
            "          -5.7952e-03,  3.4890e-03],\n",
            "         [-2.3588e-03,  3.1020e-03,  2.5612e-03,  ..., -8.1908e-04,\n",
            "          -1.7319e-03,  1.0427e-03],\n",
            "         ...,\n",
            "         [ 2.0192e-03, -2.6554e-03, -2.1924e-03,  ...,  7.0116e-04,\n",
            "           1.4825e-03, -8.9255e-04],\n",
            "         [ 9.1083e-03, -1.1978e-02, -9.8897e-03,  ...,  3.1628e-03,\n",
            "           6.6874e-03, -4.0261e-03],\n",
            "         [ 4.9246e-03, -6.4763e-03, -5.3471e-03,  ...,  1.7101e-03,\n",
            "           3.6157e-03, -2.1768e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2011, 0.1522, 0.1643,  ..., 0.1338, 0.1832, 0.0849],\n",
            "        [0.2033, 0.1535, 0.1659,  ..., 0.1356, 0.1840, 0.0847],\n",
            "        [0.2034, 0.1538, 0.1662,  ..., 0.1378, 0.1867, 0.0841],\n",
            "        ...,\n",
            "        [0.2036, 0.1536, 0.1660,  ..., 0.1357, 0.1842, 0.0848],\n",
            "        [0.2061, 0.1542, 0.1666,  ..., 0.1363, 0.1856, 0.0854],\n",
            "        [0.2003, 0.1537, 0.1662,  ..., 0.1355, 0.1866, 0.0841]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 0.0059, -0.0070, -0.0030,  ..., -0.0007,  0.0054, -0.0079],\n",
            "         [-0.0019,  0.0023,  0.0010,  ...,  0.0002, -0.0018,  0.0026],\n",
            "         [-0.0089,  0.0106,  0.0045,  ...,  0.0011, -0.0081,  0.0119],\n",
            "         ...,\n",
            "         [-0.0205,  0.0245,  0.0104,  ...,  0.0026, -0.0188,  0.0274],\n",
            "         [ 0.0138, -0.0165, -0.0070,  ..., -0.0017,  0.0126, -0.0185],\n",
            "         [-0.0079,  0.0094,  0.0040,  ...,  0.0010, -0.0072,  0.0106]],\n",
            "\n",
            "        [[ 0.0039, -0.0038, -0.0045,  ...,  0.0024,  0.0021,  0.0006],\n",
            "         [-0.0034,  0.0033,  0.0039,  ..., -0.0021, -0.0018, -0.0005],\n",
            "         [-0.0010,  0.0010,  0.0012,  ..., -0.0006, -0.0006, -0.0001],\n",
            "         ...,\n",
            "         [ 0.0014, -0.0013, -0.0016,  ...,  0.0008,  0.0007,  0.0002],\n",
            "         [ 0.0085, -0.0084, -0.0099,  ...,  0.0053,  0.0047,  0.0012],\n",
            "         [ 0.0051, -0.0050, -0.0059,  ...,  0.0032,  0.0028,  0.0007]],\n",
            "\n",
            "        [[ 0.0035, -0.0076, -0.0091,  ...,  0.0045,  0.0030, -0.0011],\n",
            "         [-0.0088,  0.0191,  0.0229,  ..., -0.0114, -0.0075,  0.0026],\n",
            "         [-0.0025,  0.0054,  0.0065,  ..., -0.0032, -0.0021,  0.0007],\n",
            "         ...,\n",
            "         [ 0.0027, -0.0060, -0.0072,  ...,  0.0036,  0.0024, -0.0008],\n",
            "         [ 0.0098, -0.0214, -0.0257,  ...,  0.0128,  0.0085, -0.0030],\n",
            "         [ 0.0010, -0.0021, -0.0025,  ...,  0.0012,  0.0008, -0.0003]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0008, -0.0005, -0.0006,  ..., -0.0002,  0.0001, -0.0003],\n",
            "         [-0.0057,  0.0039,  0.0044,  ...,  0.0013, -0.0011,  0.0018],\n",
            "         [-0.0024,  0.0016,  0.0018,  ...,  0.0006, -0.0004,  0.0007],\n",
            "         ...,\n",
            "         [ 0.0040, -0.0027, -0.0031,  ..., -0.0010,  0.0008, -0.0013],\n",
            "         [ 0.0108, -0.0073, -0.0083,  ..., -0.0025,  0.0020, -0.0034],\n",
            "         [ 0.0037, -0.0025, -0.0028,  ..., -0.0009,  0.0007, -0.0012]],\n",
            "\n",
            "        [[ 0.0098, -0.0121, -0.0029,  ..., -0.0017,  0.0061, -0.0120],\n",
            "         [-0.0052,  0.0065,  0.0015,  ...,  0.0009, -0.0032,  0.0064],\n",
            "         [-0.0039,  0.0049,  0.0012,  ...,  0.0007, -0.0024,  0.0048],\n",
            "         ...,\n",
            "         [-0.0227,  0.0281,  0.0066,  ...,  0.0040, -0.0141,  0.0277],\n",
            "         [ 0.0175, -0.0216, -0.0051,  ..., -0.0031,  0.0108, -0.0213],\n",
            "         [-0.0043,  0.0053,  0.0013,  ...,  0.0008, -0.0027,  0.0052]],\n",
            "\n",
            "        [[ 0.0045, -0.0066, -0.0057,  ...,  0.0015,  0.0027, -0.0020],\n",
            "         [-0.0074,  0.0110,  0.0094,  ..., -0.0026, -0.0044,  0.0034],\n",
            "         [-0.0027,  0.0040,  0.0034,  ..., -0.0009, -0.0016,  0.0012],\n",
            "         ...,\n",
            "         [ 0.0015, -0.0022, -0.0019,  ...,  0.0005,  0.0009, -0.0007],\n",
            "         [ 0.0084, -0.0125, -0.0107,  ...,  0.0029,  0.0050, -0.0039],\n",
            "         [ 0.0044, -0.0065, -0.0056,  ...,  0.0015,  0.0026, -0.0020]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2011, 0.1519, 0.1659,  ..., 0.1350, 0.1846, 0.0857],\n",
            "        [0.2030, 0.1560, 0.1692,  ..., 0.1338, 0.1851, 0.0848],\n",
            "        [0.2048, 0.1545, 0.1670,  ..., 0.1391, 0.1877, 0.0843],\n",
            "        ...,\n",
            "        [0.2055, 0.1541, 0.1664,  ..., 0.1361, 0.1854, 0.0853],\n",
            "        [0.2008, 0.1539, 0.1674,  ..., 0.1345, 0.1841, 0.0839],\n",
            "        [0.2052, 0.1546, 0.1673,  ..., 0.1394, 0.1879, 0.0843]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 5.1859e-03, -5.7133e-03, -1.9419e-03,  ..., -4.7956e-04,\n",
            "           4.0614e-03, -5.9388e-03],\n",
            "         [-4.1383e-03,  4.5591e-03,  1.5496e-03,  ...,  3.8268e-04,\n",
            "          -3.2409e-03,  4.7391e-03],\n",
            "         [-1.0533e-02,  1.1604e-02,  3.9443e-03,  ...,  9.7407e-04,\n",
            "          -8.2492e-03,  1.2063e-02],\n",
            "         ...,\n",
            "         [-2.2278e-02,  2.4543e-02,  8.3420e-03,  ...,  2.0601e-03,\n",
            "          -1.7447e-02,  2.5512e-02],\n",
            "         [ 1.4777e-02, -1.6280e-02, -5.5333e-03,  ..., -1.3665e-03,\n",
            "           1.1573e-02, -1.6922e-02],\n",
            "         [-6.9229e-03,  7.6269e-03,  2.5923e-03,  ...,  6.4019e-04,\n",
            "          -5.4217e-03,  7.9280e-03]],\n",
            "\n",
            "        [[ 3.1563e-03, -3.4667e-03, -3.7883e-03,  ...,  1.7754e-03,\n",
            "           1.4922e-03,  4.5447e-04],\n",
            "         [-2.7359e-03,  3.0050e-03,  3.2837e-03,  ..., -1.5389e-03,\n",
            "          -1.2934e-03, -3.9393e-04],\n",
            "         [-1.3754e-03,  1.5107e-03,  1.6508e-03,  ..., -7.7367e-04,\n",
            "          -6.5025e-04, -1.9804e-04],\n",
            "         ...,\n",
            "         [ 1.2005e-03, -1.3185e-03, -1.4408e-03,  ...,  6.7526e-04,\n",
            "           5.6754e-04,  1.7285e-04],\n",
            "         [ 9.1323e-03, -1.0031e-02, -1.0961e-02,  ...,  5.1370e-03,\n",
            "           4.3175e-03,  1.3149e-03],\n",
            "         [ 5.9306e-03, -6.5140e-03, -7.1181e-03,  ...,  3.3360e-03,\n",
            "           2.8038e-03,  8.5394e-04]],\n",
            "\n",
            "        [[ 2.8145e-03, -6.8597e-03, -8.6003e-03,  ...,  3.9869e-03,\n",
            "           2.6029e-03, -4.7991e-04],\n",
            "         [-7.1758e-03,  1.7489e-02,  2.1927e-02,  ..., -1.0165e-02,\n",
            "          -6.6362e-03,  1.2236e-03],\n",
            "         [-2.1884e-03,  5.3338e-03,  6.6873e-03,  ..., -3.1001e-03,\n",
            "          -2.0239e-03,  3.7316e-04],\n",
            "         ...,\n",
            "         [ 2.6376e-03, -6.4285e-03, -8.0597e-03,  ...,  3.7363e-03,\n",
            "           2.4393e-03, -4.4975e-04],\n",
            "         [ 8.6788e-03, -2.1153e-02, -2.6520e-02,  ...,  1.2294e-02,\n",
            "           8.0262e-03, -1.4799e-03],\n",
            "         [ 5.4005e-04, -1.3163e-03, -1.6503e-03,  ...,  7.6502e-04,\n",
            "           4.9945e-04, -9.2086e-05]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 3.9583e-04, -2.8889e-04, -3.0932e-04,  ..., -1.0971e-04,\n",
            "           1.1251e-04, -1.5369e-04],\n",
            "         [-6.5197e-03,  4.7584e-03,  5.0948e-03,  ...,  1.8071e-03,\n",
            "          -1.8531e-03,  2.5314e-03],\n",
            "         [-2.7584e-03,  2.0132e-03,  2.1555e-03,  ...,  7.6455e-04,\n",
            "          -7.8402e-04,  1.0710e-03],\n",
            "         ...,\n",
            "         [ 5.6173e-03, -4.0998e-03, -4.3896e-03,  ..., -1.5570e-03,\n",
            "           1.5966e-03, -2.1810e-03],\n",
            "         [ 1.1210e-02, -8.1817e-03, -8.7602e-03,  ..., -3.1072e-03,\n",
            "           3.1863e-03, -4.3525e-03],\n",
            "         [ 3.4704e-03, -2.5328e-03, -2.7119e-03,  ..., -9.6189e-04,\n",
            "           9.8640e-04, -1.3474e-03]],\n",
            "\n",
            "        [[ 1.0507e-02, -1.2296e-02, -3.2960e-03,  ..., -1.2787e-03,\n",
            "           7.4594e-03, -1.2493e-02],\n",
            "         [-5.4821e-03,  6.4159e-03,  1.7198e-03,  ...,  6.6717e-04,\n",
            "          -3.8921e-03,  6.5183e-03],\n",
            "         [-4.1019e-03,  4.8006e-03,  1.2868e-03,  ...,  4.9920e-04,\n",
            "          -2.9122e-03,  4.8772e-03],\n",
            "         ...,\n",
            "         [-2.3481e-02,  2.7481e-02,  7.3662e-03,  ...,  2.8577e-03,\n",
            "          -1.6671e-02,  2.7920e-02],\n",
            "         [ 1.7865e-02, -2.0908e-02, -5.6043e-03,  ..., -2.1742e-03,\n",
            "           1.2683e-02, -2.1242e-02],\n",
            "         [-4.7410e-03,  5.5487e-03,  1.4873e-03,  ...,  5.7698e-04,\n",
            "          -3.3660e-03,  5.6372e-03]],\n",
            "\n",
            "        [[ 4.9971e-03, -6.7467e-03, -6.4002e-03,  ...,  1.6752e-03,\n",
            "           2.9963e-03, -2.5520e-03],\n",
            "         [-7.2706e-03,  9.8161e-03,  9.3119e-03,  ..., -2.4373e-03,\n",
            "          -4.3595e-03,  3.7131e-03],\n",
            "         [-2.4919e-03,  3.3643e-03,  3.1915e-03,  ..., -8.3534e-04,\n",
            "          -1.4941e-03,  1.2726e-03],\n",
            "         ...,\n",
            "         [ 1.3396e-03, -1.8086e-03, -1.7157e-03,  ...,  4.4908e-04,\n",
            "           8.0325e-04, -6.8414e-04],\n",
            "         [ 8.2448e-03, -1.1131e-02, -1.0560e-02,  ...,  2.7639e-03,\n",
            "           4.9436e-03, -4.2106e-03],\n",
            "         [ 3.7682e-03, -5.0875e-03, -4.8262e-03,  ...,  1.2632e-03,\n",
            "           2.2595e-03, -1.9244e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2054, 0.1541, 0.1664,  ..., 0.1361, 0.1853, 0.0853],\n",
            "        [0.2010, 0.1539, 0.1673,  ..., 0.1345, 0.1840, 0.0839],\n",
            "        [0.2041, 0.1541, 0.1665,  ..., 0.1383, 0.1871, 0.0843],\n",
            "        ...,\n",
            "        [0.2017, 0.1537, 0.1666,  ..., 0.1347, 0.1836, 0.0839],\n",
            "        [0.2021, 0.1536, 0.1663,  ..., 0.1348, 0.1834, 0.0840],\n",
            "        [0.2022, 0.1577, 0.1684,  ..., 0.1338, 0.1844, 0.0824]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 5.7450e-03, -6.4381e-03, -1.3923e-03,  ..., -5.3456e-04,\n",
            "           4.3426e-03, -6.4589e-03],\n",
            "         [-7.0829e-03,  7.9374e-03,  1.7165e-03,  ...,  6.5904e-04,\n",
            "          -5.3539e-03,  7.9630e-03],\n",
            "         [-1.0997e-02,  1.2324e-02,  2.6651e-03,  ...,  1.0233e-03,\n",
            "          -8.3127e-03,  1.2364e-02],\n",
            "         ...,\n",
            "         [-2.3345e-02,  2.6162e-02,  5.6576e-03,  ...,  2.1722e-03,\n",
            "          -1.7647e-02,  2.6247e-02],\n",
            "         [ 1.4869e-02, -1.6663e-02, -3.6035e-03,  ..., -1.3836e-03,\n",
            "           1.1240e-02, -1.6717e-02],\n",
            "         [-4.9429e-03,  5.5393e-03,  1.1979e-03,  ...,  4.5993e-04,\n",
            "          -3.7363e-03,  5.5572e-03]],\n",
            "\n",
            "        [[ 3.1647e-03, -3.1725e-03, -3.7700e-03,  ...,  2.1695e-03,\n",
            "           1.7482e-03,  5.6592e-04],\n",
            "         [-2.7410e-03,  2.7477e-03,  3.2653e-03,  ..., -1.8791e-03,\n",
            "          -1.5141e-03, -4.9016e-04],\n",
            "         [-1.6097e-03,  1.6137e-03,  1.9176e-03,  ..., -1.1035e-03,\n",
            "          -8.8923e-04, -2.8786e-04],\n",
            "         ...,\n",
            "         [ 6.5709e-04, -6.5870e-04, -7.8278e-04,  ...,  4.5046e-04,\n",
            "           3.6298e-04,  1.1750e-04],\n",
            "         [ 8.7439e-03, -8.7653e-03, -1.0416e-02,  ...,  5.9943e-03,\n",
            "           4.8302e-03,  1.5636e-03],\n",
            "         [ 6.7175e-03, -6.7340e-03, -8.0025e-03,  ...,  4.6052e-03,\n",
            "           3.7108e-03,  1.2013e-03]],\n",
            "\n",
            "        [[ 2.9041e-03, -6.2099e-03, -8.2017e-03,  ...,  3.0322e-03,\n",
            "           2.3714e-03, -4.8986e-04],\n",
            "         [-7.6904e-03,  1.6445e-02,  2.1719e-02,  ..., -8.0297e-03,\n",
            "          -6.2798e-03,  1.2972e-03],\n",
            "         [-2.6577e-03,  5.6829e-03,  7.5057e-03,  ..., -2.7749e-03,\n",
            "          -2.1702e-03,  4.4829e-04],\n",
            "         ...,\n",
            "         [ 3.1230e-03, -6.6780e-03, -8.8198e-03,  ...,  3.2608e-03,\n",
            "           2.5501e-03, -5.2678e-04],\n",
            "         [ 9.9228e-03, -2.1218e-02, -2.8024e-02,  ...,  1.0361e-02,\n",
            "           8.1026e-03, -1.6738e-03],\n",
            "         [ 6.8004e-04, -1.4542e-03, -1.9206e-03,  ...,  7.1005e-04,\n",
            "           5.5530e-04, -1.1471e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.0728e-05,  9.3059e-06,  8.7258e-06,  ...,  2.8403e-06,\n",
            "          -2.5824e-06,  4.2021e-06],\n",
            "         [-4.5831e-03,  3.9755e-03,  3.7276e-03,  ...,  1.2134e-03,\n",
            "          -1.1032e-03,  1.7951e-03],\n",
            "         [-2.7695e-03,  2.4024e-03,  2.2526e-03,  ...,  7.3324e-04,\n",
            "          -6.6666e-04,  1.0848e-03],\n",
            "         ...,\n",
            "         [ 5.7175e-03, -4.9595e-03, -4.6503e-03,  ..., -1.5137e-03,\n",
            "           1.3763e-03, -2.2395e-03],\n",
            "         [ 1.0924e-02, -9.4759e-03, -8.8852e-03,  ..., -2.8922e-03,\n",
            "           2.6296e-03, -4.2789e-03],\n",
            "         [ 2.1402e-03, -1.8565e-03, -1.7407e-03,  ..., -5.6662e-04,\n",
            "           5.1517e-04, -8.3830e-04]],\n",
            "\n",
            "        [[ 1.0592e-02, -1.3156e-02, -2.9585e-03,  ..., -3.8463e-04,\n",
            "           6.7834e-03, -1.2273e-02],\n",
            "         [-5.4388e-03,  6.7556e-03,  1.5191e-03,  ...,  1.9750e-04,\n",
            "          -3.4832e-03,  6.3021e-03],\n",
            "         [-4.3947e-03,  5.4586e-03,  1.2275e-03,  ...,  1.5959e-04,\n",
            "          -2.8145e-03,  5.0922e-03],\n",
            "         ...,\n",
            "         [-2.2744e-02,  2.8250e-02,  6.3526e-03,  ...,  8.2590e-04,\n",
            "          -1.4566e-02,  2.6354e-02],\n",
            "         [ 1.8478e-02, -2.2952e-02, -5.1612e-03,  ..., -6.7101e-04,\n",
            "           1.1834e-02, -2.1411e-02],\n",
            "         [-4.1570e-03,  5.1634e-03,  1.1611e-03,  ...,  1.5096e-04,\n",
            "          -2.6623e-03,  4.8169e-03]],\n",
            "\n",
            "        [[ 5.1286e-03, -6.3526e-03, -6.2588e-03,  ...,  1.7177e-03,\n",
            "           2.1850e-03, -2.6095e-03],\n",
            "         [-8.8197e-03,  1.0924e-02,  1.0763e-02,  ..., -2.9540e-03,\n",
            "          -3.7575e-03,  4.4874e-03],\n",
            "         [-3.0608e-03,  3.7913e-03,  3.7353e-03,  ..., -1.0252e-03,\n",
            "          -1.3040e-03,  1.5573e-03],\n",
            "         ...,\n",
            "         [ 2.8504e-03, -3.5306e-03, -3.4784e-03,  ...,  9.5467e-04,\n",
            "           1.2143e-03, -1.4503e-03],\n",
            "         [ 1.0285e-02, -1.2739e-02, -1.2551e-02,  ...,  3.4446e-03,\n",
            "           4.3816e-03, -5.2328e-03],\n",
            "         [ 4.1082e-03, -5.0886e-03, -5.0134e-03,  ...,  1.3759e-03,\n",
            "           1.7502e-03, -2.0902e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2012, 0.1522, 0.1642,  ..., 0.1338, 0.1832, 0.0848],\n",
            "        [0.2015, 0.1538, 0.1668,  ..., 0.1346, 0.1838, 0.0839],\n",
            "        [0.2040, 0.1541, 0.1665,  ..., 0.1383, 0.1871, 0.0843],\n",
            "        ...,\n",
            "        [0.2016, 0.1538, 0.1667,  ..., 0.1347, 0.1837, 0.0839],\n",
            "        [0.2018, 0.1558, 0.1702,  ..., 0.1350, 0.1855, 0.0830],\n",
            "        [0.2024, 0.1580, 0.1685,  ..., 0.1341, 0.1840, 0.0827]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 6.1631e-03, -7.8803e-03, -2.2867e-03,  ..., -7.5344e-04,\n",
            "           5.9012e-03, -7.2550e-03],\n",
            "         [-6.9067e-03,  8.8311e-03,  2.5626e-03,  ...,  8.4435e-04,\n",
            "          -6.6133e-03,  8.1304e-03],\n",
            "         [-9.4079e-03,  1.2029e-02,  3.4906e-03,  ...,  1.1501e-03,\n",
            "          -9.0082e-03,  1.1075e-02],\n",
            "         ...,\n",
            "         [-2.0144e-02,  2.5757e-02,  7.4741e-03,  ...,  2.4627e-03,\n",
            "          -1.9288e-02,  2.3713e-02],\n",
            "         [ 1.3329e-02, -1.7043e-02, -4.9455e-03,  ..., -1.6295e-03,\n",
            "           1.2763e-02, -1.5691e-02],\n",
            "         [-4.3290e-03,  5.5352e-03,  1.6062e-03,  ...,  5.2922e-04,\n",
            "          -4.1451e-03,  5.0960e-03]],\n",
            "\n",
            "        [[ 4.0632e-03, -4.1245e-03, -4.8807e-03,  ...,  2.1859e-03,\n",
            "           2.0149e-03,  8.2458e-04],\n",
            "         [-1.5640e-03,  1.5875e-03,  1.8786e-03,  ..., -8.4137e-04,\n",
            "          -7.7554e-04, -3.1739e-04],\n",
            "         [-5.6570e-04,  5.7422e-04,  6.7951e-04,  ..., -3.0433e-04,\n",
            "          -2.8052e-04, -1.1480e-04],\n",
            "         ...,\n",
            "         [-1.0792e-04,  1.0955e-04,  1.2963e-04,  ..., -5.8058e-05,\n",
            "          -5.3516e-05, -2.1901e-05],\n",
            "         [ 7.9251e-03, -8.0445e-03, -9.5196e-03,  ...,  4.2635e-03,\n",
            "           3.9299e-03,  1.6083e-03],\n",
            "         [ 6.1154e-03, -6.2075e-03, -7.3457e-03,  ...,  3.2899e-03,\n",
            "           3.0325e-03,  1.2410e-03]],\n",
            "\n",
            "        [[ 2.7495e-03, -6.7625e-03, -8.9995e-03,  ...,  4.1423e-03,\n",
            "           2.3155e-03, -4.2823e-04],\n",
            "         [-6.7535e-03,  1.6611e-02,  2.2106e-02,  ..., -1.0175e-02,\n",
            "          -5.6876e-03,  1.0519e-03],\n",
            "         [-2.1559e-03,  5.3025e-03,  7.0566e-03,  ..., -3.2480e-03,\n",
            "          -1.8156e-03,  3.3578e-04],\n",
            "         ...,\n",
            "         [ 3.2141e-03, -7.9054e-03, -1.0521e-02,  ...,  4.8424e-03,\n",
            "           2.7069e-03, -5.0060e-04],\n",
            "         [ 7.9115e-03, -1.9459e-02, -2.5896e-02,  ...,  1.1919e-02,\n",
            "           6.6628e-03, -1.2322e-03],\n",
            "         [ 1.2057e-04, -2.9656e-04, -3.9466e-04,  ...,  1.8165e-04,\n",
            "           1.0154e-04, -1.8779e-05]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.3412e-04, -1.1012e-04, -1.1137e-04,  ..., -2.9505e-05,\n",
            "           6.3007e-05, -4.9634e-05],\n",
            "         [-4.4313e-03,  3.6386e-03,  3.6799e-03,  ...,  9.7489e-04,\n",
            "          -2.0818e-03,  1.6400e-03],\n",
            "         [-7.3766e-04,  6.0569e-04,  6.1257e-04,  ...,  1.6228e-04,\n",
            "          -3.4655e-04,  2.7300e-04],\n",
            "         ...,\n",
            "         [ 1.7539e-04, -1.4402e-04, -1.4565e-04,  ..., -3.8586e-05,\n",
            "           8.2399e-05, -6.4910e-05],\n",
            "         [ 8.6357e-03, -7.0909e-03, -7.1714e-03,  ..., -1.8999e-03,\n",
            "           4.0570e-03, -3.1960e-03],\n",
            "         [ 2.3684e-03, -1.9447e-03, -1.9668e-03,  ..., -5.2105e-04,\n",
            "           1.1127e-03, -8.7652e-04]],\n",
            "\n",
            "        [[ 1.1738e-02, -1.2220e-02, -3.5855e-03,  ..., -1.8633e-03,\n",
            "           7.0555e-03, -1.1621e-02],\n",
            "         [-5.3834e-03,  5.6045e-03,  1.6444e-03,  ...,  8.5458e-04,\n",
            "          -3.2359e-03,  5.3295e-03],\n",
            "         [-4.8244e-03,  5.0225e-03,  1.4737e-03,  ...,  7.6584e-04,\n",
            "          -2.8998e-03,  4.7761e-03],\n",
            "         ...,\n",
            "         [-2.7277e-02,  2.8397e-02,  8.3321e-03,  ...,  4.3300e-03,\n",
            "          -1.6396e-02,  2.7004e-02],\n",
            "         [ 1.9913e-02, -2.0731e-02, -6.0827e-03,  ..., -3.1611e-03,\n",
            "           1.1969e-02, -1.9714e-02],\n",
            "         [-4.4984e-03,  4.6832e-03,  1.3741e-03,  ...,  7.1409e-04,\n",
            "          -2.7039e-03,  4.4534e-03]],\n",
            "\n",
            "        [[ 4.4240e-03, -6.1355e-03, -6.1509e-03,  ...,  1.3133e-03,\n",
            "           1.8661e-03, -2.5673e-03],\n",
            "         [-7.5599e-03,  1.0484e-02,  1.0511e-02,  ..., -2.2442e-03,\n",
            "          -3.1888e-03,  4.3871e-03],\n",
            "         [-3.3143e-03,  4.5965e-03,  4.6080e-03,  ..., -9.8387e-04,\n",
            "          -1.3980e-03,  1.9233e-03],\n",
            "         ...,\n",
            "         [ 2.6811e-03, -3.7182e-03, -3.7276e-03,  ...,  7.9588e-04,\n",
            "           1.1309e-03, -1.5558e-03],\n",
            "         [ 9.8161e-03, -1.3614e-02, -1.3648e-02,  ...,  2.9140e-03,\n",
            "           4.1405e-03, -5.6964e-03],\n",
            "         [ 3.7070e-03, -5.1411e-03, -5.1540e-03,  ...,  1.1004e-03,\n",
            "           1.5636e-03, -2.1512e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2008, 0.1539, 0.1674,  ..., 0.1345, 0.1841, 0.0839],\n",
            "        [0.2059, 0.1567, 0.1693,  ..., 0.1364, 0.1855, 0.0860],\n",
            "        [0.2005, 0.1537, 0.1662,  ..., 0.1356, 0.1866, 0.0841],\n",
            "        ...,\n",
            "        [0.2017, 0.1530, 0.1662,  ..., 0.1351, 0.1840, 0.0845],\n",
            "        [0.2028, 0.1538, 0.1686,  ..., 0.1367, 0.1857, 0.0834],\n",
            "        [0.2021, 0.1515, 0.1652,  ..., 0.1355, 0.1845, 0.0858]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 5.7497e-03, -7.9819e-03, -2.6660e-03,  ..., -1.0738e-03,\n",
            "           6.0905e-03, -6.8126e-03],\n",
            "         [-5.8488e-03,  8.1195e-03,  2.7119e-03,  ...,  1.0923e-03,\n",
            "          -6.1954e-03,  6.9299e-03],\n",
            "         [-6.7409e-03,  9.3580e-03,  3.1256e-03,  ...,  1.2590e-03,\n",
            "          -7.1405e-03,  7.9870e-03],\n",
            "         ...,\n",
            "         [-2.1049e-02,  2.9221e-02,  9.7600e-03,  ...,  3.9312e-03,\n",
            "          -2.2297e-02,  2.4940e-02],\n",
            "         [ 1.3202e-02, -1.8327e-02, -6.1213e-03,  ..., -2.4656e-03,\n",
            "           1.3984e-02, -1.5642e-02],\n",
            "         [-4.1620e-03,  5.7778e-03,  1.9298e-03,  ...,  7.7731e-04,\n",
            "          -4.4087e-03,  4.9314e-03]],\n",
            "\n",
            "        [[ 3.9644e-03, -4.1717e-03, -4.8332e-03,  ...,  2.1968e-03,\n",
            "           2.0075e-03,  8.3674e-04],\n",
            "         [-1.4212e-03,  1.4955e-03,  1.7326e-03,  ..., -7.8752e-04,\n",
            "          -7.1967e-04, -2.9996e-04],\n",
            "         [-6.6341e-04,  6.9809e-04,  8.0879e-04,  ..., -3.6761e-04,\n",
            "          -3.3594e-04, -1.4002e-04],\n",
            "         ...,\n",
            "         [-2.5867e-04,  2.7219e-04,  3.1536e-04,  ..., -1.4334e-04,\n",
            "          -1.3099e-04, -5.4596e-05],\n",
            "         [ 7.7017e-03, -8.1043e-03, -9.3895e-03,  ...,  4.2677e-03,\n",
            "           3.9001e-03,  1.6255e-03],\n",
            "         [ 6.0378e-03, -6.3534e-03, -7.3609e-03,  ...,  3.3457e-03,\n",
            "           3.0574e-03,  1.2743e-03]],\n",
            "\n",
            "        [[ 3.2585e-03, -7.3736e-03, -9.7802e-03,  ...,  4.6990e-03,\n",
            "           2.2193e-03, -5.1404e-04],\n",
            "         [-7.0813e-03,  1.6024e-02,  2.1254e-02,  ..., -1.0212e-02,\n",
            "          -4.8228e-03,  1.1171e-03],\n",
            "         [-2.9537e-03,  6.6839e-03,  8.8653e-03,  ..., -4.2595e-03,\n",
            "          -2.0117e-03,  4.6595e-04],\n",
            "         ...,\n",
            "         [ 3.4170e-03, -7.7323e-03, -1.0256e-02,  ...,  4.9276e-03,\n",
            "           2.3272e-03, -5.3904e-04],\n",
            "         [ 7.7582e-03, -1.7556e-02, -2.3286e-02,  ...,  1.1188e-02,\n",
            "           5.2838e-03, -1.2239e-03],\n",
            "         [ 3.8399e-04, -8.6891e-04, -1.1525e-03,  ...,  5.5374e-04,\n",
            "           2.6152e-04, -6.0575e-05]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.4851e-04, -1.3928e-04, -1.5299e-04,  ..., -4.0820e-05,\n",
            "           8.7488e-05, -5.0113e-05],\n",
            "         [-3.6014e-03,  3.3775e-03,  3.7099e-03,  ...,  9.8987e-04,\n",
            "          -2.1216e-03,  1.2152e-03],\n",
            "         [-1.8669e-04,  1.7509e-04,  1.9232e-04,  ...,  5.1314e-05,\n",
            "          -1.0998e-04,  6.2997e-05],\n",
            "         ...,\n",
            "         [ 2.5796e-04, -2.4192e-04, -2.6574e-04,  ..., -7.0903e-05,\n",
            "           1.5196e-04, -8.7046e-05],\n",
            "         [ 6.3924e-03, -5.9950e-03, -6.5851e-03,  ..., -1.7570e-03,\n",
            "           3.7657e-03, -2.1570e-03],\n",
            "         [ 1.6929e-03, -1.5877e-03, -1.7439e-03,  ..., -4.6531e-04,\n",
            "           9.9729e-04, -5.7125e-04]],\n",
            "\n",
            "        [[ 1.1081e-02, -1.3031e-02, -3.4595e-03,  ..., -1.3448e-03,\n",
            "           6.7752e-03, -1.1262e-02],\n",
            "         [-5.6013e-03,  6.5869e-03,  1.7488e-03,  ...,  6.7977e-04,\n",
            "          -3.4248e-03,  5.6927e-03],\n",
            "         [-4.3209e-03,  5.0812e-03,  1.3490e-03,  ...,  5.2438e-04,\n",
            "          -2.6420e-03,  4.3915e-03],\n",
            "         ...,\n",
            "         [-2.6122e-02,  3.0719e-02,  8.1555e-03,  ...,  3.1702e-03,\n",
            "          -1.5972e-02,  2.6549e-02],\n",
            "         [ 1.7679e-02, -2.0790e-02, -5.5195e-03,  ..., -2.1455e-03,\n",
            "           1.0810e-02, -1.7968e-02],\n",
            "         [-5.8142e-03,  6.8373e-03,  1.8152e-03,  ...,  7.0561e-04,\n",
            "          -3.5550e-03,  5.9091e-03]],\n",
            "\n",
            "        [[ 4.6657e-03, -5.6724e-03, -6.4283e-03,  ...,  1.9633e-03,\n",
            "           1.3623e-03, -2.9114e-03],\n",
            "         [-8.9558e-03,  1.0888e-02,  1.2339e-02,  ..., -3.7686e-03,\n",
            "          -2.6150e-03,  5.5884e-03],\n",
            "         [-4.1748e-03,  5.0756e-03,  5.7520e-03,  ..., -1.7568e-03,\n",
            "          -1.2190e-03,  2.6051e-03],\n",
            "         ...,\n",
            "         [ 4.2148e-03, -5.1242e-03, -5.8070e-03,  ...,  1.7736e-03,\n",
            "           1.2307e-03, -2.6300e-03],\n",
            "         [ 1.2722e-02, -1.5467e-02, -1.7529e-02,  ...,  5.3535e-03,\n",
            "           3.7148e-03, -7.9388e-03],\n",
            "         [ 4.1826e-03, -5.0851e-03, -5.7628e-03,  ...,  1.7600e-03,\n",
            "           1.2213e-03, -2.6100e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2046, 0.1542, 0.1655,  ..., 0.1347, 0.1842, 0.0859],\n",
            "        [0.2009, 0.1521, 0.1644,  ..., 0.1339, 0.1831, 0.0850],\n",
            "        [0.2055, 0.1548, 0.1677,  ..., 0.1398, 0.1882, 0.0844],\n",
            "        ...,\n",
            "        [0.2045, 0.1537, 0.1661,  ..., 0.1359, 0.1848, 0.0851],\n",
            "        [0.2045, 0.1527, 0.1682,  ..., 0.1372, 0.1870, 0.0839],\n",
            "        [0.2027, 0.1514, 0.1648,  ..., 0.1358, 0.1849, 0.0860]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 7.1334e-03, -7.7197e-03, -3.2248e-03,  ..., -7.2511e-04,\n",
            "           6.5399e-03, -7.3537e-03],\n",
            "         [-7.9657e-03,  8.6204e-03,  3.6010e-03,  ...,  8.0972e-04,\n",
            "          -7.3029e-03,  8.2117e-03],\n",
            "         [-8.5005e-03,  9.1991e-03,  3.8428e-03,  ...,  8.6407e-04,\n",
            "          -7.7932e-03,  8.7630e-03],\n",
            "         ...,\n",
            "         [-2.1273e-02,  2.3021e-02,  9.6166e-03,  ...,  2.1623e-03,\n",
            "          -1.9503e-02,  2.1929e-02],\n",
            "         [ 1.3130e-02, -1.4209e-02, -5.9356e-03,  ..., -1.3347e-03,\n",
            "           1.2038e-02, -1.3536e-02],\n",
            "         [-4.3091e-03,  4.6633e-03,  1.9480e-03,  ...,  4.3802e-04,\n",
            "          -3.9506e-03,  4.4422e-03]],\n",
            "\n",
            "        [[ 5.2935e-03, -1.2850e-02, -3.4466e-03,  ..., -2.5315e-03,\n",
            "           7.3982e-03, -1.0439e-02],\n",
            "         [-3.9167e-03,  9.5078e-03,  2.5502e-03,  ...,  1.8731e-03,\n",
            "          -5.4740e-03,  7.7240e-03],\n",
            "         [-3.5316e-03,  8.5729e-03,  2.2994e-03,  ...,  1.6889e-03,\n",
            "          -4.9357e-03,  6.9645e-03],\n",
            "         ...,\n",
            "         [-1.3734e-02,  3.3339e-02,  8.9423e-03,  ...,  6.5681e-03,\n",
            "          -1.9195e-02,  2.7085e-02],\n",
            "         [ 1.0730e-02, -2.6047e-02, -6.9863e-03,  ..., -5.1314e-03,\n",
            "           1.4996e-02, -2.1160e-02],\n",
            "         [-6.1247e-03,  1.4868e-02,  3.9879e-03,  ...,  2.9291e-03,\n",
            "          -8.5600e-03,  1.2079e-02]],\n",
            "\n",
            "        [[ 3.2420e-03, -7.2590e-03, -1.0166e-02,  ...,  4.5963e-03,\n",
            "           2.1147e-03,  1.7960e-04],\n",
            "         [-6.4509e-03,  1.4444e-02,  2.0228e-02,  ..., -9.1457e-03,\n",
            "          -4.2078e-03, -3.5736e-04],\n",
            "         [-2.8086e-03,  6.2885e-03,  8.8068e-03,  ..., -3.9818e-03,\n",
            "          -1.8320e-03, -1.5559e-04],\n",
            "         ...,\n",
            "         [ 4.1707e-03, -9.3384e-03, -1.3078e-02,  ...,  5.9129e-03,\n",
            "           2.7205e-03,  2.3105e-04],\n",
            "         [ 7.4397e-03, -1.6658e-02, -2.3329e-02,  ...,  1.0548e-02,\n",
            "           4.8528e-03,  4.1214e-04],\n",
            "         [ 3.9215e-05, -8.7805e-05, -1.2297e-04,  ...,  5.5597e-05,\n",
            "           2.5580e-05,  2.1724e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 5.2615e-04, -5.2175e-04, -5.1788e-04,  ..., -1.0090e-04,\n",
            "           2.9048e-04, -2.7379e-04],\n",
            "         [-1.9807e-03,  1.9641e-03,  1.9496e-03,  ...,  3.7984e-04,\n",
            "          -1.0935e-03,  1.0307e-03],\n",
            "         [ 1.6364e-03, -1.6227e-03, -1.6107e-03,  ..., -3.1381e-04,\n",
            "           9.0342e-04, -8.5153e-04],\n",
            "         ...,\n",
            "         [-7.7898e-04,  7.7246e-04,  7.6674e-04,  ...,  1.4939e-04,\n",
            "          -4.3006e-04,  4.0536e-04],\n",
            "         [ 4.5148e-03, -4.4770e-03, -4.4438e-03,  ..., -8.6581e-04,\n",
            "           2.4925e-03, -2.3494e-03],\n",
            "         [ 3.1546e-03, -3.1282e-03, -3.1050e-03,  ..., -6.0497e-04,\n",
            "           1.7416e-03, -1.6416e-03]],\n",
            "\n",
            "        [[ 9.7577e-03, -1.2031e-02, -2.8600e-03,  ..., -1.2647e-03,\n",
            "           5.1399e-03, -9.3537e-03],\n",
            "         [-6.5447e-03,  8.0693e-03,  1.9182e-03,  ...,  8.4823e-04,\n",
            "          -3.4474e-03,  6.2737e-03],\n",
            "         [-4.7964e-03,  5.9137e-03,  1.4058e-03,  ...,  6.2165e-04,\n",
            "          -2.5265e-03,  4.5978e-03],\n",
            "         ...,\n",
            "         [-2.3560e-02,  2.9049e-02,  6.9055e-03,  ...,  3.0536e-03,\n",
            "          -1.2410e-02,  2.2585e-02],\n",
            "         [ 1.7008e-02, -2.0970e-02, -4.9850e-03,  ..., -2.2043e-03,\n",
            "           8.9590e-03, -1.6304e-02],\n",
            "         [-5.3725e-03,  6.6240e-03,  1.5747e-03,  ...,  6.9631e-04,\n",
            "          -2.8300e-03,  5.1501e-03]],\n",
            "\n",
            "        [[ 5.2712e-03, -6.2998e-03, -6.9331e-03,  ...,  1.7799e-03,\n",
            "           1.5204e-03, -2.7304e-03],\n",
            "         [-8.4969e-03,  1.0155e-02,  1.1176e-02,  ..., -2.8691e-03,\n",
            "          -2.4509e-03,  4.4013e-03],\n",
            "         [-3.5497e-03,  4.2424e-03,  4.6689e-03,  ..., -1.1986e-03,\n",
            "          -1.0239e-03,  1.8387e-03],\n",
            "         ...,\n",
            "         [ 4.1575e-03, -4.9687e-03, -5.4683e-03,  ...,  1.4038e-03,\n",
            "           1.1992e-03, -2.1535e-03],\n",
            "         [ 1.2261e-02, -1.4654e-02, -1.6127e-02,  ...,  4.1402e-03,\n",
            "           3.5367e-03, -6.3511e-03],\n",
            "         [ 3.7934e-03, -4.5337e-03, -4.9895e-03,  ...,  1.2809e-03,\n",
            "           1.0942e-03, -1.9649e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2044, 0.1524, 0.1657,  ..., 0.1362, 0.1863, 0.0848],\n",
            "        [0.2058, 0.1542, 0.1689,  ..., 0.1384, 0.1866, 0.0841],\n",
            "        [0.2006, 0.1537, 0.1661,  ..., 0.1356, 0.1866, 0.0840],\n",
            "        ...,\n",
            "        [0.2007, 0.1520, 0.1662,  ..., 0.1349, 0.1848, 0.0858],\n",
            "        [0.2030, 0.1560, 0.1691,  ..., 0.1338, 0.1851, 0.0848],\n",
            "        [0.2056, 0.1548, 0.1663,  ..., 0.1351, 0.1846, 0.0863]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 7.1209e-03, -6.7163e-03, -2.5655e-03,  ..., -4.9446e-04,\n",
            "           5.3087e-03, -6.5584e-03],\n",
            "         [-8.3199e-03,  7.8471e-03,  2.9975e-03,  ...,  5.7772e-04,\n",
            "          -6.2026e-03,  7.6627e-03],\n",
            "         [-8.7060e-03,  8.2112e-03,  3.1365e-03,  ...,  6.0453e-04,\n",
            "          -6.4904e-03,  8.0183e-03],\n",
            "         ...,\n",
            "         [-2.3125e-02,  2.1811e-02,  8.3315e-03,  ...,  1.6058e-03,\n",
            "          -1.7240e-02,  2.1299e-02],\n",
            "         [ 1.4344e-02, -1.3529e-02, -5.1678e-03,  ..., -9.9603e-04,\n",
            "           1.0694e-02, -1.3211e-02],\n",
            "         [-4.7172e-03,  4.4492e-03,  1.6995e-03,  ...,  3.2756e-04,\n",
            "          -3.5167e-03,  4.3446e-03]],\n",
            "\n",
            "        [[ 5.1909e-03, -1.3753e-02, -3.6349e-03,  ..., -2.9845e-03,\n",
            "           6.6784e-03, -1.1195e-02],\n",
            "         [-3.6350e-03,  9.6306e-03,  2.5454e-03,  ...,  2.0899e-03,\n",
            "          -4.6766e-03,  7.8391e-03],\n",
            "         [-3.8781e-03,  1.0275e-02,  2.7156e-03,  ...,  2.2297e-03,\n",
            "          -4.9894e-03,  8.3635e-03],\n",
            "         ...,\n",
            "         [-1.2364e-02,  3.2757e-02,  8.6577e-03,  ...,  7.1085e-03,\n",
            "          -1.5907e-02,  2.6663e-02],\n",
            "         [ 1.1027e-02, -2.9214e-02, -7.7213e-03,  ..., -6.3396e-03,\n",
            "           1.4186e-02, -2.3780e-02],\n",
            "         [-6.2299e-03,  1.6506e-02,  4.3625e-03,  ...,  3.5818e-03,\n",
            "          -8.0152e-03,  1.3435e-02]],\n",
            "\n",
            "        [[ 3.3293e-03, -6.8568e-03, -1.0293e-02,  ...,  5.2333e-03,\n",
            "           2.2169e-03, -1.1325e-04],\n",
            "         [-6.3700e-03,  1.3119e-02,  1.9694e-02,  ..., -1.0013e-02,\n",
            "          -4.2416e-03,  2.1669e-04],\n",
            "         [-3.1144e-03,  6.4143e-03,  9.6288e-03,  ..., -4.8956e-03,\n",
            "          -2.0738e-03,  1.0594e-04],\n",
            "         ...,\n",
            "         [ 4.1825e-03, -8.6140e-03, -1.2931e-02,  ...,  6.5744e-03,\n",
            "           2.7850e-03, -1.4228e-04],\n",
            "         [ 7.0114e-03, -1.4440e-02, -2.1677e-02,  ...,  1.1021e-02,\n",
            "           4.6687e-03, -2.3851e-04],\n",
            "         [ 2.0591e-04, -4.2408e-04, -6.3661e-04,  ...,  3.2367e-04,\n",
            "           1.3711e-04, -7.0045e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.9213e-04,  2.9136e-04,  2.7316e-04,  ...,  6.1375e-05,\n",
            "          -1.4295e-04,  1.1883e-04],\n",
            "         [-2.7610e-03,  2.7538e-03,  2.5817e-03,  ...,  5.8009e-04,\n",
            "          -1.3511e-03,  1.1231e-03],\n",
            "         [ 7.2308e-04, -7.2118e-04, -6.7613e-04,  ..., -1.5192e-04,\n",
            "           3.5383e-04, -2.9413e-04],\n",
            "         ...,\n",
            "         [-1.4501e-04,  1.4463e-04,  1.3559e-04,  ...,  3.0466e-05,\n",
            "          -7.0958e-05,  5.8986e-05],\n",
            "         [ 5.4730e-03, -5.4586e-03, -5.1176e-03,  ..., -1.1499e-03,\n",
            "           2.6781e-03, -2.2263e-03],\n",
            "         [ 3.4658e-03, -3.4567e-03, -3.2408e-03,  ..., -7.2816e-04,\n",
            "           1.6959e-03, -1.4098e-03]],\n",
            "\n",
            "        [[ 9.8983e-03, -1.2126e-02, -2.7616e-03,  ..., -1.0813e-03,\n",
            "           3.9271e-03, -9.2490e-03],\n",
            "         [-6.9800e-03,  8.5508e-03,  1.9474e-03,  ...,  7.6252e-04,\n",
            "          -2.7693e-03,  6.5221e-03],\n",
            "         [-6.6348e-03,  8.1279e-03,  1.8511e-03,  ...,  7.2481e-04,\n",
            "          -2.6323e-03,  6.1996e-03],\n",
            "         ...,\n",
            "         [-2.3574e-02,  2.8879e-02,  6.5769e-03,  ...,  2.5753e-03,\n",
            "          -9.3526e-03,  2.2027e-02],\n",
            "         [ 1.7121e-02, -2.0974e-02, -4.7766e-03,  ..., -1.8703e-03,\n",
            "           6.7925e-03, -1.5998e-02],\n",
            "         [-4.9578e-03,  6.0735e-03,  1.3832e-03,  ...,  5.4160e-04,\n",
            "          -1.9670e-03,  4.6325e-03]],\n",
            "\n",
            "        [[ 6.6312e-03, -7.5929e-03, -9.3401e-03,  ...,  2.8970e-03,\n",
            "           1.4938e-03, -3.8282e-03],\n",
            "         [-8.3877e-03,  9.6042e-03,  1.1814e-02,  ..., -3.6644e-03,\n",
            "          -1.8895e-03,  4.8423e-03],\n",
            "         [-3.5079e-03,  4.0166e-03,  4.9409e-03,  ..., -1.5325e-03,\n",
            "          -7.9020e-04,  2.0251e-03],\n",
            "         ...,\n",
            "         [ 4.9974e-03, -5.7222e-03, -7.0389e-03,  ...,  2.1832e-03,\n",
            "           1.1257e-03, -2.8850e-03],\n",
            "         [ 1.3228e-02, -1.5147e-02, -1.8632e-02,  ...,  5.7790e-03,\n",
            "           2.9799e-03, -7.6367e-03],\n",
            "         [ 3.7361e-03, -4.2780e-03, -5.2624e-03,  ...,  1.6322e-03,\n",
            "           8.4163e-04, -2.1569e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2016, 0.1524, 0.1642,  ..., 0.1338, 0.1833, 0.0847],\n",
            "        [0.2053, 0.1540, 0.1663,  ..., 0.1360, 0.1852, 0.0852],\n",
            "        [0.2050, 0.1546, 0.1672,  ..., 0.1392, 0.1878, 0.0843],\n",
            "        ...,\n",
            "        [0.2009, 0.1521, 0.1644,  ..., 0.1339, 0.1831, 0.0850],\n",
            "        [0.2018, 0.1537, 0.1665,  ..., 0.1347, 0.1836, 0.0840],\n",
            "        [0.2036, 0.1516, 0.1647,  ..., 0.1360, 0.1856, 0.0860]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "repr, std, cov 0.08351482450962067 0.49164506793022156 0.00019928290566895157\n",
            "train jepa world_state_ tensor([[[ 8.4335e-03, -8.4411e-03, -3.3908e-03,  ..., -6.3689e-04,\n",
            "           6.8546e-03, -8.1079e-03],\n",
            "         [-6.1188e-03,  4.9547e-03,  1.6134e-03,  ...,  3.3888e-04,\n",
            "          -3.6102e-03,  5.0643e-03],\n",
            "         [-5.9816e-03,  4.6313e-03,  1.4235e-03,  ...,  3.0891e-04,\n",
            "          -3.2817e-03,  4.8022e-03],\n",
            "         ...,\n",
            "         [-2.4370e-02,  2.3446e-02,  9.1139e-03,  ...,  1.7408e-03,\n",
            "          -1.8706e-02,  2.2768e-02],\n",
            "         [ 1.6351e-02, -1.6166e-02, -6.4298e-03,  ..., -1.2138e-03,\n",
            "           1.3057e-02, -1.5580e-02],\n",
            "         [-5.1646e-03,  5.0371e-03,  1.9808e-03,  ...,  3.7610e-04,\n",
            "          -4.0437e-03,  4.8728e-03]],\n",
            "\n",
            "        [[ 5.0761e-03, -1.3320e-02, -3.5557e-03,  ..., -2.8878e-03,\n",
            "           6.4123e-03, -1.0904e-02],\n",
            "         [-3.2120e-03,  8.0367e-03,  2.2535e-03,  ...,  1.7339e-03,\n",
            "          -3.6962e-03,  6.7701e-03],\n",
            "         [-3.0130e-03,  7.0146e-03,  2.1186e-03,  ...,  1.5016e-03,\n",
            "          -2.9842e-03,  6.1769e-03],\n",
            "         ...,\n",
            "         [-1.3513e-02,  3.7088e-02,  9.4509e-03,  ...,  8.0758e-03,\n",
            "          -1.8571e-02,  2.9568e-02],\n",
            "         [ 1.0940e-02, -2.8889e-02, -7.6617e-03,  ..., -6.2670e-03,\n",
            "           1.3986e-02, -2.3561e-02],\n",
            "         [-6.4181e-03,  1.7215e-02,  4.4923e-03,  ...,  3.7402e-03,\n",
            "          -8.4514e-03,  1.3911e-02]],\n",
            "\n",
            "        [[ 4.6297e-03, -9.3857e-03, -1.4487e-02,  ...,  7.3747e-03,\n",
            "           2.3401e-03, -7.2907e-04],\n",
            "         [-8.3856e-03,  1.7039e-02,  2.6195e-02,  ..., -1.3332e-02,\n",
            "          -4.4326e-03,  1.1712e-03],\n",
            "         [-4.3316e-03,  8.7813e-03,  1.3555e-02,  ..., -6.8999e-03,\n",
            "          -2.1891e-03,  6.8233e-04],\n",
            "         ...,\n",
            "         [ 5.9722e-03, -1.2094e-02, -1.8703e-02,  ...,  9.5216e-03,\n",
            "           2.9546e-03, -9.8980e-04],\n",
            "         [ 7.0164e-03, -1.4450e-02, -2.1693e-02,  ...,  1.1029e-02,\n",
            "           4.6692e-03, -2.4087e-04],\n",
            "         [-6.0434e-04,  1.1516e-03,  1.9768e-03,  ..., -1.0106e-03,\n",
            "           6.0343e-05,  3.7669e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 3.2541e-04, -4.9314e-04, -3.8392e-04,  ..., -8.5883e-05,\n",
            "           8.7029e-05, -3.9427e-05],\n",
            "         [-6.8616e-04,  1.1793e-04,  3.7402e-04,  ...,  8.5314e-05,\n",
            "          -5.7837e-04,  5.9139e-04],\n",
            "         [-1.0230e-03,  1.4970e-03,  1.1817e-03,  ...,  2.6445e-04,\n",
            "          -2.9642e-04,  1.5333e-04],\n",
            "         ...,\n",
            "         [-2.6813e-04,  3.0103e-04,  2.6659e-04,  ...,  5.9825e-05,\n",
            "          -1.1681e-04,  9.0538e-05],\n",
            "         [ 3.0615e-03, -2.3952e-03, -2.5517e-03,  ..., -5.7483e-04,\n",
            "           1.7801e-03, -1.6083e-03],\n",
            "         [ 1.0764e-03, -4.2133e-04, -6.9841e-04,  ..., -1.5839e-04,\n",
            "           8.0612e-04, -7.9749e-04]],\n",
            "\n",
            "        [[ 9.2209e-03, -1.0834e-02, -2.5325e-03,  ..., -9.2836e-04,\n",
            "           3.3693e-03, -8.3610e-03],\n",
            "         [-8.2992e-03,  1.1066e-02,  2.3935e-03,  ...,  1.0604e-03,\n",
            "          -3.8556e-03,  8.2514e-03],\n",
            "         [-6.2086e-03,  7.3152e-03,  1.7069e-03,  ...,  6.2856e-04,\n",
            "          -2.2814e-03,  5.6409e-03],\n",
            "         ...,\n",
            "         [-2.5792e-02,  3.3109e-02,  7.3270e-03,  ...,  3.0762e-03,\n",
            "          -1.1179e-02,  2.4935e-02],\n",
            "         [ 1.8267e-02, -2.3160e-02, -5.1643e-03,  ..., -2.1293e-03,\n",
            "           7.7367e-03, -1.7501e-02],\n",
            "         [-8.0330e-03,  1.1938e-02,  2.4231e-03,  ...,  1.2361e-03,\n",
            "          -4.4993e-03,  8.6638e-03]],\n",
            "\n",
            "        [[ 7.6967e-03, -8.6415e-03, -1.0903e-02,  ...,  3.4077e-03,\n",
            "           1.7886e-03, -4.6887e-03],\n",
            "         [-1.2456e-02,  1.3608e-02,  1.7782e-02,  ..., -5.6144e-03,\n",
            "          -3.0152e-03,  8.1274e-03],\n",
            "         [-5.9867e-03,  6.4559e-03,  8.5770e-03,  ..., -2.7206e-03,\n",
            "          -1.4761e-03,  4.0267e-03],\n",
            "         ...,\n",
            "         [ 9.4808e-03, -1.0134e-02, -1.3615e-02,  ...,  4.3322e-03,\n",
            "           2.3664e-03, -6.5053e-03],\n",
            "         [ 1.5944e-02, -1.7819e-02, -2.2616e-02,  ...,  7.0808e-03,\n",
            "           3.7314e-03, -9.8298e-03],\n",
            "         [ 1.6157e-03, -2.1914e-03, -2.1520e-03,  ...,  6.1583e-04,\n",
            "           2.5485e-04, -4.4464e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2196, 0.1613, 0.1817,  ..., 0.1509, 0.2049, 0.0807],\n",
            "        [0.2178, 0.1616, 0.1808,  ..., 0.1497, 0.2029, 0.0805],\n",
            "        [0.2201, 0.1632, 0.1828,  ..., 0.1537, 0.2065, 0.0810],\n",
            "        ...,\n",
            "        [0.2128, 0.1615, 0.1791,  ..., 0.1451, 0.1987, 0.0807],\n",
            "        [0.2147, 0.1628, 0.1810,  ..., 0.1467, 0.1999, 0.0794],\n",
            "        [0.2151, 0.1621, 0.1805,  ..., 0.1501, 0.2034, 0.0812]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 9.0485e-03, -9.0564e-03, -3.3344e-03,  ..., -6.4335e-04,\n",
            "           6.9772e-03, -8.5396e-03],\n",
            "         [-6.0997e-03,  5.1518e-03,  2.1118e-03,  ...,  4.0624e-04,\n",
            "          -4.2809e-03,  5.3808e-03],\n",
            "         [-6.0223e-03,  4.9533e-03,  2.0661e-03,  ...,  3.9725e-04,\n",
            "          -4.1676e-03,  5.2601e-03],\n",
            "         ...,\n",
            "         [-2.4593e-02,  2.3593e-02,  8.9169e-03,  ...,  1.7191e-03,\n",
            "          -1.8510e-02,  2.2807e-02],\n",
            "         [ 1.4771e-02, -1.4047e-02, -5.3380e-03,  ..., -1.0290e-03,\n",
            "           1.1063e-02, -1.3650e-02],\n",
            "         [-1.5466e-03,  6.0004e-04,  4.3475e-04,  ...,  8.2670e-05,\n",
            "          -7.7243e-04,  1.0860e-03]],\n",
            "\n",
            "        [[ 4.8641e-03, -1.2421e-02, -3.4187e-03,  ..., -2.7381e-03,\n",
            "           5.8787e-03, -1.0312e-02],\n",
            "         [-2.9873e-03,  6.9909e-03,  2.1168e-03,  ...,  1.6015e-03,\n",
            "          -3.0915e-03,  6.0908e-03],\n",
            "         [-2.9897e-03,  6.6540e-03,  2.1278e-03,  ...,  1.5598e-03,\n",
            "          -2.8152e-03,  5.9653e-03],\n",
            "         ...,\n",
            "         [-1.3572e-02,  3.7679e-02,  9.4567e-03,  ...,  8.0191e-03,\n",
            "          -1.8862e-02,  2.9923e-02],\n",
            "         [ 1.0902e-02, -2.8707e-02, -7.6390e-03,  ..., -6.2459e-03,\n",
            "           1.3882e-02, -2.3444e-02],\n",
            "         [-7.0187e-03,  1.9720e-02,  4.8843e-03,  ...,  4.1765e-03,\n",
            "          -9.9454e-03,  1.5564e-02]],\n",
            "\n",
            "        [[ 4.6397e-03, -9.2770e-03, -1.4469e-02,  ...,  7.2345e-03,\n",
            "           2.7161e-03, -1.1447e-03],\n",
            "         [-8.4983e-03,  1.7050e-02,  2.6477e-02,  ..., -1.3263e-02,\n",
            "          -5.0525e-03,  1.8919e-03],\n",
            "         [-4.4573e-03,  8.8945e-03,  1.3909e-02,  ..., -6.9465e-03,\n",
            "          -2.5854e-03,  1.1630e-03],\n",
            "         ...,\n",
            "         [ 5.8802e-03, -1.1750e-02, -1.8342e-02,  ...,  9.1673e-03,\n",
            "           3.4318e-03, -1.4787e-03],\n",
            "         [ 7.4593e-03, -1.5268e-02, -2.3105e-02,  ...,  1.1705e-02,\n",
            "           4.8393e-03, -5.9111e-04],\n",
            "         [-8.3812e-04,  1.5042e-03,  2.6908e-03,  ..., -1.2708e-03,\n",
            "          -2.6065e-04,  8.1480e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.2961e-03, -1.8831e-03, -1.4651e-03,  ..., -2.0895e-04,\n",
            "           3.4294e-04, -3.0683e-04],\n",
            "         [ 2.3013e-04, -1.3413e-03, -6.9186e-04,  ...,  7.0999e-05,\n",
            "          -4.3601e-04,  3.2150e-04],\n",
            "         [-1.4874e-03,  2.3050e-03,  1.7430e-03,  ...,  2.2429e-04,\n",
            "          -3.2239e-04,  2.9826e-04],\n",
            "         ...,\n",
            "         [-1.1221e-03,  1.4824e-03,  1.2050e-03,  ...,  1.9677e-04,\n",
            "          -3.6989e-04,  3.2086e-04],\n",
            "         [ 2.3780e-03, -1.2214e-03, -1.7304e-03,  ..., -6.2310e-04,\n",
            "           1.7313e-03, -1.3968e-03],\n",
            "         [ 1.4300e-03, -6.6954e-04, -1.0127e-03,  ..., -3.8167e-04,\n",
            "           1.0731e-03, -8.6421e-04]],\n",
            "\n",
            "        [[ 8.6474e-03, -9.8557e-03, -2.3798e-03,  ..., -6.1831e-04,\n",
            "           2.9525e-03, -7.6911e-03],\n",
            "         [-9.0521e-03,  1.2311e-02,  2.5797e-03,  ...,  1.5295e-03,\n",
            "          -4.3836e-03,  9.1027e-03],\n",
            "         [-6.7735e-03,  8.3797e-03,  1.8934e-03,  ...,  7.7615e-04,\n",
            "          -2.7404e-03,  6.3723e-03],\n",
            "         ...,\n",
            "         [-2.5359e-02,  3.2119e-02,  7.1218e-03,  ...,  3.2362e-03,\n",
            "          -1.0744e-02,  2.4251e-02],\n",
            "         [ 1.6861e-02, -2.0502e-02, -4.6973e-03,  ..., -1.7741e-03,\n",
            "           6.5901e-03, -1.5674e-02],\n",
            "         [-5.4282e-03,  6.9272e-03,  1.5268e-03,  ...,  7.1573e-04,\n",
            "          -2.3335e-03,  5.2184e-03]],\n",
            "\n",
            "        [[ 7.7438e-03, -8.6493e-03, -1.1192e-02,  ...,  3.5822e-03,\n",
            "           1.6963e-03, -4.8152e-03],\n",
            "         [-1.3312e-02,  1.4280e-02,  2.0009e-02,  ..., -6.6974e-03,\n",
            "          -2.7857e-03,  9.2108e-03],\n",
            "         [-6.8394e-03,  7.1800e-03,  1.0485e-02,  ..., -3.5844e-03,\n",
            "          -1.3965e-03,  4.9805e-03],\n",
            "         ...,\n",
            "         [ 9.0747e-03, -9.5936e-03, -1.3824e-02,  ...,  4.6944e-03,\n",
            "           1.8678e-03, -6.5020e-03],\n",
            "         [ 1.6017e-02, -1.7795e-02, -2.3273e-02,  ...,  7.4968e-03,\n",
            "           3.4875e-03, -1.0111e-02],\n",
            "         [ 1.0445e-03, -1.7223e-03, -7.8341e-04,  ..., -2.5568e-05,\n",
            "           3.5175e-04,  2.3083e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2191, 0.1614, 0.1810,  ..., 0.1501, 0.2034, 0.0816],\n",
            "        [0.2140, 0.1628, 0.1809,  ..., 0.1464, 0.1996, 0.0792],\n",
            "        [0.2150, 0.1621, 0.1804,  ..., 0.1500, 0.2033, 0.0812],\n",
            "        ...,\n",
            "        [0.2158, 0.1608, 0.1797,  ..., 0.1489, 0.2013, 0.0815],\n",
            "        [0.2138, 0.1615, 0.1800,  ..., 0.1472, 0.2003, 0.0806],\n",
            "        [0.2192, 0.1629, 0.1823,  ..., 0.1530, 0.2057, 0.0811]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 9.2378e-03, -9.8937e-03, -3.4615e-03,  ..., -6.9759e-04,\n",
            "           7.5989e-03, -9.1659e-03],\n",
            "         [-7.1187e-03,  6.0440e-03,  2.4890e-03,  ...,  4.6245e-04,\n",
            "          -4.9030e-03,  6.1831e-03],\n",
            "         [-4.2360e-03,  1.5015e-03,  1.2445e-03,  ...,  1.7559e-04,\n",
            "          -1.6545e-03,  2.5123e-03],\n",
            "         ...,\n",
            "         [-2.3111e-02,  2.1790e-02,  8.3256e-03,  ...,  1.6045e-03,\n",
            "          -1.7225e-02,  2.1282e-02],\n",
            "         [ 1.5553e-02, -1.5344e-02, -5.6797e-03,  ..., -1.1121e-03,\n",
            "           1.2002e-02, -1.4701e-02],\n",
            "         [-2.1654e-03,  6.1880e-04,  6.1939e-04,  ...,  8.2690e-05,\n",
            "          -7.5606e-04,  1.2014e-03]],\n",
            "\n",
            "        [[ 4.7970e-03, -1.2457e-02, -3.4143e-03,  ..., -2.7343e-03,\n",
            "           5.8919e-03, -1.0365e-02],\n",
            "         [-2.5711e-03,  6.1301e-03,  1.9497e-03,  ...,  1.4143e-03,\n",
            "          -2.5525e-03,  5.5987e-03],\n",
            "         [-3.0553e-03,  7.5675e-03,  2.2549e-03,  ...,  1.7072e-03,\n",
            "          -3.3466e-03,  6.6307e-03],\n",
            "         ...,\n",
            "         [-1.4309e-02,  3.9158e-02,  9.7471e-03,  ...,  8.3439e-03,\n",
            "          -1.9791e-02,  3.0761e-02],\n",
            "         [ 1.0646e-02, -2.7963e-02, -7.5084e-03,  ..., -6.0983e-03,\n",
            "           1.3427e-02, -2.2979e-02],\n",
            "         [-6.7491e-03,  1.8214e-02,  4.6531e-03,  ...,  3.9115e-03,\n",
            "          -9.0516e-03,  1.4528e-02]],\n",
            "\n",
            "        [[ 4.1558e-03, -8.5897e-03, -1.3386e-02,  ...,  6.6775e-03,\n",
            "           2.5201e-03, -7.3978e-04],\n",
            "         [-8.2551e-03,  1.7072e-02,  2.6747e-02,  ..., -1.3307e-02,\n",
            "          -4.9331e-03,  1.6456e-03],\n",
            "         [-4.2187e-03,  8.7296e-03,  1.3761e-02,  ..., -6.8251e-03,\n",
            "          -2.4789e-03,  9.4302e-04],\n",
            "         ...,\n",
            "         [ 5.5478e-03, -1.1477e-02, -1.8039e-02,  ...,  8.9600e-03,\n",
            "           3.2859e-03, -1.1772e-03],\n",
            "         [ 7.5024e-03, -1.5470e-02, -2.3514e-02,  ...,  1.1879e-02,\n",
            "           4.8488e-03, -6.1070e-04],\n",
            "         [-6.2311e-04,  1.3141e-03,  2.4652e-03,  ..., -1.1249e-03,\n",
            "          -1.6701e-04,  6.2140e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 7.9126e-04, -9.1069e-04, -8.1570e-04,  ..., -3.5733e-05,\n",
            "           2.1528e-06, -1.9166e-04],\n",
            "         [ 8.7410e-04, -1.2795e-03, -1.0717e-03,  ...,  2.5426e-04,\n",
            "          -8.6421e-04,  8.1322e-05],\n",
            "         [-1.5177e-03,  1.7651e-03,  1.5760e-03,  ...,  4.8934e-05,\n",
            "           5.3712e-05,  3.4807e-04],\n",
            "         ...,\n",
            "         [ 5.7945e-04, -6.5919e-04, -5.9253e-04,  ..., -3.4470e-05,\n",
            "           2.6071e-05, -1.4864e-04],\n",
            "         [ 1.3351e-03, -8.6753e-04, -9.5888e-04,  ..., -7.7897e-04,\n",
            "           2.1239e-03, -1.0404e-03],\n",
            "         [ 1.6792e-03, -1.4743e-03, -1.4451e-03,  ..., -5.6801e-04,\n",
            "           1.4567e-03, -8.9777e-04]],\n",
            "\n",
            "        [[ 7.6869e-03, -8.4577e-03, -2.1472e-03,  ..., -2.8084e-04,\n",
            "           2.2319e-03, -6.4498e-03],\n",
            "         [-9.7408e-03,  1.3130e-02,  2.7144e-03,  ...,  1.7619e-03,\n",
            "          -4.8857e-03,  1.0017e-02],\n",
            "         [-6.2084e-03,  7.4206e-03,  1.7326e-03,  ...,  5.7046e-04,\n",
            "          -2.3054e-03,  5.6598e-03],\n",
            "         ...,\n",
            "         [-2.5824e-02,  3.2612e-02,  7.2021e-03,  ...,  3.3900e-03,\n",
            "          -1.1078e-02,  2.4876e-02],\n",
            "         [ 1.7719e-02, -2.1966e-02, -4.9428e-03,  ..., -2.0869e-03,\n",
            "           7.2512e-03, -1.6755e-02],\n",
            "         [-4.5221e-03,  5.3508e-03,  1.2621e-03,  ...,  3.8389e-04,\n",
            "          -1.6330e-03,  4.0810e-03]],\n",
            "\n",
            "        [[ 6.7935e-03, -7.7713e-03, -9.5937e-03,  ...,  2.9853e-03,\n",
            "           1.5221e-03, -3.9505e-03],\n",
            "         [-1.3334e-02,  1.5040e-02,  1.9540e-02,  ..., -6.3567e-03,\n",
            "          -2.7530e-03,  8.5682e-03],\n",
            "         [-7.2502e-03,  8.1294e-03,  1.0786e-02,  ..., -3.5694e-03,\n",
            "          -1.4435e-03,  4.8440e-03],\n",
            "         ...,\n",
            "         [ 8.9073e-03, -1.0019e-02, -1.3146e-02,  ...,  4.3113e-03,\n",
            "           1.8083e-03, -5.8301e-03],\n",
            "         [ 1.6072e-02, -1.8272e-02, -2.3073e-02,  ...,  7.3267e-03,\n",
            "           3.4763e-03, -9.7785e-03],\n",
            "         [ 9.4842e-04, -1.2144e-03, -9.0829e-04,  ...,  1.1491e-04,\n",
            "           3.5496e-04, -5.7095e-05]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2165, 0.1630, 0.1813,  ..., 0.1474, 0.2006, 0.0799],\n",
            "        [0.2184, 0.1613, 0.1805,  ..., 0.1498, 0.2027, 0.0814],\n",
            "        [0.2191, 0.1629, 0.1823,  ..., 0.1529, 0.2056, 0.0811],\n",
            "        ...,\n",
            "        [0.2147, 0.1628, 0.1809,  ..., 0.1467, 0.1998, 0.0793],\n",
            "        [0.2170, 0.1632, 0.1837,  ..., 0.1502, 0.2032, 0.0785],\n",
            "        [0.2132, 0.1620, 0.1800,  ..., 0.1487, 0.2030, 0.0811]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 9.7039e-03, -1.0912e-02, -3.4333e-03,  ..., -7.2008e-04,\n",
            "           8.1727e-03, -9.9045e-03],\n",
            "         [-5.2737e-03,  2.8993e-03,  1.9740e-03,  ...,  3.1165e-04,\n",
            "          -2.8251e-03,  3.7166e-03],\n",
            "         [-4.7375e-03,  1.7653e-03,  1.8033e-03,  ...,  2.5789e-04,\n",
            "          -2.0903e-03,  2.8774e-03],\n",
            "         ...,\n",
            "         [-2.5319e-02,  2.5375e-02,  9.0686e-03,  ...,  1.7974e-03,\n",
            "          -1.9673e-02,  2.4141e-02],\n",
            "         [ 1.5448e-02, -1.5322e-02, -5.5388e-03,  ..., -1.0925e-03,\n",
            "           1.1918e-02, -1.4641e-02],\n",
            "         [-4.1768e-03,  3.5714e-03,  1.5179e-03,  ...,  2.8035e-04,\n",
            "          -2.9176e-03,  3.6446e-03]],\n",
            "\n",
            "        [[ 4.7207e-03, -1.2211e-02, -3.3278e-03,  ..., -2.6682e-03,\n",
            "           5.6950e-03, -1.0217e-02],\n",
            "         [-2.3760e-03,  5.5037e-03,  1.7233e-03,  ...,  1.2431e-03,\n",
            "          -2.0439e-03,  5.2230e-03],\n",
            "         [-2.8962e-03,  7.0562e-03,  2.0745e-03,  ...,  1.5693e-03,\n",
            "          -2.9361e-03,  6.3231e-03],\n",
            "         ...,\n",
            "         [-1.4413e-02,  3.9472e-02,  9.9955e-03,  ...,  8.4865e-03,\n",
            "          -2.0191e-02,  3.0921e-02],\n",
            "         [ 1.0384e-02, -2.7106e-02, -7.3014e-03,  ..., -5.9071e-03,\n",
            "           1.2842e-02, -2.2443e-02],\n",
            "         [-6.3946e-03,  1.7045e-02,  4.4700e-03,  ...,  3.6926e-03,\n",
            "          -8.3595e-03,  1.3777e-02]],\n",
            "\n",
            "        [[ 4.0059e-03, -8.4001e-03, -1.3154e-02,  ...,  6.7112e-03,\n",
            "           2.5564e-03, -7.1535e-04],\n",
            "         [-7.9568e-03,  1.6739e-02,  2.6402e-02,  ..., -1.3479e-02,\n",
            "          -5.0378e-03,  1.6286e-03],\n",
            "         [-4.1984e-03,  8.8867e-03,  1.4211e-02,  ..., -7.2632e-03,\n",
            "          -2.6177e-03,  1.0705e-03],\n",
            "         ...,\n",
            "         [ 5.5319e-03, -1.1692e-02, -1.8636e-02,  ...,  9.5220e-03,\n",
            "           3.4621e-03, -1.3430e-03],\n",
            "         [ 7.3791e-03, -1.5279e-02, -2.3231e-02,  ...,  1.1824e-02,\n",
            "           4.8532e-03, -5.6570e-04],\n",
            "         [-7.8628e-04,  1.8390e-03,  3.5579e-03,  ..., -1.8435e-03,\n",
            "          -3.6070e-04,  8.7587e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.4562e-03, -2.9068e-03, -2.4749e-03,  ...,  1.9201e-05,\n",
            "           7.0795e-05, -3.8153e-04],\n",
            "         [ 1.3840e-03, -2.0697e-03, -1.5630e-03,  ...,  5.1648e-04,\n",
            "          -1.0287e-03,  3.6848e-04],\n",
            "         [-5.6852e-04,  7.8181e-04,  6.1536e-04,  ..., -1.3210e-04,\n",
            "           2.5338e-04, -5.8985e-05],\n",
            "         ...,\n",
            "         [ 5.4158e-06, -3.0416e-05, -1.4819e-05,  ...,  2.8158e-05,\n",
            "          -5.9259e-05,  3.1600e-05],\n",
            "         [ 2.0136e-03, -1.4331e-03, -1.6586e-03,  ..., -1.0968e-03,\n",
            "           2.4091e-03, -1.5965e-03],\n",
            "         [-5.0628e-05,  6.3525e-04,  2.7537e-04,  ..., -6.7420e-04,\n",
            "           1.4225e-03, -7.6961e-04]],\n",
            "\n",
            "        [[ 7.1612e-03, -7.4062e-03, -2.1882e-03,  ..., -2.7965e-04,\n",
            "           2.1449e-03, -6.0971e-03],\n",
            "         [-1.0817e-02,  1.5166e-02,  2.7510e-03,  ...,  1.8863e-03,\n",
            "          -5.2675e-03,  1.0940e-02],\n",
            "         [-4.6647e-03,  4.7307e-03,  1.4384e-03,  ...,  1.4776e-04,\n",
            "          -1.3495e-03,  3.9308e-03],\n",
            "         ...,\n",
            "         [-2.4965e-02,  3.1278e-02,  6.8683e-03,  ...,  2.9828e-03,\n",
            "          -1.0259e-02,  2.3629e-02],\n",
            "         [ 1.8111e-02, -2.2681e-02, -4.9840e-03,  ..., -2.1604e-03,\n",
            "           7.4374e-03, -1.7138e-02],\n",
            "         [-1.5959e-03,  2.7655e-04,  6.7900e-04,  ..., -4.4307e-04,\n",
            "           2.2208e-04,  7.6115e-04]],\n",
            "\n",
            "        [[ 7.3240e-03, -8.3563e-03, -1.0352e-02,  ...,  3.2683e-03,\n",
            "           1.5818e-03, -4.3863e-03],\n",
            "         [-1.4097e-02,  1.5895e-02,  2.0149e-02,  ..., -6.7246e-03,\n",
            "          -2.6147e-03,  9.4414e-03],\n",
            "         [-7.4447e-03,  8.3544e-03,  1.0688e-02,  ..., -3.6427e-03,\n",
            "          -1.2903e-03,  5.1965e-03],\n",
            "         ...,\n",
            "         [ 9.4166e-03, -1.0591e-02, -1.3490e-02,  ...,  4.5520e-03,\n",
            "           1.6871e-03, -6.4450e-03],\n",
            "         [ 1.5568e-02, -1.7725e-02, -2.2049e-02,  ...,  7.0335e-03,\n",
            "           3.2771e-03, -9.5219e-03],\n",
            "         [ 6.3278e-04, -8.5862e-04, -7.3194e-04,  ..., -3.1251e-05,\n",
            "           4.4740e-04,  3.4303e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2138, 0.1618, 0.1792,  ..., 0.1453, 0.1988, 0.0807],\n",
            "        [0.2127, 0.1630, 0.1810,  ..., 0.1460, 0.1995, 0.0789],\n",
            "        [0.2155, 0.1622, 0.1806,  ..., 0.1504, 0.2035, 0.0812],\n",
            "        ...,\n",
            "        [0.2151, 0.1617, 0.1801,  ..., 0.1478, 0.2006, 0.0806],\n",
            "        [0.2144, 0.1628, 0.1809,  ..., 0.1465, 0.1997, 0.0793],\n",
            "        [0.2177, 0.1626, 0.1816,  ..., 0.1519, 0.2045, 0.0812]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.0387e-02, -1.1887e-02, -3.7872e-03,  ..., -8.6529e-04,\n",
            "           8.4765e-03, -1.0875e-02],\n",
            "         [-5.6185e-03,  3.5697e-03,  1.9869e-03,  ...,  2.7099e-04,\n",
            "          -3.5823e-03,  4.0922e-03],\n",
            "         [-3.7340e-03,  3.3861e-04,  1.2765e-03,  ...,  3.9980e-05,\n",
            "          -1.6678e-03,  1.4465e-03],\n",
            "         ...,\n",
            "         [-2.5122e-02,  2.4973e-02,  9.0784e-03,  ...,  1.8325e-03,\n",
            "          -1.9177e-02,  2.3938e-02],\n",
            "         [ 1.4860e-02, -1.4346e-02, -5.3608e-03,  ..., -1.0546e-03,\n",
            "           1.1194e-02, -1.3893e-02],\n",
            "         [-4.1257e-03,  3.5125e-03,  1.4782e-03,  ...,  2.6039e-04,\n",
            "          -2.9430e-03,  3.5628e-03]],\n",
            "\n",
            "        [[ 4.7140e-03, -1.2083e-02, -3.3200e-03,  ..., -2.6312e-03,\n",
            "           5.7185e-03, -1.0225e-02],\n",
            "         [-2.4961e-03,  5.6430e-03,  1.7934e-03,  ...,  1.2463e-03,\n",
            "          -2.3843e-03,  5.5225e-03],\n",
            "         [-2.7958e-03,  6.4853e-03,  2.0010e-03,  ...,  1.4281e-03,\n",
            "          -2.8110e-03,  6.1620e-03],\n",
            "         ...,\n",
            "         [-1.4507e-02,  4.0260e-02,  1.0073e-02,  ...,  8.6958e-03,\n",
            "          -2.0220e-02,  3.1023e-02],\n",
            "         [ 1.0182e-02, -2.6258e-02, -7.1638e-03,  ..., -5.7143e-03,\n",
            "           1.2487e-02, -2.2062e-02],\n",
            "         [-6.1730e-03,  1.6306e-02,  4.3249e-03,  ...,  3.5397e-03,\n",
            "          -7.9006e-03,  1.3319e-02]],\n",
            "\n",
            "        [[ 4.1257e-03, -8.8430e-03, -1.4031e-02,  ...,  7.0391e-03,\n",
            "           2.7240e-03, -7.6255e-04],\n",
            "         [-7.8108e-03,  1.6713e-02,  2.6457e-02,  ..., -1.3280e-02,\n",
            "          -5.1591e-03,  1.3913e-03],\n",
            "         [-3.8608e-03,  8.2758e-03,  1.3132e-02,  ..., -6.5879e-03,\n",
            "          -2.5491e-03,  7.1447e-04],\n",
            "         ...,\n",
            "         [ 5.5855e-03, -1.2113e-02, -1.9517e-02,  ...,  9.7557e-03,\n",
            "           3.6784e-03, -1.2861e-03],\n",
            "         [ 7.4177e-03, -1.5454e-02, -2.3584e-02,  ...,  1.1943e-02,\n",
            "           4.9275e-03, -5.6981e-04],\n",
            "         [-6.7933e-04,  1.7837e-03,  3.5188e-03,  ..., -1.6836e-03,\n",
            "          -4.2661e-04,  7.1472e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.7255e-03, -2.0753e-03, -1.7237e-03,  ...,  1.3159e-04,\n",
            "          -5.4553e-05, -3.1404e-04],\n",
            "         [ 1.7610e-03, -2.5506e-03, -1.8939e-03,  ...,  7.3746e-04,\n",
            "          -1.1530e-03,  1.5292e-04],\n",
            "         [-3.2935e-04,  5.1332e-04,  3.6550e-04,  ..., -1.8854e-04,\n",
            "           3.0772e-04, -6.8336e-05],\n",
            "         ...,\n",
            "         [-4.9320e-04,  5.5305e-04,  4.8021e-04,  ...,  1.8349e-05,\n",
            "          -8.6213e-05,  1.3369e-04],\n",
            "         [ 2.4686e-03, -1.9345e-03, -2.1441e-03,  ..., -1.2544e-03,\n",
            "           2.5465e-03, -1.5817e-03],\n",
            "         [ 1.2595e-03, -8.6876e-04, -1.0572e-03,  ..., -8.0494e-04,\n",
            "           1.5993e-03, -9.3646e-04]],\n",
            "\n",
            "        [[ 7.2420e-03, -7.7757e-03, -2.3205e-03,  ..., -5.2502e-04,\n",
            "           1.9104e-03, -6.0534e-03],\n",
            "         [-1.0556e-02,  1.4407e-02,  2.5411e-03,  ...,  1.5114e-03,\n",
            "          -5.4840e-03,  1.0824e-02],\n",
            "         [-4.6820e-03,  4.9298e-03,  1.5268e-03,  ...,  3.1583e-04,\n",
            "          -1.1497e-03,  3.8502e-03],\n",
            "         ...,\n",
            "         [-2.5212e-02,  3.1562e-02,  6.8490e-03,  ...,  2.9184e-03,\n",
            "          -1.0597e-02,  2.3998e-02],\n",
            "         [ 1.8694e-02, -2.3550e-02, -5.0378e-03,  ..., -2.1997e-03,\n",
            "           7.9867e-03, -1.7890e-02],\n",
            "         [-4.3240e-03,  5.0356e-03,  1.2779e-03,  ...,  4.0888e-04,\n",
            "          -1.4858e-03,  3.8701e-03]],\n",
            "\n",
            "        [[ 7.1582e-03, -8.2296e-03, -1.0151e-02,  ...,  3.2100e-03,\n",
            "           1.5320e-03, -4.3237e-03],\n",
            "         [-1.2775e-02,  1.4904e-02,  1.8568e-02,  ..., -6.2696e-03,\n",
            "          -2.2077e-03,  8.9666e-03],\n",
            "         [-7.3973e-03,  8.7151e-03,  1.0928e-02,  ..., -3.8420e-03,\n",
            "          -1.0723e-03,  5.6813e-03],\n",
            "         ...,\n",
            "         [ 9.9273e-03, -1.1678e-02, -1.4627e-02,  ...,  5.1106e-03,\n",
            "           1.4833e-03, -7.5193e-03],\n",
            "         [ 1.5660e-02, -1.8084e-02, -2.2375e-02,  ...,  7.2228e-03,\n",
            "           3.1562e-03, -9.9223e-03],\n",
            "         [ 4.0650e-04, -2.5572e-04, -1.3719e-04,  ..., -3.4493e-04,\n",
            "           6.0011e-04,  9.7306e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2123, 0.1615, 0.1791,  ..., 0.1450, 0.1986, 0.0807],\n",
            "        [0.2126, 0.1630, 0.1810,  ..., 0.1459, 0.1995, 0.0789],\n",
            "        [0.2130, 0.1620, 0.1800,  ..., 0.1487, 0.2030, 0.0811],\n",
            "        ...,\n",
            "        [0.2196, 0.1646, 0.1826,  ..., 0.1481, 0.2014, 0.0817],\n",
            "        [0.2162, 0.1629, 0.1812,  ..., 0.1472, 0.2005, 0.0798],\n",
            "        [0.2138, 0.1603, 0.1799,  ..., 0.1477, 0.2007, 0.0820]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.1916e-02, -1.4519e-02, -4.0882e-03,  ..., -1.0462e-03,\n",
            "           1.0144e-02, -1.3531e-02],\n",
            "         [-4.7060e-03,  1.9668e-03,  1.8500e-03,  ...,  1.6193e-04,\n",
            "          -2.5586e-03,  2.4083e-03],\n",
            "         [-4.4901e-03,  1.3515e-03,  1.7980e-03,  ...,  1.1948e-04,\n",
            "          -2.2395e-03,  1.8887e-03],\n",
            "         ...,\n",
            "         [-2.4867e-02,  2.4645e-02,  8.8845e-03,  ...,  1.8062e-03,\n",
            "          -1.8996e-02,  2.3831e-02],\n",
            "         [ 1.5497e-02, -1.5404e-02, -5.5338e-03,  ..., -1.1286e-03,\n",
            "           1.1856e-02, -1.4887e-02],\n",
            "         [-5.5499e-03,  5.8041e-03,  1.9639e-03,  ...,  4.2336e-04,\n",
            "          -4.3564e-03,  5.5553e-03]],\n",
            "\n",
            "        [[ 4.9592e-03, -1.3154e-02, -3.4929e-03,  ..., -2.8694e-03,\n",
            "           6.3085e-03, -1.0860e-02],\n",
            "         [-6.7151e-04,  1.9671e-03,  7.2962e-04,  ...,  6.1828e-04,\n",
            "           5.3779e-05,  3.5543e-03],\n",
            "         [-2.7195e-03,  7.2785e-03,  2.0057e-03,  ...,  1.6543e-03,\n",
            "          -3.1400e-03,  6.6882e-03],\n",
            "         ...,\n",
            "         [-1.5861e-02,  4.1801e-02,  1.0800e-02,  ...,  8.8452e-03,\n",
            "          -2.1489e-02,  3.1720e-02],\n",
            "         [ 9.5568e-03, -2.5413e-02, -6.8207e-03,  ..., -5.6098e-03,\n",
            "           1.1840e-02, -2.1654e-02],\n",
            "         [-5.0536e-03,  1.3464e-02,  3.6417e-03,  ...,  2.9977e-03,\n",
            "          -6.1375e-03,  1.1734e-02]],\n",
            "\n",
            "        [[ 4.0979e-03, -8.5473e-03, -1.3808e-02,  ...,  6.7824e-03,\n",
            "           2.6937e-03, -7.2202e-04],\n",
            "         [-7.6846e-03,  1.6011e-02,  2.5707e-02,  ..., -1.2663e-02,\n",
            "          -5.0571e-03,  1.2579e-03],\n",
            "         [-3.9044e-03,  8.1518e-03,  1.3242e-02,  ..., -6.4878e-03,\n",
            "          -2.5638e-03,  7.3164e-04],\n",
            "         ...,\n",
            "         [ 5.6802e-03, -1.1908e-02, -1.9781e-02,  ...,  9.5931e-03,\n",
            "           3.7141e-03, -1.3286e-03],\n",
            "         [ 7.6991e-03, -1.5953e-02, -2.4822e-02,  ...,  1.2407e-02,\n",
            "           5.0953e-03, -7.8320e-04],\n",
            "         [-1.2370e-03,  2.7496e-03,  5.9628e-03,  ..., -2.5846e-03,\n",
            "          -7.5797e-04,  1.1359e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.6449e-03, -1.7726e-03, -1.4410e-03,  ...,  4.8437e-06,\n",
            "          -9.1635e-06, -3.2122e-04],\n",
            "         [ 2.0887e-03, -2.4139e-03, -1.7100e-03,  ...,  4.3855e-04,\n",
            "          -1.0161e-03,  2.1377e-05],\n",
            "         [-5.1241e-04,  5.9531e-04,  4.1723e-04,  ..., -1.1586e-04,\n",
            "           2.6850e-04, -1.3455e-05],\n",
            "         ...,\n",
            "         [-9.2907e-04,  9.8009e-04,  8.2945e-04,  ...,  5.3349e-05,\n",
            "          -1.2511e-04,  2.3711e-04],\n",
            "         [ 1.5353e-03, -1.2627e-03, -1.6329e-03,  ..., -1.0349e-03,\n",
            "           2.4062e-03, -1.3317e-03],\n",
            "         [ 1.2994e-03, -1.1482e-03, -1.3236e-03,  ..., -6.6493e-04,\n",
            "           1.5463e-03, -9.1764e-04]],\n",
            "\n",
            "        [[ 7.0286e-03, -7.3407e-03, -2.2086e-03,  ..., -5.2996e-04,\n",
            "           1.8133e-03, -6.3556e-03],\n",
            "         [-1.0402e-02,  1.4257e-02,  2.6068e-03,  ...,  1.4200e-03,\n",
            "          -5.2900e-03,  9.9726e-03],\n",
            "         [-3.9122e-03,  3.5880e-03,  1.3265e-03,  ...,  2.0171e-04,\n",
            "          -6.2691e-04,  3.4545e-03],\n",
            "         ...,\n",
            "         [-2.6273e-02,  3.3380e-02,  7.0970e-03,  ...,  3.0939e-03,\n",
            "          -1.1341e-02,  2.4749e-02],\n",
            "         [ 1.7843e-02, -2.2178e-02, -4.9157e-03,  ..., -2.0091e-03,\n",
            "           7.3245e-03, -1.6726e-02],\n",
            "         [-3.8049e-03,  4.1512e-03,  1.1611e-03,  ...,  3.2011e-04,\n",
            "          -1.1178e-03,  3.4702e-03]],\n",
            "\n",
            "        [[ 7.4193e-03, -8.5132e-03, -1.0705e-02,  ...,  3.4071e-03,\n",
            "           1.6454e-03, -4.5697e-03],\n",
            "         [-1.2409e-02,  1.4300e-02,  1.8776e-02,  ..., -6.2670e-03,\n",
            "          -2.6632e-03,  8.6254e-03],\n",
            "         [-7.2715e-03,  8.4112e-03,  1.1456e-02,  ..., -3.9682e-03,\n",
            "          -1.5143e-03,  5.5656e-03],\n",
            "         ...,\n",
            "         [ 1.0300e-02, -1.1914e-02, -1.6220e-02,  ...,  5.6153e-03,\n",
            "           2.1460e-03, -7.8737e-03],\n",
            "         [ 1.5539e-02, -1.7845e-02, -2.2632e-02,  ...,  7.2744e-03,\n",
            "           3.4244e-03, -9.8103e-03],\n",
            "         [ 9.3988e-04, -1.0130e-03, -4.2153e-04,  ..., -1.7748e-04,\n",
            "           3.0364e-04,  4.7356e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2144, 0.1620, 0.1794,  ..., 0.1455, 0.1989, 0.0807],\n",
            "        [0.2155, 0.1623, 0.1799,  ..., 0.1459, 0.1992, 0.0808],\n",
            "        [0.2154, 0.1622, 0.1806,  ..., 0.1503, 0.2035, 0.0812],\n",
            "        ...,\n",
            "        [0.2167, 0.1628, 0.1805,  ..., 0.1465, 0.1996, 0.0808],\n",
            "        [0.2139, 0.1615, 0.1800,  ..., 0.1473, 0.2004, 0.0806],\n",
            "        [0.2190, 0.1629, 0.1822,  ..., 0.1528, 0.2055, 0.0811]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.0868e-02, -1.3861e-02, -3.4094e-03,  ..., -5.2470e-04,\n",
            "           9.0578e-03, -1.2953e-02],\n",
            "         [-5.3675e-03,  2.2179e-03,  2.3326e-03,  ...,  5.5390e-04,\n",
            "          -3.2489e-03,  2.6246e-03],\n",
            "         [-5.1374e-03,  1.4072e-03,  2.3329e-03,  ...,  5.7573e-04,\n",
            "          -2.9202e-03,  1.9286e-03],\n",
            "         ...,\n",
            "         [-2.4839e-02,  2.5078e-02,  8.7173e-03,  ...,  1.6196e-03,\n",
            "          -1.8954e-02,  2.4222e-02],\n",
            "         [ 1.5561e-02, -1.5850e-02, -5.4420e-03,  ..., -1.0059e-03,\n",
            "           1.1912e-02, -1.5288e-02],\n",
            "         [-5.6761e-03,  6.2774e-03,  1.9154e-03,  ...,  3.3529e-04,\n",
            "          -4.4761e-03,  5.9809e-03]],\n",
            "\n",
            "        [[ 5.1845e-03, -1.3731e-02, -3.6306e-03,  ..., -2.9800e-03,\n",
            "           6.6642e-03, -1.1182e-02],\n",
            "         [-5.1817e-04, -1.1789e-03,  4.6353e-04,  ..., -6.6650e-05,\n",
            "           2.2298e-03,  1.8275e-03],\n",
            "         [-2.4570e-03,  5.3461e-03,  1.7664e-03,  ...,  1.2464e-03,\n",
            "          -1.8404e-03,  5.6224e-03],\n",
            "         ...,\n",
            "         [-1.5444e-02,  4.3438e-02,  1.0715e-02,  ...,  9.2394e-03,\n",
            "          -2.2731e-02,  3.2604e-02],\n",
            "         [ 1.0208e-02, -2.6376e-02, -7.1747e-03,  ..., -5.7735e-03,\n",
            "           1.2373e-02, -2.2201e-02],\n",
            "         [-6.0689e-03,  1.5947e-02,  4.2549e-03,  ...,  3.4705e-03,\n",
            "          -7.6584e-03,  1.3125e-02]],\n",
            "\n",
            "        [[ 4.1590e-03, -8.7340e-03, -1.3827e-02,  ...,  6.7023e-03,\n",
            "           2.6013e-03, -6.1644e-04],\n",
            "         [-7.5028e-03,  1.5682e-02,  2.4519e-02,  ..., -1.2019e-02,\n",
            "          -4.7665e-03,  9.0367e-04],\n",
            "         [-4.1505e-03,  8.7584e-03,  1.4042e-02,  ..., -6.7299e-03,\n",
            "          -2.5539e-03,  7.3429e-04],\n",
            "         ...,\n",
            "         [ 5.5602e-03, -1.1731e-02, -1.8799e-02,  ...,  9.0136e-03,\n",
            "           3.4234e-03, -9.7778e-04],\n",
            "         [ 7.1771e-03, -1.4815e-02, -2.2383e-02,  ...,  1.1315e-02,\n",
            "           4.7455e-03, -3.3903e-04],\n",
            "         [-1.3649e-03,  3.1298e-03,  6.0546e-03,  ..., -2.4574e-03,\n",
            "          -5.9077e-04,  9.4563e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.4170e-03, -1.5944e-03, -1.4800e-03,  ..., -3.1411e-05,\n",
            "          -9.0344e-05, -3.6836e-04],\n",
            "         [ 2.2919e-03, -2.8214e-03, -2.6015e-03,  ...,  3.0577e-04,\n",
            "          -1.1956e-03, -3.1724e-04],\n",
            "         [ 3.7123e-04, -3.3297e-04, -3.1521e-04,  ..., -1.3282e-04,\n",
            "           3.4300e-04, -1.9384e-04],\n",
            "         ...,\n",
            "         [-4.8254e-04,  5.1704e-04,  4.8182e-04,  ...,  4.8790e-05,\n",
            "          -8.1347e-05,  1.5520e-04],\n",
            "         [ 1.9134e-03, -1.5312e-03, -1.4663e-03,  ..., -9.5662e-04,\n",
            "           2.5686e-03, -1.2116e-03],\n",
            "         [ 2.9208e-04,  4.4990e-05,  1.4752e-05,  ..., -5.5586e-04,\n",
            "           1.5983e-03, -5.0514e-04]],\n",
            "\n",
            "        [[ 6.4912e-03, -6.3121e-03, -1.5015e-03,  ..., -6.4125e-04,\n",
            "           1.0474e-03, -5.5669e-03],\n",
            "         [-1.2447e-02,  1.7880e-02,  3.9693e-03,  ...,  1.4687e-03,\n",
            "          -7.3901e-03,  1.2431e-02],\n",
            "         [-4.9114e-03,  5.1871e-03,  1.2137e-03,  ...,  5.0220e-04,\n",
            "          -1.1757e-03,  4.3370e-03],\n",
            "         ...,\n",
            "         [-2.5742e-02,  3.2579e-02,  7.3789e-03,  ...,  2.8554e-03,\n",
            "          -1.1186e-02,  2.4371e-02],\n",
            "         [ 1.8012e-02, -2.2494e-02, -5.1060e-03,  ..., -1.9854e-03,\n",
            "           7.5455e-03, -1.6960e-02],\n",
            "         [-4.1172e-03,  4.6391e-03,  1.0723e-03,  ...,  4.3303e-04,\n",
            "          -1.2565e-03,  3.7241e-03]],\n",
            "\n",
            "        [[ 7.0327e-03, -8.0853e-03, -9.9784e-03,  ...,  3.1390e-03,\n",
            "           1.5498e-03, -4.1592e-03],\n",
            "         [-1.2560e-02,  1.4721e-02,  1.8446e-02,  ..., -6.1790e-03,\n",
            "          -2.4718e-03,  8.2817e-03],\n",
            "         [-7.2954e-03,  8.6615e-03,  1.0961e-02,  ..., -3.8153e-03,\n",
            "          -1.3188e-03,  5.1473e-03],\n",
            "         ...,\n",
            "         [ 9.8094e-03, -1.1623e-02, -1.4688e-02,  ...,  5.0834e-03,\n",
            "           1.7974e-03, -6.8518e-03],\n",
            "         [ 1.5173e-02, -1.7531e-02, -2.1723e-02,  ...,  6.9510e-03,\n",
            "           3.2513e-03, -9.2397e-03],\n",
            "         [-1.9149e-04,  5.3860e-04,  9.8068e-04,  ..., -7.3497e-04,\n",
            "           2.9344e-04,  1.0808e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2161, 0.1625, 0.1802,  ..., 0.1462, 0.1994, 0.0808],\n",
            "        [0.2150, 0.1644, 0.1837,  ..., 0.1481, 0.2016, 0.0779],\n",
            "        [0.2126, 0.1621, 0.1800,  ..., 0.1484, 0.2029, 0.0811],\n",
            "        ...,\n",
            "        [0.2161, 0.1625, 0.1802,  ..., 0.1462, 0.1994, 0.0808],\n",
            "        [0.2122, 0.1631, 0.1811,  ..., 0.1458, 0.1995, 0.0788],\n",
            "        [0.2156, 0.1622, 0.1807,  ..., 0.1505, 0.2036, 0.0812]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.1157e-02, -1.4349e-02, -3.4739e-03,  ..., -3.3178e-04,\n",
            "           9.1198e-03, -1.3663e-02],\n",
            "         [-5.9884e-03,  3.4384e-03,  2.4728e-03,  ...,  6.7169e-04,\n",
            "          -4.0012e-03,  3.5589e-03],\n",
            "         [-4.9952e-03,  1.1946e-03,  2.3014e-03,  ...,  7.5409e-04,\n",
            "          -2.9868e-03,  1.4868e-03],\n",
            "         ...,\n",
            "         [-2.5688e-02,  2.6657e-02,  8.9082e-03,  ...,  1.5025e-03,\n",
            "          -1.9660e-02,  2.5809e-02],\n",
            "         [ 1.5582e-02, -1.5870e-02, -5.4464e-03,  ..., -9.4613e-04,\n",
            "           1.1863e-02, -1.5390e-02],\n",
            "         [-6.2115e-03,  7.2748e-03,  2.0358e-03,  ...,  2.6733e-04,\n",
            "          -4.9277e-03,  6.9749e-03]],\n",
            "\n",
            "        [[ 4.7347e-03, -1.2139e-02, -3.2703e-03,  ..., -2.6194e-03,\n",
            "           5.6572e-03, -1.0270e-02],\n",
            "         [-6.8271e-04, -8.1295e-04,  1.8596e-04,  ..., -2.7230e-04,\n",
            "           1.9313e-03,  1.8582e-03],\n",
            "         [-2.5036e-03,  5.4123e-03,  1.6171e-03,  ...,  1.1299e-03,\n",
            "          -1.9128e-03,  5.5788e-03],\n",
            "         ...,\n",
            "         [-1.4851e-02,  4.1556e-02,  1.0646e-02,  ...,  9.0987e-03,\n",
            "          -2.1474e-02,  3.1702e-02],\n",
            "         [ 9.9373e-03, -2.5361e-02, -6.8508e-03,  ..., -5.4681e-03,\n",
            "           1.1748e-02, -2.1573e-02],\n",
            "         [-6.7535e-03,  1.8358e-02,  4.7808e-03,  ...,  4.0007e-03,\n",
            "          -9.1869e-03,  1.4496e-02]],\n",
            "\n",
            "        [[ 3.9878e-03, -8.2560e-03, -1.2840e-02,  ...,  6.3684e-03,\n",
            "           2.5972e-03, -5.5194e-04],\n",
            "         [-7.8578e-03,  1.6281e-02,  2.5449e-02,  ..., -1.2578e-02,\n",
            "          -5.1011e-03,  1.2079e-03],\n",
            "         [-4.0362e-03,  8.3731e-03,  1.3195e-02,  ..., -6.4846e-03,\n",
            "          -2.6063e-03,  7.2008e-04],\n",
            "         ...,\n",
            "         [ 5.8786e-03, -1.2218e-02, -1.9492e-02,  ...,  9.4982e-03,\n",
            "           3.7647e-03, -1.2722e-03],\n",
            "         [ 7.6788e-03, -1.5858e-02, -2.4259e-02,  ...,  1.2172e-02,\n",
            "           5.0542e-03, -6.8315e-04],\n",
            "         [-1.6486e-03,  3.5167e-03,  6.5374e-03,  ..., -2.8732e-03,\n",
            "          -9.3416e-04,  1.2285e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.1605e-03, -2.5021e-03, -2.1490e-03,  ...,  4.0420e-05,\n",
            "           2.9370e-05, -3.0233e-04],\n",
            "         [ 2.4379e-03, -3.1675e-03, -2.5526e-03,  ...,  5.3567e-04,\n",
            "          -9.8581e-04,  2.3037e-04],\n",
            "         [-4.9304e-04,  6.6393e-04,  5.2490e-04,  ..., -1.4153e-04,\n",
            "           2.6839e-04, -8.5301e-05],\n",
            "         ...,\n",
            "         [ 6.2449e-04, -7.3180e-04, -6.2436e-04,  ...,  2.3891e-05,\n",
            "          -1.6894e-05, -7.3153e-05],\n",
            "         [ 2.0931e-03, -1.6091e-03, -1.7797e-03,  ..., -1.1210e-03,\n",
            "           2.4407e-03, -1.6459e-03],\n",
            "         [-5.0242e-04,  1.0629e-03,  6.7820e-04,  ..., -6.9425e-04,\n",
            "           1.4171e-03, -7.2839e-04]],\n",
            "\n",
            "        [[ 7.5140e-03, -8.2401e-03, -2.1188e-03,  ..., -4.0898e-04,\n",
            "           2.0870e-03, -6.7487e-03],\n",
            "         [-1.0795e-02,  1.4768e-02,  2.9757e-03,  ...,  1.8382e-03,\n",
            "          -5.7132e-03,  1.0522e-02],\n",
            "         [-4.6853e-03,  4.9508e-03,  1.3255e-03,  ...,  1.7507e-04,\n",
            "          -1.1278e-03,  4.1552e-03],\n",
            "         ...,\n",
            "         [-2.6630e-02,  3.3859e-02,  7.4007e-03,  ...,  3.4370e-03,\n",
            "          -1.1711e-02,  2.5232e-02],\n",
            "         [ 1.9936e-02, -2.5562e-02, -5.5356e-03,  ..., -2.6643e-03,\n",
            "           8.9655e-03, -1.8950e-02],\n",
            "         [-5.6443e-03,  7.1922e-03,  1.5682e-03,  ...,  7.3518e-04,\n",
            "          -2.4968e-03,  5.3524e-03]],\n",
            "\n",
            "        [[ 6.9913e-03, -8.0602e-03, -9.9801e-03,  ...,  3.1189e-03,\n",
            "           1.5670e-03, -4.1316e-03],\n",
            "         [-1.1922e-02,  1.4190e-02,  1.8095e-02,  ..., -5.8426e-03,\n",
            "          -2.6086e-03,  7.8194e-03],\n",
            "         [-6.6812e-03,  8.1341e-03,  1.0580e-02,  ..., -3.4882e-03,\n",
            "          -1.4358e-03,  4.6981e-03],\n",
            "         ...,\n",
            "         [ 1.0834e-02, -1.3295e-02, -1.7410e-02,  ...,  5.7801e-03,\n",
            "           2.3132e-03, -7.8010e-03],\n",
            "         [ 1.4391e-02, -1.6656e-02, -2.0699e-02,  ...,  6.4957e-03,\n",
            "           3.2165e-03, -8.6162e-03],\n",
            "         [ 7.8615e-04, -4.5031e-04, -2.0062e-05,  ..., -1.8586e-04,\n",
            "           2.4143e-04,  3.2792e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2168, 0.1618, 0.1805,  ..., 0.1491, 0.2020, 0.0803],\n",
            "        [0.2133, 0.1650, 0.1839,  ..., 0.1439, 0.2002, 0.0794],\n",
            "        [0.2149, 0.1621, 0.1804,  ..., 0.1499, 0.2033, 0.0812],\n",
            "        ...,\n",
            "        [0.2192, 0.1614, 0.1811,  ..., 0.1502, 0.2034, 0.0816],\n",
            "        [0.2135, 0.1652, 0.1837,  ..., 0.1443, 0.2000, 0.0798],\n",
            "        [0.2139, 0.1619, 0.1801,  ..., 0.1492, 0.2031, 0.0812]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.0886e-02, -1.3044e-02, -2.9753e-03,  ..., -5.6419e-04,\n",
            "           8.9811e-03, -1.2671e-02],\n",
            "         [-6.0899e-03,  4.0995e-03,  2.7547e-03,  ...,  5.3643e-04,\n",
            "          -4.0276e-03,  4.0425e-03],\n",
            "         [-5.4584e-03,  2.7534e-03,  2.7830e-03,  ...,  5.4439e-04,\n",
            "          -3.3229e-03,  2.7460e-03],\n",
            "         ...,\n",
            "         [-2.7362e-02,  2.8932e-02,  8.7927e-03,  ...,  1.6842e-03,\n",
            "          -2.1373e-02,  2.8178e-02],\n",
            "         [ 1.6278e-02, -1.6779e-02, -5.3783e-03,  ..., -1.0318e-03,\n",
            "           1.2580e-02, -1.6351e-02],\n",
            "         [-6.9463e-03,  8.1953e-03,  1.9421e-03,  ...,  3.6884e-04,\n",
            "          -5.6909e-03,  7.9635e-03]],\n",
            "\n",
            "        [[ 4.6100e-03, -1.1907e-02, -3.3015e-03,  ..., -2.5894e-03,\n",
            "           5.5117e-03, -1.0089e-02],\n",
            "         [-6.9130e-04,  2.7474e-04,  8.5604e-04,  ...,  8.7740e-05,\n",
            "           1.2355e-03,  2.2380e-03],\n",
            "         [-2.1225e-03,  4.6949e-03,  1.7081e-03,  ...,  1.0356e-03,\n",
            "          -1.4635e-03,  5.0229e-03],\n",
            "         ...,\n",
            "         [-1.5064e-02,  4.1338e-02,  1.0207e-02,  ...,  8.9450e-03,\n",
            "          -2.1330e-02,  3.1801e-02],\n",
            "         [ 9.8463e-03, -2.5462e-02, -7.0439e-03,  ..., -5.5368e-03,\n",
            "           1.1816e-02, -2.1534e-02],\n",
            "         [-7.0585e-03,  1.9139e-02,  4.8380e-03,  ...,  4.1454e-03,\n",
            "          -9.6793e-03,  1.5012e-02]],\n",
            "\n",
            "        [[ 4.5967e-03, -9.0517e-03, -1.4043e-02,  ...,  6.7438e-03,\n",
            "           2.8428e-03, -9.2872e-04],\n",
            "         [-7.8486e-03,  1.5680e-02,  2.4069e-02,  ..., -1.1775e-02,\n",
            "          -4.9718e-03,  1.1680e-03],\n",
            "         [-3.8697e-03,  7.7223e-03,  1.1863e-02,  ..., -5.7957e-03,\n",
            "          -2.4468e-03,  5.9192e-04],\n",
            "         ...,\n",
            "         [ 6.3241e-03, -1.2323e-02, -1.9267e-02,  ...,  9.1269e-03,\n",
            "           3.8427e-03, -1.5202e-03],\n",
            "         [ 7.8969e-03, -1.5974e-02, -2.4297e-02,  ...,  1.2077e-02,\n",
            "           5.1060e-03, -8.0826e-04],\n",
            "         [-1.5606e-03,  2.6352e-03,  4.5898e-03,  ..., -1.7817e-03,\n",
            "          -7.3531e-04,  1.1296e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 9.2800e-04, -1.1621e-03, -1.0268e-03,  ..., -8.3284e-05,\n",
            "           1.5530e-05, -3.5886e-05],\n",
            "         [ 1.9752e-03, -2.8881e-03, -2.4644e-03,  ...,  1.8551e-05,\n",
            "          -7.3590e-04,  5.2255e-04],\n",
            "         [ 3.2449e-04, -2.4637e-04, -2.5146e-04,  ..., -1.0466e-04,\n",
            "           3.0206e-04, -2.4359e-04],\n",
            "         ...,\n",
            "         [ 9.6327e-04, -1.1756e-03, -1.0452e-03,  ..., -1.0093e-04,\n",
            "           7.2992e-05, -8.1548e-05],\n",
            "         [ 2.9336e-03, -2.4336e-03, -2.4120e-03,  ..., -8.4879e-04,\n",
            "           2.3483e-03, -1.9043e-03],\n",
            "         [ 3.8307e-04,  2.1550e-04,  4.3692e-05,  ..., -3.6267e-04,\n",
            "           1.2955e-03, -1.0189e-03]],\n",
            "\n",
            "        [[ 7.4981e-03, -7.2401e-03, -1.4598e-03,  ..., -2.1868e-04,\n",
            "           1.6410e-03, -6.2951e-03],\n",
            "         [-1.0905e-02,  1.6540e-02,  4.0760e-03,  ...,  2.1731e-03,\n",
            "          -6.5074e-03,  1.1352e-02],\n",
            "         [-5.6236e-03,  6.0697e-03,  1.3027e-03,  ...,  3.6139e-04,\n",
            "          -1.6692e-03,  4.9551e-03],\n",
            "         ...,\n",
            "         [-2.7254e-02,  3.6371e-02,  8.5732e-03,  ...,  3.8982e-03,\n",
            "          -1.2859e-02,  2.6557e-02],\n",
            "         [ 1.8426e-02, -2.3630e-02, -5.4844e-03,  ..., -2.3394e-03,\n",
            "           8.0356e-03, -1.7604e-02],\n",
            "         [-5.6417e-03,  7.4657e-03,  1.7541e-03,  ...,  7.8742e-04,\n",
            "          -2.6184e-03,  5.4743e-03]],\n",
            "\n",
            "        [[ 7.0463e-03, -8.1215e-03, -1.0060e-02,  ...,  3.1174e-03,\n",
            "           1.5832e-03, -4.1568e-03],\n",
            "         [-1.2743e-02,  1.5150e-02,  1.9366e-02,  ..., -5.9767e-03,\n",
            "          -2.8276e-03,  8.2894e-03],\n",
            "         [-7.1229e-03,  8.6189e-03,  1.1209e-02,  ..., -3.4516e-03,\n",
            "          -1.5688e-03,  4.8860e-03],\n",
            "         ...,\n",
            "         [ 1.1055e-02, -1.3434e-02, -1.7542e-02,  ...,  5.3991e-03,\n",
            "           2.4304e-03, -7.6791e-03],\n",
            "         [ 1.5002e-02, -1.7405e-02, -2.1707e-02,  ...,  6.7205e-03,\n",
            "           3.3618e-03, -9.0402e-03],\n",
            "         [-2.3266e-04,  7.7466e-04,  1.6189e-03,  ..., -4.7467e-04,\n",
            "          -1.3155e-05,  9.8397e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2132, 0.1629, 0.1809,  ..., 0.1461, 0.1995, 0.0790],\n",
            "        [0.2154, 0.1618, 0.1802,  ..., 0.1479, 0.2007, 0.0805],\n",
            "        [0.2177, 0.1626, 0.1816,  ..., 0.1520, 0.2045, 0.0812],\n",
            "        ...,\n",
            "        [0.2150, 0.1628, 0.1810,  ..., 0.1468, 0.2000, 0.0794],\n",
            "        [0.2141, 0.1628, 0.1809,  ..., 0.1464, 0.1996, 0.0792],\n",
            "        [0.2151, 0.1621, 0.1805,  ..., 0.1500, 0.2034, 0.0812]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.0239e-02, -1.2130e-02, -2.8360e-03,  ..., -3.4886e-04,\n",
            "           8.4391e-03, -1.1837e-02],\n",
            "         [-5.8928e-03,  3.6329e-03,  2.7869e-03,  ...,  6.9107e-04,\n",
            "          -3.7658e-03,  3.5537e-03],\n",
            "         [-5.7445e-03,  3.0692e-03,  2.8796e-03,  ...,  7.4282e-04,\n",
            "          -3.5171e-03,  3.0046e-03],\n",
            "         ...,\n",
            "         [-2.7805e-02,  2.9937e-02,  8.7376e-03,  ...,  1.3872e-03,\n",
            "          -2.1939e-02,  2.9222e-02],\n",
            "         [ 1.6532e-02, -1.7327e-02, -5.3576e-03,  ..., -8.9388e-04,\n",
            "           1.2890e-02, -1.6914e-02],\n",
            "         [-6.6601e-03,  7.8226e-03,  1.8681e-03,  ...,  2.3682e-04,\n",
            "          -5.4674e-03,  7.6339e-03]],\n",
            "\n",
            "        [[ 4.7946e-03, -1.2317e-02, -3.3462e-03,  ..., -2.6963e-03,\n",
            "           5.7894e-03, -1.0397e-02],\n",
            "         [-8.1404e-04, -5.9010e-04,  4.9032e-04,  ...,  3.8683e-05,\n",
            "           1.6510e-03,  2.1641e-03],\n",
            "         [-2.3497e-03,  4.7369e-03,  1.6021e-03,  ...,  1.1183e-03,\n",
            "          -1.5610e-03,  5.2886e-03],\n",
            "         ...,\n",
            "         [-1.4847e-02,  4.1755e-02,  1.0467e-02,  ...,  8.9143e-03,\n",
            "          -2.1477e-02,  3.1660e-02],\n",
            "         [ 9.9177e-03, -2.5196e-02, -6.9134e-03,  ..., -5.5333e-03,\n",
            "           1.1699e-02, -2.1549e-02],\n",
            "         [-6.7643e-03,  1.8442e-02,  4.7517e-03,  ...,  3.9704e-03,\n",
            "          -9.2138e-03,  1.4510e-02]],\n",
            "\n",
            "        [[ 4.2733e-03, -8.6190e-03, -1.3486e-02,  ...,  6.5137e-03,\n",
            "           2.5750e-03, -9.5275e-04],\n",
            "         [-7.9881e-03,  1.6140e-02,  2.5167e-02,  ..., -1.2208e-02,\n",
            "          -4.8555e-03,  1.6556e-03],\n",
            "         [-4.2723e-03,  8.5757e-03,  1.3545e-02,  ..., -6.4660e-03,\n",
            "          -2.5131e-03,  1.1356e-03],\n",
            "         ...,\n",
            "         [ 6.4163e-03, -1.2784e-02, -2.0487e-02,  ...,  9.6042e-03,\n",
            "           3.6325e-03, -2.1287e-03],\n",
            "         [ 8.0898e-03, -1.6453e-02, -2.5325e-02,  ...,  1.2484e-02,\n",
            "           5.0778e-03, -1.1975e-03],\n",
            "         [-1.6615e-03,  3.0618e-03,  5.6797e-03,  ..., -2.2091e-03,\n",
            "          -5.7135e-04,  1.6536e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.3592e-03, -2.4746e-03, -1.9493e-03,  ...,  6.7829e-05,\n",
            "           3.9804e-04, -7.9243e-05],\n",
            "         [ 2.4760e-03, -2.7097e-03, -1.8082e-03,  ...,  5.9284e-04,\n",
            "          -2.8251e-04,  7.3189e-04],\n",
            "         [ 5.2140e-04, -5.1078e-04, -5.0707e-04,  ..., -1.5241e-04,\n",
            "           3.1268e-04, -2.7907e-04],\n",
            "         ...,\n",
            "         [ 1.1749e-03, -1.2324e-03, -9.7083e-04,  ...,  3.3679e-05,\n",
            "           1.9836e-04, -3.9620e-05],\n",
            "         [ 1.6148e-03, -1.4336e-03, -1.8835e-03,  ..., -1.1593e-03,\n",
            "           1.8909e-03, -1.9381e-03],\n",
            "         [ 9.0384e-05,  6.4677e-05, -4.1131e-04,  ..., -7.3638e-04,\n",
            "           1.0072e-03, -1.1576e-03]],\n",
            "\n",
            "        [[ 8.1431e-03, -8.8019e-03, -1.9299e-03,  ..., -3.3845e-04,\n",
            "           2.0701e-03, -7.0803e-03],\n",
            "         [-1.0603e-02,  1.5411e-02,  3.6638e-03,  ...,  2.2957e-03,\n",
            "          -6.6018e-03,  1.0998e-02],\n",
            "         [-6.5722e-03,  8.0094e-03,  1.8214e-03,  ...,  6.9832e-04,\n",
            "          -2.5661e-03,  6.1222e-03],\n",
            "         ...,\n",
            "         [-2.6431e-02,  3.4289e-02,  7.9306e-03,  ...,  3.7844e-03,\n",
            "          -1.2375e-02,  2.5557e-02],\n",
            "         [ 1.8357e-02, -2.3314e-02, -5.3621e-03,  ..., -2.3934e-03,\n",
            "           8.1000e-03, -1.7525e-02],\n",
            "         [-7.3220e-03,  1.0551e-02,  2.5034e-03,  ...,  1.5422e-03,\n",
            "          -4.4682e-03,  7.5537e-03]],\n",
            "\n",
            "        [[ 7.2994e-03, -8.3347e-03, -1.0369e-02,  ...,  3.2482e-03,\n",
            "           1.6429e-03, -4.2585e-03],\n",
            "         [-1.2718e-02,  1.4411e-02,  1.8479e-02,  ..., -5.9402e-03,\n",
            "          -2.8561e-03,  7.6306e-03],\n",
            "         [-6.8278e-03,  7.7022e-03,  1.0050e-02,  ..., -3.2773e-03,\n",
            "          -1.5313e-03,  4.1629e-03],\n",
            "         ...,\n",
            "         [ 1.1402e-02, -1.2832e-02, -1.6896e-02,  ...,  5.5493e-03,\n",
            "           2.5555e-03, -7.0091e-03],\n",
            "         [ 1.4817e-02, -1.6911e-02, -2.1078e-02,  ...,  6.6143e-03,\n",
            "           3.3346e-03, -8.6601e-03],\n",
            "         [-1.2019e-03,  1.2039e-03,  2.3375e-03,  ..., -9.6305e-04,\n",
            "          -2.6070e-04,  1.0228e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2162, 0.1625, 0.1802,  ..., 0.1462, 0.1994, 0.0808],\n",
            "        [0.2133, 0.1629, 0.1809,  ..., 0.1461, 0.1995, 0.0790],\n",
            "        [0.2127, 0.1621, 0.1800,  ..., 0.1485, 0.2029, 0.0811],\n",
            "        ...,\n",
            "        [0.2174, 0.1633, 0.1809,  ..., 0.1469, 0.2000, 0.0810],\n",
            "        [0.2168, 0.1631, 0.1814,  ..., 0.1475, 0.2007, 0.0801],\n",
            "        [0.2144, 0.1620, 0.1803,  ..., 0.1495, 0.2031, 0.0812]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.0448e-02, -1.3639e-02, -3.2903e-03,  ..., -2.8770e-04,\n",
            "           1.0297e-02, -1.2910e-02],\n",
            "         [-5.6652e-03,  2.3235e-03,  2.4192e-03,  ...,  7.4269e-04,\n",
            "          -2.2224e-03,  2.5954e-03],\n",
            "         [-6.8605e-03,  4.3714e-03,  2.7345e-03,  ...,  7.1921e-04,\n",
            "          -3.7235e-03,  4.4957e-03],\n",
            "         ...,\n",
            "         [-2.7870e-02,  3.1684e-02,  9.3652e-03,  ...,  1.3109e-03,\n",
            "          -2.4354e-02,  3.0356e-02],\n",
            "         [ 1.7137e-02, -1.9340e-02, -5.7763e-03,  ..., -8.2246e-04,\n",
            "           1.4881e-02, -1.8542e-02],\n",
            "         [-7.6168e-03,  1.0482e-02,  2.3312e-03,  ...,  1.4736e-04,\n",
            "          -7.8642e-03,  9.8795e-03]],\n",
            "\n",
            "        [[ 4.6781e-03, -1.1939e-02, -3.2168e-03,  ..., -2.5829e-03,\n",
            "           5.5501e-03, -1.0136e-02],\n",
            "         [-8.3362e-04, -2.7629e-04,  2.6151e-04,  ..., -1.0370e-04,\n",
            "           1.4872e-03,  2.0586e-03],\n",
            "         [-2.3474e-03,  4.8614e-03,  1.4677e-03,  ...,  1.0311e-03,\n",
            "          -1.6214e-03,  5.2049e-03],\n",
            "         ...,\n",
            "         [-1.5178e-02,  4.2710e-02,  1.0952e-02,  ...,  9.3124e-03,\n",
            "          -2.2100e-02,  3.2471e-02],\n",
            "         [ 1.0271e-02, -2.6542e-02, -7.1052e-03,  ..., -5.7480e-03,\n",
            "           1.2524e-02, -2.2220e-02],\n",
            "         [-7.2869e-03,  2.0244e-02,  5.2242e-03,  ...,  4.4095e-03,\n",
            "          -1.0341e-02,  1.5616e-02]],\n",
            "\n",
            "        [[ 4.3175e-03, -8.4832e-03, -1.3294e-02,  ...,  6.1802e-03,\n",
            "           2.7652e-03, -1.0670e-03],\n",
            "         [-7.4645e-03,  1.4921e-02,  2.3018e-02,  ..., -1.1062e-02,\n",
            "          -4.8489e-03,  1.2730e-03],\n",
            "         [-3.9233e-03,  7.7454e-03,  1.2085e-02,  ..., -5.6706e-03,\n",
            "          -2.5226e-03,  8.8659e-04],\n",
            "         ...,\n",
            "         [ 6.1743e-03, -1.1892e-02, -1.8980e-02,  ...,  8.4831e-03,\n",
            "           3.8901e-03, -2.0647e-03],\n",
            "         [ 7.8034e-03, -1.5744e-02, -2.4082e-02,  ...,  1.1780e-02,\n",
            "           5.1081e-03, -1.0030e-03],\n",
            "         [-1.1414e-03,  1.7932e-03,  3.4552e-03,  ..., -9.6735e-04,\n",
            "          -6.1041e-04,  1.2933e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1938e-03, -1.3192e-03, -1.1554e-03,  ..., -3.2656e-05,\n",
            "           1.2627e-04,  1.1094e-04],\n",
            "         [ 2.3170e-03, -2.7502e-03, -2.3002e-03,  ...,  2.5875e-04,\n",
            "          -4.3105e-04,  1.0962e-03],\n",
            "         [ 1.1395e-03, -1.1726e-03, -1.0765e-03,  ..., -1.7827e-04,\n",
            "           4.2928e-04, -2.9635e-04],\n",
            "         ...,\n",
            "         [ 2.0820e-04, -2.3821e-04, -2.0398e-04,  ...,  8.1151e-06,\n",
            "          -6.9647e-06,  5.7110e-05],\n",
            "         [ 1.7943e-03, -1.4713e-03, -1.5810e-03,  ..., -9.1708e-04,\n",
            "           2.0116e-03, -2.2067e-03],\n",
            "         [ 2.0268e-03, -1.8970e-03, -1.8573e-03,  ..., -6.3710e-04,\n",
            "           1.4352e-03, -1.4022e-03]],\n",
            "\n",
            "        [[ 7.6932e-03, -8.0657e-03, -1.9018e-03,  ..., -2.8633e-04,\n",
            "           1.9494e-03, -6.4396e-03],\n",
            "         [-1.1705e-02,  1.7250e-02,  3.7896e-03,  ...,  2.4659e-03,\n",
            "          -7.0068e-03,  1.2542e-02],\n",
            "         [-5.3018e-03,  5.6736e-03,  1.3313e-03,  ...,  2.4423e-04,\n",
            "          -1.4368e-03,  4.5013e-03],\n",
            "         ...,\n",
            "         [-2.5307e-02,  3.2070e-02,  7.2527e-03,  ...,  3.2002e-03,\n",
            "          -1.0907e-02,  2.4235e-02],\n",
            "         [ 1.8460e-02, -2.3440e-02, -5.2988e-03,  ..., -2.3532e-03,\n",
            "           7.9937e-03, -1.7704e-02],\n",
            "         [-6.1447e-03,  8.2588e-03,  1.8460e-03,  ...,  9.6951e-04,\n",
            "          -3.0315e-03,  6.1447e-03]],\n",
            "\n",
            "        [[ 7.4862e-03, -8.5193e-03, -1.0820e-02,  ...,  3.4022e-03,\n",
            "           1.6932e-03, -4.4449e-03],\n",
            "         [-1.3143e-02,  1.4757e-02,  2.0045e-02,  ..., -6.4741e-03,\n",
            "          -2.9984e-03,  8.2717e-03],\n",
            "         [-6.7501e-03,  7.5295e-03,  1.0552e-02,  ..., -3.4482e-03,\n",
            "          -1.5463e-03,  4.3633e-03],\n",
            "         ...,\n",
            "         [ 1.1288e-02, -1.2537e-02, -1.7925e-02,  ...,  5.8998e-03,\n",
            "           2.5926e-03, -7.4212e-03],\n",
            "         [ 1.5274e-02, -1.7363e-02, -2.2173e-02,  ...,  6.9878e-03,\n",
            "           3.4570e-03, -9.1121e-03],\n",
            "         [-5.3518e-04,  3.4983e-04,  2.1300e-03,  ..., -8.9151e-04,\n",
            "          -1.5444e-04,  9.2340e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2165, 0.1618, 0.1804,  ..., 0.1488, 0.2017, 0.0803],\n",
            "        [0.2137, 0.1629, 0.1809,  ..., 0.1462, 0.1996, 0.0791],\n",
            "        [0.2170, 0.1624, 0.1812,  ..., 0.1514, 0.2041, 0.0812],\n",
            "        ...,\n",
            "        [0.2164, 0.1630, 0.1812,  ..., 0.1473, 0.2005, 0.0799],\n",
            "        [0.2155, 0.1642, 0.1836,  ..., 0.1487, 0.2018, 0.0780],\n",
            "        [0.2138, 0.1619, 0.1801,  ..., 0.1492, 0.2031, 0.0812]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.0010e-02, -1.2390e-02, -3.5573e-03,  ..., -3.4255e-04,\n",
            "           9.8454e-03, -1.1457e-02],\n",
            "         [-4.4988e-03,  3.4195e-04,  1.6856e-03,  ...,  7.7867e-04,\n",
            "          -2.0158e-04,  1.1829e-03],\n",
            "         [-5.9818e-03,  2.8606e-03,  2.2012e-03,  ...,  7.4779e-04,\n",
            "          -2.2121e-03,  3.3986e-03],\n",
            "         ...,\n",
            "         [-2.8449e-02,  3.2267e-02,  1.0159e-02,  ...,  1.3258e-03,\n",
            "          -2.5601e-02,  3.0326e-02],\n",
            "         [ 1.7304e-02, -1.9344e-02, -6.1842e-03,  ..., -8.4034e-04,\n",
            "           1.5343e-02, -1.8231e-02],\n",
            "         [-8.1503e-03,  1.1192e-02,  2.8782e-03,  ...,  1.4701e-04,\n",
            "          -8.9084e-03,  1.0166e-02]],\n",
            "\n",
            "        [[ 4.4139e-03, -1.1228e-02, -3.0499e-03,  ..., -2.3947e-03,\n",
            "           5.1967e-03, -9.6909e-03],\n",
            "         [-8.0762e-04,  4.4223e-04,  4.1675e-04,  ..., -5.6012e-05,\n",
            "           7.1491e-04,  2.3678e-03],\n",
            "         [-2.7889e-03,  6.7351e-03,  1.8956e-03,  ...,  1.4030e-03,\n",
            "          -2.9125e-03,  6.2557e-03],\n",
            "         ...,\n",
            "         [-1.4402e-02,  3.9382e-02,  1.0192e-02,  ...,  8.6557e-03,\n",
            "          -1.9794e-02,  3.0608e-02],\n",
            "         [ 9.6547e-03, -2.4756e-02, -6.6884e-03,  ..., -5.2984e-03,\n",
            "           1.1570e-02, -2.1125e-02],\n",
            "         [-6.4954e-03,  1.7368e-02,  4.5623e-03,  ...,  3.7833e-03,\n",
            "          -8.5213e-03,  1.3949e-02]],\n",
            "\n",
            "        [[ 3.9120e-03, -8.1975e-03, -1.2875e-02,  ...,  6.2449e-03,\n",
            "           2.5417e-03, -1.0339e-03],\n",
            "         [-7.8865e-03,  1.6608e-02,  2.6412e-02,  ..., -1.2646e-02,\n",
            "          -5.0870e-03,  2.6125e-03],\n",
            "         [-4.0648e-03,  8.6009e-03,  1.3839e-02,  ..., -6.5454e-03,\n",
            "          -2.6036e-03,  1.6075e-03],\n",
            "         ...,\n",
            "         [ 5.1507e-03, -1.0842e-02, -1.7220e-02,  ...,  8.2552e-03,\n",
            "           3.3247e-03, -1.6719e-03],\n",
            "         [ 7.7130e-03, -1.6055e-02, -2.4785e-02,  ...,  1.2239e-02,\n",
            "           5.0598e-03, -1.3470e-03],\n",
            "         [-1.1337e-03,  2.6580e-03,  5.2982e-03,  ..., -2.0018e-03,\n",
            "          -6.0965e-04,  2.1094e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 4.0078e-03, -4.1719e-03, -3.1520e-03,  ...,  1.5840e-04,\n",
            "           7.6124e-04, -3.5971e-04],\n",
            "         [ 2.1885e-03, -2.3837e-03, -1.3609e-03,  ...,  6.9177e-04,\n",
            "          -3.1029e-04,  5.7230e-04],\n",
            "         [ 1.0842e-03, -1.0960e-03, -9.6379e-04,  ..., -1.4377e-04,\n",
            "           4.2977e-04, -3.3432e-04],\n",
            "         ...,\n",
            "         [ 1.9826e-04, -2.1168e-04, -1.3785e-04,  ...,  3.8212e-05,\n",
            "           1.2258e-06,  2.0783e-05],\n",
            "         [ 9.4713e-04, -7.6086e-04, -1.5124e-03,  ..., -1.2520e-03,\n",
            "           1.7264e-03, -1.7226e-03],\n",
            "         [ 2.0321e-03, -1.9686e-03, -2.0988e-03,  ..., -7.6051e-04,\n",
            "           1.3945e-03, -1.2503e-03]],\n",
            "\n",
            "        [[ 7.3087e-03, -7.7721e-03, -1.6059e-03,  ..., -3.6656e-04,\n",
            "           1.5521e-03, -6.3111e-03],\n",
            "         [-1.1871e-02,  1.6773e-02,  4.1299e-03,  ...,  2.1124e-03,\n",
            "          -7.2544e-03,  1.2070e-02],\n",
            "         [-4.3687e-03,  4.3182e-03,  8.3979e-04,  ...,  9.9351e-05,\n",
            "          -5.5409e-04,  3.6287e-03],\n",
            "         ...,\n",
            "         [-2.6298e-02,  3.3459e-02,  7.7927e-03,  ...,  3.3272e-03,\n",
            "          -1.1851e-02,  2.5118e-02],\n",
            "         [ 1.6715e-02, -2.0292e-02, -4.5957e-03,  ..., -1.7584e-03,\n",
            "           6.4207e-03, -1.5538e-02],\n",
            "         [-5.4201e-03,  6.8507e-03,  1.5895e-03,  ...,  6.6920e-04,\n",
            "          -2.3909e-03,  5.1570e-03]],\n",
            "\n",
            "        [[ 7.5355e-03, -8.6356e-03, -1.0793e-02,  ...,  3.2891e-03,\n",
            "           1.6632e-03, -4.4071e-03],\n",
            "         [-1.2969e-02,  1.4887e-02,  1.9177e-02,  ..., -5.6509e-03,\n",
            "          -2.7476e-03,  7.7750e-03],\n",
            "         [-6.9203e-03,  7.9510e-03,  1.0425e-02,  ..., -3.0120e-03,\n",
            "          -1.4293e-03,  4.2093e-03],\n",
            "         ...,\n",
            "         [ 1.0853e-02, -1.2473e-02, -1.6449e-02,  ...,  4.7221e-03,\n",
            "           2.2225e-03, -6.6330e-03],\n",
            "         [ 1.4878e-02, -1.7049e-02, -2.1283e-02,  ...,  6.4943e-03,\n",
            "           3.2889e-03, -8.6927e-03],\n",
            "         [-6.3033e-04,  7.5637e-04,  1.7543e-03,  ..., -2.6098e-04,\n",
            "           2.3816e-05,  6.3795e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2184, 0.1637, 0.1822,  ..., 0.1483, 0.2016, 0.0806],\n",
            "        [0.2145, 0.1628, 0.1809,  ..., 0.1466, 0.1998, 0.0793],\n",
            "        [0.2149, 0.1621, 0.1804,  ..., 0.1499, 0.2033, 0.0812],\n",
            "        ...,\n",
            "        [0.2138, 0.1629, 0.1809,  ..., 0.1463, 0.1996, 0.0791],\n",
            "        [0.2125, 0.1630, 0.1810,  ..., 0.1459, 0.1995, 0.0788],\n",
            "        [0.2196, 0.1630, 0.1825,  ..., 0.1533, 0.2060, 0.0811]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.0279e-02, -1.2798e-02, -3.1065e-03,  ..., -3.1332e-04,\n",
            "           9.8341e-03, -1.2174e-02],\n",
            "         [-4.8839e-03,  1.2296e-03,  2.4088e-03,  ...,  7.7483e-04,\n",
            "          -1.2783e-03,  1.5516e-03],\n",
            "         [-6.0328e-03,  3.0629e-03,  2.6786e-03,  ...,  7.5788e-04,\n",
            "          -2.6593e-03,  3.2639e-03],\n",
            "         ...,\n",
            "         [-2.9493e-02,  3.4076e-02,  9.4225e-03,  ...,  1.2405e-03,\n",
            "          -2.6367e-02,  3.2625e-02],\n",
            "         [ 1.7668e-02, -1.9931e-02, -5.7373e-03,  ..., -8.0533e-04,\n",
            "           1.5458e-02, -1.9123e-02],\n",
            "         [-7.9775e-03,  1.0728e-02,  2.2581e-03,  ...,  1.4052e-04,\n",
            "          -8.1893e-03,  1.0143e-02]],\n",
            "\n",
            "        [[ 4.1069e-03, -1.0324e-02, -2.8638e-03,  ..., -2.1558e-03,\n",
            "           4.7070e-03, -9.2670e-03],\n",
            "         [-2.1293e-03,  4.8684e-03,  1.4743e-03,  ...,  9.3888e-04,\n",
            "          -1.9384e-03,  5.1617e-03],\n",
            "         [-3.1384e-03,  7.9352e-03,  2.1895e-03,  ...,  1.6642e-03,\n",
            "          -3.6442e-03,  7.0481e-03],\n",
            "         ...,\n",
            "         [-1.3953e-02,  3.7782e-02,  9.7878e-03,  ...,  8.3230e-03,\n",
            "          -1.8796e-02,  2.9489e-02],\n",
            "         [ 9.4989e-03, -2.4382e-02, -6.6345e-03,  ..., -5.1718e-03,\n",
            "           1.1408e-02, -2.1063e-02],\n",
            "         [-5.9758e-03,  1.5702e-02,  4.1817e-03,  ...,  3.3876e-03,\n",
            "          -7.5531e-03,  1.2983e-02]],\n",
            "\n",
            "        [[ 4.2109e-03, -8.2271e-03, -1.3149e-02,  ...,  6.4665e-03,\n",
            "           2.4498e-03, -1.2501e-03],\n",
            "         [-8.5359e-03,  1.6486e-02,  2.6710e-02,  ..., -1.3043e-02,\n",
            "          -4.8138e-03,  3.0095e-03],\n",
            "         [-4.4444e-03,  8.4814e-03,  1.3937e-02,  ..., -6.7559e-03,\n",
            "          -2.4252e-03,  1.8209e-03],\n",
            "         ...,\n",
            "         [ 6.0511e-03, -1.1518e-02, -1.8983e-02,  ...,  9.1881e-03,\n",
            "           3.2787e-03, -2.5517e-03],\n",
            "         [ 7.2420e-03, -1.4799e-02, -2.2424e-02,  ...,  1.1344e-02,\n",
            "           4.7296e-03, -5.3590e-04],\n",
            "         [-1.5495e-03,  2.3043e-03,  5.0494e-03,  ..., -2.1318e-03,\n",
            "          -3.2667e-04,  2.2565e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.3087e-03, -2.5521e-03, -2.1171e-03,  ...,  1.0822e-04,\n",
            "           3.5668e-04, -1.9871e-04],\n",
            "         [ 3.3540e-03, -3.9317e-03, -3.0382e-03,  ...,  6.9023e-04,\n",
            "          -1.7637e-04,  3.7655e-04],\n",
            "         [ 1.1349e-03, -1.1715e-03, -1.0546e-03,  ..., -1.4450e-04,\n",
            "           4.3295e-04, -3.4442e-04],\n",
            "         ...,\n",
            "         [-7.9877e-04,  8.5938e-04,  7.3642e-04,  ...,  1.8691e-05,\n",
            "          -1.9655e-04,  1.3880e-04],\n",
            "         [ 6.4555e-04, -1.8081e-04, -6.8100e-04,  ..., -1.2368e-03,\n",
            "           1.7508e-03, -1.6369e-03],\n",
            "         [ 2.2059e-03, -2.0792e-03, -2.0828e-03,  ..., -7.5085e-04,\n",
            "           1.4539e-03, -1.2560e-03]],\n",
            "\n",
            "        [[ 9.0601e-03, -1.0629e-02, -2.3677e-03,  ..., -8.2626e-04,\n",
            "           2.9891e-03, -8.1189e-03],\n",
            "         [-1.0590e-02,  1.4995e-02,  3.6434e-03,  ...,  1.8610e-03,\n",
            "          -6.8084e-03,  1.1389e-02],\n",
            "         [-5.0962e-03,  5.3809e-03,  1.1281e-03,  ...,  2.5661e-04,\n",
            "          -9.1065e-04,  4.1251e-03],\n",
            "         ...,\n",
            "         [-2.5225e-02,  3.1828e-02,  7.3530e-03,  ...,  3.0779e-03,\n",
            "          -1.1201e-02,  2.4254e-02],\n",
            "         [ 1.5970e-02, -1.8920e-02, -4.2360e-03,  ..., -1.5202e-03,\n",
            "           5.5052e-03, -1.4446e-02],\n",
            "         [-6.8821e-03,  9.5089e-03,  2.2873e-03,  ...,  1.1272e-03,\n",
            "          -4.1201e-03,  7.2269e-03]],\n",
            "\n",
            "        [[ 8.6843e-03, -9.7830e-03, -1.2900e-02,  ...,  4.1692e-03,\n",
            "           1.5918e-03, -5.2875e-03],\n",
            "         [-1.4704e-02,  1.6341e-02,  2.2766e-02,  ..., -7.5781e-03,\n",
            "          -2.1910e-03,  9.3313e-03],\n",
            "         [-7.0245e-03,  7.7678e-03,  1.1039e-02,  ..., -3.7116e-03,\n",
            "          -9.5810e-04,  4.5245e-03],\n",
            "         ...,\n",
            "         [ 1.1165e-02, -1.2301e-02, -1.7734e-02,  ...,  6.0051e-03,\n",
            "           1.4202e-03, -7.2687e-03],\n",
            "         [ 1.5551e-02, -1.7625e-02, -2.2661e-02,  ...,  7.2186e-03,\n",
            "           3.0908e-03, -9.2879e-03],\n",
            "         [-4.3522e-04,  1.7149e-04,  1.9708e-03,  ..., -9.5259e-04,\n",
            "           6.4247e-04,  8.0784e-04]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2186, 0.1614, 0.1812,  ..., 0.1503, 0.2037, 0.0806],\n",
            "        [0.2135, 0.1615, 0.1801,  ..., 0.1471, 0.2004, 0.0806],\n",
            "        [0.2144, 0.1620, 0.1803,  ..., 0.1495, 0.2031, 0.0812],\n",
            "        ...,\n",
            "        [0.2191, 0.1614, 0.1810,  ..., 0.1501, 0.2033, 0.0816],\n",
            "        [0.2172, 0.1632, 0.1816,  ..., 0.1477, 0.2009, 0.0802],\n",
            "        [0.2155, 0.1622, 0.1806,  ..., 0.1503, 0.2035, 0.0812]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.0315e-02, -1.2055e-02, -3.1739e-03,  ..., -1.5286e-04,\n",
            "           9.5760e-03, -1.1707e-02],\n",
            "         [-5.8639e-03,  3.7424e-03,  2.5297e-03,  ...,  8.4039e-04,\n",
            "          -2.9214e-03,  3.7038e-03],\n",
            "         [-5.5907e-03,  3.0048e-03,  2.5432e-03,  ...,  9.3769e-04,\n",
            "          -2.3285e-03,  2.9968e-03],\n",
            "         ...,\n",
            "         [-2.9570e-02,  3.2583e-02,  9.5591e-03,  ...,  9.1648e-04,\n",
            "          -2.5851e-02,  3.1688e-02],\n",
            "         [ 1.8188e-02, -1.9954e-02, -5.9000e-03,  ..., -5.8491e-04,\n",
            "           1.5829e-02, -1.9407e-02],\n",
            "         [-8.5276e-03,  1.0818e-02,  2.4253e-03,  ..., -7.9959e-05,\n",
            "          -8.6074e-03,  1.0487e-02]],\n",
            "\n",
            "        [[ 4.3786e-03, -1.0969e-02, -3.1747e-03,  ..., -2.4292e-03,\n",
            "           5.3188e-03, -9.5728e-03],\n",
            "         [-2.4199e-03,  5.4667e-03,  1.8570e-03,  ...,  1.2593e-03,\n",
            "          -2.6430e-03,  5.4132e-03],\n",
            "         [-3.6170e-03,  9.3800e-03,  2.5677e-03,  ...,  2.0512e-03,\n",
            "          -4.5524e-03,  7.8421e-03],\n",
            "         ...,\n",
            "         [-1.3590e-02,  3.6958e-02,  9.3523e-03,  ...,  7.9465e-03,\n",
            "          -1.7959e-02,  2.9111e-02],\n",
            "         [ 9.8533e-03, -2.5193e-02, -7.0565e-03,  ..., -5.5376e-03,\n",
            "           1.2222e-02, -2.1437e-02],\n",
            "         [-6.1677e-03,  1.6292e-02,  4.3272e-03,  ...,  3.5393e-03,\n",
            "          -7.9110e-03,  1.3311e-02]],\n",
            "\n",
            "        [[ 3.8204e-03, -7.8006e-03, -1.2437e-02,  ...,  6.1202e-03,\n",
            "           2.4227e-03, -6.8091e-04],\n",
            "         [-8.5738e-03,  1.7354e-02,  2.9312e-02,  ..., -1.3993e-02,\n",
            "          -5.1653e-03,  2.7639e-03],\n",
            "         [-3.9492e-03,  8.0185e-03,  1.3272e-02,  ..., -6.4031e-03,\n",
            "          -2.4237e-03,  1.0708e-03],\n",
            "         ...,\n",
            "         [ 5.7610e-03, -1.1647e-02, -1.9820e-02,  ...,  9.4250e-03,\n",
            "           3.4466e-03, -1.9668e-03],\n",
            "         [ 7.7859e-03, -1.5929e-02, -2.5057e-02,  ...,  1.2420e-02,\n",
            "           4.9933e-03, -1.1337e-03],\n",
            "         [-9.7632e-04,  1.8478e-03,  4.5232e-03,  ..., -1.8113e-03,\n",
            "          -3.5839e-04,  1.3595e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 3.0831e-03, -3.3322e-03, -2.6487e-03,  ...,  2.4302e-04,\n",
            "           4.2471e-04, -3.3540e-04],\n",
            "         [ 3.1427e-03, -3.5843e-03, -2.5290e-03,  ...,  8.9781e-04,\n",
            "          -3.5817e-04,  3.2862e-04],\n",
            "         [ 2.3046e-04, -1.9233e-04, -2.4968e-04,  ..., -1.7843e-04,\n",
            "           2.7098e-04, -2.2784e-04],\n",
            "         ...,\n",
            "         [-1.8577e-03,  1.9833e-03,  1.6183e-03,  ..., -6.1707e-05,\n",
            "          -3.5900e-04,  2.8948e-04],\n",
            "         [ 3.9318e-04, -5.1288e-06, -7.2011e-04,  ..., -1.4232e-03,\n",
            "           1.8238e-03, -1.5427e-03],\n",
            "         [ 2.9465e-03, -2.8992e-03, -2.7912e-03,  ..., -7.5610e-04,\n",
            "           1.6086e-03, -1.3399e-03]],\n",
            "\n",
            "        [[ 9.0200e-03, -1.0531e-02, -2.3103e-03,  ..., -8.3199e-04,\n",
            "           2.8746e-03, -8.0340e-03],\n",
            "         [-9.0071e-03,  1.2231e-02,  2.9889e-03,  ...,  1.3380e-03,\n",
            "          -5.1985e-03,  9.3263e-03],\n",
            "         [-4.7839e-03,  4.7674e-03,  9.0002e-04,  ...,  1.9933e-04,\n",
            "          -4.1414e-04,  3.6390e-03],\n",
            "         ...,\n",
            "         [-2.7125e-02,  3.5327e-02,  8.4017e-03,  ...,  3.5835e-03,\n",
            "          -1.3609e-02,  2.6940e-02],\n",
            "         [ 1.6383e-02, -1.9635e-02, -4.3977e-03,  ..., -1.6610e-03,\n",
            "           5.9089e-03, -1.4978e-02],\n",
            "         [-7.3320e-03,  1.0384e-02,  2.6031e-03,  ...,  1.2157e-03,\n",
            "          -4.8123e-03,  7.9170e-03]],\n",
            "\n",
            "        [[ 8.3781e-03, -9.3386e-03, -1.2545e-02,  ...,  4.2658e-03,\n",
            "           1.7076e-03, -5.2250e-03],\n",
            "         [-1.4585e-02,  1.5797e-02,  2.3184e-02,  ..., -8.5199e-03,\n",
            "          -2.6479e-03,  9.7972e-03],\n",
            "         [-7.2982e-03,  7.8042e-03,  1.1895e-02,  ..., -4.5024e-03,\n",
            "          -1.2541e-03,  5.0557e-03],\n",
            "         ...,\n",
            "         [ 1.1312e-02, -1.2033e-02, -1.8625e-02,  ...,  7.1313e-03,\n",
            "           1.8986e-03, -7.9343e-03],\n",
            "         [ 1.5787e-02, -1.7704e-02, -2.3328e-02,  ...,  7.7843e-03,\n",
            "           3.2931e-03, -9.6831e-03],\n",
            "         [-4.8242e-04, -6.2628e-05,  2.4776e-03,  ..., -1.6731e-03,\n",
            "           3.2532e-04,  1.2161e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2152, 0.1622, 0.1797,  ..., 0.1458, 0.1991, 0.0808],\n",
            "        [0.2148, 0.1644, 0.1838,  ..., 0.1479, 0.2015, 0.0779],\n",
            "        [0.2188, 0.1628, 0.1821,  ..., 0.1527, 0.2054, 0.0811],\n",
            "        ...,\n",
            "        [0.2195, 0.1645, 0.1824,  ..., 0.1480, 0.2013, 0.0817],\n",
            "        [0.2181, 0.1636, 0.1820,  ..., 0.1481, 0.2014, 0.0805],\n",
            "        [0.2141, 0.1659, 0.1823,  ..., 0.1456, 0.2017, 0.0787]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.0394e-02, -1.3391e-02, -2.9983e-03,  ..., -3.5415e-04,\n",
            "           9.7525e-03, -1.2193e-02],\n",
            "         [-6.3139e-03,  3.7562e-03,  2.7322e-03,  ...,  6.6372e-04,\n",
            "          -3.4790e-03,  4.2091e-03],\n",
            "         [-6.6219e-03,  3.9611e-03,  2.8610e-03,  ...,  6.9387e-04,\n",
            "          -3.6608e-03,  4.4303e-03],\n",
            "         ...,\n",
            "         [-2.7424e-02,  3.0578e-02,  8.8999e-03,  ...,  1.4215e-03,\n",
            "          -2.3077e-02,  2.8700e-02],\n",
            "         [ 1.6418e-02, -1.7758e-02, -5.4420e-03,  ..., -9.0713e-04,\n",
            "           1.3509e-02, -1.6781e-02],\n",
            "         [-8.0135e-03,  1.1171e-02,  2.1354e-03,  ...,  1.8624e-04,\n",
            "          -7.9922e-03,  1.0020e-02]],\n",
            "\n",
            "        [[ 4.6486e-03, -1.2028e-02, -3.2143e-03,  ..., -2.6145e-03,\n",
            "           5.7779e-03, -1.0170e-02],\n",
            "         [-3.0480e-03,  7.7633e-03,  2.0902e-03,  ...,  1.6895e-03,\n",
            "          -3.7019e-03,  6.7305e-03],\n",
            "         [-3.3582e-03,  8.6208e-03,  2.3124e-03,  ...,  1.8750e-03,\n",
            "          -4.1261e-03,  7.3815e-03],\n",
            "         ...,\n",
            "         [-1.3477e-02,  3.6298e-02,  9.5208e-03,  ...,  7.8678e-03,\n",
            "          -1.7755e-02,  2.8766e-02],\n",
            "         [ 9.5050e-03, -2.4374e-02, -6.5414e-03,  ..., -5.3017e-03,\n",
            "           1.1660e-02, -2.0906e-02],\n",
            "         [-5.5492e-03,  1.4340e-02,  3.8346e-03,  ...,  3.1175e-03,\n",
            "          -6.8849e-03,  1.2150e-02]],\n",
            "\n",
            "        [[ 3.8717e-03, -8.0161e-03, -1.2980e-02,  ...,  6.2931e-03,\n",
            "           2.4578e-03, -9.1859e-04],\n",
            "         [-8.1332e-03,  1.6888e-02,  2.8427e-02,  ..., -1.3458e-02,\n",
            "          -5.0247e-03,  2.8347e-03],\n",
            "         [-4.2851e-03,  8.9164e-03,  1.5427e-02,  ..., -7.1831e-03,\n",
            "          -2.5937e-03,  1.8442e-03],\n",
            "         ...,\n",
            "         [ 5.3196e-03, -1.1045e-02, -1.8563e-02,  ...,  8.7965e-03,\n",
            "           3.2900e-03, -1.8307e-03],\n",
            "         [ 7.3651e-03, -1.5196e-02, -2.3429e-02,  ...,  1.1712e-02,\n",
            "           4.8258e-03, -7.6376e-04],\n",
            "         [-5.0038e-04,  1.0855e-03,  2.8618e-03,  ..., -1.0564e-03,\n",
            "          -1.7656e-04,  1.0417e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.4634e-03, -2.9298e-03, -2.4979e-03,  ...,  1.9357e-05,\n",
            "           4.8112e-04, -2.5051e-04],\n",
            "         [ 3.6948e-03, -4.7929e-03, -3.9104e-03,  ...,  4.8165e-04,\n",
            "           1.1104e-04,  2.5780e-04],\n",
            "         [-3.9794e-05,  1.7059e-04,  9.1038e-05,  ..., -1.4029e-04,\n",
            "           1.8106e-04, -1.9188e-04],\n",
            "         ...,\n",
            "         [-2.0194e-03,  2.3357e-03,  2.0205e-03,  ...,  5.9047e-05,\n",
            "          -4.9546e-04,  3.1022e-04],\n",
            "         [ 3.5970e-04,  5.1865e-04,  2.4450e-05,  ..., -1.0719e-03,\n",
            "           1.5201e-03, -1.5409e-03],\n",
            "         [ 2.0125e-03, -1.7579e-03, -1.7793e-03,  ..., -7.0600e-04,\n",
            "           1.3668e-03, -1.2150e-03]],\n",
            "\n",
            "        [[ 9.7019e-03, -1.1753e-02, -2.6768e-03,  ..., -1.0298e-03,\n",
            "           3.6907e-03, -8.9645e-03],\n",
            "         [-9.8799e-03,  1.4058e-02,  3.1995e-03,  ...,  1.5239e-03,\n",
            "          -6.2591e-03,  1.0722e-02],\n",
            "         [-5.0684e-03,  5.1533e-03,  1.1747e-03,  ...,  3.1355e-04,\n",
            "          -7.4729e-04,  3.9308e-03],\n",
            "         ...,\n",
            "         [-2.6303e-02,  3.4062e-02,  7.7554e-03,  ...,  3.2919e-03,\n",
            "          -1.2637e-02,  2.5981e-02],\n",
            "         [ 1.6559e-02, -1.9907e-02, -4.5340e-03,  ..., -1.7229e-03,\n",
            "           6.1166e-03, -1.5184e-02],\n",
            "         [-6.6918e-03,  9.3666e-03,  2.1319e-03,  ...,  9.9689e-04,\n",
            "          -4.0538e-03,  7.1441e-03]],\n",
            "\n",
            "        [[ 8.1436e-03, -9.1766e-03, -1.2157e-02,  ...,  4.2800e-03,\n",
            "           1.7663e-03, -5.1863e-03],\n",
            "         [-1.5175e-02,  1.6711e-02,  2.4453e-02,  ..., -9.8707e-03,\n",
            "          -3.1122e-03,  1.0937e-02],\n",
            "         [-7.5277e-03,  8.2256e-03,  1.2427e-02,  ..., -5.2084e-03,\n",
            "          -1.5144e-03,  5.6347e-03],\n",
            "         ...,\n",
            "         [ 1.2345e-02, -1.3416e-02, -2.0722e-02,  ...,  8.9025e-03,\n",
            "           2.4495e-03, -9.4830e-03],\n",
            "         [ 1.6260e-02, -1.8321e-02, -2.4278e-02,  ...,  8.5515e-03,\n",
            "           3.5261e-03, -1.0359e-02],\n",
            "         [-1.1661e-03,  8.5485e-04,  3.8664e-03,  ..., -2.8505e-03,\n",
            "          -4.1519e-05,  2.2449e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2177, 0.1612, 0.1802,  ..., 0.1495, 0.2022, 0.0814],\n",
            "        [0.2187, 0.1638, 0.1824,  ..., 0.1485, 0.2019, 0.0806],\n",
            "        [0.2201, 0.1633, 0.1828,  ..., 0.1538, 0.2066, 0.0810],\n",
            "        ...,\n",
            "        [0.2189, 0.1614, 0.1808,  ..., 0.1500, 0.2032, 0.0815],\n",
            "        [0.2189, 0.1638, 0.1826,  ..., 0.1487, 0.2022, 0.0807],\n",
            "        [0.2162, 0.1623, 0.1809,  ..., 0.1509, 0.2037, 0.0811]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.1214e-02, -1.4522e-02, -3.8466e-03,  ..., -4.1729e-05,\n",
            "           1.0128e-02, -1.2487e-02],\n",
            "         [-6.6494e-03,  4.6612e-03,  2.4746e-03,  ...,  7.6251e-04,\n",
            "          -4.2357e-03,  5.2432e-03],\n",
            "         [-7.4666e-03,  5.8476e-03,  2.7486e-03,  ...,  7.4162e-04,\n",
            "          -5.0312e-03,  6.2232e-03],\n",
            "         ...,\n",
            "         [-2.9261e-02,  3.3512e-02,  1.0252e-02,  ...,  9.2709e-04,\n",
            "          -2.4464e-02,  3.0185e-02],\n",
            "         [ 1.6224e-02, -1.7115e-02, -5.7564e-03,  ..., -7.8803e-04,\n",
            "           1.2908e-02, -1.5935e-02],\n",
            "         [-7.8571e-03,  1.0438e-02,  2.6824e-03,  ..., -1.9783e-05,\n",
            "          -7.2138e-03,  8.8926e-03]],\n",
            "\n",
            "        [[ 4.8999e-03, -1.2893e-02, -3.4423e-03,  ..., -2.7921e-03,\n",
            "           6.2698e-03, -1.0648e-02],\n",
            "         [-2.2508e-03,  5.5410e-03,  1.6290e-03,  ...,  1.1747e-03,\n",
            "          -2.7331e-03,  5.2405e-03],\n",
            "         [-2.5694e-03,  6.4082e-03,  1.8493e-03,  ...,  1.3644e-03,\n",
            "          -3.1519e-03,  5.9066e-03],\n",
            "         ...,\n",
            "         [-1.3018e-02,  3.4689e-02,  9.0905e-03,  ...,  7.5408e-03,\n",
            "          -1.6825e-02,  2.7891e-02],\n",
            "         [ 8.8478e-03, -2.2777e-02, -6.2790e-03,  ..., -4.8992e-03,\n",
            "           1.1127e-02, -1.9689e-02],\n",
            "         [-5.1460e-03,  1.3303e-02,  3.6449e-03,  ...,  2.8652e-03,\n",
            "          -6.4933e-03,  1.1400e-02]],\n",
            "\n",
            "        [[ 4.0052e-03, -8.7588e-03, -1.4487e-02,  ...,  7.2133e-03,\n",
            "           2.6953e-03, -1.2037e-03],\n",
            "         [-7.8403e-03,  1.7257e-02,  2.8817e-02,  ..., -1.4320e-02,\n",
            "          -5.2822e-03,  2.5886e-03],\n",
            "         [-3.5234e-03,  7.5650e-03,  1.2166e-02,  ..., -6.0935e-03,\n",
            "          -2.3632e-03,  7.6564e-04],\n",
            "         ...,\n",
            "         [ 5.2375e-03, -1.1583e-02, -1.9477e-02,  ...,  9.6650e-03,\n",
            "           3.5317e-03, -1.8443e-03],\n",
            "         [ 7.2864e-03, -1.5214e-02, -2.3384e-02,  ...,  1.1827e-02,\n",
            "           4.8633e-03, -6.8219e-04],\n",
            "         [-3.3273e-04,  1.0916e-03,  2.7056e-03,  ..., -1.2542e-03,\n",
            "          -2.4411e-04,  8.6193e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.0379e-03, -2.3213e-03, -2.1300e-03,  ...,  1.5584e-04,\n",
            "           6.2896e-04, -8.9673e-05],\n",
            "         [ 2.9845e-03, -3.6887e-03, -3.3441e-03,  ...,  8.1302e-04,\n",
            "           5.5237e-04,  6.0898e-04],\n",
            "         [-3.7197e-04,  5.0670e-04,  4.5330e-04,  ..., -1.9631e-04,\n",
            "          -8.9508e-06, -1.9614e-04],\n",
            "         ...,\n",
            "         [-4.8185e-04,  5.2233e-04,  4.8301e-04,  ...,  1.6810e-05,\n",
            "          -1.8255e-04,  8.9129e-05],\n",
            "         [ 1.5620e-03, -1.0732e-03, -1.0838e-03,  ..., -1.3084e-03,\n",
            "           1.3824e-03, -1.8763e-03],\n",
            "         [ 2.2001e-03, -2.0375e-03, -1.9353e-03,  ..., -7.7947e-04,\n",
            "           1.2766e-03, -1.2966e-03]],\n",
            "\n",
            "        [[ 1.0249e-02, -1.2926e-02, -2.9519e-03,  ..., -1.1519e-03,\n",
            "           4.3923e-03, -9.8328e-03],\n",
            "         [-7.9601e-03,  1.0786e-02,  2.4790e-03,  ...,  9.5972e-04,\n",
            "          -4.0685e-03,  8.1527e-03],\n",
            "         [-5.3043e-03,  5.0939e-03,  1.1293e-03,  ...,  4.5711e-04,\n",
            "          -8.6858e-04,  3.9860e-03],\n",
            "         ...,\n",
            "         [-2.6965e-02,  3.6612e-02,  8.4166e-03,  ...,  3.2576e-03,\n",
            "          -1.3848e-02,  2.7670e-02],\n",
            "         [ 1.8336e-02, -2.3744e-02, -5.4356e-03,  ..., -2.1148e-03,\n",
            "           8.4030e-03, -1.8019e-02],\n",
            "         [-6.7862e-03,  1.0243e-02,  2.3750e-03,  ...,  9.0948e-04,\n",
            "          -4.3907e-03,  7.6744e-03]],\n",
            "\n",
            "        [[ 8.4359e-03, -9.4449e-03, -1.2546e-02,  ...,  4.6722e-03,\n",
            "           1.7232e-03, -5.3267e-03],\n",
            "         [-1.6371e-02,  1.7796e-02,  2.5998e-02,  ..., -1.1517e-02,\n",
            "          -2.9044e-03,  1.1471e-02],\n",
            "         [-7.4087e-03,  8.0196e-03,  1.1871e-02,  ..., -5.3696e-03,\n",
            "          -1.2861e-03,  5.2641e-03],\n",
            "         ...,\n",
            "         [ 1.2500e-02, -1.3422e-02, -2.0369e-02,  ...,  9.5635e-03,\n",
            "           2.0796e-03, -9.1150e-03],\n",
            "         [ 1.6972e-02, -1.8988e-02, -2.5283e-02,  ...,  9.4613e-03,\n",
            "           3.4557e-03, -1.0745e-02],\n",
            "         [-4.4111e-04,  8.6143e-06,  2.1591e-03,  ..., -2.4767e-03,\n",
            "           3.1059e-04,  1.3116e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2177, 0.1634, 0.1818,  ..., 0.1479, 0.2011, 0.0804],\n",
            "        [0.2162, 0.1629, 0.1812,  ..., 0.1472, 0.2005, 0.0798],\n",
            "        [0.2171, 0.1625, 0.1813,  ..., 0.1515, 0.2041, 0.0812],\n",
            "        ...,\n",
            "        [0.2153, 0.1628, 0.1810,  ..., 0.1469, 0.2001, 0.0795],\n",
            "        [0.2178, 0.1616, 0.1808,  ..., 0.1498, 0.2030, 0.0805],\n",
            "        [0.2156, 0.1622, 0.1806,  ..., 0.1504, 0.2036, 0.0812]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.0556e-02, -1.4889e-02, -3.9500e-03,  ..., -1.4849e-04,\n",
            "           9.8344e-03, -1.2052e-02],\n",
            "         [-7.1475e-03,  5.0580e-03,  2.5250e-03,  ...,  6.9579e-04,\n",
            "          -4.6581e-03,  5.7879e-03],\n",
            "         [-7.8852e-03,  6.2587e-03,  2.8058e-03,  ...,  6.8718e-04,\n",
            "          -5.4092e-03,  6.7058e-03],\n",
            "         ...,\n",
            "         [-2.8949e-02,  3.5667e-02,  1.0679e-02,  ...,  1.0192e-03,\n",
            "          -2.4913e-02,  3.0612e-02],\n",
            "         [ 1.5625e-02, -1.6577e-02, -5.6842e-03,  ..., -8.6698e-04,\n",
            "           1.2382e-02, -1.5260e-02],\n",
            "         [-6.6114e-03,  8.9556e-03,  2.4629e-03,  ...,  1.3678e-04,\n",
            "          -6.0123e-03,  7.3739e-03]],\n",
            "\n",
            "        [[ 4.8327e-03, -1.2812e-02, -3.4017e-03,  ..., -2.7934e-03,\n",
            "           6.1643e-03, -1.0576e-02],\n",
            "         [-2.1088e-03,  5.6222e-03,  1.5517e-03,  ...,  1.2759e-03,\n",
            "          -2.4864e-03,  5.2031e-03],\n",
            "         [-2.6505e-03,  7.0505e-03,  1.9164e-03,  ...,  1.5749e-03,\n",
            "          -3.2277e-03,  6.2431e-03],\n",
            "         ...,\n",
            "         [-1.3587e-02,  3.5971e-02,  9.4544e-03,  ...,  7.7612e-03,\n",
            "          -1.7663e-02,  2.8777e-02],\n",
            "         [ 8.4820e-03, -2.2531e-02, -6.0646e-03,  ..., -4.9824e-03,\n",
            "           1.0534e-02, -1.9385e-02],\n",
            "         [-4.7113e-03,  1.2517e-02,  3.3737e-03,  ...,  2.7718e-03,\n",
            "          -5.8357e-03,  1.0812e-02]],\n",
            "\n",
            "        [[ 4.6875e-03, -9.2675e-03, -1.6407e-02,  ...,  8.4118e-03,\n",
            "           2.9640e-03, -1.8937e-03],\n",
            "         [-8.1893e-03,  1.6348e-02,  2.7883e-02,  ..., -1.4270e-02,\n",
            "          -5.2424e-03,  2.6016e-03],\n",
            "         [-3.8441e-03,  7.7094e-03,  1.2913e-02,  ..., -6.6032e-03,\n",
            "          -2.4752e-03,  1.0625e-03],\n",
            "         ...,\n",
            "         [ 5.7218e-03, -1.1346e-02, -1.9860e-02,  ...,  1.0177e-02,\n",
            "           3.6318e-03, -2.1602e-03],\n",
            "         [ 7.1420e-03, -1.4672e-02, -2.2265e-02,  ...,  1.1327e-02,\n",
            "           4.7406e-03, -4.0978e-04],\n",
            "         [-6.5796e-04,  1.1092e-03,  3.2520e-03,  ..., -1.6979e-03,\n",
            "          -3.3811e-04,  1.1254e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.2864e-03, -2.4527e-03, -2.1634e-03,  ...,  2.9669e-04,\n",
            "           5.1354e-04,  1.1427e-06],\n",
            "         [ 2.5027e-03, -2.8478e-03, -2.3921e-03,  ...,  1.0605e-03,\n",
            "          -1.0929e-05,  8.8288e-04],\n",
            "         [ 4.6088e-04, -4.4215e-04, -4.2837e-04,  ..., -1.7585e-04,\n",
            "           2.8707e-04, -2.8217e-04],\n",
            "         ...,\n",
            "         [-1.1836e-03,  1.2498e-03,  1.1170e-03,  ..., -6.4312e-05,\n",
            "          -3.3537e-04,  1.0639e-04],\n",
            "         [ 2.2448e-03, -2.0232e-03, -2.0672e-03,  ..., -1.4445e-03,\n",
            "           1.8562e-03, -2.0790e-03],\n",
            "         [ 1.9783e-03, -1.8737e-03, -1.8352e-03,  ..., -8.6391e-04,\n",
            "           1.3172e-03, -1.3419e-03]],\n",
            "\n",
            "        [[ 1.0192e-02, -1.2793e-02, -2.9164e-03,  ..., -1.1389e-03,\n",
            "           4.3166e-03, -9.7328e-03],\n",
            "         [-7.9467e-03,  1.0748e-02,  2.4570e-03,  ...,  9.5214e-04,\n",
            "          -4.0512e-03,  8.1145e-03],\n",
            "         [-5.2578e-03,  4.9982e-03,  1.1251e-03,  ...,  4.5469e-04,\n",
            "          -8.0612e-04,  3.9312e-03],\n",
            "         ...,\n",
            "         [-2.6997e-02,  3.6659e-02,  8.3815e-03,  ...,  3.2468e-03,\n",
            "          -1.3893e-02,  2.7666e-02],\n",
            "         [ 1.8384e-02, -2.3845e-02, -5.4425e-03,  ..., -2.1181e-03,\n",
            "           8.4678e-03, -1.8078e-02],\n",
            "         [-6.9811e-03,  1.0672e-02,  2.4498e-03,  ...,  9.3852e-04,\n",
            "          -4.6503e-03,  7.9656e-03]],\n",
            "\n",
            "        [[ 8.5238e-03, -9.3579e-03, -1.2547e-02,  ...,  4.5597e-03,\n",
            "           1.7850e-03, -5.2976e-03],\n",
            "         [-1.7734e-02,  1.8320e-02,  2.7650e-02,  ..., -1.1875e-02,\n",
            "          -3.3275e-03,  1.2098e-02],\n",
            "         [-8.1082e-03,  8.3068e-03,  1.2736e-02,  ..., -5.5740e-03,\n",
            "          -1.4981e-03,  5.5967e-03],\n",
            "         ...,\n",
            "         [ 1.3308e-02, -1.3472e-02, -2.1120e-02,  ...,  9.4844e-03,\n",
            "           2.4045e-03, -9.3372e-03],\n",
            "         [ 1.7343e-02, -1.8985e-02, -2.5605e-02,  ...,  9.3944e-03,\n",
            "           3.6131e-03, -1.0832e-02],\n",
            "         [-1.5827e-03,  6.8218e-04,  3.7498e-03,  ..., -3.0405e-03,\n",
            "           2.3217e-05,  1.9725e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2127, 0.1603, 0.1798,  ..., 0.1472, 0.2005, 0.0820],\n",
            "        [0.2136, 0.1629, 0.1809,  ..., 0.1462, 0.1995, 0.0791],\n",
            "        [0.2182, 0.1627, 0.1818,  ..., 0.1523, 0.2049, 0.0811],\n",
            "        ...,\n",
            "        [0.2170, 0.1611, 0.1800,  ..., 0.1493, 0.2019, 0.0814],\n",
            "        [0.2125, 0.1630, 0.1810,  ..., 0.1459, 0.1995, 0.0788],\n",
            "        [0.2178, 0.1626, 0.1816,  ..., 0.1520, 0.2046, 0.0812]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.0381e-02, -1.4436e-02, -3.5591e-03,  ..., -5.9071e-04,\n",
            "           9.2989e-03, -1.1970e-02],\n",
            "         [-7.3362e-03,  5.5173e-03,  2.6976e-03,  ...,  5.4868e-04,\n",
            "          -4.9984e-03,  6.0296e-03],\n",
            "         [-7.3726e-03,  5.0533e-03,  2.7301e-03,  ...,  5.6516e-04,\n",
            "          -4.8582e-03,  5.8047e-03],\n",
            "         ...,\n",
            "         [-2.7615e-02,  3.2445e-02,  9.7002e-03,  ...,  1.7384e-03,\n",
            "          -2.2737e-02,  2.8753e-02],\n",
            "         [ 1.5469e-02, -1.6192e-02, -5.5106e-03,  ..., -1.0292e-03,\n",
            "           1.2070e-02, -1.5078e-02],\n",
            "         [-6.2965e-03,  8.1893e-03,  2.1809e-03,  ...,  3.7418e-04,\n",
            "          -5.4499e-03,  6.9663e-03]],\n",
            "\n",
            "        [[ 5.4591e-03, -1.4447e-02, -3.7944e-03,  ..., -3.0998e-03,\n",
            "           7.0466e-03, -1.1689e-02],\n",
            "         [-2.3239e-03,  6.2370e-03,  1.7655e-03,  ...,  1.5262e-03,\n",
            "          -2.8769e-03,  5.4231e-03],\n",
            "         [-2.1755e-03,  5.8678e-03,  1.7029e-03,  ...,  1.4976e-03,\n",
            "          -2.6523e-03,  5.2260e-03],\n",
            "         ...,\n",
            "         [-1.3031e-02,  3.4482e-02,  9.0543e-03,  ...,  7.3952e-03,\n",
            "          -1.6822e-02,  2.7892e-02],\n",
            "         [ 8.2531e-03, -2.2035e-02, -6.0716e-03,  ..., -5.1472e-03,\n",
            "           1.0379e-02, -1.8669e-02],\n",
            "         [-4.2175e-03,  1.1297e-02,  3.1654e-03,  ...,  2.7166e-03,\n",
            "          -5.2528e-03,  9.7270e-03]],\n",
            "\n",
            "        [[ 4.4698e-03, -9.1032e-03, -1.6499e-02,  ...,  7.9747e-03,\n",
            "           2.5373e-03, -2.0280e-03],\n",
            "         [-8.3609e-03,  1.7041e-02,  3.0527e-02,  ..., -1.4798e-02,\n",
            "          -4.8010e-03,  3.5591e-03],\n",
            "         [-3.5092e-03,  7.1919e-03,  1.1777e-02,  ..., -5.8446e-03,\n",
            "          -2.1847e-03,  7.6877e-04],\n",
            "         ...,\n",
            "         [ 5.3250e-03, -1.0864e-02, -1.9147e-02,  ...,  9.3206e-03,\n",
            "           3.1060e-03, -2.0604e-03],\n",
            "         [ 7.2660e-03, -1.4942e-02, -2.3062e-02,  ...,  1.1633e-02,\n",
            "           4.7402e-03, -6.6599e-04],\n",
            "         [-7.7584e-04,  1.5096e-03,  4.7053e-03,  ..., -2.0361e-03,\n",
            "          -1.3873e-04,  1.6412e-03]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.7029e-03, -2.6133e-03, -2.4746e-03,  ...,  3.6880e-04,\n",
            "           6.5051e-04, -1.0967e-04],\n",
            "         [ 3.1714e-03, -2.9997e-03, -2.8610e-03,  ...,  1.1890e-03,\n",
            "           2.2059e-04,  6.7051e-04],\n",
            "         [-2.0991e-04,  1.8367e-04,  1.7984e-04,  ..., -2.4769e-04,\n",
            "           1.0666e-04, -2.2295e-04],\n",
            "         ...,\n",
            "         [-6.0946e-04,  5.9507e-04,  5.6170e-04,  ..., -1.7208e-05,\n",
            "          -1.9400e-04,  9.4421e-05],\n",
            "         [ 1.2371e-03, -1.3505e-03, -1.2314e-03,  ..., -1.5847e-03,\n",
            "           1.5559e-03, -1.9031e-03],\n",
            "         [ 2.1433e-03, -2.1740e-03, -2.0274e-03,  ..., -8.6391e-04,\n",
            "           1.3456e-03, -1.3089e-03]],\n",
            "\n",
            "        [[ 8.7837e-03, -2.4454e-02, -4.6522e-04,  ..., -1.7367e-03,\n",
            "           1.2229e-02, -1.8829e-02],\n",
            "         [-7.2944e-03,  5.0737e-03,  2.5951e-03,  ...,  5.7767e-04,\n",
            "          -4.2778e-04,  3.8199e-03],\n",
            "         [-6.5628e-03,  8.9244e-03,  1.7027e-03,  ...,  7.6715e-04,\n",
            "          -3.1686e-03,  6.8185e-03],\n",
            "         ...,\n",
            "         [-2.1907e-02,  4.7308e-02,  3.1440e-03,  ...,  3.5550e-03,\n",
            "          -2.1763e-02,  3.6349e-02],\n",
            "         [ 1.6589e-02, -2.6856e-02, -3.6809e-03,  ..., -2.1830e-03,\n",
            "           1.0753e-02, -2.0569e-02],\n",
            "         [-3.4289e-03,  2.2983e-02, -1.7665e-03,  ...,  1.4405e-03,\n",
            "          -1.3354e-02,  1.7773e-02]],\n",
            "\n",
            "        [[ 8.2176e-03, -9.0393e-03, -1.2201e-02,  ...,  4.2722e-03,\n",
            "           1.8322e-03, -5.1105e-03],\n",
            "         [-1.5675e-02,  1.6248e-02,  2.4956e-02,  ..., -9.9812e-03,\n",
            "          -3.4438e-03,  1.0732e-02],\n",
            "         [-7.2920e-03,  7.4665e-03,  1.1765e-02,  ..., -4.8127e-03,\n",
            "          -1.5973e-03,  5.0836e-03],\n",
            "         ...,\n",
            "         [ 1.2336e-02, -1.2413e-02, -2.0273e-02,  ...,  8.5446e-03,\n",
            "           2.6910e-03, -8.8164e-03],\n",
            "         [ 1.7060e-02, -1.8640e-02, -2.5542e-02,  ...,  9.1006e-03,\n",
            "           3.7972e-03, -1.0734e-02],\n",
            "         [-1.1425e-03,  1.6970e-04,  3.5353e-03,  ..., -2.5968e-03,\n",
            "          -1.9896e-04,  1.7862e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.2123, 0.1630, 0.1811,  ..., 0.1458, 0.1995, 0.0788],\n",
            "        [0.2139, 0.1655, 0.1838,  ..., 0.1448, 0.2003, 0.0798],\n",
            "        [0.2205, 0.1634, 0.1832,  ..., 0.1542, 0.2070, 0.0809],\n",
            "        ...,\n",
            "        [0.2125, 0.1630, 0.1810,  ..., 0.1459, 0.1995, 0.0788],\n",
            "        [0.2145, 0.1620, 0.1803,  ..., 0.1496, 0.2032, 0.0812],\n",
            "        [0.2140, 0.1658, 0.1822,  ..., 0.1456, 0.2017, 0.0786]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "repr, std, cov 0.12200552970170975 0.4881178140640259 0.0019524479284882545\n",
            "3 #### train ####\n",
            "train jepa world_state_ tensor([[[-8.0358e-04,  1.6686e-03,  9.6781e-04,  ...,  2.7888e-04,\n",
            "          -7.1258e-05, -2.0395e-04],\n",
            "         [ 1.6877e-03, -3.5044e-03, -2.0326e-03,  ..., -5.8571e-04,\n",
            "           1.4966e-04,  4.2834e-04],\n",
            "         [-6.6594e-03,  1.3828e-02,  8.0204e-03,  ...,  2.3111e-03,\n",
            "          -5.9053e-04, -1.6901e-03],\n",
            "         ...,\n",
            "         [ 4.5659e-05, -9.4808e-05, -5.4990e-05,  ..., -1.5846e-05,\n",
            "           4.0488e-06,  1.1588e-05],\n",
            "         [-7.4391e-04,  1.5447e-03,  8.9594e-04,  ...,  2.5817e-04,\n",
            "          -6.5967e-05, -1.8880e-04],\n",
            "         [ 1.4228e-03, -2.9544e-03, -1.7136e-03,  ..., -4.9378e-04,\n",
            "           1.2617e-04,  3.6111e-04]],\n",
            "\n",
            "        [[ 7.6752e-04, -8.5788e-04, -8.6903e-04,  ..., -3.8332e-05,\n",
            "           3.7415e-04, -1.0097e-04],\n",
            "         [ 1.1398e-04, -1.2739e-04, -1.2905e-04,  ..., -5.6923e-06,\n",
            "           5.5561e-05, -1.4994e-05],\n",
            "         [-1.3474e-02,  1.5060e-02,  1.5255e-02,  ...,  6.7291e-04,\n",
            "          -6.5680e-03,  1.7726e-03],\n",
            "         ...,\n",
            "         [-1.1100e-04,  1.2406e-04,  1.2567e-04,  ...,  5.5434e-06,\n",
            "          -5.4108e-05,  1.4602e-05],\n",
            "         [ 6.8638e-03, -7.6718e-03, -7.7716e-03,  ..., -3.4280e-04,\n",
            "           3.3460e-03, -9.0299e-04],\n",
            "         [-1.8178e-03,  2.0318e-03,  2.0582e-03,  ...,  9.0788e-05,\n",
            "          -8.8615e-04,  2.3915e-04]],\n",
            "\n",
            "        [[ 5.5703e-04, -5.8526e-04, -8.4630e-04,  ...,  2.5255e-04,\n",
            "           2.6088e-04, -3.8703e-05],\n",
            "         [-6.9618e-04,  7.3145e-04,  1.0577e-03,  ..., -3.1563e-04,\n",
            "          -3.2605e-04,  4.8372e-05],\n",
            "         [-1.7569e-02,  1.8459e-02,  2.6693e-02,  ..., -7.9654e-03,\n",
            "          -8.2283e-03,  1.2207e-03],\n",
            "         ...,\n",
            "         [-1.7861e-05,  1.8766e-05,  2.7136e-05,  ..., -8.0978e-06,\n",
            "          -8.3650e-06,  1.2410e-06],\n",
            "         [ 4.4338e-03, -4.6585e-03, -6.7363e-03,  ...,  2.0102e-03,\n",
            "           2.0765e-03, -3.0807e-04],\n",
            "         [ 3.5406e-03, -3.7200e-03, -5.3792e-03,  ...,  1.6052e-03,\n",
            "           1.6582e-03, -2.4601e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 6.5305e-03, -8.0567e-03, -1.3589e-02,  ...,  2.8016e-03,\n",
            "           3.3949e-03, -6.2245e-03],\n",
            "         [-9.3055e-03,  1.1480e-02,  1.9363e-02,  ..., -3.9921e-03,\n",
            "          -4.8375e-03,  8.8695e-03],\n",
            "         [-3.9850e-03,  4.9162e-03,  8.2919e-03,  ..., -1.7096e-03,\n",
            "          -2.0716e-03,  3.7983e-03],\n",
            "         ...,\n",
            "         [ 5.7988e-03, -7.1540e-03, -1.2066e-02,  ...,  2.4877e-03,\n",
            "           3.0145e-03, -5.5271e-03],\n",
            "         [ 9.5611e-03, -1.1796e-02, -1.9895e-02,  ...,  4.1017e-03,\n",
            "           4.9704e-03, -9.1132e-03],\n",
            "         [-1.2975e-03,  1.6007e-03,  2.6998e-03,  ..., -5.5663e-04,\n",
            "          -6.7452e-04,  1.2367e-03]],\n",
            "\n",
            "        [[ 6.9229e-03, -9.1774e-03, -1.1412e-02,  ...,  2.7109e-03,\n",
            "           2.4683e-03, -4.4302e-03],\n",
            "         [-1.2646e-02,  1.6764e-02,  2.0846e-02,  ..., -4.9519e-03,\n",
            "          -4.5087e-03,  8.0923e-03],\n",
            "         [-2.8706e-03,  3.8054e-03,  4.7320e-03,  ..., -1.1241e-03,\n",
            "          -1.0235e-03,  1.8370e-03],\n",
            "         ...,\n",
            "         [ 5.4796e-03, -7.2641e-03, -9.0328e-03,  ...,  2.1457e-03,\n",
            "           1.9537e-03, -3.5066e-03],\n",
            "         [ 1.0415e-02, -1.3807e-02, -1.7169e-02,  ...,  4.0785e-03,\n",
            "           3.7135e-03, -6.6650e-03],\n",
            "         [-1.1696e-03,  1.5505e-03,  1.9280e-03,  ..., -4.5800e-04,\n",
            "          -4.1701e-04,  7.4846e-04]],\n",
            "\n",
            "        [[ 4.2997e-03, -6.7490e-03, -6.6492e-03,  ..., -1.8337e-03,\n",
            "          -6.5307e-04, -2.3596e-03],\n",
            "         [-7.2468e-03,  1.1375e-02,  1.1207e-02,  ...,  3.0906e-03,\n",
            "           1.1007e-03,  3.9769e-03],\n",
            "         [-4.3004e-03,  6.7501e-03,  6.6503e-03,  ...,  1.8340e-03,\n",
            "           6.5318e-04,  2.3600e-03],\n",
            "         ...,\n",
            "         [ 8.0472e-03, -1.2631e-02, -1.2444e-02,  ..., -3.4319e-03,\n",
            "          -1.2223e-03, -4.4161e-03],\n",
            "         [ 1.0531e-02, -1.6530e-02, -1.6286e-02,  ..., -4.4914e-03,\n",
            "          -1.5996e-03, -5.7795e-03],\n",
            "         [-3.6888e-03,  5.7900e-03,  5.7044e-03,  ...,  1.5732e-03,\n",
            "           5.6027e-04,  2.0243e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1897, 0.1302, 0.1847,  ..., 0.0884, 0.1891, 0.0521],\n",
            "        [0.1931, 0.1341, 0.1871,  ..., 0.0888, 0.1897, 0.0529],\n",
            "        [0.1943, 0.1305, 0.1868,  ..., 0.0918, 0.1945, 0.0518],\n",
            "        ...,\n",
            "        [0.1874, 0.1308, 0.1838,  ..., 0.0897, 0.1911, 0.0532],\n",
            "        [0.1883, 0.1310, 0.1844,  ..., 0.0906, 0.1914, 0.0531],\n",
            "        [0.1906, 0.1299, 0.1849,  ..., 0.0895, 0.1897, 0.0536]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[-5.9272e-04,  1.2145e-03,  6.9336e-04,  ...,  2.0264e-04,\n",
            "          -5.1093e-05, -1.3956e-04],\n",
            "         [ 1.9367e-03, -3.9684e-03, -2.2656e-03,  ..., -6.6212e-04,\n",
            "           1.6695e-04,  4.5603e-04],\n",
            "         [-6.4810e-03,  1.3280e-02,  7.5815e-03,  ...,  2.2157e-03,\n",
            "          -5.5866e-04, -1.5260e-03],\n",
            "         ...,\n",
            "         [ 1.4791e-04, -3.0307e-04, -1.7302e-04,  ..., -5.0566e-05,\n",
            "           1.2750e-05,  3.4827e-05],\n",
            "         [-7.6520e-04,  1.5679e-03,  8.9513e-04,  ...,  2.6160e-04,\n",
            "          -6.5960e-05, -1.8018e-04],\n",
            "         [ 1.6051e-03, -3.2889e-03, -1.8776e-03,  ..., -5.4874e-04,\n",
            "           1.3836e-04,  3.7794e-04]],\n",
            "\n",
            "        [[-5.3689e-05,  6.1972e-05,  5.7226e-05,  ...,  5.9085e-07,\n",
            "          -2.6098e-05,  4.5387e-06],\n",
            "         [ 7.5341e-04, -8.6965e-04, -8.0304e-04,  ..., -8.2914e-06,\n",
            "           3.6622e-04, -6.3690e-05],\n",
            "         [-1.2917e-02,  1.4910e-02,  1.3768e-02,  ...,  1.4216e-04,\n",
            "          -6.2790e-03,  1.0920e-03],\n",
            "         ...,\n",
            "         [ 1.1901e-04, -1.3738e-04, -1.2686e-04,  ..., -1.3098e-06,\n",
            "           5.7852e-05, -1.0061e-05],\n",
            "         [ 5.1609e-03, -5.9572e-03, -5.5010e-03,  ..., -5.6797e-05,\n",
            "           2.5087e-03, -4.3629e-04],\n",
            "         [-2.0640e-03,  2.3825e-03,  2.2000e-03,  ...,  2.2715e-05,\n",
            "          -1.0033e-03,  1.7449e-04]],\n",
            "\n",
            "        [[ 1.9487e-03, -1.7105e-03, -2.6737e-03,  ...,  7.3109e-04,\n",
            "           6.4832e-04, -1.2891e-04],\n",
            "         [-3.7548e-04,  3.2959e-04,  5.1518e-04,  ..., -1.4087e-04,\n",
            "          -1.2492e-04,  2.4838e-05],\n",
            "         [-1.8876e-02,  1.6569e-02,  2.5899e-02,  ..., -7.0817e-03,\n",
            "          -6.2800e-03,  1.2487e-03],\n",
            "         ...,\n",
            "         [-3.2773e-04,  2.8768e-04,  4.4967e-04,  ..., -1.2296e-04,\n",
            "          -1.0904e-04,  2.1680e-05],\n",
            "         [ 3.1970e-03, -2.8063e-03, -4.3866e-03,  ...,  1.1994e-03,\n",
            "           1.0636e-03, -2.1149e-04],\n",
            "         [ 5.1301e-03, -4.5031e-03, -7.0389e-03,  ...,  1.9247e-03,\n",
            "           1.7068e-03, -3.3936e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 7.6309e-03, -8.2600e-03, -1.4166e-02,  ...,  3.5790e-03,\n",
            "           3.5829e-03, -5.9899e-03],\n",
            "         [-1.0761e-02,  1.1648e-02,  1.9976e-02,  ..., -5.0472e-03,\n",
            "          -5.0526e-03,  8.4470e-03],\n",
            "         [-4.3381e-03,  4.6957e-03,  8.0529e-03,  ..., -2.0346e-03,\n",
            "          -2.0368e-03,  3.4052e-03],\n",
            "         ...,\n",
            "         [ 7.1151e-03, -7.7016e-03, -1.3208e-02,  ...,  3.3371e-03,\n",
            "           3.3407e-03, -5.5850e-03],\n",
            "         [ 1.0756e-02, -1.1643e-02, -1.9966e-02,  ...,  5.0447e-03,\n",
            "           5.0501e-03, -8.4428e-03],\n",
            "         [-1.2670e-03,  1.3715e-03,  2.3520e-03,  ..., -5.9426e-04,\n",
            "          -5.9490e-04,  9.9456e-04]],\n",
            "\n",
            "        [[ 7.4126e-03, -9.3739e-03, -1.0753e-02,  ...,  3.1916e-03,\n",
            "           2.2652e-03, -4.0732e-03],\n",
            "         [-1.2558e-02,  1.5881e-02,  1.8217e-02,  ..., -5.4071e-03,\n",
            "          -3.8376e-03,  6.9007e-03],\n",
            "         [-2.9204e-03,  3.6931e-03,  4.2364e-03,  ..., -1.2574e-03,\n",
            "          -8.9242e-04,  1.6047e-03],\n",
            "         ...,\n",
            "         [ 5.8937e-03, -7.4532e-03, -8.5496e-03,  ...,  2.5376e-03,\n",
            "           1.8010e-03, -3.2386e-03],\n",
            "         [ 1.1451e-02, -1.4481e-02, -1.6611e-02,  ...,  4.9305e-03,\n",
            "           3.4993e-03, -6.2924e-03],\n",
            "         [-1.4862e-03,  1.8794e-03,  2.1559e-03,  ..., -6.3990e-04,\n",
            "          -4.5415e-04,  8.1665e-04]],\n",
            "\n",
            "        [[ 4.4233e-03, -7.3389e-03, -6.9226e-03,  ..., -1.7550e-03,\n",
            "          -5.3386e-04, -2.4027e-03],\n",
            "         [-6.9321e-03,  1.1501e-02,  1.0849e-02,  ...,  2.7504e-03,\n",
            "           8.3666e-04,  3.7655e-03],\n",
            "         [-4.6939e-03,  7.7879e-03,  7.3462e-03,  ...,  1.8624e-03,\n",
            "           5.6652e-04,  2.5497e-03],\n",
            "         ...,\n",
            "         [ 7.7626e-03, -1.2879e-02, -1.2149e-02,  ..., -3.0799e-03,\n",
            "          -9.3690e-04, -4.2166e-03],\n",
            "         [ 1.0436e-02, -1.7315e-02, -1.6333e-02,  ..., -4.1406e-03,\n",
            "          -1.2595e-03, -5.6688e-03],\n",
            "         [-4.1292e-03,  6.8510e-03,  6.4625e-03,  ...,  1.6383e-03,\n",
            "           4.9837e-04,  2.2430e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1868, 0.1310, 0.1836,  ..., 0.0858, 0.1861, 0.0523],\n",
            "        [0.1922, 0.1301, 0.1855,  ..., 0.0903, 0.1911, 0.0532],\n",
            "        [0.1939, 0.1305, 0.1865,  ..., 0.0915, 0.1939, 0.0517],\n",
            "        ...,\n",
            "        [0.1919, 0.1319, 0.1865,  ..., 0.0935, 0.1943, 0.0525],\n",
            "        [0.1905, 0.1318, 0.1856,  ..., 0.0925, 0.1932, 0.0526],\n",
            "        [0.1899, 0.1297, 0.1848,  ..., 0.0891, 0.1892, 0.0538]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[-8.6379e-04,  1.7807e-03,  1.0091e-03,  ...,  2.7409e-04,\n",
            "          -6.5432e-05, -2.1239e-04],\n",
            "         [ 2.0676e-03, -4.2622e-03, -2.4153e-03,  ..., -6.5606e-04,\n",
            "           1.5662e-04,  5.0838e-04],\n",
            "         [-6.9059e-03,  1.4236e-02,  8.0673e-03,  ...,  2.1913e-03,\n",
            "          -5.2312e-04, -1.6981e-03],\n",
            "         ...,\n",
            "         [ 3.4007e-05, -7.0104e-05, -3.9727e-05,  ..., -1.0791e-05,\n",
            "           2.5761e-06,  8.3619e-06],\n",
            "         [-3.9279e-04,  8.0973e-04,  4.5885e-04,  ...,  1.2464e-04,\n",
            "          -2.9754e-05, -9.6582e-05],\n",
            "         [ 1.8831e-03, -3.8819e-03, -2.1998e-03,  ..., -5.9753e-04,\n",
            "           1.4265e-04,  4.6303e-04]],\n",
            "\n",
            "        [[ 2.6894e-04, -3.2162e-04, -2.9578e-04,  ...,  1.6805e-05,\n",
            "           1.3748e-04,  6.9758e-06],\n",
            "         [ 1.5516e-03, -1.8555e-03, -1.7065e-03,  ...,  9.6956e-05,\n",
            "           7.9316e-04,  4.0246e-05],\n",
            "         [-1.2595e-02,  1.5061e-02,  1.3852e-02,  ..., -7.8700e-04,\n",
            "          -6.4382e-03, -3.2668e-04],\n",
            "         ...,\n",
            "         [-4.2174e-04,  5.0435e-04,  4.6384e-04,  ..., -2.6354e-05,\n",
            "          -2.1559e-04, -1.0939e-05],\n",
            "         [ 4.8451e-03, -5.7941e-03, -5.3287e-03,  ...,  3.0276e-04,\n",
            "           2.4768e-03,  1.2567e-04],\n",
            "         [-6.1521e-04,  7.3571e-04,  6.7662e-04,  ..., -3.8443e-05,\n",
            "          -3.1449e-04, -1.5957e-05]],\n",
            "\n",
            "        [[ 2.1099e-03, -1.8080e-03, -2.8131e-03,  ...,  7.7605e-04,\n",
            "           7.7101e-04, -2.7049e-04],\n",
            "         [-8.3124e-04,  7.1228e-04,  1.1083e-03,  ..., -3.0573e-04,\n",
            "          -3.0375e-04,  1.0656e-04],\n",
            "         [-1.9231e-02,  1.6479e-02,  2.5640e-02,  ..., -7.0734e-03,\n",
            "          -7.0275e-03,  2.4654e-03],\n",
            "         ...,\n",
            "         [-3.7452e-04,  3.2092e-04,  4.9933e-04,  ..., -1.3775e-04,\n",
            "          -1.3686e-04,  4.8012e-05],\n",
            "         [ 3.9764e-03, -3.4074e-03, -5.3016e-03,  ...,  1.4625e-03,\n",
            "           1.4530e-03, -5.0976e-04],\n",
            "         [ 4.6131e-03, -3.9530e-03, -6.1505e-03,  ...,  1.6967e-03,\n",
            "           1.6857e-03, -5.9138e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 7.0515e-03, -8.4620e-03, -1.5592e-02,  ...,  4.2002e-03,\n",
            "           3.3146e-03, -5.3696e-03],\n",
            "         [-9.0505e-03,  1.0861e-02,  2.0013e-02,  ..., -5.3908e-03,\n",
            "          -4.2543e-03,  6.8918e-03],\n",
            "         [-3.5675e-03,  4.2811e-03,  7.8887e-03,  ..., -2.1250e-03,\n",
            "          -1.6770e-03,  2.7166e-03],\n",
            "         ...,\n",
            "         [ 6.3388e-03, -7.6067e-03, -1.4017e-02,  ...,  3.7757e-03,\n",
            "           2.9796e-03, -4.8269e-03],\n",
            "         [ 8.9383e-03, -1.0726e-02, -1.9765e-02,  ...,  5.3241e-03,\n",
            "           4.2016e-03, -6.8064e-03],\n",
            "         [-1.3321e-03,  1.5986e-03,  2.9456e-03,  ..., -7.9347e-04,\n",
            "          -6.2618e-04,  1.0144e-03]],\n",
            "\n",
            "        [[ 7.9314e-03, -9.8936e-03, -1.1550e-02,  ...,  2.5436e-03,\n",
            "           2.3141e-03, -4.3729e-03],\n",
            "         [-1.2856e-02,  1.6036e-02,  1.8721e-02,  ..., -4.1229e-03,\n",
            "          -3.7509e-03,  7.0879e-03],\n",
            "         [-2.0936e-03,  2.6116e-03,  3.0488e-03,  ..., -6.7143e-04,\n",
            "          -6.1085e-04,  1.1543e-03],\n",
            "         ...,\n",
            "         [ 5.9415e-03, -7.4113e-03, -8.6520e-03,  ...,  1.9054e-03,\n",
            "           1.7335e-03, -3.2757e-03],\n",
            "         [ 1.1769e-02, -1.4681e-02, -1.7138e-02,  ...,  3.7744e-03,\n",
            "           3.4338e-03, -6.4888e-03],\n",
            "         [-1.1190e-03,  1.3958e-03,  1.6295e-03,  ..., -3.5887e-04,\n",
            "          -3.2649e-04,  6.1695e-04]],\n",
            "\n",
            "        [[ 4.3582e-03, -6.6013e-03, -6.7759e-03,  ..., -1.5351e-03,\n",
            "          -6.7280e-04, -2.6901e-03],\n",
            "         [-6.7810e-03,  1.0271e-02,  1.0543e-02,  ...,  2.3885e-03,\n",
            "           1.0468e-03,  4.1855e-03],\n",
            "         [-5.7075e-03,  8.6451e-03,  8.8736e-03,  ...,  2.0104e-03,\n",
            "           8.8109e-04,  3.5229e-03],\n",
            "         ...,\n",
            "         [ 7.8541e-03, -1.1896e-02, -1.2211e-02,  ..., -2.7664e-03,\n",
            "          -1.2125e-03, -4.8478e-03],\n",
            "         [ 1.0088e-02, -1.5280e-02, -1.5684e-02,  ..., -3.5533e-03,\n",
            "          -1.5573e-03, -6.2268e-03],\n",
            "         [-3.9441e-03,  5.9741e-03,  6.1320e-03,  ...,  1.3892e-03,\n",
            "           6.0887e-04,  2.4345e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1906, 0.1304, 0.1851,  ..., 0.0889, 0.1898, 0.0518],\n",
            "        [0.1877, 0.1311, 0.1841,  ..., 0.0858, 0.1866, 0.0523],\n",
            "        [0.1943, 0.1304, 0.1871,  ..., 0.0916, 0.1934, 0.0530],\n",
            "        ...,\n",
            "        [0.1882, 0.1310, 0.1843,  ..., 0.0905, 0.1914, 0.0531],\n",
            "        [0.1877, 0.1309, 0.1841,  ..., 0.0900, 0.1912, 0.0531],\n",
            "        [0.1930, 0.1301, 0.1859,  ..., 0.0907, 0.1917, 0.0531]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[-4.9264e-04,  1.0283e-03,  5.9761e-04,  ...,  1.5754e-04,\n",
            "          -7.0106e-05, -1.3596e-04],\n",
            "         [ 1.9730e-03, -4.1184e-03, -2.3935e-03,  ..., -6.3094e-04,\n",
            "           2.8078e-04,  5.4453e-04],\n",
            "         [-6.7095e-03,  1.4005e-02,  8.1391e-03,  ...,  2.1456e-03,\n",
            "          -9.5482e-04, -1.8517e-03],\n",
            "         ...,\n",
            "         [ 1.7063e-04, -3.5616e-04, -2.0699e-04,  ..., -5.4563e-05,\n",
            "           2.4282e-05,  4.7091e-05],\n",
            "         [-6.7396e-04,  1.4068e-03,  8.1757e-04,  ...,  2.1552e-04,\n",
            "          -9.5910e-05, -1.8600e-04],\n",
            "         [ 1.6631e-03, -3.4714e-03, -2.0174e-03,  ..., -5.3181e-04,\n",
            "           2.3667e-04,  4.5898e-04]],\n",
            "\n",
            "        [[ 6.5463e-04, -7.2912e-04, -7.0457e-04,  ...,  4.0964e-05,\n",
            "           2.9082e-04,  2.8678e-05],\n",
            "         [ 1.5380e-03, -1.7130e-03, -1.6553e-03,  ...,  9.6240e-05,\n",
            "           6.8323e-04,  6.7374e-05],\n",
            "         [-1.4713e-02,  1.6387e-02,  1.5835e-02,  ..., -9.2068e-04,\n",
            "          -6.5362e-03, -6.4454e-04],\n",
            "         ...,\n",
            "         [-3.8071e-04,  4.2403e-04,  4.0976e-04,  ..., -2.3824e-05,\n",
            "          -1.6913e-04, -1.6678e-05],\n",
            "         [ 5.8621e-03, -6.5291e-03, -6.3094e-03,  ...,  3.6683e-04,\n",
            "           2.6042e-03,  2.5681e-04],\n",
            "         [-9.5438e-04,  1.0630e-03,  1.0272e-03,  ..., -5.9721e-05,\n",
            "          -4.2398e-04, -4.1809e-05]],\n",
            "\n",
            "        [[ 1.6422e-03, -1.4732e-03, -2.3084e-03,  ...,  5.5264e-04,\n",
            "           6.4927e-04, -2.5774e-04],\n",
            "         [-8.6518e-04,  7.7614e-04,  1.2162e-03,  ..., -2.9116e-04,\n",
            "          -3.4207e-04,  1.3579e-04],\n",
            "         [-1.8360e-02,  1.6471e-02,  2.5809e-02,  ..., -6.1788e-03,\n",
            "          -7.2592e-03,  2.8816e-03],\n",
            "         ...,\n",
            "         [-9.3058e-04,  8.3481e-04,  1.3081e-03,  ..., -3.1317e-04,\n",
            "          -3.6793e-04,  1.4605e-04],\n",
            "         [ 3.3181e-03, -2.9766e-03, -4.6642e-03,  ...,  1.1166e-03,\n",
            "           1.3119e-03, -5.2077e-04],\n",
            "         [ 6.3363e-03, -5.6842e-03, -8.9069e-03,  ...,  2.1324e-03,\n",
            "           2.5052e-03, -9.9448e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 5.9608e-03, -7.8622e-03, -1.4132e-02,  ...,  3.3773e-03,\n",
            "           2.9045e-03, -5.5018e-03],\n",
            "         [-8.0214e-03,  1.0580e-02,  1.9018e-02,  ..., -4.5448e-03,\n",
            "          -3.9086e-03,  7.4037e-03],\n",
            "         [-3.7236e-03,  4.9113e-03,  8.8280e-03,  ..., -2.1097e-03,\n",
            "          -1.8144e-03,  3.4368e-03],\n",
            "         ...,\n",
            "         [ 5.6166e-03, -7.4081e-03, -1.3316e-02,  ...,  3.1822e-03,\n",
            "           2.7368e-03, -5.1840e-03],\n",
            "         [ 7.9135e-03, -1.0438e-02, -1.8762e-02,  ...,  4.4836e-03,\n",
            "           3.8560e-03, -7.3041e-03],\n",
            "         [-1.2354e-03,  1.6295e-03,  2.9289e-03,  ..., -6.9994e-04,\n",
            "          -6.0197e-04,  1.1403e-03]],\n",
            "\n",
            "        [[ 8.5360e-03, -9.0879e-03, -1.0291e-02,  ...,  2.1489e-03,\n",
            "           2.1370e-03, -4.9093e-03],\n",
            "         [-1.2294e-02,  1.3089e-02,  1.4821e-02,  ..., -3.0949e-03,\n",
            "          -3.0778e-03,  7.0705e-03],\n",
            "         [-2.8919e-03,  3.0788e-03,  3.4863e-03,  ..., -7.2800e-04,\n",
            "          -7.2398e-04,  1.6632e-03],\n",
            "         ...,\n",
            "         [ 5.8903e-03, -6.2712e-03, -7.1012e-03,  ...,  1.4828e-03,\n",
            "           1.4746e-03, -3.3877e-03],\n",
            "         [ 1.3084e-02, -1.3930e-02, -1.5774e-02,  ...,  3.2939e-03,\n",
            "           3.2756e-03, -7.5251e-03],\n",
            "         [-1.9896e-03,  2.1182e-03,  2.3986e-03,  ..., -5.0086e-04,\n",
            "          -4.9809e-04,  1.1443e-03]],\n",
            "\n",
            "        [[ 4.3353e-03, -6.4134e-03, -7.0004e-03,  ..., -8.5061e-04,\n",
            "          -2.3825e-04, -3.1122e-03],\n",
            "         [-8.0861e-03,  1.1962e-02,  1.3057e-02,  ...,  1.5865e-03,\n",
            "           4.4437e-04,  5.8047e-03],\n",
            "         [-5.5722e-03,  8.2433e-03,  8.9978e-03,  ...,  1.0933e-03,\n",
            "           3.0623e-04,  4.0001e-03],\n",
            "         ...,\n",
            "         [ 7.3284e-03, -1.0841e-02, -1.1834e-02,  ..., -1.4379e-03,\n",
            "          -4.0274e-04, -5.2608e-03],\n",
            "         [ 1.1161e-02, -1.6511e-02, -1.8022e-02,  ..., -2.1898e-03,\n",
            "          -6.1335e-04, -8.0120e-03],\n",
            "         [-4.0984e-03,  6.0629e-03,  6.6179e-03,  ...,  8.0412e-04,\n",
            "           2.2523e-04,  2.9421e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1886, 0.1312, 0.1844,  ..., 0.0859, 0.1867, 0.0524],\n",
            "        [0.1916, 0.1305, 0.1855,  ..., 0.0897, 0.1908, 0.0516],\n",
            "        [0.1904, 0.1298, 0.1848,  ..., 0.0893, 0.1895, 0.0537],\n",
            "        ...,\n",
            "        [0.1924, 0.1319, 0.1870,  ..., 0.0939, 0.1947, 0.0524],\n",
            "        [0.1880, 0.1310, 0.1842,  ..., 0.0903, 0.1913, 0.0531],\n",
            "        [0.1941, 0.1303, 0.1868,  ..., 0.0914, 0.1930, 0.0530]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[-7.4265e-04,  1.6054e-03,  9.4408e-04,  ...,  2.5967e-04,\n",
            "          -5.9274e-05, -2.2389e-04],\n",
            "         [ 2.2743e-03, -4.9163e-03, -2.8911e-03,  ..., -7.9522e-04,\n",
            "           1.8152e-04,  6.8564e-04],\n",
            "         [-6.5974e-03,  1.4262e-02,  8.3869e-03,  ...,  2.3069e-03,\n",
            "          -5.2657e-04, -1.9890e-03],\n",
            "         ...,\n",
            "         [ 1.3220e-04, -2.8577e-04, -1.6806e-04,  ..., -4.6224e-05,\n",
            "           1.0551e-05,  3.9855e-05],\n",
            "         [-5.4793e-04,  1.1845e-03,  6.9655e-04,  ...,  1.9159e-04,\n",
            "          -4.3733e-05, -1.6519e-04],\n",
            "         [ 1.9142e-03, -4.1378e-03, -2.4334e-03,  ..., -6.6931e-04,\n",
            "           1.5278e-04,  5.7708e-04]],\n",
            "\n",
            "        [[ 4.0701e-05, -4.3158e-05, -4.2210e-05,  ...,  3.4920e-07,\n",
            "           1.8759e-05,  8.8806e-07],\n",
            "         [ 2.2560e-03, -2.3922e-03, -2.3397e-03,  ...,  1.9356e-05,\n",
            "           1.0398e-03,  4.9225e-05],\n",
            "         [-1.6226e-02,  1.7205e-02,  1.6827e-02,  ..., -1.3921e-04,\n",
            "          -7.4784e-03, -3.5403e-04],\n",
            "         ...,\n",
            "         [-1.0551e-03,  1.1188e-03,  1.0942e-03,  ..., -9.0523e-06,\n",
            "          -4.8629e-04, -2.3021e-05],\n",
            "         [ 5.3541e-03, -5.6773e-03, -5.5526e-03,  ...,  4.5936e-05,\n",
            "           2.4677e-03,  1.1682e-04],\n",
            "         [-7.1075e-04,  7.5366e-04,  7.3710e-04,  ..., -6.0980e-06,\n",
            "          -3.2758e-04, -1.5508e-05]],\n",
            "\n",
            "        [[ 2.3113e-03, -1.8679e-03, -3.0348e-03,  ...,  7.5687e-04,\n",
            "           9.0119e-04, -2.6265e-04],\n",
            "         [-7.4100e-04,  5.9886e-04,  9.7297e-04,  ..., -2.4266e-04,\n",
            "          -2.8893e-04,  8.4206e-05],\n",
            "         [-1.8899e-02,  1.5274e-02,  2.4815e-02,  ..., -6.1888e-03,\n",
            "          -7.3689e-03,  2.1476e-03],\n",
            "         ...,\n",
            "         [-1.6504e-03,  1.3338e-03,  2.1670e-03,  ..., -5.4045e-04,\n",
            "          -6.4350e-04,  1.8754e-04],\n",
            "         [ 3.8822e-03, -3.1376e-03, -5.0976e-03,  ...,  1.2713e-03,\n",
            "           1.5137e-03, -4.4117e-04],\n",
            "         [ 5.6538e-03, -4.5693e-03, -7.4237e-03,  ...,  1.8514e-03,\n",
            "           2.2045e-03, -6.4249e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 4.9029e-03, -7.6183e-03, -1.3208e-02,  ...,  3.4534e-03,\n",
            "           3.2581e-03, -4.7322e-03],\n",
            "         [-7.2946e-03,  1.1335e-02,  1.9651e-02,  ..., -5.1380e-03,\n",
            "          -4.8474e-03,  7.0406e-03],\n",
            "         [-3.2488e-03,  5.0480e-03,  8.7519e-03,  ..., -2.2883e-03,\n",
            "          -2.1589e-03,  3.1357e-03],\n",
            "         ...,\n",
            "         [ 4.6365e-03, -7.2044e-03, -1.2490e-02,  ...,  3.2657e-03,\n",
            "           3.0811e-03, -4.4751e-03],\n",
            "         [ 7.0399e-03, -1.0939e-02, -1.8965e-02,  ...,  4.9586e-03,\n",
            "           4.6782e-03, -6.7948e-03],\n",
            "         [-1.3295e-03,  2.0659e-03,  3.5817e-03,  ..., -9.3647e-04,\n",
            "          -8.8352e-04,  1.2833e-03]],\n",
            "\n",
            "        [[ 8.6146e-03, -8.9904e-03, -9.3381e-03,  ...,  2.0347e-03,\n",
            "           1.4969e-03, -4.3769e-03],\n",
            "         [-1.3100e-02,  1.3672e-02,  1.4201e-02,  ..., -3.0943e-03,\n",
            "          -2.2765e-03,  6.6562e-03],\n",
            "         [-1.3970e-03,  1.4579e-03,  1.5143e-03,  ..., -3.2996e-04,\n",
            "          -2.4275e-04,  7.0978e-04],\n",
            "         ...,\n",
            "         [ 7.0380e-03, -7.3451e-03, -7.6291e-03,  ...,  1.6623e-03,\n",
            "           1.2230e-03, -3.5759e-03],\n",
            "         [ 1.3904e-02, -1.4511e-02, -1.5072e-02,  ...,  3.2841e-03,\n",
            "           2.4161e-03, -7.0644e-03],\n",
            "         [-2.0926e-03,  2.1839e-03,  2.2683e-03,  ..., -4.9426e-04,\n",
            "          -3.6363e-04,  1.0632e-03]],\n",
            "\n",
            "        [[ 3.9309e-03, -6.4246e-03, -7.4963e-03,  ..., -8.5338e-04,\n",
            "           6.0081e-05, -3.1303e-03],\n",
            "         [-8.2504e-03,  1.3484e-02,  1.5734e-02,  ...,  1.7911e-03,\n",
            "          -1.2610e-04,  6.5699e-03],\n",
            "         [-4.7888e-03,  7.8266e-03,  9.1322e-03,  ...,  1.0396e-03,\n",
            "          -7.3193e-05,  3.8134e-03],\n",
            "         ...,\n",
            "         [ 6.7162e-03, -1.0977e-02, -1.2808e-02,  ..., -1.4580e-03,\n",
            "           1.0265e-04, -5.3482e-03],\n",
            "         [ 1.0088e-02, -1.6488e-02, -1.9239e-02,  ..., -2.1901e-03,\n",
            "           1.5419e-04, -8.0336e-03],\n",
            "         [-4.2438e-03,  6.9359e-03,  8.0929e-03,  ...,  9.2130e-04,\n",
            "          -6.4863e-05,  3.3794e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1924, 0.1330, 0.1870,  ..., 0.0893, 0.1904, 0.0515],\n",
            "        [0.1924, 0.1301, 0.1856,  ..., 0.0904, 0.1912, 0.0532],\n",
            "        [0.1932, 0.1342, 0.1872,  ..., 0.0888, 0.1898, 0.0529],\n",
            "        ...,\n",
            "        [0.1900, 0.1316, 0.1854,  ..., 0.0920, 0.1927, 0.0527],\n",
            "        [0.1916, 0.1319, 0.1863,  ..., 0.0933, 0.1941, 0.0525],\n",
            "        [0.1908, 0.1300, 0.1849,  ..., 0.0896, 0.1899, 0.0535]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[-8.6635e-04,  1.9033e-03,  1.1335e-03,  ...,  2.7819e-04,\n",
            "          -1.0760e-04, -2.8380e-04],\n",
            "         [ 1.9397e-03, -4.2613e-03, -2.5378e-03,  ..., -6.2285e-04,\n",
            "           2.4091e-04,  6.3540e-04],\n",
            "         [-6.3278e-03,  1.3902e-02,  8.2789e-03,  ...,  2.0319e-03,\n",
            "          -7.8593e-04, -2.0729e-03],\n",
            "         ...,\n",
            "         [-1.2125e-04,  2.6637e-04,  1.5863e-04,  ...,  3.8934e-05,\n",
            "          -1.5059e-05, -3.9719e-05],\n",
            "         [-4.9246e-04,  1.0819e-03,  6.4431e-04,  ...,  1.5813e-04,\n",
            "          -6.1166e-05, -1.6132e-04],\n",
            "         [ 1.6292e-03, -3.5793e-03, -2.1316e-03,  ..., -5.2317e-04,\n",
            "           2.0236e-04,  5.3371e-04]],\n",
            "\n",
            "        [[-8.6364e-04,  8.8239e-04,  8.5619e-04,  ...,  2.2546e-06,\n",
            "          -3.1263e-04,  2.1992e-05],\n",
            "         [ 1.5653e-03, -1.5993e-03, -1.5518e-03,  ..., -4.0863e-06,\n",
            "           5.6663e-04, -3.9860e-05],\n",
            "         [-1.4453e-02,  1.4767e-02,  1.4328e-02,  ...,  3.7730e-05,\n",
            "          -5.2319e-03,  3.6804e-04],\n",
            "         ...,\n",
            "         [-1.0110e-03,  1.0329e-03,  1.0022e-03,  ...,  2.6391e-06,\n",
            "          -3.6596e-04,  2.5744e-05],\n",
            "         [ 6.1112e-03, -6.2439e-03, -6.0585e-03,  ..., -1.5954e-05,\n",
            "           2.2122e-03, -1.5562e-04],\n",
            "         [-1.5931e-03,  1.6276e-03,  1.5793e-03,  ...,  4.1587e-06,\n",
            "          -5.7668e-04,  4.0567e-05]],\n",
            "\n",
            "        [[ 3.0500e-03, -2.6969e-03, -4.0743e-03,  ...,  8.8862e-04,\n",
            "           1.3434e-03, -2.6256e-04],\n",
            "         [-7.2396e-05,  6.4015e-05,  9.6710e-05,  ..., -2.1093e-05,\n",
            "          -3.1887e-05,  6.2322e-06],\n",
            "         [-1.8750e-02,  1.6579e-02,  2.5047e-02,  ..., -5.4628e-03,\n",
            "          -8.2584e-03,  1.6141e-03],\n",
            "         ...,\n",
            "         [-1.9088e-03,  1.6878e-03,  2.5498e-03,  ..., -5.5612e-04,\n",
            "          -8.4072e-04,  1.6432e-04],\n",
            "         [ 2.9506e-03, -2.6090e-03, -3.9415e-03,  ...,  8.5964e-04,\n",
            "           1.2996e-03, -2.5400e-04],\n",
            "         [ 7.1790e-03, -6.3479e-03, -9.5900e-03,  ...,  2.0916e-03,\n",
            "           3.1620e-03, -6.1801e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 5.1514e-03, -7.8652e-03, -1.3242e-02,  ...,  3.0495e-03,\n",
            "           2.3222e-03, -4.6848e-03],\n",
            "         [-7.7035e-03,  1.1762e-02,  1.9802e-02,  ..., -4.5602e-03,\n",
            "          -3.4726e-03,  7.0057e-03],\n",
            "         [-3.3876e-03,  5.1722e-03,  8.7076e-03,  ..., -2.0053e-03,\n",
            "          -1.5271e-03,  3.0807e-03],\n",
            "         ...,\n",
            "         [ 5.3046e-03, -8.0991e-03, -1.3635e-02,  ...,  3.1401e-03,\n",
            "           2.3912e-03, -4.8241e-03],\n",
            "         [ 7.6264e-03, -1.1644e-02, -1.9603e-02,  ...,  4.5146e-03,\n",
            "           3.4378e-03, -6.9356e-03],\n",
            "         [-1.8487e-03,  2.8226e-03,  4.7520e-03,  ..., -1.0944e-03,\n",
            "          -8.3336e-04,  1.6812e-03]],\n",
            "\n",
            "        [[ 7.8622e-03, -9.6623e-03, -9.1526e-03,  ...,  1.8081e-03,\n",
            "           1.2350e-03, -3.9827e-03],\n",
            "         [-1.1707e-02,  1.4388e-02,  1.3629e-02,  ..., -2.6923e-03,\n",
            "          -1.8390e-03,  5.9305e-03],\n",
            "         [-2.6301e-04,  3.2323e-04,  3.0618e-04,  ..., -6.0484e-05,\n",
            "          -4.1315e-05,  1.3323e-04],\n",
            "         ...,\n",
            "         [ 5.6490e-03, -6.9424e-03, -6.5762e-03,  ...,  1.2991e-03,\n",
            "           8.8737e-04, -2.8616e-03],\n",
            "         [ 1.2694e-02, -1.5601e-02, -1.4778e-02,  ...,  2.9193e-03,\n",
            "           1.9941e-03, -6.4306e-03],\n",
            "         [-2.1307e-03,  2.6185e-03,  2.4804e-03,  ..., -4.8999e-04,\n",
            "          -3.3470e-04,  1.0793e-03]],\n",
            "\n",
            "        [[ 4.5057e-03, -6.3239e-03, -7.8717e-03,  ..., -8.0638e-04,\n",
            "           9.3797e-06, -3.1319e-03],\n",
            "         [-9.2981e-03,  1.3050e-02,  1.6244e-02,  ...,  1.6641e-03,\n",
            "          -1.9356e-05,  6.4630e-03],\n",
            "         [-4.9423e-03,  6.9367e-03,  8.6345e-03,  ...,  8.8452e-04,\n",
            "          -1.0289e-05,  3.4353e-03],\n",
            "         ...,\n",
            "         [ 7.0287e-03, -9.8651e-03, -1.2280e-02,  ..., -1.2579e-03,\n",
            "           1.4632e-05, -4.8856e-03],\n",
            "         [ 1.0940e-02, -1.5354e-02, -1.9112e-02,  ..., -1.9579e-03,\n",
            "           2.2774e-05, -7.6041e-03],\n",
            "         [-5.0058e-03,  7.0259e-03,  8.7455e-03,  ...,  8.9589e-04,\n",
            "          -1.0421e-05,  3.4795e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1922, 0.1334, 0.1861,  ..., 0.0881, 0.1889, 0.0526],\n",
            "        [0.1874, 0.1310, 0.1839,  ..., 0.0857, 0.1865, 0.0523],\n",
            "        [0.1914, 0.1324, 0.1863,  ..., 0.0887, 0.1894, 0.0511],\n",
            "        ...,\n",
            "        [0.1890, 0.1313, 0.1848,  ..., 0.0913, 0.1920, 0.0529],\n",
            "        [0.1886, 0.1311, 0.1846,  ..., 0.0909, 0.1917, 0.0530],\n",
            "        [0.1925, 0.1301, 0.1856,  ..., 0.0904, 0.1912, 0.0532]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[-8.4338e-04,  1.8010e-03,  1.0577e-03,  ...,  3.8325e-04,\n",
            "          -7.2139e-05, -1.9282e-04],\n",
            "         [ 2.2118e-03, -4.7232e-03, -2.7739e-03,  ..., -1.0051e-03,\n",
            "           1.8919e-04,  5.0567e-04],\n",
            "         [-6.3571e-03,  1.3576e-02,  7.9729e-03,  ...,  2.8888e-03,\n",
            "          -5.4376e-04, -1.4534e-03],\n",
            "         ...,\n",
            "         [-1.3535e-03,  2.8905e-03,  1.6976e-03,  ...,  6.1508e-04,\n",
            "          -1.1577e-04, -3.0945e-04],\n",
            "         [-9.8710e-04,  2.1079e-03,  1.2380e-03,  ...,  4.4856e-04,\n",
            "          -8.4432e-05, -2.2567e-04],\n",
            "         [ 1.7633e-03, -3.7656e-03, -2.2115e-03,  ..., -8.0130e-04,\n",
            "           1.5083e-04,  4.0314e-04]],\n",
            "\n",
            "        [[-6.7854e-04,  6.9121e-04,  6.7473e-04,  ...,  3.2380e-05,\n",
            "          -2.8695e-04, -3.0359e-05],\n",
            "         [ 1.1897e-03, -1.2119e-03, -1.1830e-03,  ..., -5.6771e-05,\n",
            "           5.0310e-04,  5.3228e-05],\n",
            "         [-1.4561e-02,  1.4832e-02,  1.4479e-02,  ...,  6.9482e-04,\n",
            "          -6.1574e-03, -6.5146e-04],\n",
            "         ...,\n",
            "         [-1.8359e-03,  1.8702e-03,  1.8256e-03,  ...,  8.7609e-05,\n",
            "          -7.7638e-04, -8.2141e-05],\n",
            "         [ 6.5080e-03, -6.6295e-03, -6.4715e-03,  ..., -3.1056e-04,\n",
            "           2.7521e-03,  2.9118e-04],\n",
            "         [-6.6381e-04,  6.7621e-04,  6.6008e-04,  ...,  3.1677e-05,\n",
            "          -2.8072e-04, -2.9700e-05]],\n",
            "\n",
            "        [[ 2.1857e-03, -2.0684e-03, -2.9917e-03,  ...,  8.6777e-04,\n",
            "           8.4244e-04, -1.8710e-04],\n",
            "         [-1.2789e-03,  1.2103e-03,  1.7505e-03,  ..., -5.0775e-04,\n",
            "          -4.9293e-04,  1.0947e-04],\n",
            "         [-1.7851e-02,  1.6893e-02,  2.4434e-02,  ..., -7.0872e-03,\n",
            "          -6.8802e-03,  1.5280e-03],\n",
            "         ...,\n",
            "         [-1.7021e-03,  1.6108e-03,  2.3298e-03,  ..., -6.7579e-04,\n",
            "          -6.5606e-04,  1.4570e-04],\n",
            "         [ 4.7723e-03, -4.5162e-03, -6.5322e-03,  ...,  1.8947e-03,\n",
            "           1.8394e-03, -4.0851e-04],\n",
            "         [ 6.1644e-03, -5.8336e-03, -8.4376e-03,  ...,  2.4474e-03,\n",
            "           2.3760e-03, -5.2767e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 4.8862e-03, -7.5906e-03, -1.2415e-02,  ...,  3.1974e-03,\n",
            "           3.1562e-03, -5.4237e-03],\n",
            "         [-7.4523e-03,  1.1577e-02,  1.8935e-02,  ..., -4.8766e-03,\n",
            "          -4.8137e-03,  8.2721e-03],\n",
            "         [-2.8909e-03,  4.4909e-03,  7.3454e-03,  ..., -1.8917e-03,\n",
            "          -1.8673e-03,  3.2089e-03],\n",
            "         ...,\n",
            "         [ 5.2739e-03, -8.1929e-03, -1.3400e-02,  ...,  3.4511e-03,\n",
            "           3.4066e-03, -5.8541e-03],\n",
            "         [ 7.2266e-03, -1.1226e-02, -1.8362e-02,  ...,  4.7289e-03,\n",
            "           4.6679e-03, -8.0216e-03],\n",
            "         [-1.7483e-03,  2.7159e-03,  4.4422e-03,  ..., -1.1440e-03,\n",
            "          -1.1293e-03,  1.9406e-03]],\n",
            "\n",
            "        [[ 7.0311e-03, -8.7093e-03, -8.7867e-03,  ...,  9.4521e-04,\n",
            "           1.2204e-03, -2.3751e-03],\n",
            "         [-1.2263e-02,  1.5190e-02,  1.5326e-02,  ..., -1.6486e-03,\n",
            "          -2.1287e-03,  4.1426e-03],\n",
            "         [-4.1085e-04,  5.0892e-04,  5.1344e-04,  ..., -5.5232e-05,\n",
            "          -7.1315e-05,  1.3879e-04],\n",
            "         ...,\n",
            "         [ 5.3768e-03, -6.6602e-03, -6.7194e-03,  ...,  7.2282e-04,\n",
            "           9.3330e-04, -1.8163e-03],\n",
            "         [ 1.2882e-02, -1.5956e-02, -1.6098e-02,  ...,  1.7317e-03,\n",
            "           2.2360e-03, -4.3513e-03],\n",
            "         [-2.7364e-03,  3.3895e-03,  3.4196e-03,  ..., -3.6786e-04,\n",
            "          -4.7497e-04,  9.2433e-04]],\n",
            "\n",
            "        [[ 4.7313e-03, -7.5123e-03, -8.5719e-03,  ..., -6.8066e-04,\n",
            "          -1.8639e-04, -3.3180e-03],\n",
            "         [-9.2813e-03,  1.4737e-02,  1.6815e-02,  ...,  1.3352e-03,\n",
            "           3.6563e-04,  6.5089e-03],\n",
            "         [-4.9078e-03,  7.7925e-03,  8.8916e-03,  ...,  7.0605e-04,\n",
            "           1.9334e-04,  3.4418e-03],\n",
            "         ...,\n",
            "         [ 6.8626e-03, -1.0896e-02, -1.2433e-02,  ..., -9.8726e-04,\n",
            "          -2.7034e-04, -4.8127e-03],\n",
            "         [ 1.0463e-02, -1.6613e-02, -1.8956e-02,  ..., -1.5053e-03,\n",
            "          -4.1219e-04, -7.3378e-03],\n",
            "         [-4.6608e-03,  7.4003e-03,  8.4441e-03,  ...,  6.7051e-04,\n",
            "           1.8361e-04,  3.2686e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1879, 0.1293, 0.1844,  ..., 0.0883, 0.1888, 0.0541],\n",
            "        [0.1903, 0.1298, 0.1848,  ..., 0.0893, 0.1894, 0.0537],\n",
            "        [0.1888, 0.1299, 0.1843,  ..., 0.0880, 0.1886, 0.0522],\n",
            "        ...,\n",
            "        [0.1929, 0.1320, 0.1876,  ..., 0.0944, 0.1952, 0.0524],\n",
            "        [0.1924, 0.1319, 0.1870,  ..., 0.0939, 0.1947, 0.0524],\n",
            "        [0.1901, 0.1298, 0.1848,  ..., 0.0892, 0.1893, 0.0537]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[-7.7423e-04,  1.4879e-03,  9.3668e-04,  ...,  2.5222e-04,\n",
            "          -6.8832e-05, -1.5072e-04],\n",
            "         [ 2.2457e-03, -4.3158e-03, -2.7169e-03,  ..., -7.3157e-04,\n",
            "           1.9965e-04,  4.3717e-04],\n",
            "         [-6.9071e-03,  1.3274e-02,  8.3564e-03,  ...,  2.2501e-03,\n",
            "          -6.1407e-04, -1.3446e-03],\n",
            "         ...,\n",
            "         [-5.2472e-04,  1.0084e-03,  6.3482e-04,  ...,  1.7094e-04,\n",
            "          -4.6650e-05, -1.0215e-04],\n",
            "         [-5.1084e-04,  9.8174e-04,  6.1802e-04,  ...,  1.6641e-04,\n",
            "          -4.5415e-05, -9.9447e-05],\n",
            "         [ 1.7580e-03, -3.3786e-03, -2.1269e-03,  ..., -5.7271e-04,\n",
            "           1.5630e-04,  3.4224e-04]],\n",
            "\n",
            "        [[-1.0636e-03,  1.0366e-03,  1.0272e-03,  ...,  4.3905e-05,\n",
            "          -4.4526e-04, -4.4719e-06],\n",
            "         [ 1.6958e-03, -1.6528e-03, -1.6378e-03,  ..., -7.0002e-05,\n",
            "           7.0992e-04,  7.1300e-06],\n",
            "         [-1.4469e-02,  1.4102e-02,  1.3974e-02,  ...,  5.9727e-04,\n",
            "          -6.0572e-03, -6.0834e-05],\n",
            "         ...,\n",
            "         [-1.3896e-03,  1.3544e-03,  1.3421e-03,  ...,  5.7363e-05,\n",
            "          -5.8174e-04, -5.8426e-06],\n",
            "         [ 5.8759e-03, -5.7269e-03, -5.6748e-03,  ..., -2.4255e-04,\n",
            "           2.4598e-03,  2.4705e-05],\n",
            "         [-6.4431e-04,  6.2798e-04,  6.2227e-04,  ...,  2.6597e-05,\n",
            "          -2.6973e-04, -2.7090e-06]],\n",
            "\n",
            "        [[ 4.0764e-03, -3.6539e-03, -5.2789e-03,  ...,  1.0279e-03,\n",
            "           1.7374e-03, -1.5179e-04],\n",
            "         [-1.1980e-03,  1.0738e-03,  1.5514e-03,  ..., -3.0207e-04,\n",
            "          -5.1060e-04,  4.4610e-05],\n",
            "         [-1.7932e-02,  1.6073e-02,  2.3222e-02,  ..., -4.5215e-03,\n",
            "          -7.6427e-03,  6.6772e-04],\n",
            "         ...,\n",
            "         [-1.9371e-03,  1.7363e-03,  2.5086e-03,  ..., -4.8845e-04,\n",
            "          -8.2562e-04,  7.2133e-05],\n",
            "         [ 5.0645e-03, -4.5396e-03, -6.5586e-03,  ...,  1.2770e-03,\n",
            "           2.1586e-03, -1.8859e-04],\n",
            "         [ 5.0103e-03, -4.4910e-03, -6.4883e-03,  ...,  1.2634e-03,\n",
            "           2.1354e-03, -1.8657e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 5.2406e-03, -7.6561e-03, -1.2828e-02,  ...,  2.8751e-03,\n",
            "           3.1997e-03, -5.2511e-03],\n",
            "         [-7.5072e-03,  1.0967e-02,  1.8376e-02,  ..., -4.1186e-03,\n",
            "          -4.5836e-03,  7.5223e-03],\n",
            "         [-3.1146e-03,  4.5502e-03,  7.6241e-03,  ..., -1.7087e-03,\n",
            "          -1.9016e-03,  3.1209e-03],\n",
            "         ...,\n",
            "         [ 5.5045e-03, -8.0416e-03, -1.3474e-02,  ...,  3.0198e-03,\n",
            "           3.3608e-03, -5.5155e-03],\n",
            "         [ 7.4633e-03, -1.0903e-02, -1.8269e-02,  ...,  4.0945e-03,\n",
            "           4.5567e-03, -7.4783e-03],\n",
            "         [-1.6715e-03,  2.4420e-03,  4.0916e-03,  ..., -9.1702e-04,\n",
            "          -1.0206e-03,  1.6749e-03]],\n",
            "\n",
            "        [[ 7.8607e-03, -8.7483e-03, -8.2073e-03,  ...,  6.3798e-04,\n",
            "           1.1495e-03, -2.7153e-03],\n",
            "         [-1.2203e-02,  1.3580e-02,  1.2741e-02,  ..., -9.9038e-04,\n",
            "          -1.7845e-03,  4.2151e-03],\n",
            "         [-5.7537e-04,  6.4033e-04,  6.0074e-04,  ..., -4.6697e-05,\n",
            "          -8.4140e-05,  1.9875e-04],\n",
            "         ...,\n",
            "         [ 5.8336e-03, -6.4923e-03, -6.0909e-03,  ...,  4.7346e-04,\n",
            "           8.5310e-04, -2.0151e-03],\n",
            "         [ 1.4342e-02, -1.5961e-02, -1.4974e-02,  ...,  1.1640e-03,\n",
            "           2.0973e-03, -4.9541e-03],\n",
            "         [-3.7743e-03,  4.2005e-03,  3.9408e-03,  ..., -3.0633e-04,\n",
            "          -5.5195e-04,  1.3038e-03]],\n",
            "\n",
            "        [[ 4.5007e-03, -7.6396e-03, -8.4211e-03,  ..., -4.4411e-04,\n",
            "           1.0109e-05, -3.6233e-03],\n",
            "         [-9.6513e-03,  1.6383e-02,  1.8058e-02,  ...,  9.5236e-04,\n",
            "          -2.1678e-05,  7.7699e-03],\n",
            "         [-4.1505e-03,  7.0453e-03,  7.7659e-03,  ...,  4.0956e-04,\n",
            "          -9.3226e-06,  3.3414e-03],\n",
            "         ...,\n",
            "         [ 6.4212e-03, -1.0900e-02, -1.2015e-02,  ..., -6.3362e-04,\n",
            "           1.4423e-05, -5.1695e-03],\n",
            "         [ 9.7161e-03, -1.6493e-02, -1.8180e-02,  ..., -9.5875e-04,\n",
            "           2.1824e-05, -7.8221e-03],\n",
            "         [-3.9178e-03,  6.6502e-03,  7.3305e-03,  ...,  3.8659e-04,\n",
            "          -8.7999e-06,  3.1541e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1924, 0.1336, 0.1863,  ..., 0.0883, 0.1891, 0.0527],\n",
            "        [0.1878, 0.1311, 0.1841,  ..., 0.0858, 0.1866, 0.0523],\n",
            "        [0.1877, 0.1323, 0.1854,  ..., 0.0876, 0.1882, 0.0501],\n",
            "        ...,\n",
            "        [0.1914, 0.1319, 0.1861,  ..., 0.0931, 0.1939, 0.0526],\n",
            "        [0.1911, 0.1319, 0.1859,  ..., 0.0929, 0.1937, 0.0526],\n",
            "        [0.1938, 0.1302, 0.1865,  ..., 0.0912, 0.1925, 0.0530]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[-1.0542e-03,  2.0511e-03,  1.2686e-03,  ...,  3.5331e-04,\n",
            "          -9.5992e-05, -2.3721e-04],\n",
            "         [ 2.2123e-03, -4.3045e-03, -2.6623e-03,  ..., -7.4147e-04,\n",
            "           2.0145e-04,  4.9780e-04],\n",
            "         [-6.8821e-03,  1.3390e-02,  8.2820e-03,  ...,  2.3065e-03,\n",
            "          -6.2666e-04, -1.5486e-03],\n",
            "         ...,\n",
            "         [-2.2776e-04,  4.4314e-04,  2.7408e-04,  ...,  7.6333e-05,\n",
            "          -2.0739e-05, -5.1248e-05],\n",
            "         [-5.6433e-04,  1.0980e-03,  6.7912e-04,  ...,  1.8914e-04,\n",
            "          -5.1386e-05, -1.2698e-04],\n",
            "         [ 1.7964e-03, -3.4952e-03, -2.1618e-03,  ..., -6.0207e-04,\n",
            "           1.6358e-04,  4.0421e-04]],\n",
            "\n",
            "        [[-6.3412e-04,  6.3695e-04,  6.0738e-04,  ...,  1.0851e-05,\n",
            "          -2.4948e-04, -1.1515e-05],\n",
            "         [ 1.7618e-03, -1.7696e-03, -1.6875e-03,  ..., -3.0146e-05,\n",
            "           6.9313e-04,  3.1990e-05],\n",
            "         [-1.5221e-02,  1.5289e-02,  1.4580e-02,  ...,  2.6046e-04,\n",
            "          -5.9886e-03, -2.7640e-04],\n",
            "         ...,\n",
            "         [-1.5650e-03,  1.5720e-03,  1.4990e-03,  ...,  2.6779e-05,\n",
            "          -6.1571e-04, -2.8417e-05],\n",
            "         [ 5.7316e-03, -5.7572e-03, -5.4899e-03,  ..., -9.8076e-05,\n",
            "           2.2550e-03,  1.0408e-04],\n",
            "         [-1.7814e-03,  1.7894e-03,  1.7063e-03,  ...,  3.0483e-05,\n",
            "          -7.0086e-04, -3.2347e-05]],\n",
            "\n",
            "        [[ 5.2530e-03, -4.7994e-03, -6.4693e-03,  ...,  1.3405e-03,\n",
            "           1.9478e-03, -4.1161e-04],\n",
            "         [-1.1465e-03,  1.0475e-03,  1.4120e-03,  ..., -2.9257e-04,\n",
            "          -4.2513e-04,  8.9837e-05],\n",
            "         [-1.9843e-02,  1.8129e-02,  2.4437e-02,  ..., -5.0635e-03,\n",
            "          -7.3576e-03,  1.5548e-03],\n",
            "         ...,\n",
            "         [-2.4738e-03,  2.2602e-03,  3.0466e-03,  ..., -6.3128e-04,\n",
            "          -9.1729e-04,  1.9384e-04],\n",
            "         [ 4.4143e-03, -4.0331e-03, -5.4364e-03,  ...,  1.1265e-03,\n",
            "           1.6368e-03, -3.4589e-04],\n",
            "         [ 5.0759e-03, -4.6376e-03, -6.2512e-03,  ...,  1.2953e-03,\n",
            "           1.8821e-03, -3.9773e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 4.9764e-03, -8.4424e-03, -1.2552e-02,  ...,  2.8110e-03,\n",
            "           3.9671e-03, -5.9080e-03],\n",
            "         [-7.1210e-03,  1.2081e-02,  1.7961e-02,  ..., -4.0224e-03,\n",
            "          -5.6767e-03,  8.4540e-03],\n",
            "         [-2.7619e-03,  4.6855e-03,  6.9663e-03,  ..., -1.5601e-03,\n",
            "          -2.2017e-03,  3.2789e-03],\n",
            "         ...,\n",
            "         [ 4.8873e-03, -8.2912e-03, -1.2327e-02,  ...,  2.7607e-03,\n",
            "           3.8960e-03, -5.8022e-03],\n",
            "         [ 6.9577e-03, -1.1804e-02, -1.7549e-02,  ...,  3.9302e-03,\n",
            "           5.5465e-03, -8.2602e-03],\n",
            "         [-1.5320e-03,  2.5990e-03,  3.8641e-03,  ..., -8.6538e-04,\n",
            "          -1.2213e-03,  1.8188e-03]],\n",
            "\n",
            "        [[ 8.5117e-03, -9.9196e-03, -8.6931e-03,  ...,  7.3194e-04,\n",
            "           7.6769e-04, -2.6500e-03],\n",
            "         [-1.2653e-02,  1.4746e-02,  1.2923e-02,  ..., -1.0881e-03,\n",
            "          -1.1412e-03,  3.9395e-03],\n",
            "         [-4.8336e-04,  5.6332e-04,  4.9367e-04,  ..., -4.1566e-05,\n",
            "          -4.3596e-05,  1.5049e-04],\n",
            "         ...,\n",
            "         [ 6.4251e-03, -7.4879e-03, -6.5621e-03,  ...,  5.5251e-04,\n",
            "           5.7950e-04, -2.0004e-03],\n",
            "         [ 1.4101e-02, -1.6434e-02, -1.4402e-02,  ...,  1.2126e-03,\n",
            "           1.2718e-03, -4.3903e-03],\n",
            "         [-4.3051e-03,  5.0172e-03,  4.3969e-03,  ..., -3.7021e-04,\n",
            "          -3.8829e-04,  1.3404e-03]],\n",
            "\n",
            "        [[ 4.9455e-03, -7.8368e-03, -8.6047e-03,  ..., -1.8404e-04,\n",
            "           3.0128e-05, -3.9797e-03],\n",
            "         [-9.4443e-03,  1.4966e-02,  1.6432e-02,  ...,  3.5146e-04,\n",
            "          -5.7534e-05,  7.5999e-03],\n",
            "         [-3.9049e-03,  6.1878e-03,  6.7941e-03,  ...,  1.4532e-04,\n",
            "          -2.3788e-05,  3.1423e-03],\n",
            "         ...,\n",
            "         [ 6.4015e-03, -1.0144e-02, -1.1138e-02,  ..., -2.3823e-04,\n",
            "           3.8997e-05, -5.1513e-03],\n",
            "         [ 9.8242e-03, -1.5568e-02, -1.7093e-02,  ..., -3.6560e-04,\n",
            "           5.9849e-05, -7.9056e-03],\n",
            "         [-3.9984e-03,  6.3360e-03,  6.9568e-03,  ...,  1.4880e-04,\n",
            "          -2.4358e-05,  3.2175e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1880, 0.1293, 0.1844,  ..., 0.0884, 0.1887, 0.0541],\n",
            "        [0.1938, 0.1302, 0.1865,  ..., 0.0912, 0.1925, 0.0530],\n",
            "        [0.1911, 0.1323, 0.1862,  ..., 0.0886, 0.1892, 0.0510],\n",
            "        ...,\n",
            "        [0.1870, 0.1309, 0.1836,  ..., 0.0894, 0.1909, 0.0534],\n",
            "        [0.1906, 0.1318, 0.1856,  ..., 0.0925, 0.1933, 0.0526],\n",
            "        [0.1920, 0.1319, 0.1866,  ..., 0.0935, 0.1943, 0.0525]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[-7.6274e-04,  1.5921e-03,  9.2793e-04,  ...,  3.3480e-04,\n",
            "          -3.1112e-05, -1.6949e-04],\n",
            "         [ 2.0989e-03, -4.3810e-03, -2.5534e-03,  ..., -9.2128e-04,\n",
            "           8.5612e-05,  4.6640e-04],\n",
            "         [-6.2162e-03,  1.2975e-02,  7.5625e-03,  ...,  2.7286e-03,\n",
            "          -2.5356e-04, -1.3813e-03],\n",
            "         ...,\n",
            "         [-6.9625e-04,  1.4533e-03,  8.4704e-04,  ...,  3.0562e-04,\n",
            "          -2.8400e-05, -1.5472e-04],\n",
            "         [-9.2314e-04,  1.9269e-03,  1.1231e-03,  ...,  4.0521e-04,\n",
            "          -3.7655e-05, -2.0514e-04],\n",
            "         [ 1.4995e-03, -3.1298e-03, -1.8242e-03,  ..., -6.5818e-04,\n",
            "           6.1163e-05,  3.3320e-04]],\n",
            "\n",
            "        [[ 1.5217e-04, -1.6336e-04, -1.5121e-04,  ..., -7.7580e-06,\n",
            "           5.1231e-05,  4.8675e-06],\n",
            "         [ 2.7443e-03, -2.9461e-03, -2.7270e-03,  ..., -1.3991e-04,\n",
            "           9.2395e-04,  8.7784e-05],\n",
            "         [-1.4685e-02,  1.5764e-02,  1.4592e-02,  ...,  7.4866e-04,\n",
            "          -4.9439e-03, -4.6972e-04],\n",
            "         ...,\n",
            "         [-1.4393e-03,  1.5451e-03,  1.4302e-03,  ...,  7.3377e-05,\n",
            "          -4.8456e-04, -4.6038e-05],\n",
            "         [ 3.8496e-03, -4.1326e-03, -3.8253e-03,  ..., -1.9626e-04,\n",
            "           1.2960e-03,  1.2314e-04],\n",
            "         [-1.0061e-03,  1.0801e-03,  9.9975e-04,  ...,  5.1294e-05,\n",
            "          -3.3873e-04, -3.2182e-05]],\n",
            "\n",
            "        [[ 3.6457e-03, -3.8696e-03, -4.7657e-03,  ...,  9.6456e-04,\n",
            "           1.4620e-03, -5.4899e-04],\n",
            "         [-1.8909e-03,  2.0070e-03,  2.4718e-03,  ..., -5.0029e-04,\n",
            "          -7.5828e-04,  2.8474e-04],\n",
            "         [-1.8856e-02,  2.0014e-02,  2.4649e-02,  ..., -4.9889e-03,\n",
            "          -7.5616e-03,  2.8395e-03],\n",
            "         ...,\n",
            "         [-2.6615e-03,  2.8250e-03,  3.4792e-03,  ..., -7.0417e-04,\n",
            "          -1.0673e-03,  4.0079e-04],\n",
            "         [ 4.4899e-03, -4.7656e-03, -5.8692e-03,  ...,  1.1879e-03,\n",
            "           1.8005e-03, -6.7611e-04],\n",
            "         [ 4.9524e-03, -5.2565e-03, -6.4739e-03,  ...,  1.3103e-03,\n",
            "           1.9860e-03, -7.4577e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 5.0349e-03, -8.6493e-03, -1.2755e-02,  ...,  2.9706e-03,\n",
            "           3.9107e-03, -6.5266e-03],\n",
            "         [-7.2092e-03,  1.2385e-02,  1.8263e-02,  ..., -4.2535e-03,\n",
            "          -5.5995e-03,  9.3452e-03],\n",
            "         [-2.8211e-03,  4.8463e-03,  7.1468e-03,  ..., -1.6645e-03,\n",
            "          -2.1912e-03,  3.6569e-03],\n",
            "         ...,\n",
            "         [ 4.7709e-03, -8.1959e-03, -1.2086e-02,  ...,  2.8149e-03,\n",
            "           3.7056e-03, -6.1844e-03],\n",
            "         [ 7.3556e-03, -1.2636e-02, -1.8634e-02,  ...,  4.3399e-03,\n",
            "           5.7132e-03, -9.5349e-03],\n",
            "         [-1.5055e-03,  2.5863e-03,  3.8140e-03,  ..., -8.8827e-04,\n",
            "          -1.1694e-03,  1.9516e-03]],\n",
            "\n",
            "        [[ 9.9761e-03, -1.0682e-02, -8.7056e-03,  ...,  9.8531e-04,\n",
            "           1.4930e-03, -2.0264e-03],\n",
            "         [-1.3413e-02,  1.4363e-02,  1.1705e-02,  ..., -1.3248e-03,\n",
            "          -2.0074e-03,  2.7246e-03],\n",
            "         [-3.2995e-04,  3.5331e-04,  2.8793e-04,  ..., -3.2588e-05,\n",
            "          -4.9380e-05,  6.7022e-05],\n",
            "         ...,\n",
            "         [ 7.1287e-03, -7.6334e-03, -6.2208e-03,  ...,  7.0409e-04,\n",
            "           1.0669e-03, -1.4480e-03],\n",
            "         [ 1.4716e-02, -1.5758e-02, -1.2842e-02,  ...,  1.4535e-03,\n",
            "           2.2025e-03, -2.9893e-03],\n",
            "         [-4.2534e-03,  4.5546e-03,  3.7117e-03,  ..., -4.2010e-04,\n",
            "          -6.3657e-04,  8.6399e-04]],\n",
            "\n",
            "        [[ 4.4450e-03, -7.6919e-03, -8.7494e-03,  ..., -2.0351e-04,\n",
            "           1.8388e-04, -3.9515e-03],\n",
            "         [-8.4488e-03,  1.4621e-02,  1.6630e-02,  ...,  3.8682e-04,\n",
            "          -3.4952e-04,  7.5109e-03],\n",
            "         [-3.7124e-03,  6.4242e-03,  7.3073e-03,  ...,  1.6997e-04,\n",
            "          -1.5358e-04,  3.3002e-03],\n",
            "         ...,\n",
            "         [ 5.8639e-03, -1.0147e-02, -1.1542e-02,  ..., -2.6848e-04,\n",
            "           2.4258e-04, -5.2130e-03],\n",
            "         [ 9.0272e-03, -1.5621e-02, -1.7769e-02,  ..., -4.1331e-04,\n",
            "           3.7344e-04, -8.0251e-03],\n",
            "         [-3.6935e-03,  6.3916e-03,  7.2703e-03,  ...,  1.6911e-04,\n",
            "          -1.5280e-04,  3.2835e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1884, 0.1321, 0.1856,  ..., 0.0875, 0.1882, 0.0503],\n",
            "        [0.1931, 0.1301, 0.1860,  ..., 0.0908, 0.1917, 0.0531],\n",
            "        [0.1937, 0.1346, 0.1879,  ..., 0.0893, 0.1902, 0.0532],\n",
            "        ...,\n",
            "        [0.1897, 0.1382, 0.1871,  ..., 0.0879, 0.1905, 0.0505],\n",
            "        [0.1916, 0.1319, 0.1863,  ..., 0.0933, 0.1941, 0.0525],\n",
            "        [0.1881, 0.1293, 0.1844,  ..., 0.0884, 0.1887, 0.0541]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[-9.8352e-04,  2.1564e-03,  1.2640e-03,  ...,  3.3530e-04,\n",
            "          -1.6189e-05, -2.6543e-04],\n",
            "         [ 1.8324e-03, -4.0175e-03, -2.3550e-03,  ..., -6.2469e-04,\n",
            "           3.0161e-05,  4.9453e-04],\n",
            "         [-6.6661e-03,  1.4615e-02,  8.5671e-03,  ...,  2.2726e-03,\n",
            "          -1.0972e-04, -1.7991e-03],\n",
            "         ...,\n",
            "         [ 1.5712e-04, -3.4449e-04, -2.0193e-04,  ..., -5.3566e-05,\n",
            "           2.5863e-06,  4.2405e-05],\n",
            "         [-2.4706e-04,  5.4169e-04,  3.1752e-04,  ...,  8.4228e-05,\n",
            "          -4.0667e-06, -6.6678e-05],\n",
            "         [ 1.4210e-03, -3.1157e-03, -1.8263e-03,  ..., -4.8446e-04,\n",
            "           2.3391e-05,  3.8352e-04]],\n",
            "\n",
            "        [[ 1.3179e-04, -1.4709e-04, -1.3590e-04,  ..., -7.9725e-06,\n",
            "           4.4975e-05,  1.2388e-05],\n",
            "         [ 2.4342e-03, -2.7167e-03, -2.5101e-03,  ..., -1.4725e-04,\n",
            "           8.3070e-04,  2.2881e-04],\n",
            "         [-1.3289e-02,  1.4832e-02,  1.3704e-02,  ...,  8.0394e-04,\n",
            "          -4.5352e-03, -1.2492e-03],\n",
            "         ...,\n",
            "         [-3.2915e-03,  3.6736e-03,  3.3942e-03,  ...,  1.9912e-04,\n",
            "          -1.1233e-03, -3.0940e-04],\n",
            "         [ 2.6715e-03, -2.9816e-03, -2.7549e-03,  ..., -1.6161e-04,\n",
            "           9.1169e-04,  2.5112e-04],\n",
            "         [-4.0977e-04,  4.5733e-04,  4.2256e-04,  ...,  2.4789e-05,\n",
            "          -1.3984e-04, -3.8518e-05]],\n",
            "\n",
            "        [[ 4.9970e-03, -5.0770e-03, -6.1609e-03,  ...,  1.4001e-03,\n",
            "           2.1351e-03, -5.7563e-04],\n",
            "         [-1.9087e-03,  1.9393e-03,  2.3533e-03,  ..., -5.3479e-04,\n",
            "          -8.1558e-04,  2.1988e-04],\n",
            "         [-1.9869e-02,  2.0187e-02,  2.4497e-02,  ..., -5.5670e-03,\n",
            "          -8.4898e-03,  2.2888e-03],\n",
            "         ...,\n",
            "         [-2.1097e-03,  2.1435e-03,  2.6011e-03,  ..., -5.9110e-04,\n",
            "          -9.0144e-04,  2.4302e-04],\n",
            "         [ 5.6332e-03, -5.7235e-03, -6.9454e-03,  ...,  1.5783e-03,\n",
            "           2.4070e-03, -6.4892e-04],\n",
            "         [ 3.8138e-03, -3.8749e-03, -4.7021e-03,  ...,  1.0686e-03,\n",
            "           1.6296e-03, -4.3933e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 5.1041e-03, -9.3928e-03, -1.3289e-02,  ...,  2.7210e-03,\n",
            "           3.5218e-03, -7.1045e-03],\n",
            "         [-7.0118e-03,  1.2903e-02,  1.8256e-02,  ..., -3.7380e-03,\n",
            "          -4.8380e-03,  9.7598e-03],\n",
            "         [-3.1295e-03,  5.7590e-03,  8.1482e-03,  ..., -1.6683e-03,\n",
            "          -2.1593e-03,  4.3560e-03],\n",
            "         ...,\n",
            "         [ 4.8385e-03, -8.9041e-03, -1.2598e-02,  ...,  2.5794e-03,\n",
            "           3.3385e-03, -6.7348e-03],\n",
            "         [ 7.3546e-03, -1.3534e-02, -1.9149e-02,  ...,  3.9207e-03,\n",
            "           5.0746e-03, -1.0237e-02],\n",
            "         [-1.3277e-03,  2.4434e-03,  3.4570e-03,  ..., -7.0781e-04,\n",
            "          -9.1612e-04,  1.8481e-03]],\n",
            "\n",
            "        [[ 8.9745e-03, -1.0436e-02, -9.5465e-03,  ...,  9.9288e-04,\n",
            "           9.3220e-04, -1.8276e-03],\n",
            "         [-1.2794e-02,  1.4877e-02,  1.3609e-02,  ..., -1.4155e-03,\n",
            "          -1.3289e-03,  2.6054e-03],\n",
            "         [-3.8711e-04,  4.5015e-04,  4.1178e-04,  ..., -4.2827e-05,\n",
            "          -4.0210e-05,  7.8832e-05],\n",
            "         ...,\n",
            "         [ 6.8035e-03, -7.9114e-03, -7.2371e-03,  ...,  7.5270e-04,\n",
            "           7.0669e-04, -1.3855e-03],\n",
            "         [ 1.3663e-02, -1.5888e-02, -1.4534e-02,  ...,  1.5116e-03,\n",
            "           1.4192e-03, -2.7824e-03],\n",
            "         [-3.6559e-03,  4.2513e-03,  3.8889e-03,  ..., -4.0447e-04,\n",
            "          -3.7975e-04,  7.4450e-04]],\n",
            "\n",
            "        [[ 4.8350e-03, -7.7186e-03, -9.0304e-03,  ..., -4.2747e-04,\n",
            "           7.0525e-05, -4.1483e-03],\n",
            "         [-8.6271e-03,  1.3772e-02,  1.6113e-02,  ...,  7.6275e-04,\n",
            "          -1.2584e-04,  7.4019e-03],\n",
            "         [-3.6888e-03,  5.8888e-03,  6.8897e-03,  ...,  3.2614e-04,\n",
            "          -5.3806e-05,  3.1649e-03],\n",
            "         ...,\n",
            "         [ 5.5716e-03, -8.8946e-03, -1.0406e-02,  ..., -4.9260e-04,\n",
            "           8.1270e-05, -4.7803e-03],\n",
            "         [ 9.2350e-03, -1.4743e-02, -1.7248e-02,  ..., -8.1649e-04,\n",
            "           1.3470e-04, -7.9234e-03],\n",
            "         [-3.8711e-03,  6.1798e-03,  7.2301e-03,  ...,  3.4225e-04,\n",
            "          -5.6465e-05,  3.3213e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1898, 0.1322, 0.1861,  ..., 0.0880, 0.1886, 0.0507],\n",
            "        [0.1902, 0.1298, 0.1848,  ..., 0.0893, 0.1894, 0.0537],\n",
            "        [0.1924, 0.1301, 0.1856,  ..., 0.0904, 0.1912, 0.0532],\n",
            "        ...,\n",
            "        [0.1900, 0.1316, 0.1854,  ..., 0.0920, 0.1927, 0.0527],\n",
            "        [0.1874, 0.1308, 0.1838,  ..., 0.0897, 0.1910, 0.0532],\n",
            "        [0.1927, 0.1382, 0.1901,  ..., 0.0874, 0.1896, 0.0541]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[-5.1521e-04,  1.0597e-03,  6.0671e-04,  ...,  1.6114e-04,\n",
            "          -4.4743e-05, -9.6632e-05],\n",
            "         [ 1.8622e-03, -3.8301e-03, -2.1929e-03,  ..., -5.8243e-04,\n",
            "           1.6172e-04,  3.4927e-04],\n",
            "         [-6.7655e-03,  1.3915e-02,  7.9669e-03,  ...,  2.1160e-03,\n",
            "          -5.8754e-04, -1.2689e-03],\n",
            "         ...,\n",
            "         [-1.8570e-04,  3.8194e-04,  2.1868e-04,  ...,  5.8082e-05,\n",
            "          -1.6127e-05, -3.4830e-05],\n",
            "         [-7.1849e-04,  1.4777e-03,  8.4608e-04,  ...,  2.2472e-04,\n",
            "          -6.2396e-05, -1.3476e-04],\n",
            "         [ 1.7262e-03, -3.5503e-03, -2.0327e-03,  ..., -5.3989e-04,\n",
            "           1.4991e-04,  3.2376e-04]],\n",
            "\n",
            "        [[ 1.0845e-04, -1.2977e-04, -1.1616e-04,  ..., -7.5668e-06,\n",
            "           3.6909e-05,  1.0736e-05],\n",
            "         [ 2.6220e-03, -3.1374e-03, -2.8085e-03,  ..., -1.8294e-04,\n",
            "           8.9235e-04,  2.5957e-04],\n",
            "         [-1.2432e-02,  1.4875e-02,  1.3316e-02,  ...,  8.6738e-04,\n",
            "          -4.2309e-03, -1.2307e-03],\n",
            "         ...,\n",
            "         [-3.2235e-03,  3.8570e-03,  3.4526e-03,  ...,  2.2490e-04,\n",
            "          -1.0970e-03, -3.1910e-04],\n",
            "         [ 1.6968e-03, -2.0304e-03, -1.8175e-03,  ..., -1.1839e-04,\n",
            "           5.7748e-04,  1.6798e-04],\n",
            "         [ 6.3001e-04, -7.5384e-04, -6.7481e-04,  ..., -4.3957e-05,\n",
            "           2.1441e-04,  6.2367e-05]],\n",
            "\n",
            "        [[ 6.2452e-03, -5.5735e-03, -7.7153e-03,  ...,  1.4919e-03,\n",
            "           2.1873e-03, -9.3836e-04],\n",
            "         [-1.1825e-03,  1.0553e-03,  1.4608e-03,  ..., -2.8248e-04,\n",
            "          -4.1414e-04,  1.7767e-04],\n",
            "         [-1.9303e-02,  1.7227e-02,  2.3847e-02,  ..., -4.6113e-03,\n",
            "          -6.7606e-03,  2.9003e-03],\n",
            "         ...,\n",
            "         [-4.9002e-03,  4.3731e-03,  6.0536e-03,  ..., -1.1706e-03,\n",
            "          -1.7162e-03,  7.3626e-04],\n",
            "         [ 5.3881e-03, -4.8085e-03, -6.6564e-03,  ...,  1.2872e-03,\n",
            "           1.8871e-03, -8.0957e-04],\n",
            "         [ 4.1280e-03, -3.6839e-03, -5.0996e-03,  ...,  9.8613e-04,\n",
            "           1.4458e-03, -6.2024e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 5.5896e-03, -9.3982e-03, -1.3573e-02,  ...,  2.6306e-03,\n",
            "           3.5779e-03, -7.3032e-03],\n",
            "         [-7.8758e-03,  1.3242e-02,  1.9125e-02,  ..., -3.7065e-03,\n",
            "          -5.0413e-03,  1.0290e-02],\n",
            "         [-3.3518e-03,  5.6356e-03,  8.1391e-03,  ..., -1.5774e-03,\n",
            "          -2.1455e-03,  4.3793e-03],\n",
            "         ...,\n",
            "         [ 5.3884e-03, -9.0599e-03, -1.3085e-02,  ...,  2.5359e-03,\n",
            "           3.4491e-03, -7.0404e-03],\n",
            "         [ 8.1270e-03, -1.3664e-02, -1.9734e-02,  ...,  3.8247e-03,\n",
            "           5.2021e-03, -1.0618e-02],\n",
            "         [-1.3514e-03,  2.2722e-03,  3.2816e-03,  ..., -6.3599e-04,\n",
            "          -8.6504e-04,  1.7657e-03]],\n",
            "\n",
            "        [[ 8.0207e-03, -1.0130e-02, -8.7192e-03,  ...,  2.9815e-04,\n",
            "           5.8055e-04, -2.0775e-03],\n",
            "         [-1.1933e-02,  1.5071e-02,  1.2972e-02,  ..., -4.4359e-04,\n",
            "          -8.6373e-04,  3.0910e-03],\n",
            "         [-1.2889e-03,  1.6278e-03,  1.4011e-03,  ..., -4.7911e-05,\n",
            "          -9.3289e-05,  3.3385e-04],\n",
            "         ...,\n",
            "         [ 6.3838e-03, -8.0625e-03, -6.9398e-03,  ...,  2.3730e-04,\n",
            "           4.6207e-04, -1.6536e-03],\n",
            "         [ 1.2469e-02, -1.5748e-02, -1.3555e-02,  ...,  4.6351e-04,\n",
            "           9.0253e-04, -3.2298e-03],\n",
            "         [-3.9526e-03,  4.9919e-03,  4.2968e-03,  ..., -1.4693e-04,\n",
            "          -2.8609e-04,  1.0238e-03]],\n",
            "\n",
            "        [[ 4.0718e-03, -7.4992e-03, -8.7893e-03,  ..., -9.1887e-04,\n",
            "           4.8032e-04, -3.2239e-03],\n",
            "         [-7.4121e-03,  1.3651e-02,  1.6000e-02,  ...,  1.6727e-03,\n",
            "          -8.7435e-04,  5.8688e-03],\n",
            "         [-3.6398e-03,  6.7036e-03,  7.8568e-03,  ...,  8.2139e-04,\n",
            "          -4.2936e-04,  2.8819e-03],\n",
            "         ...,\n",
            "         [ 4.2514e-03, -7.8300e-03, -9.1770e-03,  ..., -9.5940e-04,\n",
            "           5.0150e-04, -3.3661e-03],\n",
            "         [ 7.6907e-03, -1.4164e-02, -1.6601e-02,  ..., -1.7356e-03,\n",
            "           9.0722e-04, -6.0893e-03],\n",
            "         [-3.3449e-03,  6.1606e-03,  7.2204e-03,  ...,  7.5485e-04,\n",
            "          -3.9458e-04,  2.6485e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1900, 0.1318, 0.1848,  ..., 0.0866, 0.1873, 0.0524],\n",
            "        [0.1936, 0.1336, 0.1879,  ..., 0.0901, 0.1915, 0.0520],\n",
            "        [0.1888, 0.1300, 0.1843,  ..., 0.0880, 0.1887, 0.0522],\n",
            "        ...,\n",
            "        [0.1882, 0.1310, 0.1843,  ..., 0.0904, 0.1914, 0.0531],\n",
            "        [0.1918, 0.1319, 0.1864,  ..., 0.0934, 0.1942, 0.0525],\n",
            "        [0.1911, 0.1319, 0.1860,  ..., 0.0929, 0.1937, 0.0526]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 1.2572e-05, -2.5675e-05, -1.5516e-05,  ..., -2.7925e-06,\n",
            "           6.0954e-07,  3.0366e-06],\n",
            "         [ 2.1866e-03, -4.4658e-03, -2.6987e-03,  ..., -4.8571e-04,\n",
            "           1.0602e-04,  5.2817e-04],\n",
            "         [-6.7745e-03,  1.3836e-02,  8.3611e-03,  ...,  1.5048e-03,\n",
            "          -3.2846e-04, -1.6363e-03],\n",
            "         ...,\n",
            "         [-1.1102e-04,  2.2674e-04,  1.3702e-04,  ...,  2.4661e-05,\n",
            "          -5.3828e-06, -2.6816e-05],\n",
            "         [-1.6514e-04,  3.3726e-04,  2.0381e-04,  ...,  3.6681e-05,\n",
            "          -8.0066e-06, -3.9888e-05],\n",
            "         [ 1.5535e-03, -3.1727e-03, -1.9173e-03,  ..., -3.4507e-04,\n",
            "           7.5320e-05,  3.7523e-04]],\n",
            "\n",
            "        [[-1.2923e-03,  1.6587e-03,  1.4277e-03,  ...,  8.3589e-05,\n",
            "          -5.1680e-04, -1.3813e-04],\n",
            "         [ 3.4058e-03, -4.3714e-03, -3.7626e-03,  ..., -2.2029e-04,\n",
            "           1.3620e-03,  3.6403e-04],\n",
            "         [-1.2432e-02,  1.5956e-02,  1.3734e-02,  ...,  8.0410e-04,\n",
            "          -4.9714e-03, -1.3287e-03],\n",
            "         ...,\n",
            "         [-2.6372e-03,  3.3849e-03,  2.9135e-03,  ...,  1.7058e-04,\n",
            "          -1.0546e-03, -2.8188e-04],\n",
            "         [ 7.8579e-04, -1.0086e-03, -8.6809e-04,  ..., -5.0826e-05,\n",
            "           3.1424e-04,  8.3989e-05],\n",
            "         [ 1.0453e-03, -1.3417e-03, -1.1548e-03,  ..., -6.7614e-05,\n",
            "           4.1803e-04,  1.1173e-04]],\n",
            "\n",
            "        [[ 5.1691e-03, -5.1037e-03, -5.6925e-03,  ...,  1.2886e-03,\n",
            "           1.8329e-03,  1.5253e-04],\n",
            "         [-1.6416e-03,  1.6208e-03,  1.8078e-03,  ..., -4.0924e-04,\n",
            "          -5.8209e-04, -4.8441e-05],\n",
            "         [-2.0303e-02,  2.0046e-02,  2.2358e-02,  ..., -5.0613e-03,\n",
            "          -7.1991e-03, -5.9910e-04],\n",
            "         ...,\n",
            "         [-1.8269e-03,  1.8037e-03,  2.0118e-03,  ..., -4.5542e-04,\n",
            "          -6.4778e-04, -5.3908e-05],\n",
            "         [ 4.5912e-03, -4.5331e-03, -5.0560e-03,  ...,  1.1445e-03,\n",
            "           1.6280e-03,  1.3548e-04],\n",
            "         [ 2.4454e-03, -2.4145e-03, -2.6930e-03,  ...,  6.0962e-04,\n",
            "           8.6710e-04,  7.2160e-05]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 5.8654e-03, -9.8323e-03, -1.3210e-02,  ...,  1.8908e-03,\n",
            "           3.3726e-03, -7.4022e-03],\n",
            "         [-7.6796e-03,  1.2874e-02,  1.7296e-02,  ..., -2.4756e-03,\n",
            "          -4.4158e-03,  9.6918e-03],\n",
            "         [-3.9165e-03,  6.5654e-03,  8.8208e-03,  ..., -1.2626e-03,\n",
            "          -2.2520e-03,  4.9427e-03],\n",
            "         ...,\n",
            "         [ 5.5910e-03, -9.3724e-03, -1.2592e-02,  ...,  1.8024e-03,\n",
            "           3.2149e-03, -7.0560e-03],\n",
            "         [ 8.3796e-03, -1.4047e-02, -1.8873e-02,  ...,  2.7013e-03,\n",
            "           4.8183e-03, -1.0575e-02],\n",
            "         [-1.7546e-03,  2.9413e-03,  3.9518e-03,  ..., -5.6563e-04,\n",
            "          -1.0089e-03,  2.2144e-03]],\n",
            "\n",
            "        [[ 7.7461e-03, -1.0537e-02, -8.7002e-03,  ...,  1.6526e-04,\n",
            "           3.8832e-04, -2.2572e-03],\n",
            "         [-1.1307e-02,  1.5380e-02,  1.2699e-02,  ..., -2.4123e-04,\n",
            "          -5.6681e-04,  3.2947e-03],\n",
            "         [-1.8918e-03,  2.5735e-03,  2.1249e-03,  ..., -4.0362e-05,\n",
            "          -9.4841e-05,  5.5128e-04],\n",
            "         ...,\n",
            "         [ 6.0363e-03, -8.2112e-03, -6.7798e-03,  ...,  1.2878e-04,\n",
            "           3.0261e-04, -1.7590e-03],\n",
            "         [ 1.2320e-02, -1.6759e-02, -1.3838e-02,  ...,  2.6285e-04,\n",
            "           6.1763e-04, -3.5901e-03],\n",
            "         [-3.7485e-03,  5.0991e-03,  4.2102e-03,  ..., -7.9974e-05,\n",
            "          -1.8792e-04,  1.0923e-03]],\n",
            "\n",
            "        [[ 3.7510e-03, -7.5601e-03, -8.5118e-03,  ..., -9.8668e-04,\n",
            "           2.6582e-04, -2.9006e-03],\n",
            "         [-6.9757e-03,  1.4059e-02,  1.5829e-02,  ...,  1.8349e-03,\n",
            "          -4.9434e-04,  5.3941e-03],\n",
            "         [-3.1940e-03,  6.4373e-03,  7.2477e-03,  ...,  8.4015e-04,\n",
            "          -2.2634e-04,  2.4698e-03],\n",
            "         ...,\n",
            "         [ 3.9535e-03, -7.9682e-03, -8.9713e-03,  ..., -1.0399e-03,\n",
            "           2.8017e-04, -3.0572e-03],\n",
            "         [ 7.1763e-03, -1.4464e-02, -1.6284e-02,  ..., -1.8877e-03,\n",
            "           5.0856e-04, -5.5493e-03],\n",
            "         [-3.1800e-03,  6.4092e-03,  7.2160e-03,  ...,  8.3648e-04,\n",
            "          -2.2535e-04,  2.4590e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1890, 0.1313, 0.1845,  ..., 0.0861, 0.1868, 0.0524],\n",
            "        [0.1917, 0.1301, 0.1853,  ..., 0.0900, 0.1906, 0.0533],\n",
            "        [0.1927, 0.1332, 0.1872,  ..., 0.0894, 0.1907, 0.0516],\n",
            "        ...,\n",
            "        [0.1904, 0.1318, 0.1856,  ..., 0.0924, 0.1931, 0.0526],\n",
            "        [0.1922, 0.1301, 0.1855,  ..., 0.0903, 0.1911, 0.0532],\n",
            "        [0.1887, 0.1311, 0.1846,  ..., 0.0910, 0.1917, 0.0530]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[-3.6027e-06,  7.1266e-06,  4.5671e-06,  ...,  9.5417e-07,\n",
            "          -3.1091e-08, -6.8299e-07],\n",
            "         [ 2.6034e-03, -5.1498e-03, -3.3002e-03,  ..., -6.8950e-04,\n",
            "           2.2467e-05,  4.9354e-04],\n",
            "         [-6.4643e-03,  1.2787e-02,  8.1947e-03,  ...,  1.7121e-03,\n",
            "          -5.5786e-05, -1.2255e-03],\n",
            "         ...,\n",
            "         [-2.5202e-04,  4.9853e-04,  3.1948e-04,  ...,  6.6748e-05,\n",
            "          -2.1749e-06, -4.7778e-05],\n",
            "         [-6.2044e-04,  1.2273e-03,  7.8653e-04,  ...,  1.6432e-04,\n",
            "          -5.3544e-06, -1.1762e-04],\n",
            "         [ 1.6887e-03, -3.3404e-03, -2.1407e-03,  ..., -4.4724e-04,\n",
            "           1.4573e-05,  3.2013e-04]],\n",
            "\n",
            "        [[-1.4541e-03,  1.9812e-03,  1.5468e-03,  ...,  1.0920e-04,\n",
            "          -5.7470e-04, -2.2799e-04],\n",
            "         [ 3.1835e-03, -4.3374e-03, -3.3863e-03,  ..., -2.3907e-04,\n",
            "           1.2582e-03,  4.9913e-04],\n",
            "         [-1.2461e-02,  1.6978e-02,  1.3255e-02,  ...,  9.3578e-04,\n",
            "          -4.9248e-03, -1.9537e-03],\n",
            "         ...,\n",
            "         [-1.4806e-03,  2.0172e-03,  1.5749e-03,  ...,  1.1119e-04,\n",
            "          -5.8515e-04, -2.3214e-04],\n",
            "         [ 1.0496e-03, -1.4301e-03, -1.1165e-03,  ..., -7.8823e-05,\n",
            "           4.1483e-04,  1.6457e-04],\n",
            "         [ 9.2792e-04, -1.2643e-03, -9.8702e-04,  ..., -6.9684e-05,\n",
            "           3.6673e-04,  1.4549e-04]],\n",
            "\n",
            "        [[ 4.6827e-03, -4.2990e-03, -5.4393e-03,  ...,  1.3816e-03,\n",
            "           1.8345e-03, -3.7862e-04],\n",
            "         [-1.4171e-03,  1.3010e-03,  1.6461e-03,  ..., -4.1811e-04,\n",
            "          -5.5517e-04,  1.1458e-04],\n",
            "         [-2.0332e-02,  1.8666e-02,  2.3617e-02,  ..., -5.9990e-03,\n",
            "          -7.9654e-03,  1.6439e-03],\n",
            "         ...,\n",
            "         [-2.7414e-03,  2.5168e-03,  3.1844e-03,  ..., -8.0886e-04,\n",
            "          -1.0740e-03,  2.2166e-04],\n",
            "         [ 5.6799e-03, -5.2145e-03, -6.5976e-03,  ...,  1.6759e-03,\n",
            "           2.2252e-03, -4.5924e-04],\n",
            "         [ 1.9346e-03, -1.7761e-03, -2.2472e-03,  ...,  5.7081e-04,\n",
            "           7.5791e-04, -1.5642e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 4.8714e-03, -9.2967e-03, -1.2322e-02,  ...,  1.9418e-03,\n",
            "           3.5903e-03, -6.8171e-03],\n",
            "         [-7.6594e-03,  1.4617e-02,  1.9374e-02,  ..., -3.0532e-03,\n",
            "          -5.6451e-03,  1.0719e-02],\n",
            "         [-3.7436e-03,  7.1443e-03,  9.4691e-03,  ..., -1.4923e-03,\n",
            "          -2.7591e-03,  5.2388e-03],\n",
            "         ...,\n",
            "         [ 5.4086e-03, -1.0322e-02, -1.3681e-02,  ...,  2.1560e-03,\n",
            "           3.9862e-03, -7.5689e-03],\n",
            "         [ 7.4020e-03, -1.4126e-02, -1.8723e-02,  ...,  2.9506e-03,\n",
            "           5.4554e-03, -1.0358e-02],\n",
            "         [-1.7265e-03,  3.2949e-03,  4.3671e-03,  ..., -6.8822e-04,\n",
            "          -1.2725e-03,  2.4161e-03]],\n",
            "\n",
            "        [[ 7.7551e-03, -9.9858e-03, -9.4846e-03,  ...,  4.3749e-04,\n",
            "           1.0515e-03, -3.3941e-03],\n",
            "         [-1.0281e-02,  1.3238e-02,  1.2574e-02,  ..., -5.7998e-04,\n",
            "          -1.3939e-03,  4.4996e-03],\n",
            "         [-2.7588e-03,  3.5524e-03,  3.3741e-03,  ..., -1.5563e-04,\n",
            "          -3.7405e-04,  1.2074e-03],\n",
            "         ...,\n",
            "         [ 4.9928e-03, -6.4290e-03, -6.1063e-03,  ...,  2.8166e-04,\n",
            "           6.7694e-04, -2.1852e-03],\n",
            "         [ 1.1181e-02, -1.4397e-02, -1.3675e-02,  ...,  6.3075e-04,\n",
            "           1.5160e-03, -4.8935e-03],\n",
            "         [-2.8889e-03,  3.7199e-03,  3.5332e-03,  ..., -1.6297e-04,\n",
            "          -3.9169e-04,  1.2644e-03]],\n",
            "\n",
            "        [[ 4.0658e-03, -6.9831e-03, -7.2786e-03,  ..., -8.3660e-04,\n",
            "           2.1580e-04, -2.8616e-03],\n",
            "         [-8.1532e-03,  1.4003e-02,  1.4596e-02,  ...,  1.6776e-03,\n",
            "          -4.3274e-04,  5.7385e-03],\n",
            "         [-4.1769e-03,  7.1738e-03,  7.4774e-03,  ...,  8.5945e-04,\n",
            "          -2.2169e-04,  2.9398e-03],\n",
            "         ...,\n",
            "         [ 4.4973e-03, -7.7242e-03, -8.0510e-03,  ..., -9.2539e-04,\n",
            "           2.3870e-04, -3.1654e-03],\n",
            "         [ 8.0568e-03, -1.3838e-02, -1.4423e-02,  ..., -1.6578e-03,\n",
            "           4.2762e-04, -5.6706e-03],\n",
            "         [-3.7614e-03,  6.4602e-03,  6.7336e-03,  ...,  7.7396e-04,\n",
            "          -1.9964e-04,  2.6474e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1916, 0.1325, 0.1864,  ..., 0.0889, 0.1897, 0.0512],\n",
            "        [0.1893, 0.1315, 0.1846,  ..., 0.0862, 0.1869, 0.0524],\n",
            "        [0.1917, 0.1330, 0.1857,  ..., 0.0878, 0.1886, 0.0525],\n",
            "        ...,\n",
            "        [0.1895, 0.1314, 0.1851,  ..., 0.0917, 0.1924, 0.0528],\n",
            "        [0.1900, 0.1297, 0.1848,  ..., 0.0892, 0.1892, 0.0538],\n",
            "        [0.1875, 0.1308, 0.1839,  ..., 0.0898, 0.1911, 0.0532]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[-2.8588e-04,  5.7494e-04,  3.6710e-04,  ...,  7.5691e-05,\n",
            "          -1.1729e-06, -5.4517e-05],\n",
            "         [ 2.1164e-03, -4.2564e-03, -2.7177e-03,  ..., -5.6035e-04,\n",
            "           8.6832e-06,  4.0360e-04],\n",
            "         [-7.1347e-03,  1.4349e-02,  9.1618e-03,  ...,  1.8890e-03,\n",
            "          -2.9272e-05, -1.3606e-03],\n",
            "         ...,\n",
            "         [ 4.0637e-04, -8.1726e-04, -5.2183e-04,  ..., -1.0759e-04,\n",
            "           1.6673e-06,  7.7494e-05],\n",
            "         [ 8.5410e-05, -1.7177e-04, -1.0968e-04,  ..., -2.2614e-05,\n",
            "           3.5042e-07,  1.6288e-05],\n",
            "         [ 1.5148e-03, -3.0464e-03, -1.9451e-03,  ..., -4.0106e-04,\n",
            "           6.2148e-06,  2.8886e-04]],\n",
            "\n",
            "        [[-8.6560e-04,  1.0759e-03,  9.0480e-04,  ...,  8.5696e-05,\n",
            "          -2.8214e-04, -1.5536e-04],\n",
            "         [ 3.0768e-03, -3.8244e-03, -3.2161e-03,  ..., -3.0461e-04,\n",
            "           1.0029e-03,  5.5222e-04],\n",
            "         [-1.2771e-02,  1.5874e-02,  1.3349e-02,  ...,  1.2643e-03,\n",
            "          -4.1626e-03, -2.2920e-03],\n",
            "         ...,\n",
            "         [-1.7521e-03,  2.1778e-03,  1.8314e-03,  ...,  1.7346e-04,\n",
            "          -5.7109e-04, -3.1446e-04],\n",
            "         [ 1.9033e-03, -2.3658e-03, -1.9895e-03,  ..., -1.8843e-04,\n",
            "           6.2037e-04,  3.4160e-04],\n",
            "         [ 4.9948e-04, -6.2085e-04, -5.2210e-04,  ..., -4.9450e-05,\n",
            "           1.6281e-04,  8.9646e-05]],\n",
            "\n",
            "        [[ 5.1804e-03, -4.3259e-03, -6.1238e-03,  ...,  1.3996e-03,\n",
            "           1.7864e-03, -4.5504e-04],\n",
            "         [-1.1796e-03,  9.8506e-04,  1.3945e-03,  ..., -3.1870e-04,\n",
            "          -4.0678e-04,  1.0362e-04],\n",
            "         [-1.9793e-02,  1.6528e-02,  2.3398e-02,  ..., -5.3474e-03,\n",
            "          -6.8254e-03,  1.7386e-03],\n",
            "         ...,\n",
            "         [-2.7911e-03,  2.3307e-03,  3.2993e-03,  ..., -7.5404e-04,\n",
            "          -9.6245e-04,  2.4516e-04],\n",
            "         [ 6.0480e-03, -5.0504e-03, -7.1493e-03,  ...,  1.6339e-03,\n",
            "           2.0855e-03, -5.3124e-04],\n",
            "         [ 3.6664e-03, -3.0617e-03, -4.3341e-03,  ...,  9.9053e-04,\n",
            "           1.2643e-03, -3.2205e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 6.0467e-03, -8.7175e-03, -1.1427e-02,  ...,  1.7336e-03,\n",
            "           3.3879e-03, -6.1384e-03],\n",
            "         [-9.7939e-03,  1.4120e-02,  1.8509e-02,  ..., -2.8079e-03,\n",
            "          -5.4874e-03,  9.9424e-03],\n",
            "         [-4.2512e-03,  6.1290e-03,  8.0342e-03,  ..., -1.2188e-03,\n",
            "          -2.3819e-03,  4.3157e-03],\n",
            "         ...,\n",
            "         [ 7.0006e-03, -1.0093e-02, -1.3230e-02,  ...,  2.0071e-03,\n",
            "           3.9223e-03, -7.1067e-03],\n",
            "         [ 1.0115e-02, -1.4583e-02, -1.9117e-02,  ...,  2.9001e-03,\n",
            "           5.6675e-03, -1.0269e-02],\n",
            "         [-1.7755e-03,  2.5598e-03,  3.3555e-03,  ..., -5.0904e-04,\n",
            "          -9.9480e-04,  1.8025e-03]],\n",
            "\n",
            "        [[ 7.7421e-03, -1.0881e-02, -1.0013e-02,  ...,  8.9627e-04,\n",
            "           2.3383e-04, -2.4780e-03],\n",
            "         [-1.0940e-02,  1.5375e-02,  1.4149e-02,  ..., -1.2665e-03,\n",
            "          -3.3042e-04,  3.5016e-03],\n",
            "         [-1.8163e-03,  2.5526e-03,  2.3490e-03,  ..., -2.1026e-04,\n",
            "          -5.4857e-05,  5.8135e-04],\n",
            "         ...,\n",
            "         [ 4.6414e-03, -6.5229e-03, -6.0025e-03,  ...,  5.3731e-04,\n",
            "           1.4018e-04, -1.4856e-03],\n",
            "         [ 1.1983e-02, -1.6841e-02, -1.5497e-02,  ...,  1.3872e-03,\n",
            "           3.6191e-04, -3.8354e-03],\n",
            "         [-2.8720e-03,  4.0363e-03,  3.7143e-03,  ..., -3.3248e-04,\n",
            "          -8.6742e-05,  9.1925e-04]],\n",
            "\n",
            "        [[ 4.4409e-03, -6.3769e-03, -7.7274e-03,  ..., -9.4191e-04,\n",
            "           5.2397e-04, -2.8289e-03],\n",
            "         [-8.3774e-03,  1.2029e-02,  1.4577e-02,  ...,  1.7768e-03,\n",
            "          -9.8842e-04,  5.3365e-03],\n",
            "         [-4.6160e-03,  6.6283e-03,  8.0321e-03,  ...,  9.7905e-04,\n",
            "          -5.4463e-04,  2.9405e-03],\n",
            "         ...,\n",
            "         [ 4.8289e-03, -6.9340e-03, -8.4025e-03,  ..., -1.0242e-03,\n",
            "           5.6975e-04, -3.0761e-03],\n",
            "         [ 8.4978e-03, -1.2202e-02, -1.4787e-02,  ..., -1.8024e-03,\n",
            "           1.0026e-03, -5.4132e-03],\n",
            "         [-4.3682e-03,  6.2725e-03,  7.6009e-03,  ...,  9.2649e-04,\n",
            "          -5.1539e-04,  2.7826e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1875, 0.1299, 0.1840,  ..., 0.0879, 0.1888, 0.0522],\n",
            "        [0.1924, 0.1301, 0.1856,  ..., 0.0904, 0.1912, 0.0532],\n",
            "        [0.1870, 0.1310, 0.1837,  ..., 0.0858, 0.1863, 0.0522],\n",
            "        ...,\n",
            "        [0.1920, 0.1319, 0.1866,  ..., 0.0935, 0.1943, 0.0525],\n",
            "        [0.1887, 0.1312, 0.1846,  ..., 0.0910, 0.1918, 0.0529],\n",
            "        [0.1909, 0.1319, 0.1858,  ..., 0.0928, 0.1935, 0.0526]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "train jepa world_state_ tensor([[[ 3.5548e-04, -6.8019e-04, -4.1218e-04,  ..., -7.6452e-05,\n",
            "           4.0657e-05,  3.4298e-05],\n",
            "         [ 1.7924e-03, -3.4297e-03, -2.0783e-03,  ..., -3.8549e-04,\n",
            "           2.0500e-04,  1.7294e-04],\n",
            "         [-6.5212e-03,  1.2478e-02,  7.5614e-03,  ...,  1.4025e-03,\n",
            "          -7.4585e-04, -6.2921e-04],\n",
            "         ...,\n",
            "         [-4.8166e-05,  9.2165e-05,  5.5850e-05,  ...,  1.0359e-05,\n",
            "          -5.5089e-06, -4.6474e-06],\n",
            "         [-6.9124e-04,  1.3227e-03,  8.0150e-04,  ...,  1.4866e-04,\n",
            "          -7.9059e-05, -6.6695e-05],\n",
            "         [ 1.7715e-03, -3.3897e-03, -2.0541e-03,  ..., -3.8100e-04,\n",
            "           2.0261e-04,  1.7092e-04]],\n",
            "\n",
            "        [[-1.0164e-04,  1.3721e-04,  1.1147e-04,  ...,  1.2162e-05,\n",
            "          -3.3950e-05, -1.4896e-05],\n",
            "         [ 2.2998e-03, -3.1047e-03, -2.5223e-03,  ..., -2.7520e-04,\n",
            "           7.6819e-04,  3.3705e-04],\n",
            "         [-1.1121e-02,  1.5013e-02,  1.2197e-02,  ...,  1.3307e-03,\n",
            "          -3.7146e-03, -1.6298e-03],\n",
            "         ...,\n",
            "         [-1.7505e-03,  2.3631e-03,  1.9198e-03,  ...,  2.0947e-04,\n",
            "          -5.8470e-04, -2.5654e-04],\n",
            "         [ 2.2595e-03, -3.0503e-03, -2.4781e-03,  ..., -2.7038e-04,\n",
            "           7.5472e-04,  3.3114e-04],\n",
            "         [ 7.5686e-04, -1.0217e-03, -8.3009e-04,  ..., -9.0569e-05,\n",
            "           2.5281e-04,  1.1092e-04]],\n",
            "\n",
            "        [[ 8.2207e-03, -6.7455e-03, -9.2671e-03,  ...,  2.4415e-03,\n",
            "           2.5498e-03, -5.8415e-04],\n",
            "         [-2.4200e-03,  1.9857e-03,  2.7280e-03,  ..., -7.1873e-04,\n",
            "          -7.5059e-04,  1.7196e-04],\n",
            "         [-2.0079e-02,  1.6476e-02,  2.2635e-02,  ..., -5.9634e-03,\n",
            "          -6.2278e-03,  1.4268e-03],\n",
            "         ...,\n",
            "         [-3.0008e-03,  2.4623e-03,  3.3828e-03,  ..., -8.9124e-04,\n",
            "          -9.3075e-04,  2.1323e-04],\n",
            "         [ 6.2993e-03, -5.1688e-03, -7.1011e-03,  ...,  1.8709e-03,\n",
            "           1.9538e-03, -4.4761e-04],\n",
            "         [ 3.3490e-03, -2.7480e-03, -3.7753e-03,  ...,  9.9465e-04,\n",
            "           1.0387e-03, -2.3797e-04]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 4.9889e-03, -8.6645e-03, -1.0943e-02,  ...,  8.1322e-04,\n",
            "           2.8795e-03, -4.6365e-03],\n",
            "         [-8.4606e-03,  1.4694e-02,  1.8557e-02,  ..., -1.3791e-03,\n",
            "          -4.8833e-03,  7.8629e-03],\n",
            "         [-4.0820e-03,  7.0894e-03,  8.9533e-03,  ..., -6.6538e-04,\n",
            "          -2.3560e-03,  3.7936e-03],\n",
            "         ...,\n",
            "         [ 6.3574e-03, -1.1041e-02, -1.3944e-02,  ...,  1.0363e-03,\n",
            "           3.6694e-03, -5.9083e-03],\n",
            "         [ 8.9264e-03, -1.5503e-02, -1.9579e-02,  ...,  1.4550e-03,\n",
            "           5.1521e-03, -8.2958e-03],\n",
            "         [-2.0973e-03,  3.6425e-03,  4.6002e-03,  ..., -3.4187e-04,\n",
            "          -1.2105e-03,  1.9492e-03]],\n",
            "\n",
            "        [[ 7.1380e-03, -1.0800e-02, -1.0363e-02,  ...,  1.7700e-04,\n",
            "           7.2939e-04, -3.0318e-03],\n",
            "         [-9.6602e-03,  1.4616e-02,  1.4025e-02,  ..., -2.3954e-04,\n",
            "          -9.8712e-04,  4.1030e-03],\n",
            "         [-2.6585e-03,  4.0223e-03,  3.8596e-03,  ..., -6.5920e-05,\n",
            "          -2.7165e-04,  1.1291e-03],\n",
            "         ...,\n",
            "         [ 3.6608e-03, -5.5389e-03, -5.3149e-03,  ...,  9.0774e-05,\n",
            "           3.7408e-04, -1.5549e-03],\n",
            "         [ 1.0593e-02, -1.6027e-02, -1.5379e-02,  ...,  2.6266e-04,\n",
            "           1.0824e-03, -4.4992e-03],\n",
            "         [-2.4560e-03,  3.7160e-03,  3.5657e-03,  ..., -6.0899e-05,\n",
            "          -2.5096e-04,  1.0431e-03]],\n",
            "\n",
            "        [[ 4.1919e-03, -5.9597e-03, -7.3705e-03,  ..., -6.8807e-04,\n",
            "           5.3916e-04, -2.9219e-03],\n",
            "         [-8.3130e-03,  1.1819e-02,  1.4617e-02,  ...,  1.3645e-03,\n",
            "          -1.0692e-03,  5.7945e-03],\n",
            "         [-4.2204e-03,  6.0003e-03,  7.4207e-03,  ...,  6.9276e-04,\n",
            "          -5.4283e-04,  2.9418e-03],\n",
            "         ...,\n",
            "         [ 5.1640e-03, -7.3419e-03, -9.0799e-03,  ..., -8.4765e-04,\n",
            "           6.6420e-04, -3.5995e-03],\n",
            "         [ 8.3670e-03, -1.1896e-02, -1.4712e-02,  ..., -1.3734e-03,\n",
            "           1.0762e-03, -5.8321e-03],\n",
            "         [-4.4006e-03,  6.2565e-03,  7.7375e-03,  ...,  7.2234e-04,\n",
            "          -5.6600e-04,  3.0674e-03]]], grad_fn=<AddBackward0>)\n",
            "train jepa sy_ tensor([[0.1908, 0.1323, 0.1862,  ..., 0.0884, 0.1891, 0.0510],\n",
            "        [0.1913, 0.1305, 0.1854,  ..., 0.0895, 0.1906, 0.0517],\n",
            "        [0.1903, 0.1320, 0.1850,  ..., 0.0869, 0.1875, 0.0524],\n",
            "        ...,\n",
            "        [0.1896, 0.1315, 0.1852,  ..., 0.0918, 0.1924, 0.0528],\n",
            "        [0.1905, 0.1398, 0.1890,  ..., 0.0898, 0.1908, 0.0513],\n",
            "        [0.1896, 0.1372, 0.1865,  ..., 0.0876, 0.1903, 0.0504]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(10):\n",
        "    # # buffer=[]\n",
        "    # print(\"#### simulate ####\")\n",
        "    # buffer = simulate(agent, buffer)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    agent.train_jepa(train_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZtFWQs1krGZ",
        "outputId": "0015e196-ec27-43a1-dda4-e0f4e30baaf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "154\n"
          ]
        }
      ],
      "source": [
        "# for x in buffer:\n",
        "#     print(len(x))\n",
        "print(len(buffer))\n",
        "data=buffer\n",
        "buffer=data[-128:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8zxYU9jpE8K"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "###save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZeny7pRU6bG",
        "outputId": "f6cc5da7-fb7a-428a-d481-e1e18a9fe024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.8929]], device='cuda:0', requires_grad=True)\n",
            "argm 0.3448590040206909 0.46135491132736206\n",
            "argm 0.34481173753738403 0.22136220335960388\n",
            "argm 0.344801664352417 0.2574569582939148\n",
            "argm 0.34480124711990356 0.2916858494281769\n",
            "argm 0.344801127910614 0.28045156598091125\n",
            "argm 0.34480106830596924 0.26978030800819397\n",
            "argm 0.3448010981082916 0.30337217450141907\n",
            "argm 0.3448011875152588 0.2915523052215576\n",
            "argm 0.344801127910614 0.2803245782852173\n",
            "argm 0.3448010981082916 0.2696596682071686\n",
            "argm 0.3448010981082916 0.3032577931880951\n",
            "argm 0.3448011875152588 0.2914435565471649\n",
            "argm 0.344801127910614 0.28022152185440063\n",
            "argm 0.3448010981082916 0.269561767578125\n",
            "argm 0.3448010981082916 0.3031650483608246\n",
            "argm 0.3448011875152588 0.29135534167289734\n",
            "argm 0.344801127910614 0.2801375687122345\n",
            "argm 0.3448010981082916 0.2694820165634155\n",
            "argm 0.3448010981082916 0.3030891716480255\n",
            "argm 0.3448011577129364 0.29128339886665344\n"
          ]
        }
      ],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "batch=sx.size(dim=0)\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*4 -2\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*3 -1.5\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone())\n",
        "print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e2)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 20\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "64a96179-0b71-43d0-8ae7-c6cfae51844c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.4)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.7.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240706_095823-5d0bao6j</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/5d0bao6j' target=\"_blank\">fanciful-shadow-1</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/5d0bao6j' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/5d0bao6j</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"procgen\",\n",
        "    config={\n",
        "        \"model\": \"res18\",\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUCet57LcPdf",
        "outputId": "a27b59d8-f5a6-418b-a8dc-cc6b7a71607b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0001010101010102\n",
            "1.0101020202020203\n",
            "1.0203050709111317\n",
            "1.0307163471449186\n",
            "1.041342288868062\n",
            "1.0521896043771044\n",
            "1.0632652844231791\n",
            "1.0745766172361917\n",
            "1.0861312045183014\n",
            "1.097936978480457\n",
            "1.1100022200022202\n",
            "1.1223355780022448\n",
            "1.1349460901146295\n",
            "1.147843204775023\n",
            "1.1610368048299085\n",
            "1.174537232793047\n",
            "1.18835531788473\n",
            "1.2025024050024051\n",
            "1.2169903857855666\n",
            "1.2318317319536833\n",
            "1.2470395311136053\n",
            "1.2626275252525254\n",
            "1.2786101521544562\n",
            "1.29500259000259\n",
            "1.311820805457169\n",
            "1.3290816055289738\n",
            "1.3468026936026936\n",
            "1.36500273000273\n",
            "1.383701397537014\n",
            "1.4029194725028058\n",
            "1.4226789016929862\n",
            "1.4430028860028858\n",
            "1.4639159713072756\n",
            "1.485444147355912\n",
            "1.5076149555254033\n",
            "1.5304576063666973\n",
            "1.5540031080031078\n",
            "1.5782844065656563\n",
            "1.6033365400032062\n",
            "1.6291968067774514\n",
            "1.6559049511508523\n",
            "1.6835033670033666\n",
            "1.7120373223763048\n",
            "1.7415552072448617\n",
            "1.7721088073719646\n",
            "1.8037536075036067\n",
            "1.8365491276400356\n",
            "1.8705592966704068\n",
            "1.9058528683056974\n",
            "1.9425038850038838\n",
            "1.980592196474548\n",
            "2.020204040404039\n",
            "2.0614326942898358\n",
            "2.104379208754207\n",
            "2.1491532344723816\n",
            "2.195873956960912\n",
            "2.244671156004488\n",
            "2.2956864095500444\n",
            "2.3490744655860922\n",
            "2.4050048100048085\n",
            "2.463663463907365\n",
            "2.5252550505050486\n",
            "2.590005180005178\n",
            "2.658163211057946\n",
            "2.730005460005458\n",
            "2.8058389450056094\n",
            "2.8860057720057695\n",
            "2.970888294711821\n",
            "3.0609152127333914\n",
            "3.15656881313131\n",
            "3.2583936135549005\n",
            "3.367006734006731\n",
            "3.483110414489722\n",
            "3.6075072150072125\n",
            "3.7411185933408126\n",
            "3.885007770007767\n",
            "4.040408080808078\n",
            "4.208758417508415\n",
            "4.391747913921824\n",
            "4.591372819100089\n",
            "4.810009620009617\n",
            "5.050510101010098\n",
            "5.316326422115893\n",
            "5.61167789001122\n",
            "5.941776589423644\n",
            "6.3131376262626215\n",
            "6.734013468013463\n",
            "7.215014430014424\n",
            "7.7700155400155335\n",
            "8.417516835016828\n",
            "9.182745638200176\n",
            "10.101020202020194\n",
            "11.22335578002244\n",
            "12.626275252525245\n",
            "14.43002886002885\n",
            "16.83503367003366\n",
            "20.202040404040392\n",
            "25.25255050505049\n",
            "33.67006734006732\n",
            "50.50510101010098\n"
          ]
        }
      ],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-nT5j864BIn",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "# buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "### trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wUhKd009Qvk3"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}