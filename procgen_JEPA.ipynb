{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WkwnVjJTrW1",
        "outputId": "70d921d9-966f-4fbd-b26d-74e2d1a86bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -qq procgen\n",
        "# !pip install -qq procgen faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "# import faiss\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        # print(\"update\", self.linsx.shape, lsx.shape)\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEY9MmwZhA8a",
        "outputId": "153d6ad9-4be2-45bc-9316-bb5ef77f2916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1278976\n",
            "torch.Size([4, 256])\n",
            "1278979\n",
            "torch.Size([4, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model=256, drop=0.5):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278976\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Dropout(p=drop),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 1024):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278979\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model,4*d_list[4]), nn.ReLU(),\n",
        "            # nn.Linear(d_list[5],4*d_list[4]), nn.ReLU(),\n",
        "            nn.Unflatten(-1, (d_list[4],2,2)),\n",
        "            # nn.Unflatten(-1, (d_list[5],1,1)),\n",
        "            # nn.ConvTranspose2d(d_list[5], d_list[4], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[4], d_list[3], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[3], d_list[2], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.decoder(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "conv = Conv().to(device)\n",
        "print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = conv(input)\n",
        "print(out.shape)\n",
        "\n",
        "deconv = Deconv(256).to(device)\n",
        "print(sum(p.numel() for p in deconv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,256), device=device)\n",
        "out = deconv(input)\n",
        "print(out.shape)\n",
        "\n",
        "# print(conv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "Bos81kQf1dwh"
      },
      "outputs": [],
      "source": [
        "# @title transfer_sd store_sd load_sd\n",
        "\n",
        "def transfer_sd(tgt_sd, src_sd): #\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in src_sd.keys(): continue\n",
        "            tgt_wht, src_wht = tgt_sd[wht_name], src_sd[wht_name]\n",
        "            # print(wht_name, tgt_wht.shape, src_wht.shape)\n",
        "            if tgt_wht.shape==src_wht.shape:\n",
        "                tgt_wht.copy_(src_wht)\n",
        "                continue\n",
        "            if tgt_wht.shape[0] != src_wht.shape[0]: continue # output dim diff\n",
        "            if len(tgt_wht.shape)==2: tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "    return tgt_sd\n",
        "\n",
        "def store_sd(all_sd, new_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in new_sd.keys():\n",
        "            if not wht_name in all_sd.keys():\n",
        "                # print(wht_name, new_sd[wht_name].shape)\n",
        "                all_sd[wht_name] = (new_sd[wht_name],)\n",
        "                continue\n",
        "            all_tpl, new_wht = all_sd[wht_name], new_sd[wht_name]\n",
        "            for all_wht in all_tpl:\n",
        "                print(wht_name, all_wht.shape, new_wht.shape)\n",
        "                if all_wht.shape==new_wht.shape:\n",
        "                    all_wht = new_wht\n",
        "                    break\n",
        "                if all_wht.shape[0] != new_wht.shape[0]: continue # diff output shape\n",
        "                if len(all_wht.shape)==2: all_wht[:, :new_wht.shape[1]] = new_wht[:, :all_wht.shape[1]]\n",
        "                break\n",
        "            if len(all_wht.shape)>=2 and len(all_wht.shape)>=2:\n",
        "                if all_wht.shape[0] != new_wht.shape[0]: all_tpl = all_tpl + (new_wht,) # wht not in all_wht\n",
        "    return all_sd\n",
        "\n",
        "def load_sd(tgt_sd, all_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in all_sd.keys(): continue\n",
        "            tgt_wht, all_tpl = tgt_sd[wht_name], all_sd[wht_name]\n",
        "            for all_wht in all_tpl:\n",
        "                # try: print(wht_name, tgt_wht.shape, all_wht.shape)\n",
        "                # except: print(wht_name, tgt_wht, all_wht)\n",
        "                if tgt_wht.shape==all_wht.shape:\n",
        "                    tgt_wht.copy_(all_wht)\n",
        "                    break\n",
        "                if tgt_wht.shape[0] != all_wht.shape[0]: continue # output dim diff\n",
        "                if len(tgt_wht.shape)==2: tgt_wht[:, :all_wht.shape[1]].copy_(all_wht[:, :tgt_wht.shape[1]])\n",
        "                break\n",
        "    return tgt_sd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256, dim_a=3, dim_z=3, dim_v=512).to(device)\n",
        "\n",
        "# modelsd = torch.load('agent.pkl', map_location=device).values()\n",
        "# tgt_sd = transfer_sd(agent.state_dict(), modelsd)\n",
        "# agent.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# all_sd = {}\n",
        "# all_sd = store_sd(all_sd, agent1.state_dict())\n",
        "# print(all_sd.keys())\n",
        "# checkpoint = {'model': all_sd}\n",
        "# torch.save(checkpoint, 'all_sd.pkl')\n",
        "\n",
        "# agent3 = Agent(d_model=256, dim_a=3, dim_z=1, dim_v=512).to(device)\n",
        "# agent3.tcost = tcost3\n",
        "# tgt_sd = load_sd(agent3.state_dict(), all_sd)\n",
        "# agent3.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "# for x,y in zip(agent1.state_dict().values(), agent3.state_dict().values()):\n",
        "#     print((x==y).all())\n",
        "\n",
        "# print(agent1.jepa.enc.cnn[1].num_batches_tracked)\n",
        "# jepa.enc.cnn.0.weight\n",
        "# print(agent1.jepa.enc.cnn[0].weight.shape)\n",
        "# print(agent1.jepa.enc.cnn[0].weight[0][0])\n",
        "# print(agent3.jepa.enc.cnn[0].weight[0][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "SFVbGqMDqcDR"
      },
      "outputs": [],
      "source": [
        "# @title rename_sd\n",
        "def rename_sd(agent_sd):\n",
        "    sd_={}\n",
        "    convert={}\n",
        "    na_=''\n",
        "    for wht_name, wht in agent_sd.items():\n",
        "        o=wht_name.split('.')\n",
        "        # print(\"####\", wht_name)\n",
        "        name=wht_name\n",
        "        for i in range(len(o)):\n",
        "            c = o[i]\n",
        "            if c.isnumeric():\n",
        "                na, me = '.'.join(o[:i]), '.'.join(o[i+1:])\n",
        "                c=int(c)\n",
        "                if na!=na_: # param name diff\n",
        "                    j=0 # reset num\n",
        "                    c_=c # track wht_name num\n",
        "                    na_=na # track param name\n",
        "                elif c_<c: # same param name, diff num\n",
        "                    j+=1\n",
        "                    c_=c\n",
        "                name = f'{na}.{j}.{me}'\n",
        "        # print(name)\n",
        "        sd_[name] = wht\n",
        "        convert[name] = wht_name\n",
        "    return sd_, convert\n",
        "\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# # modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, _ = rename_sd(modelsd)\n",
        "\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "\n",
        "# modelsd = transfer_sd(agentsd, modelsd)\n",
        "# agent.load_state_dict(modelsd, strict=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "riBHnAAkkzrd"
      },
      "outputs": [],
      "source": [
        "# @title transfer_optim me\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# def transfer_optim(tgt_sd, src_sd, tgt_optim, src_optim): #\n",
        "def transfer_optim(tgt_sd, src_sd, tgt_optim_sd, src_optim_sd): #\n",
        "    non_lst = ['running_mean', 'running_var', 'num_batches_tracked', 'num_batches_tracked', 'loss_fn']\n",
        "    tgt_lst, src_lst = [], []\n",
        "    for i, (k,v) in enumerate(tgt_sd.items()):\n",
        "        # print(i, k, v.shape, any(s in k for s in non_lst))\n",
        "        if not any(s in k for s in non_lst): tgt_lst.append(k)\n",
        "    for i, (k,v) in enumerate(src_sd.items()):\n",
        "        if not any(s in k for s in non_lst): src_lst.append(k)\n",
        "\n",
        "    # tgt_optim_st, src_optim_st = tgt_optim.state_dict()['state'], src_optim.state_dict()['state']\n",
        "    tgt_optim_st, src_optim_st = tgt_optim_sd['state'], src_optim_sd['state']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, wht_name in enumerate(tgt_lst):\n",
        "            if not wht_name in src_lst: continue\n",
        "            tgt_wht, src_wht = tgt_optim_st[tgt_lst.index(wht_name)], src_optim_st[src_lst.index(wht_name)]\n",
        "            # print(wht_name, tgt_wht, src_wht)\n",
        "            tgt_shp, src_shp = tgt_wht['exp_avg'].shape, src_wht['exp_avg'].shape\n",
        "            if tgt_shp==src_shp:\n",
        "                tgt_wht = src_wht\n",
        "                continue\n",
        "            if tgt_shp[0] != src_shp[0]: continue # output dim diff\n",
        "            if len(tgt_shp)==2:\n",
        "                tgt_wht['step'] = src_wht['step']\n",
        "                tgt_wht['exp_avg'][:, :src_shp[1]] = src_wht['exp_avg'][:, :tgt_shp[1]]\n",
        "                tgt_wht['exp_avg_sq'][:, :src_shp[1]] = src_wht['exp_avg_sq'][:, :tgt_shp[1]]\n",
        "    # return tgt_optim.state_dict()\n",
        "    return tgt_optim_sd\n",
        "\n",
        "# model_src = torch.nn.Linear(10, 5)  # Example source model\n",
        "# model_tgt = torch.nn.Linear(20, 5)  # Example target model (with more input dimensions)\n",
        "\n",
        "# model_src = nn.Sequential( # trained cost\n",
        "#     nn.Linear(10, 5, bias=False), nn.Softmax(),\n",
        "#     )\n",
        "# d_model=4\n",
        "# model_tgt = nn.Sequential( # trained cost\n",
        "#     nn.Linear(20, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, 5), nn.Softmax(),\n",
        "#     )\n",
        "\n",
        "# source_optimizer = optim.AdamW(model_src.parameters())\n",
        "# target_optimizer = optim.AdamW(model_tgt.parameters())\n",
        "\n",
        "# dummy_input = torch.randn(3, 10)\n",
        "# dummy_target = torch.randn(3, 5)\n",
        "# criterion = torch.nn.MSELoss()\n",
        "# output = model_src(dummy_input)\n",
        "# loss = criterion(output, dummy_target)\n",
        "# loss.backward()\n",
        "# source_optimizer.step()\n",
        "\n",
        "# dummy_input = torch.randn(3, 20)\n",
        "# output = model_tgt(dummy_input)\n",
        "# loss = criterion(output, dummy_target)\n",
        "# loss.backward()\n",
        "# target_optimizer.step()\n",
        "\n",
        "\n",
        "# print(source_optimizer.state_dict())\n",
        "# print(target_optimizer.state_dict())\n",
        "\n",
        "# optimsd = transfer_optim(model_tgt.state_dict(), model_src.state_dict(), target_optimizer, source_optimizer)\n",
        "# target_optimizer.load_state_dict(optimsd)\n",
        "# print(target_optimizer.state_dict())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "AfjFbveH64Io",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title TCost\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TCost(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=256): # in_dim=(1+self.jepa.pred.num_layers)*d_model\n",
        "        super().__init__()\n",
        "        self.tc = torch.tensor([-1., 0.], device=device).unsqueeze(-1) # unsqueeze(0).T\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            nn.Dropout(p=0.),\n",
        "            nn.Linear(in_dim, 2, bias=False), nn.Softmax(dim=-1),\n",
        "            # nn.Linear(in_dim, d_model), nn.ReLU(),\n",
        "            # nn.Dropout(p=0.5),\n",
        "            # nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, 2), nn.Softmax(),\n",
        "            )\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def update_loss_weight(self, train_data):\n",
        "        a = len(buffer)/len(train_data.data) # ratio dided/tt steps\n",
        "        # self.data = [step for episode in buffer for step in episode]\n",
        "        self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)], device=device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.tcost(x)@self.tc\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        out = self.tcost(x)\n",
        "        # print(\"ctost loss; x,out,y\",x, out, y)\n",
        "        # print(\"tcost loss; out\",out.min().item(),out.max().item(), out)\n",
        "        y = torch.where(y < -0.5, 0, 1)\n",
        "        # print(\"ctost loss\", out.shape, y.shape)\n",
        "        return self.loss_fn(out, y)\n",
        "\n",
        "\n",
        "# tcost=TCost(1024)\n",
        "# x=torch.rand(256,1024)\n",
        "# import time\n",
        "# start = time.time()\n",
        "# out=tcost(x)\n",
        "# # out=F.gumbel_softmax(out)\n",
        "# print(time.time()-start)\n",
        "# # nn.AdaptiveLogSoftmaxWithLoss(in_features=2, n_classes=2, cutoffs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuA25qQknUAX",
        "outputId": "98ee0296-0d8f-497c-d3cf-a1258d48e95c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-5d08c402e0cb>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v, drop=0.2):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = Conv(d_model) # pixel\n",
        "        # self.enc = ConvEnc(d_model) #\n",
        "        # self.enc = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.enc.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "        # self.pred = nn.Sequential(\n",
        "        #     nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=drop)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        # return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        return std_loss, cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy, lr=3e3, h0=None): # 3e3\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "        optim = torch.optim.SGD([z], lr=lr)\n",
        "        # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95))\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        for i in range(5): # 10\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sy_ = self.pred(sxaz)\n",
        "                sy_, _ = self.pred(sxaz, h0)\n",
        "                loss = lossfn(sy_, sy)# + self.z_coeff * torch.norm(z)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"argm\",i,loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        return z#.detach()\n",
        "\n",
        "# torch.norm(z, dim=-1)\n",
        "# -(z*torch.log(z)).sum(-1) # Shannon entropy archive.is/CaYrq\n",
        "# in RL, distribution of action, if certainty is high, entropy is low\n",
        "\n",
        "\n",
        "    # def loss(self, x, y, a, z=None):\n",
        "    #     sx, sy = self.enc(x), self.enc(y)\n",
        "    #     z = self.argm(sx, a, sy)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "    #     # v_c_loss = self.v_creg(self.exp(sx))\n",
        "    #     vx, vy = self.exp(sx), self.exp(sy)\n",
        "    #     v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "    #     return repr_loss + v_c_loss\n",
        "\n",
        "    # def forward(self, sx, a): # state, ctrl\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z=torch.zeros((batch,self.dim_z),device=device)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqkI44ygzfxu",
        "outputId": "00f3f53e-7600-4f2e-d273-f087ca579877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-5685ce5b2a51>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=8, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v, drop=0.2)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=20. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=20. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=0.01 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        self.closs_coeff=100.\n",
        "        self.zloss_coeff=20.\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        self.lx, self.lz = torch.empty((0,dim_a),device=device), torch.empty((0,dim_z),device=device) # [T,dim_az]\n",
        "        self.sx = self.jepa.enc(torch.zeros((1, 3,64,64)))\n",
        "        self.la = torch.empty(0,device=device)\n",
        "\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        if len(self.la)>1 or laction!=None:\n",
        "            self.update_h0(lstate, laction)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                self.sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "                # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(self.sx, T=8, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.la, self.lx, self.lz = lact, lx, lz\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx - torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    self.la = torch.cat([torch.tensor(laction, device=device), self.la[len(laction):]], dim=-1)\n",
        "                la = self.emb(self.la[:seq_len])\n",
        "\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5) # torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        optim_z = torch.optim.SGD([lz], lr=1e1) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e0 ; 3e-2 1e-1\n",
        "        lsx, la = lsx.detach(), la.detach() # [T, d_model], [T, dim_a]\n",
        "        # print(\"update_h0 lz\", lz.data)\n",
        "        self.jepa.pred.train()\n",
        "        for i in range(1): # 1?\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0.detach()) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                loss = F.mse_loss(out_, out.squeeze(0))\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            # print(\"update_h0 loss, lz\",i,loss.item(), lz.data)\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1].unsqueeze(0)\n",
        "        # print(\"update_h0\", self.lx.data)\n",
        "        # print(self.la.shape, self.lx.shape, self.lz.shape, self.la[seq_len:].shape, self.lx[seq_len:].shape, self.lz[seq_len:].shape)\n",
        "        self.la, self.lx, self.lz = self.la[seq_len:], self.lx[seq_len:], self.lz[seq_len:] # [T, dim_a], [T, dim_z]\n",
        "        return h0\n",
        "\n",
        "    def argm_s(self, sx, x, h0, lr=3e3): # 3e3\n",
        "        T, _ = x.shape\n",
        "        batch = 64 # 16\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        optim_z = torch.optim.SGD([z], lr=1e4, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-2, (0.9, 0.999), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-0, (0.9, 0.95), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        with torch.no_grad():\n",
        "            z[:,:self.lz.shape[0]] = self.lz[:T].unsqueeze(0).repeat(batch,1,1) # [batch, seq_len, dim_z]\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        x = x.detach().repeat(batch,1,1) # [batch, seq_len, dim_a]\n",
        "        # print(\"argm\", z[0].squeeze())\n",
        "        for i in range(2): # 5\n",
        "            # print(\"argm_search\", sx.shape, x.shape, z.shape, h0.shape) # [1, 256], [batch_size, T, 3], [batch_size, T, 8], [1, 1, 256]\n",
        "            loss, lsx, lh0,c = self.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            # print(i, \"argm z loss\", z[0].squeeze().data, loss[0].squeeze().data)\n",
        "            # print(\"c\", c)\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        return z[idx]#.unsqueeze(0)\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        x = nn.Parameter(torch.empty((T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        optim_x = torch.optim.SGD([x], lr=1e3) # 1e-1,1e-0,1e4 ; 1e2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        with torch.no_grad(): x[:self.lx.shape[0]] = self.lx[:T] # [seq_len, dim_az]\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        # print(\"search\",x.squeeze().data)\n",
        "        for i in range(2): # 5\n",
        "            dist = torch.norm(self.emb.weight.data.unsqueeze(0) - x.unsqueeze(-2), dim=-1) # [1,act_space,emb_dim], [T,1,emb_dim] -> [T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            z = self.argm_s(sx, x_,h0)\n",
        "            # print(\"search\", sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [T, dim_a], [T, dim_z], [1, 1, 256]\n",
        "            loss, lsx, lh0, c = self.rnn_pred(sx, x_.unsqueeze(0), z.unsqueeze(0), h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i,x.squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "            # print(i, \"search loss\", x.squeeze().data, loss.item(), z.squeeze().data)\n",
        "            # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "            # print(\"search c\", [f'{cc.item():g}' for cc in c.squeeze(0)]) # print(f'{cc:6f}')\n",
        "        dist = torch.norm(self.emb.weight.data.unsqueeze(0) - x.unsqueeze(-2), dim=-1) # [1,act_space,emb_dim], [T,1,emb_dim] -> [T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [T]\n",
        "        # print(lact.shape, lh0.shape, x.shape, z.shape) # [1]) torch.Size([1, 1, 1, 256]) torch.Size([1, 3]) torch.Size([1, 8])\n",
        "        return lact, lh0, x.data, z # [T], [T, num_layers, batch, d_model], [T, dim_a], [T, dim_z]\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, _ = la.shape\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        # lsx = torch.empty((batch, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            # print(sx.shape, a.shape, z.shape)\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "        icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        # syh0 = torch.cat([lsx.flatten(1),lh0.permute(2,0,1,3).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "        syh0 = torch.cat([lsx[:,1:], lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,T,d_model], [T,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "        tcost = -self.tcost(syh0).unflatten(0, (batch, seq_len)).squeeze(-1)\n",
        "        c = (tcost + icost)*gamma**torch.arange(seq_len, device=device)\n",
        "        return c.sum(), lsx, lh0, c\n",
        "        # return c.sum(), lh0\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def argm(self, lsy, sy, h0, la, rwd):\n",
        "        self.tcost.eval()\n",
        "        batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "        lz = nn.Parameter(torch.zeros((batch_size, bptt, self.dim_z), device=device))\n",
        "        # torch.nn.init.xavier_uniform_(lz)\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "        # optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "        optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "        lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "        for i in range(3): # 10\n",
        "            sy_, h0_ = sy.detach(), h0.detach()\n",
        "            lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "            lsy_ = torch.empty((batch_size, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "                for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                    syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                    out_, h0_ = self.jepa.pred(syaz.unsqueeze(1), h0_) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                    lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                    lh0 = torch.cat((lh0, h0_.unsqueeze(0)), dim=0)\n",
        "                repr_loss = F.mse_loss(lsy, lsy_)\n",
        "                syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "                clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "                # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "                cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl + self.zloss_coeff * z_loss\n",
        "            cost.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "            # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "        return lz.detach()\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            state_ = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state_).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "            # sx=sy_\n",
        "            state, action, reward = Sar # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "            state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "\n",
        "            for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "                # lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "                lh0 = torch.zeros((rwd.shape[1],)+h0.shape, device=device)\n",
        "                lsy_ = torch.empty((batch_size, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    lsy = self.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "                    # print(\"st\",st[0][-1])\n",
        "                    # print(\"st\",st.max(),st.min())\n",
        "                    print(\"lsy\",lsy.max().item(),lsy.min().item())\n",
        "                    # print(\"lsy\",lsy[0])\n",
        "                    la = self.emb(act) # [batch_size, bptt, dim_a]\n",
        "                    # out = lsy - torch.cat([sy_, lsy[:,:-1]], dim=1)\n",
        "                    lz = self.argm(lsy, sy_, h0, la, rwd) # [batch_size, bptt, d_model],\n",
        "                    # lz = torch.zeros((lsy.shape[0], lsy.shape[1], self.dim_z), device=device)\n",
        "\n",
        "                    # for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                    #     syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                    #     out_, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    #     sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                    #     lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                    #     lh0 = torch.cat((lh0, h0.unsqueeze(0)), dim=0)\n",
        "\n",
        "                    # repr_loss = F.mse_loss(lsy, lsy_) # [batch_size, bptt, d_model]\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "                    repr_loss = torch.zeros(1)\n",
        "                    std_loss, cov_loss = torch.zeros(1),torch.zeros(1)\n",
        "                    # jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    jloss = 0\n",
        "\n",
        "                    # print(lsy_.requires_grad, lh0.requires_grad)\n",
        "                    syh0 = torch.cat([lsy, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "                    # syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "                    # print(\"syh0, rwd\",syh0.shape,rwd.shape)\n",
        "                    # print(\"syh0, rwd.flatten()\",syh0.shape, rwd.flatten().shape)\n",
        "                    # print(\"syh0\",syh0[0][:10])\n",
        "                    # for name, param in agent.tcost.named_parameters():\n",
        "                    #     print(\"param.data\",param.data.max(),param.data.min())\n",
        "                    #     print(\"agent.tcost\",param.data)\n",
        "                    clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                    closs = self.closs_coeff * clossl\n",
        "\n",
        "                    # pred = self.tcost(syh0).squeeze(-1).cpu().unflatten(0, rwd.shape) # [batch_size, bptt]\n",
        "                    pred = self.tcost(syh0).squeeze(-1).unflatten(0, rwd.shape) # [batch_size, bptt]\n",
        "                    print(\"pred\",pred[0])\n",
        "                    print(\"rwd\",rwd[0])\n",
        "                    mask = torch.where(abs(rwd- pred)>0.5,1,0).bool()\n",
        "                    # # print(\"rwd, pred, clossl\", rwd[mask].data, pred[mask].data, clossl.item())\n",
        "                    # try: imshow(torchvision.utils.make_grid(st[0].cpu(), nrow=10))\n",
        "                    # # try: imshow(torchvision.utils.make_grid(st[mask].cpu(), nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "                loss = jloss + closs\n",
        "\n",
        "                # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                norm = torch.norm(lsy, dim=-1)[0][0].item()\n",
        "                # z_norm = torch.norm(z)\n",
        "                # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                print(\"clossl, wrong\", clossl.item(), mask.sum())\n",
        "                # print(\"repr, std, cov, clossl, wrong\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), mask.sum())\n",
        "                # print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "                scaler.scale(loss).backward()\n",
        "                # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                scaler.step(optim)\n",
        "                scaler.update()\n",
        "                optim.zero_grad()\n",
        "                sy_, h0 = sy_.detach(), h0.detach()\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "ema_model = torch.optim.swa_utils.AveragedModel(model, multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))\n",
        "for epoch in range(300):\n",
        "      for input, target in loader:\n",
        "          optimizer.zero_grad()\n",
        "          loss_fn(model(input), target).backward()\n",
        "          optimizer.step()\n",
        "          ema_model.update_parameters(model)\n",
        "# Update bn statistics for the ema_model at the end\n",
        "torch.optim.swa_utils.update_bn(loader, ema_model)\n",
        "# Use ema_model to make predictions on test data\n",
        "preds = ema_model(test_input)\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.999)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "# !pip show torch triton\n",
        "# # !pip install --upgrade torch\n",
        "# !pip install --upgrade triton\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "# print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "# print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "# print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent.tcost._parameters['weight'].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "FwgWasBjZ04u",
        "outputId": "858913b9-8c17-4ba4-d4be-acb5f5daa560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'weight'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2d1fcd12759e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'weight'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(agent.jepa.enc.parameters().values()[0].requires_grad)\n",
        "# for name, param in agent.tcost.named_parameters():\n",
        "# # # for name, param in agent.named_parameters():\n",
        "# #     # print(name, param.requires_grad)\n",
        "#     print(name, param)\n",
        "\n",
        "for name, param in agent.tcost.named_parameters(): print(param.data)\n",
        "\n",
        "# print(agent.tcost.1.weight.data)\n",
        "\n",
        "# print(agent.tcost.named_parameters()['tcost.1.weight'])\n",
        "\n",
        "# print(vars(agent.jepa.exp.named_parameters()['exp.1.weight']))"
      ],
      "metadata": {
        "id": "49RERFWFMgA_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9a27332-e9fd-46bc-cc63-2dfff46f917a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0428, -0.1592,  0.2373,  ...,  0.0815, -0.1585, -0.0541],\n",
            "        [ 0.0443,  0.2047, -0.1938,  ..., -0.0783,  0.1589,  0.0002]],\n",
            "       device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "UEH1P802JkHU",
        "outputId": "c767bf3f-aa2c-43d8-e758-ad8e2b6fa14d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'state' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2eab810ffe5c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# sx = agent.jepa.enc(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# lact, lh0, lx, lz = agent.search(sx, T=6, h0=h0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'state' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# dim_a, dim_z = 3, 8\n",
        "# batch, T = 4,6\n",
        "# x = nn.Parameter(torch.empty((batch, T, dim_a),device=device))\n",
        "# torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "# dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# x = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "# z = nn.Parameter(torch.zeros((batch, T, dim_z),device=device))\n",
        "# torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "# state = torch.zeros((1, 3,64,64))\n",
        "# # state = torch.rand((1, 3,64,64), device=device)\n",
        "# sx = agent.jepa.enc(state)\n",
        "\n",
        "act = agent([state], k=4)\n",
        "# h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "# lact, lh0, lx, lz = agent.search(sx, T=6, h0=h0)\n",
        "# loss, lsx, lh0,c = agent.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "# print(loss,c)\n",
        "# print(lact, lh0, lx, lz)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_b7ZSW6IF1-",
        "outputId": "bf2f47fc-155f-473f-ecc7-d5f4393de241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fFXsee_cSZxhTRewD7ZkGT68NXeq8OcH\n",
            "From (redirected): https://drive.google.com/uc?id=1fFXsee_cSZxhTRewD7ZkGT68NXeq8OcH&confirm=t&uuid=bb8e6a32-7223-49b1-b754-dbc693b560bf\n",
            "To: /content/agentoptim.pkl\n",
            "100% 28.1M/28.1M [00:00<00:00, 49.6MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-34fhOMTdMvtuAeHuL28Y4taSINvOejQ\n",
            "From (redirected): https://drive.google.com/uc?id=1-34fhOMTdMvtuAeHuL28Y4taSINvOejQ&confirm=t&uuid=d24bef53-c310-4a65-8851-387b6409c8ee\n",
            "To: /content/buffergo.pkl\n",
            "100% 1.80G/1.80G [00:49<00:00, 36.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown 1bGWBbcKUgHESkbD3NfYt1WWikScVSFOj -O agentoptim.pkl # M1 gru3 tcost1\n",
        "# !gdown 1XBDhD2efIFW9lnewGRLrb362w47a8b1q -O agentoptim.pkl # B2 gru3 tcost1\n",
        "# !gdown 12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_ -O agentoptim.pkl # S3\n",
        "# !gdown 1zoZ52jctM0jed6TgD7kAwrtnuDMeA5II -O agentoptim.pkl # T4 gru1 tcost1 drop\n",
        "# !gdown 1GlZxrzdH5f28Qo4olbOi0vmAK5WDV7jc -O agentoptim.pkl # A2\n",
        "# !gdown 1UDgNtFsWGAhvqR9lwA0QbMLhUtmip4ne -O agentoptim.pkl # M1 agentoptimgru3tcost1\n",
        "# !gdown 1-0oc6yucS5JXLHX1zqbYe3NTVMuhP_5r -O agentoptim.pkl # A2 agentoptim25251c25z3\n",
        "# !gdown 1U1CuCU1FugkrzPXsvTPpIX-wzWz6szl2 -O agentoptim.pkl # T4 agentoptimargm\n",
        "# !gdown 1CWZAtiEwSnglClJbq2LJTYlKhPN10gfo -O agentoptim.pkl # S3 agentoptimargm\n",
        "# !gdown 1XAbr6l1pCmcUCKR6kYlQ_dSDsOBqRg_j -O agentoptim.pkl # B2 argm2search2\n",
        "# !gdown 1UkQuf-IC2LYErSapkF6rZM1dv3svGI5P -O agentoptim.pkl # T4 gru3 argm offline\n",
        "# !gdown 1-4sNf6mINCiD5YsBdQvCrlyqzzfS64si -O agentoptim.pkl # T4 gru3 argm offline\n",
        "# !gdown 1MV9Qj_53Vu6wpe7nOFn47M5vDj7F7-gv -O agentoptim.pkl # S3 agentoptimargm2\n",
        "# !gdown 1--1Vl3337zugQng-j1qbptFY8EvhZA-T -O agentoptim.pkl # T4 agentoptimargm3 online\n",
        "# !gdown 1XHFBVPSH4T4FpUOBKN8X20xDQLNmL7go -O agentoptim.pkl # M1 agentoptimargm4\n",
        "!gdown 1fFXsee_cSZxhTRewD7ZkGT68NXeq8OcH -O agentoptim.pkl # B2 agentoptimargm4\n",
        "\n",
        "# !gdown 1sCW9uvcdCJkCH5HQDdISLws5rMvmkmFR -O all_sd.pkl # M1 all_sd\n",
        "\n",
        "import pickle\n",
        "# !gdown 1j9hOq8_752duPB0PMYUJqabNvYoGLysX -O buffer512down.pkl # S\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY -O buffer512.pkl # S\n",
        "# with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB -O buffergo.pkl # S3\n",
        "# !gdown 1egXy0t_kn0M0oL6sbwixoVr7bqMfcB8j -O buffergo.pkl # T4\n",
        "!gdown 1-34fhOMTdMvtuAeHuL28Y4taSINvOejQ -O buffergo.pkl # B2\n",
        "with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHQ_ynlwoyJ",
        "outputId": "3ff17771-ff59-459f-9808-7f6fce990721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-a75177a6932e>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "import pickle\n",
        "\n",
        "# with open(folder+'buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptimargm4.pkl', map_location=device).values()\n",
        "modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "# modelsd = transfer_sd(agentsd, modelsd)\n",
        "# modelsd = transfer_sd(agent.state_dict(), modelsd)\n",
        "agent.load_state_dict(modelsd, strict=False)\n",
        "# # optimsd = transfer_optim(agent.state_dict(), modelsd, optim.state_dict(), optimsd)\n",
        "optim.load_state_dict(optimsd)\n",
        "\n",
        "\n",
        "\n",
        "# all_sd = torch.load(folder+'all_sd.pkl', map_location=device)\n",
        "# # all_sd = torch.load('all_sd.pkl', map_location=device)\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# # agentsd = dict((convert[k], v) for (k, v) in all_sd.items())\n",
        "# allsd = {}\n",
        "# for (k, v) in all_sd.items():\n",
        "#     try: allsd[convert[k]] = v\n",
        "#     except Exception as e: print('dict err', e)\n",
        "# # agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "# tgt_sd = load_sd(agent.state_dict(), allsd)\n",
        "# agent.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# for i, (k,v) in enumerate(modelsd.items()):\n",
        "# for i, (k,v) in enumerate(agent.state_dict().items()):\n",
        "#     print(i,k,v.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ZBfBomEBnJu0"
      },
      "outputs": [],
      "source": [
        "# buffer = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "# with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "\n",
        "# agentsd, _ = rename_sd(agent.state_dict())\n",
        "# checkpoint = {'model': agentsd, 'optimizer': optim.state_dict(),}\n",
        "checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "torch.save(checkpoint, folder+'agentoptimargm4.pkl')\n",
        "# torch.save(checkpoint, 'agentoptim.pkl')\n",
        "\n",
        "# all_sd = {}\n",
        "# agentsd, _ = rename_sd(agent.state_dict())\n",
        "# all_sd = store_sd(all_sd, agentsd)\n",
        "# # torch.save(all_sd, 'all_sd.pkl')\n",
        "# torch.save(all_sd, folder+'all_sd.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "id": "NVcknabHMxH6"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        self.data = [step for episode in self.process(buffer) for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state] # list\n",
        "        return torch.stack(state, dim=0), torch.tensor(action), torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def process(self, buffer):\n",
        "        cleaned = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "        cleaned = [episode[-random.randint(20, 100):] for episode in cleaned]\n",
        "        random.shuffle(cleaned)\n",
        "        return cleaned\n",
        "\n",
        "\n",
        "    # def add(self, episode):\n",
        "    #     self.data.append(episode)\n",
        "\n",
        "    # def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "    #     lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "    #         data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "    #         # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "    #         data=data.flatten(start_dim=-3)\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "    #         idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "    #         sample = data[idx]\n",
        "    #         index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "    #         # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "    #         index.add(data)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         # priority = -D.sum(-1) # IP\n",
        "    #         topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "    #         index_list = idx[topk.values] # most clustered\n",
        "    #         for i in reversed(index_list): data.pop(i)\n",
        "    #     return data\n",
        "\n",
        "\n",
        "    # def pop_unif(self, buffer_, n=3):\n",
        "    #     buffer_.pop(random.randrange(len(buffer_)))\n",
        "    #     return buffer_\n",
        "\n",
        "# while len(train_data.data)>10000:\n",
        "#     buffer.pop(random.randrange(len(buffer)))\n",
        "#     train_data = BufferDataset(buffer, seq_len)\n",
        "\n",
        "def collate_fn(sar):\n",
        "    state, action, reward = zip(*sar)\n",
        "    state=torch.stack(state, dim=1) # batch first -> dim=0\n",
        "    action=torch.stack(action, dim=1)\n",
        "    reward=torch.stack(reward, dim=1)\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True)\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # # [3,T,batch]\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "1e3fpbtNOiz1",
        "outputId": "2c3a1d12-128a-4131-8454-152055449fbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.,  0., -1., -1., -1.,  0.,  0., -1., -1.,  0.])\n",
            "tensor([ 0., -1., -1., -1., -1., -1., -1.,  0.,  0., -1.])\n",
            "tensor([-1.,  0., -1., -1.,  0., -1.,  0., -1., -1.,  0.])\n",
            "tensor([ 0., -1., -1., -1., -1., -1.,  0., -1., -1., -1.])\n",
            "tensor([-3.9668e-08, -1.0161e-10, -1.0000e+00, -9.9921e-01, -9.9793e-01,\n",
            "        -9.8745e-12, -5.2469e-30, -1.0000e+00, -8.1765e-14, -4.0773e-10])\n",
            "tensor([-1.4013e-45, -4.5589e-02, -6.9841e-09, -4.3396e-15, -2.7722e-01,\n",
            "        -6.7171e-07, -2.1629e-01, -1.1399e-32, -4.6333e-17, -9.9975e-01])\n",
            "tensor([-9.9998e-01, -8.0420e-20, -1.0000e+00, -1.2168e-04, -9.9999e-01,\n",
            "        -6.6515e-19, -4.1590e-30, -9.9998e-01, -9.9846e-01, -6.6974e-31])\n",
            "tensor([-1.0121e-14, -5.7647e-01, -1.0000e+00, -7.4257e-13, -9.9762e-01,\n",
            "        -2.3974e-04, -3.1355e-32, -9.9997e-01, -2.4582e-04, -1.0000e+00])\n",
            "tensor(0.7754, device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "tensor(0.3306)\n",
            "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
            "tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0])\n",
            "tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0])\n",
            "reward, pred tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.,  0., -1., -1., -1., -1.]) tensor([-3.9668e-08, -8.1765e-14, -4.5589e-02, -6.9841e-09, -4.3396e-15,\n",
            "        -2.7722e-01, -6.7171e-07, -2.1629e-01, -1.2168e-04, -9.9999e-01,\n",
            "        -6.6515e-19, -7.4257e-13, -2.3974e-04, -2.4582e-04])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1920x1440 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "data = [step for episode in buffer for step in episode]\n",
        "state, action, reward = zip(*data)\n",
        "# print(\"reward\",type(reward))\n",
        "data_targets=(torch.tensor(reward)==0).int()\n",
        "ctrain_data=list(zip(state,reward))\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, torch.tensor(y, dtype=torch.float)\n",
        "        # return x, y+1\n",
        "ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "class_count=torch.tensor([x[1] for x in class_count])\n",
        "weight=1./class_count\n",
        "weights = weight[data_targets]\n",
        "\n",
        "# batch_size = 64 #\n",
        "\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "# train_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "c_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "def make_weighted(buffer):\n",
        "    data = [step for episode in buffer for step in episode]\n",
        "    state, action, reward = zip(*data)\n",
        "    # print(\"reward\",type(reward))\n",
        "    data_targets=(torch.tensor(reward)==0).int()\n",
        "    ctrain_data=list(zip(state,reward))\n",
        "    ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "    from collections import Counter\n",
        "    class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "    class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "    class_count=torch.tensor([x[1] for x in class_count])\n",
        "    weight=1./class_count\n",
        "    weights = weight[data_targets]\n",
        "\n",
        "    # batch_size = 64 #\n",
        "    train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "    c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "    return c_loader\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "def imshow(img): # display img from torch tensor\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.axis('off')\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "trainiter = iter(c_loader)\n",
        "images, labels = next(trainiter)\n",
        "# images, labels = images.to(device), labels.to(device)\n",
        "batch=40\n",
        "images, labels = images[:batch], labels[:batch]\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=10))\n",
        "# print(labels)\n",
        "for x in range(len(labels)//10):\n",
        "    print(labels[10*x:10*x+10])\n",
        "\n",
        "# # try:\n",
        "with torch.no_grad():\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).argmax(-1).cpu()\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).squeeze(-1).cpu()\n",
        "    # _, world_state = agent.get(images.to(device))\n",
        "    # pred = agent.tcost(agent.jepa.enc(world_state.unsqueeze(1))).squeeze(-1).cpu()\n",
        "    h0 = torch.zeros((agent.jepa.pred.num_layers, batch, agent.d_model), device=device)\n",
        "    # h0 = torch.empty((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device)\n",
        "    # torch.nn.init.xavier_normal_(h0)\n",
        "    sy = agent.jepa.enc(images.to(device)) # [batch_size, d_model]\n",
        "    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "    pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "\n",
        "    # print(pred)\n",
        "    for x in range(len(pred)//10):\n",
        "        print(pred[10*x:10*x+10])\n",
        "    # print((labels==pred).sum())\n",
        "# except: pass\n",
        "print(agent.tcost.loss(syh0, labels.to(device)).squeeze(-1))\n",
        "print(F.mse_loss(labels, pred))\n",
        "\n",
        "# torch.where(abs(labels- pred)>0.5,1,0)\n",
        "for x in range(len(pred)//10):\n",
        "    print(torch.where(abs(labels- pred)>0.5,1,0)[10*x:10*x+10])\n",
        "\n",
        "mask = torch.where(abs(labels- pred)>0.5,1,0).bool()\n",
        "print(\"reward, pred\", labels[mask].data, pred[mask].data)\n",
        "try: imshow(torchvision.utils.make_grid(images[mask], nrow=10))\n",
        "except ZeroDivisionError: pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for b, Sar in enumerate(train_loader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "    state, action, reward = Sar\n",
        "    # print(state.shape, action.shape, reward.shape) # [64, 50, 3, 64, 64]) torch.Size([64, 50]) torch.Size([64, 50]\n",
        "    # if b>3:break\n",
        "    bptt=25\n",
        "    for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "        # for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "        imshow(torchvision.utils.make_grid(st[0].cpu(), nrow=10))\n",
        "        print(rwd[0])\n",
        "\n",
        "# op = iter(train_loader)\n",
        "# Sar = next(op)\n",
        "# state, action, reward = Sar\n",
        "\n",
        "# imshow(torchvision.utils.make_grid(state[0].cpu(), nrow=10))\n",
        "# print(reward[0])\n"
      ],
      "metadata": {
        "id": "Pc3mm6al-KCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "s = torch.arange(20).reshape(2,10,1) # [batch,seq,d_model]\n",
        "# print(s)\n",
        "for la in torch.split(s, 7, dim=1): # [batch,bptt,d_model]\n",
        "    # print(la)\n",
        "    print(la.shape)\n",
        "    for a in la.permute(1,0,2): # [batch,d_model]\n",
        "        print(a)\n"
      ],
      "metadata": {
        "id": "mL-RrKsu_o9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viimAIpYSJq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70264497-acd0-4475-e627-1be37d7ec960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5161, 0.4839],\n",
            "        [0.4553, 0.5447],\n",
            "        [0.6737, 0.3263],\n",
            "        [0.4455, 0.5545],\n",
            "        [0.2712, 0.7288],\n",
            "        [0.6653, 0.3347],\n",
            "        [0.5739, 0.4261],\n",
            "        [0.3988, 0.6012],\n",
            "        [0.5869, 0.4131],\n",
            "        [0.5607, 0.4393]])\n",
            "tensor(0.5933)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "labels = torch.tensor([0])\n",
        "# pred = torch.tensor([[.999,.001]])\n",
        "pred = torch.tensor([[1.,0.]])\n",
        "pred = torch.tensor([[5.,-5.]])\n",
        "# pred = torch.tensor([[.5,.5]])\n",
        "# pred = torch.tensor([[1/a, 1/(1-a)]])\n",
        "# pred = torch.tensor([[1/(1-a), 1/a]])\n",
        "# print(F.mse_loss(labels, pred))\n",
        "\n",
        "pred = torch.rand(10,2)\n",
        "pred = nn.Softmax(dim=-1)(pred)\n",
        "print(pred)\n",
        "labels = torch.where(torch.rand(10)>0.5,1,0)\n",
        "\n",
        "\n",
        "a=10\n",
        "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)]))\n",
        "print(loss_fn(pred, labels))\n",
        "\n",
        "# print((pred@torch.log(pred).T).sum())\n",
        "# print(pred,torch.log(pred).T)\n",
        "\n",
        "# 0.6931\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Ksm4ha7XA-BN"
      },
      "outputs": [],
      "source": [
        "# optim = torch.optim.SGD(agent.parameters(), 1e-1, momentum=0.9, dampening=0, weight_decay=0)\n",
        "# print(optim.param_groups[0][\"lr\"])\n",
        "# print(optim)\n",
        "optim.param_groups[0][\"lr\"] = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OksdjCeJYpYh",
        "outputId": "3e00e8b4-aab1-4c19-a72d-bbf0609bd12b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-5685ce5b2a51>:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lsy tensor(58.7500, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-5.5695e-04, -1.6775e-03, -4.1723e-07, -4.2725e-04, -9.8038e-03,\n",
            "        -1.7576e-03, -3.6895e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -1.3113e-06, -2.9802e-07, -3.5167e-06,  0.0000e+00,\n",
            "        -1.1921e-07, -1.1921e-06,  0.0000e+00, -6.5565e-07, -1.1820e-04,\n",
            "        -7.4707e-01, -9.9902e-01, -1.0000e+00, -9.9316e-01, -6.5269e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3615107238292694 tensor(88, device='cuda:0')\n",
            "lsy tensor(57., device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.7607e-04, -5.6641e-01, -9.9756e-01, -4.1138e-01, -1.1499e-01,\n",
            "        -8.4043e-06, -7.7486e-07, -3.7551e-06, -2.1636e-05, -4.4703e-06,\n",
            "        -7.9870e-06, -9.5725e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.5763e-07,\n",
            "        -1.1921e-07, -2.7061e-04, -2.5215e-03, -1.9836e-03, -5.9605e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.34642186760902405 tensor(95, device='cuda:0')\n",
            "lsy tensor(55.2188, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.0000e+00, -1.0000e+00, -1.0073e-05, -3.2425e-05, -7.3373e-05,\n",
            "        -2.1100e-05, -1.7524e-05, -5.3287e-05, -3.9339e-06, -3.0994e-06,\n",
            "        -4.3511e-06, -2.0623e-05, -8.0287e-05, -3.0100e-05, -8.6129e-05,\n",
            "        -8.8513e-05, -2.0182e-04, -5.5611e-05, -6.9618e-04, -1.3924e-04,\n",
            "        -2.2912e-04, -1.7071e-03, -4.5280e-03, -3.1006e-01, -4.4604e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3985081911087036 tensor(96, device='cuda:0')\n",
            "lsy tensor(56.7500, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-2.5909e-02, -2.9077e-01, -9.9658e-01, -9.6924e-01, -9.4141e-01,\n",
            "        -9.9951e-01, -1.0000e+00, -9.9951e-01, -9.9951e-01, -9.8291e-01,\n",
            "        -9.9805e-01, -9.9658e-01, -1.0000e+00,  0.0000e+00, -5.9605e-08,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4294e-03, -2.2590e-05,\n",
            "        -1.4305e-06,  0.0000e+00, -1.0669e-05, -8.2254e-06, -4.3631e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3594128489494324 tensor(103, device='cuda:0')\n",
            "lsy tensor(63.8125, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -2.2960e-04, -5.9605e-07,\n",
            "        -1.8711e-03, -6.3896e-03, -2.6894e-03, -2.3697e-02, -1.4381e-02,\n",
            "        -5.9605e-08, -1.0848e-05,  0.0000e+00,  0.0000e+00, -5.9605e-08,\n",
            "        -8.5754e-03, -5.5695e-04, -2.9087e-05, -1.0228e-04, -7.0000e-04,\n",
            "        -1.9073e-02, -7.6950e-05, -7.1220e-03, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3626077175140381 tensor(87, device='cuda:0')\n",
            "lsy tensor(52.2188, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0020,\n",
            "         0.0000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.36274078488349915 tensor(124, device='cuda:0')\n",
            "lsy tensor(56.7500, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-4.1485e-04, -9.6607e-04, -5.9605e-08, -9.5367e-07, -1.5843e-04,\n",
            "        -2.6822e-06, -3.5763e-06, -6.2585e-06, -4.0855e-03, -9.9951e-01,\n",
            "        -9.4482e-01, -9.9951e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.34833642840385437 tensor(114, device='cuda:0')\n",
            "lsy tensor(61.0312, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.35725581645965576 tensor(120, device='cuda:0')\n",
            "lsy tensor(68.7500, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00, -3.7789e-05, -4.1748e-02, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3442007899284363 tensor(104, device='cuda:0')\n",
            "lsy tensor(51.5938, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -5.0664e-06, -1.6394e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00, -4.9472e-05, -4.7755e-04,\n",
            "        -4.6492e-06, -8.7023e-06, -5.7793e-04, -3.5763e-05, -1.0848e-05,\n",
            "        -4.2796e-05, -4.0779e-03, -6.6309e-01, -2.2827e-01, -5.3520e-03,\n",
            "        -4.4167e-05,  0.0000e+00, -1.1921e-07,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3882538974285126 tensor(99, device='cuda:0')\n",
            "lsy tensor(59.4688, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3692229092121124 tensor(86, device='cuda:0')\n",
            "lsy tensor(54.3750, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3423447012901306 tensor(85, device='cuda:0')\n",
            "lsy tensor(57.5938, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00, -2.3842e-07, -9.8515e-04,  0.0000e+00, -5.9605e-08,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08,  0.0000e+00,\n",
            "        -5.0843e-05, -2.2471e-05, -1.0000e+00, -1.0000e+00, -1.2650e-02,\n",
            "        -2.3055e-04, -3.0994e-06, -6.5565e-07,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3594912588596344 tensor(89, device='cuda:0')\n",
            "lsy tensor(52.2500, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00, -4.7684e-07, -4.3988e-05,  0.0000e+00,  0.0000e+00,\n",
            "        -1.7881e-07, -3.8803e-05, -2.7835e-05, -1.1921e-07,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07, -1.0000e+00,\n",
            "        -1.0000e+00,  0.0000e+00, -1.3709e-06, -5.9605e-07, -1.1921e-07,\n",
            "        -5.8413e-06, -3.5524e-05, -1.2004e-04, -1.0729e-06, -3.5763e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3709624111652374 tensor(84, device='cuda:0')\n",
            "lsy tensor(52.0625, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07, -2.6822e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3435342311859131 tensor(93, device='cuda:0')\n",
            "lsy tensor(61., device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.4158438444137573 tensor(97, device='cuda:0')\n",
            "lsy tensor(56.8438, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-3.8385e-05, -5.9605e-08, -1.1921e-06,  0.0000e+00, -1.1444e-05,\n",
            "        -2.0421e-04, -2.5320e-04, -3.0935e-05, -1.8477e-06, -5.9605e-08,\n",
            "         0.0000e+00, -2.2650e-06, -2.7490e-04, -6.6650e-02, -2.6758e-01,\n",
            "        -6.2646e-01, -6.2207e-01, -9.9463e-01, -4.6265e-01,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.40732911229133606 tensor(88, device='cuda:0')\n",
            "lsy tensor(61.7500, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-8.0261e-02, -6.3037e-01, -9.9561e-01, -5.7182e-03,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -5.9605e-08,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -4.5061e-05, -1.1921e-07, -1.9610e-05,  0.0000e+00,\n",
            "        -8.9722e-02, -9.9854e-01, -8.6129e-05, -1.1921e-07,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.37427690625190735 tensor(106, device='cuda:0')\n",
            "lsy tensor(64.7500, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00, -2.9802e-07,  0.0000e+00,  0.0000e+00, -3.2568e-01,\n",
            "        -2.5630e-06, -5.6624e-06, -5.4836e-06, -2.9206e-06,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.1590e-06,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.354452520608902 tensor(71, device='cuda:0')\n",
            "lsy tensor(64.2500, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -3.6359e-06, -2.9068e-03, -1.4351e-02, -3.4561e-03,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3575673997402191 tensor(85, device='cuda:0')\n",
            "lsy tensor(59.1875, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.8656e-05, -1.9073e-06,  0.0000e+00,  0.0000e+00, -2.2650e-06,\n",
            "        -9.9902e-01, -1.0000e+00, -1.0000e+00, -2.5630e-06,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3431527614593506 tensor(94, device='cuda:0')\n",
            "lsy tensor(51.9375, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.8382e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.7881e-06, -1.0000e+00,\n",
            "        -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.38957688212394714 tensor(112, device='cuda:0')\n",
            "lsy tensor(70.6250, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -2.2650e-06,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.36869123578071594 tensor(118, device='cuda:0')\n",
            "lsy tensor(59.3438, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.0242e-03, -4.9585e-01, -2.0862e-05, -2.3842e-07, -3.9363e-04,\n",
            "        -9.9170e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.40458595752716064 tensor(117, device='cuda:0')\n",
            "lsy tensor(60.5000, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -7.9870e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3795395791530609 tensor(93, device='cuda:0')\n",
            "lsy tensor(60.3125, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -2.9802e-06, -5.9605e-08,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.34736260771751404 tensor(114, device='cuda:0')\n",
            "lsy tensor(54.6250, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.0000e+00, -1.8425e-03, -6.3753e-04, -7.6221e-01, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3882184326648712 tensor(137, device='cuda:0')\n",
            "lsy tensor(57.2500, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.38803631067276 tensor(152, device='cuda:0')\n",
            "lsy tensor(67.4375, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0228e-04,\n",
            "         0.0000e+00,  0.0000e+00, -9.8193e-01, -1.0000e+00, -1.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.34967881441116333 tensor(115, device='cuda:0')\n",
            "lsy tensor(58.5625, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3976229727268219 tensor(90, device='cuda:0')\n",
            "lsy tensor(64.8125, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.8128e-05,\n",
            "        -1.7047e-05,  0.0000e+00, -5.3174e-01, -1.1921e-07, -2.5034e-06,\n",
            "         0.0000e+00, -8.3447e-07, -2.1458e-06,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -5.5432e-06, -1.0729e-06, -1.4610e-03, -3.5167e-06,\n",
            "        -5.9605e-08, -1.1921e-07, -1.7881e-07,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.38653242588043213 tensor(97, device='cuda:0')\n",
            "lsy tensor(63.6250, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00, -1.9670e-06,  0.0000e+00,  0.0000e+00,\n",
            "        -1.0729e-06, -1.1086e-05, -7.7486e-06, -5.5115e-02, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -9.9854e-01, -3.4571e-06, -2.9206e-06,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3679701089859009 tensor(136, device='cuda:0')\n",
            "lsy tensor(58.6562, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.1921e-06, -5.3644e-06, -1.1915e-04, -6.5565e-07, -1.1921e-07,\n",
            "        -1.6749e-05, -8.9407e-07, -1.1921e-05, -5.6267e-05, -4.7278e-04,\n",
            "        -8.9169e-04, -1.2243e-04, -1.8597e-04, -3.6793e-03, -3.0816e-05,\n",
            "        -1.4901e-06,  0.0000e+00, -5.9605e-08, -1.3983e-04, -5.3650e-02,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3858043849468231 tensor(166, device='cuda:0')\n",
            "lsy tensor(66.9375, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.38309445977211 tensor(121, device='cuda:0')\n",
            "lsy tensor(50.4062, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07, -1.7703e-05,\n",
            "         0.0000e+00,  0.0000e+00, -4.5929e-03, -1.0000e+00, -1.0000e+00,\n",
            "        -3.8147e-06, -6.1810e-05, -2.5034e-06, -3.4928e-04, -2.0540e-04,\n",
            "        -1.0000e+00, -1.0000e+00, -3.8075e-04, -1.9717e-04, -1.6012e-03,\n",
            "        -1.8127e-02, -5.2783e-01, -2.9297e-03, -8.4817e-05, -3.5763e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3514716625213623 tensor(113, device='cuda:0')\n",
            "lsy tensor(65.1875, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -1.7881e-07, -3.5763e-07, -4.1723e-07, -6.5565e-07,\n",
            "         0.0000e+00, -2.9802e-07, -1.0473e-04, -5.4359e-05, -4.9472e-06,\n",
            "        -5.3048e-06, -1.7881e-07, -2.9802e-07, -5.9605e-08,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3552151620388031 tensor(110, device='cuda:0')\n",
            "lsy tensor(61.6250, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-4.1723e-07, -5.0843e-05, -1.1146e-04, -1.4453e-01, -9.7119e-01,\n",
            "        -4.4775e-01, -4.1819e-04, -5.3101e-02, -1.1731e-01, -9.8047e-01,\n",
            "        -9.9756e-01, -7.8125e-01, -2.3687e-04, -1.1921e-07, -4.7684e-07,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -3.5763e-07, -5.3048e-05,\n",
            "        -4.4703e-06, -2.3842e-07,  0.0000e+00, -5.9605e-07,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.4012291431427002 tensor(151, device='cuda:0')\n",
            "lsy tensor(53.5312, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-4.7684e-07, -2.3246e-06, -4.0352e-05, -7.9150e-01, -4.4525e-05,\n",
            "        -1.4160e-02, -5.9143e-02, -6.7932e-02, -5.1193e-03, -8.7988e-01,\n",
            "        -9.4235e-05, -6.4552e-05, -1.2755e-05, -3.6907e-04, -5.7220e-06,\n",
            "        -5.7936e-04, -3.5763e-06, -1.4524e-03, -3.4771e-03, -7.4244e-04,\n",
            "        -7.9883e-01, -2.7239e-05, -3.1829e-05, -7.3814e-04, -1.6266e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.36528411507606506 tensor(99, device='cuda:0')\n",
            "1\n",
            "lsy tensor(52.1250, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08, -7.1526e-07,\n",
            "        -2.0309e-02, -7.5000e-01, -4.3976e-02, -7.7246e-01, -8.5107e-01,\n",
            "        -3.3813e-02, -2.1744e-04, -8.8215e-05, -4.5807e-02,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.37812232971191406 tensor(101, device='cuda:0')\n",
            "lsy tensor(65., device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.7176e-03, -7.3910e-06, -4.7565e-05,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -1.2517e-06,  0.0000e+00, -2.5964e-04, -1.0000e+00,\n",
            "        -2.6226e-04, -9.9561e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3937721252441406 tensor(125, device='cuda:0')\n",
            "lsy tensor(51.0938, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -2.3842e-07,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -1.1921e-07, -8.3447e-07, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -4.6492e-06, -8.9407e-07, -9.5367e-07,\n",
            "        -1.2517e-06, -7.3314e-06, -1.6749e-05,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.35094431042671204 tensor(118, device='cuda:0')\n",
            "lsy tensor(63.8750, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.2517e-06, -4.1723e-07, -3.5763e-07, -1.4901e-06, -2.6226e-06,\n",
            "         0.0000e+00, -5.9605e-08, -1.4305e-06, -1.5497e-06, -4.2319e-06,\n",
            "        -3.6478e-05, -4.7684e-07, -5.9605e-08,  0.0000e+00, -1.7107e-05,\n",
            "        -4.7684e-07, -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3665836453437805 tensor(107, device='cuda:0')\n",
            "lsy tensor(54.8125, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.1921e-07, -7.7486e-07, -5.9605e-08, -1.1921e-07, -5.9605e-08,\n",
            "         0.0000e+00, -7.1526e-06, -5.7817e-06, -4.1723e-07,  0.0000e+00,\n",
            "        -5.9605e-08, -1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -9.5367e-07, -5.9605e-08,  0.0000e+00, -3.5763e-07, -1.6451e-05,\n",
            "        -1.7643e-05, -2.8496e-03, -8.5473e-05, -1.6999e-04, -1.4842e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3683643043041229 tensor(117, device='cuda:0')\n",
            "lsy tensor(59.1875, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-8.7023e-06, -5.9605e-08, -1.3089e-04, -1.0270e-04, -1.7345e-05,\n",
            "        -2.5570e-05, -1.2772e-02, -3.9756e-05, -7.2718e-06, -3.8743e-06,\n",
            "        -8.5235e-06, -8.3447e-07, -1.7881e-07, -1.3709e-06, -4.1723e-07,\n",
            "        -7.1526e-07,  0.0000e+00, -2.6226e-06, -4.4107e-06, -1.7285e-06,\n",
            "        -1.4305e-06,  0.0000e+00, -8.0872e-02, -9.9658e-01, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3397234380245209 tensor(84, device='cuda:0')\n",
            "lsy tensor(63.0625, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00, -6.6161e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -9.9414e-01, -3.8683e-05,\n",
            "         0.0000e+00,  0.0000e+00, -9.6875e-01, -9.9756e-01, -9.9365e-01,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3447527587413788 tensor(98, device='cuda:0')\n",
            "lsy tensor(61.5938, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -9.3701e-01,\n",
            "        -9.9902e-01, -2.3842e-07, -5.9473e-01, -5.3883e-05, -4.7028e-02,\n",
            "        -4.4525e-02, -8.1558e-03, -9.9316e-01,  0.0000e+00,  0.0000e+00,\n",
            "        -1.5295e-04, -2.3842e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3675661087036133 tensor(96, device='cuda:0')\n",
            "lsy tensor(60.1562, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.7551e-06,\n",
            "        -9.4531e-01, -9.8486e-01, -9.0088e-01,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3977780342102051 tensor(100, device='cuda:0')\n",
            "lsy tensor(67.9375, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3640185594558716 tensor(91, device='cuda:0')\n",
            "lsy tensor(57.9375, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.7881e-07, -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -9.9561e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.38043224811553955 tensor(108, device='cuda:0')\n",
            "lsy tensor(63.2500, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -1.8716e-05, -8.9407e-07, -1.3940e-01, -9.7510e-01,\n",
            "        -9.7168e-01, -9.6240e-01, -5.9605e-06, -4.8697e-05, -5.6190e-03,\n",
            "        -8.1241e-05, -2.4445e-02, -9.9951e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3817630410194397 tensor(102, device='cuda:0')\n",
            "lsy tensor(69.9375, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00, -1.1921e-07, -4.1084e-03, -1.1921e-07, -1.7881e-07,\n",
            "        -3.5763e-07, -2.9802e-07, -5.9605e-08,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -2.5616e-03, -5.0125e-03, -7.2002e-05,\n",
            "        -7.9632e-04, -4.4703e-05, -8.4448e-04, -6.5967e-01, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.35952019691467285 tensor(109, device='cuda:0')\n",
            "lsy tensor(54.0938, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.35301199555397034 tensor(95, device='cuda:0')\n",
            "lsy tensor(54.1250, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-9.7156e-06, -1.0000e+00, -1.0000e+00, -1.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3762717843055725 tensor(112, device='cuda:0')\n",
            "lsy tensor(58.6875, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.4162e-05,\n",
            "        -9.9540e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3644509017467499 tensor(101, device='cuda:0')\n",
            "lsy tensor(52.2500, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.8668e-04, -5.5432e-06, -1.7881e-04, -3.2878e-04, -1.5011e-03,\n",
            "        -4.4263e-01, -9.5010e-05, -7.3373e-05, -3.6359e-06, -9.9850e-04,\n",
            "        -1.7881e-07, -2.3842e-07, -6.6280e-04, -1.3649e-05, -1.7583e-05,\n",
            "        -1.4305e-05, -5.3644e-06, -1.2517e-06, -5.9605e-07,  0.0000e+00,\n",
            "        -4.7684e-07, -1.5783e-04, -1.6630e-05, -3.3051e-02, -2.4872e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.39101114869117737 tensor(129, device='cuda:0')\n",
            "lsy tensor(68.8125, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-2.9175e-02, -1.9073e-06, -1.7881e-07, -1.7881e-07,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -8.1970e-02, -9.4482e-01, -9.9512e-01,\n",
            "        -1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.37321552634239197 tensor(105, device='cuda:0')\n",
            "lsy tensor(56.9688, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.38474106788635254 tensor(124, device='cuda:0')\n",
            "lsy tensor(50.0938, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.9155,\n",
            "        -1.0000, -0.9888,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3538660705089569 tensor(96, device='cuda:0')\n",
            "lsy tensor(65.8750, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -4.2319e-06, -1.0729e-06,\n",
            "        -2.9802e-07, -1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -7.1526e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3686264455318451 tensor(105, device='cuda:0')\n",
            "lsy tensor(60.8125, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3618350625038147 tensor(100, device='cuda:0')\n",
            "lsy tensor(61.2812, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -1.1325e-06, -1.0796e-02, -1.0000e+00, -1.8689e-01,\n",
            "        -6.0463e-04, -9.8389e-01, -1.0490e-05,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3703250586986542 tensor(108, device='cuda:0')\n",
            "lsy tensor(54.1875, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00, -9.6777e-01, -7.8082e-06,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -1.1921e-07, -1.1921e-07, -1.1504e-05, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.4108369052410126 tensor(111, device='cuda:0')\n",
            "lsy tensor(52.5938, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-4.3164e-01, -4.7922e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3797115385532379 tensor(93, device='cuda:0')\n",
            "lsy tensor(53.0625, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.7822e-05, -7.6123e-01, -1.6003e-01, -1.0633e-03, -5.8441e-03,\n",
            "        -1.0000e+00, -1.0000e+00,  0.0000e+00, -8.3069e-02, -1.0000e+00,\n",
            "        -5.1384e-03, -1.0000e+00, -7.4121e-01, -5.7495e-02, -1.4305e-06,\n",
            "        -2.9802e-07,  0.0000e+00,  0.0000e+00, -9.9121e-01, -9.8975e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3756226599216461 tensor(102, device='cuda:0')\n",
            "lsy tensor(59.9062, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.4544e-05, -4.6777e-01, -8.7451e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3627181649208069 tensor(90, device='cuda:0')\n",
            "lsy tensor(58.9688, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0192e-05,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.35193780064582825 tensor(89, device='cuda:0')\n",
            "lsy tensor(53.2812, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.2726e-02, -1.4067e-05, -4.6338e-01, -1.6846e-02,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.4061720371246338 tensor(112, device='cuda:0')\n",
            "lsy tensor(58.4062, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -5.9605e-08, -5.9605e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -7.2122e-06, -9.6826e-01, -1.0000e+00,\n",
            "        -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.37919554114341736 tensor(115, device='cuda:0')\n",
            "lsy tensor(62.5000, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-6.4516e-04, -2.3901e-05, -6.1560e-04, -7.9193e-03, -4.7207e-05,\n",
            "        -1.9670e-06, -8.4639e-06, -1.0014e-05, -1.7941e-05, -1.2064e-03,\n",
            "        -4.1008e-05, -3.0994e-06, -4.4703e-06, -1.1620e-02, -2.7103e-03,\n",
            "        -1.2756e-02, -3.2306e-05, -6.0606e-04, -7.1526e-04, -4.0936e-04,\n",
            "        -9.4771e-06, -7.1526e-07, -1.1683e-05, -1.5056e-04, -7.7629e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.39705103635787964 tensor(95, device='cuda:0')\n",
            "lsy tensor(59.8438, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-5.4121e-04, -2.4343e-04, -1.2947e-02, -4.8685e-04, -1.2722e-03,\n",
            "        -9.5215e-03, -7.8125e-03, -7.0429e-04, -2.5606e-04, -8.4610e-03,\n",
            "        -9.5215e-03, -1.5163e-03, -6.0158e-03, -5.2605e-03, -4.7565e-05,\n",
            "        -1.4801e-02, -1.6418e-02, -2.3605e-02, -1.3405e-02, -6.4575e-02,\n",
            "        -3.3539e-02, -9.5749e-03, -1.9714e-02, -1.4244e-02, -1.0880e-02],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3885505497455597 tensor(117, device='cuda:0')\n",
            "lsy tensor(63.8438, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-9.1797e-01, -3.2568e-01, -1.8631e-02, -1.4424e-04, -4.7386e-05,\n",
            "        -2.1458e-06, -5.1260e-06, -9.4748e-04, -1.7881e-07, -1.0133e-06,\n",
            "        -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.7881e-07, -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -1.7881e-07, -3.2544e-05, -5.8770e-05],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3491790294647217 tensor(109, device='cuda:0')\n",
            "lsy tensor(58.8750, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-0.0188, -0.9941, -0.9810,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3419894874095917 tensor(90, device='cuda:0')\n",
            "lsy tensor(58.4688, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -2.9802e-07, -1.3709e-05,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3653329312801361 tensor(105, device='cuda:0')\n",
            "lsy tensor(55.3750, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-3.5763e-07, -1.4111e-01, -9.9072e-01, -4.4586e-02, -7.6050e-02,\n",
            "        -9.9945e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.5565e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3924783766269684 tensor(110, device='cuda:0')\n",
            "lsy tensor(60.3438, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-0.8252, -0.9302, -0.9487, -1.0000, -1.0000, -1.0000, -1.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "         0.0000], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.36708176136016846 tensor(106, device='cuda:0')\n",
            "lsy tensor(56.1875, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -4.7207e-05, -5.6244e-02, -1.0000e+00, -9.9854e-01],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.401666522026062 tensor(119, device='cuda:0')\n",
            "2\n",
            "lsy tensor(60.8438, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00, -9.8730e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -3.5763e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3791819214820862 tensor(86, device='cuda:0')\n",
            "lsy tensor(60.2188, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.1723e-07,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -2.0111e-04, -1.1593e-04,\n",
            "        -8.2779e-04, -1.7881e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3772207796573639 tensor(104, device='cuda:0')\n",
            "lsy tensor(63.5938, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -7.7486e-07,  0.0000e+00,\n",
            "        -3.3975e-06, -1.7881e-07, -1.2329e-01, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.38951951265335083 tensor(101, device='cuda:0')\n",
            "lsy tensor(53.5938, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.0000e+00, -4.2969e-02, -6.3133e-04, -3.0339e-05, -1.1921e-07,\n",
            "        -3.2961e-05, -9.5367e-07,  0.0000e+00, -2.3842e-07,  0.0000e+00,\n",
            "        -1.7881e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -5.9605e-08,  0.0000e+00, -5.9605e-08,\n",
            "        -5.9605e-08, -8.7619e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3767850697040558 tensor(106, device='cuda:0')\n",
            "lsy tensor(56.1250, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-3.5763e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.0133e-06,\n",
            "        -3.6560e-02, -4.7016e-04, -1.5045e-02, -4.3799e-01, -8.3740e-01,\n",
            "        -1.5918e-01, -2.9469e-03, -2.3246e-06,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -2.9802e-07,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.40918582677841187 tensor(89, device='cuda:0')\n",
            "lsy tensor(52.3750, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-5.9605e-08, -1.3113e-06, -5.0812e-03, -5.0068e-06,  0.0000e+00,\n",
            "         0.0000e+00, -3.7611e-05, -1.7639e-01, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3575607240200043 tensor(94, device='cuda:0')\n",
            "lsy tensor(52.7812, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.8014e-06,\n",
            "        -1.3173e-05, -2.5034e-06, -4.0460e-04, -9.6560e-06, -7.2718e-06,\n",
            "        -1.6224e-04, -1.0133e-06, -1.8477e-06, -3.9935e-06, -8.6844e-05,\n",
            "        -3.8218e-04, -9.9219e-01, -1.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.40002667903900146 tensor(117, device='cuda:0')\n",
            "lsy tensor(67.4375, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -1.9252e-05, -2.1315e-04,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2517e-06, -1.0931e-04],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3950493633747101 tensor(110, device='cuda:0')\n",
            "lsy tensor(48.9688, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -8.1241e-05,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.34161752462387085 tensor(89, device='cuda:0')\n",
            "lsy tensor(57.5312, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -3.5763e-07,  0.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3440839946269989 tensor(93, device='cuda:0')\n",
            "lsy tensor(48.3125, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.353506475687027 tensor(90, device='cuda:0')\n",
            "lsy tensor(53.6875, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.36612027883529663 tensor(89, device='cuda:0')\n",
            "lsy tensor(55.1875, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-7.0801e-01, -2.5101e-02, -2.6337e-02, -1.6577e-01, -1.9181e-04,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.4240e-06, -5.3072e-04,\n",
            "        -1.3292e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08,\n",
            "         0.0000e+00, -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.35637548565864563 tensor(87, device='cuda:0')\n",
            "lsy tensor(55.0938, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3918634057044983 tensor(93, device='cuda:0')\n",
            "lsy tensor(60.4375, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -2.7108e-04, -9.1504e-01,\n",
            "        -9.9854e-01, -1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3780801296234131 tensor(66, device='cuda:0')\n",
            "lsy tensor(54.5938, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.5557e-05,  0.0000e+00,  0.0000e+00, -1.1921e-07, -3.6478e-05,\n",
            "        -7.3910e-06,  0.0000e+00, -5.9605e-08,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.41590818762779236 tensor(99, device='cuda:0')\n",
            "lsy tensor(53.9688, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3668763041496277 tensor(120, device='cuda:0')\n",
            "lsy tensor(47.1875, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3494163751602173 tensor(121, device='cuda:0')\n",
            "lsy tensor(64.4375, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.7881e-06, -2.0444e-05, -2.0862e-06, -1.5497e-06, -4.4107e-06,\n",
            "        -5.9605e-08, -2.8014e-06, -3.8385e-05, -1.2970e-03, -4.2480e-02,\n",
            "        -9.8705e-04, -9.6560e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -5.9605e-08, -1.6093e-06, -7.7486e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.35662779211997986 tensor(78, device='cuda:0')\n",
            "lsy tensor(58.8125, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -2.9206e-06, -1.1921e-07,\n",
            "        -2.3842e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -4.7684e-07, -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -7.1526e-07, -6.7949e-06, -2.8014e-06],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3881298303604126 tensor(103, device='cuda:0')\n",
            "lsy tensor(52.5312, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -7.0333e-06, -1.0000e+00,\n",
            "        -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3770635724067688 tensor(84, device='cuda:0')\n",
            "lsy tensor(55.5938, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -3.2067e-04, -2.2156e-02, -8.2254e-06,\n",
            "        -8.9340e-03, -5.3644e-07, -1.5961e-02, -1.2398e-05, -1.4889e-04,\n",
            "        -2.1458e-06, -9.8535e-01, -1.0000e+00, -1.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3504512310028076 tensor(97, device='cuda:0')\n",
            "lsy tensor(53.3438, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-8.4043e-06,  0.0000e+00, -5.9605e-08, -1.1146e-05, -5.7220e-06,\n",
            "        -5.3644e-07, -1.1921e-06,  0.0000e+00, -5.4240e-06, -5.9605e-07,\n",
            "        -1.4901e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -2.8300e-04, -8.3447e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.37044060230255127 tensor(112, device='cuda:0')\n",
            "lsy tensor(53.4062, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08,\n",
            "        -1.0405e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3569165766239166 tensor(70, device='cuda:0')\n",
            "lsy tensor(56.5312, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.6093e-06, -2.9206e-05, -1.7285e-06, -3.9902e-03, -1.0590e-01,\n",
            "        -1.2598e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07, -2.9984e-03,\n",
            "        -4.4769e-02, -5.8413e-06, -9.9805e-01, -8.3447e-07,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3823031783103943 tensor(110, device='cuda:0')\n",
            "lsy tensor(50.7812, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -5.3048e-06, -4.1723e-07, -1.0000e+00, -1.0000e+00,\n",
            "        -9.9365e-01, -1.0765e-04, -2.0943e-03, -2.1231e-04, -1.6606e-04,\n",
            "        -3.8505e-05, -7.8082e-06, -3.6061e-05, -8.0943e-05, -3.1323e-01,\n",
            "        -8.0518e-01, -9.7119e-01, -1.0000e+00, -1.0000e+00, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.37598156929016113 tensor(127, device='cuda:0')\n",
            "lsy tensor(73.1875, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00, -2.9802e-06, -1.2054e-02, -3.3722e-02,\n",
            "        -5.2393e-01, -3.2187e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.2517e-06,  0.0000e+00,  0.0000e+00, -5.9605e-08,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3579877018928528 tensor(136, device='cuda:0')\n",
            "lsy tensor(56.9375, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07,\n",
            "        -1.7881e-07, -3.8743e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3710319399833679 tensor(120, device='cuda:0')\n",
            "lsy tensor(55.7500, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.4365e-04, -1.7285e-06, -1.8775e-05, -1.0908e-05, -5.1439e-05,\n",
            "        -1.2517e-06, -4.8876e-06, -2.6226e-06, -1.9073e-06, -2.3842e-07,\n",
            "        -1.1921e-07, -3.8743e-06, -1.1504e-04, -2.9325e-05, -3.4690e-05,\n",
            "        -4.4937e-03, -5.6362e-04, -6.5756e-04, -1.8060e-05, -3.9935e-05,\n",
            "        -1.3649e-05, -5.5432e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.36261144280433655 tensor(104, device='cuda:0')\n",
            "lsy tensor(52.2188, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -1.1325e-05, -1.1921e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -4.2319e-06, -4.7684e-07, -4.2915e-06, -8.3984e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.36084312200546265 tensor(95, device='cuda:0')\n",
            "lsy tensor(53.0625, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3492700159549713 tensor(106, device='cuda:0')\n",
            "lsy tensor(59.1562, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -5.9605e-08,  0.0000e+00,  0.0000e+00, -2.9802e-07,  0.0000e+00,\n",
            "        -4.7684e-07, -1.0729e-06, -3.4571e-06, -1.0000e+00, -1.0000e+00,\n",
            "        -4.3511e-06, -2.3842e-07, -1.5497e-06, -1.0729e-06, -1.1921e-07],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3999626338481903 tensor(100, device='cuda:0')\n",
            "lsy tensor(73.9375, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00, -5.9605e-08,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4152e-04, -4.0460e-04,\n",
            "        -1.0010e-02, -2.0157e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3872227370738983 tensor(84, device='cuda:0')\n",
            "lsy tensor(57.5312, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -4.3809e-05, -4.1809e-03,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.354154109954834 tensor(80, device='cuda:0')\n",
            "lsy tensor(79.1250, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4673e-02,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3627079129219055 tensor(126, device='cuda:0')\n",
            "lsy tensor(63.8125, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.3447e-07,\n",
            "        -1.7202e-04, -1.3399e-04, -1.0292e-02, -9.8291e-01, -5.3418e-01,\n",
            "        -2.1877e-03, -2.0580e-03, -1.2338e-05,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -1.6630e-05, -3.9935e-06,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.34741801023483276 tensor(99, device='cuda:0')\n",
            "lsy tensor(58.2812, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.7881e-07, -1.4191e-03, -1.0000e+00, -1.0000e+00, -1.0000e+00,\n",
            "        -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.35522305965423584 tensor(81, device='cuda:0')\n",
            "lsy tensor(55.8750, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00, -6.4240e-03, -5.6601e-04, -9.8486e-01, -1.0000e+00,\n",
            "        -9.7510e-01, -2.7847e-03, -1.8966e-04, -1.0000e+00, -5.7587e-02,\n",
            "        -9.5850e-01, -1.2323e-01, -9.9902e-01, -4.3488e-03, -5.9717e-01,\n",
            "        -1.0000e+00, -1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -1.1325e-06,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3645695149898529 tensor(103, device='cuda:0')\n",
            "3\n",
            "lsy tensor(46.3125, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.1867e-04, -8.3313e-03, -5.6183e-02, -3.4771e-03, -3.4962e-03,\n",
            "        -1.4722e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.2650e-06,\n",
            "        -9.5367e-07,  0.0000e+00, -4.5776e-05, -1.0000e+00, -4.3237e-01,\n",
            "        -6.6161e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.35831961035728455 tensor(93, device='cuda:0')\n",
            "lsy tensor(51.3125, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00, -8.8871e-05, -4.7207e-05,\n",
            "        -7.5531e-04, -5.9605e-08, -2.9802e-07,  0.0000e+00, -3.3975e-05,\n",
            "        -6.0701e-04, -3.8548e-03, -5.1022e-05, -8.0078e-02, -8.8672e-01,\n",
            "        -3.4131e-01, -1.1563e-05, -6.6757e-06, -1.3113e-06,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -5.9605e-07,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.37309813499450684 tensor(138, device='cuda:0')\n",
            "lsy tensor(55.0938, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.7881e-07,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -5.1022e-04, -1.6999e-04, -5.9009e-06,\n",
            "        -9.4531e-01, -6.4270e-02,  0.0000e+00, -3.0339e-05, -1.1921e-07,\n",
            "        -1.1492e-03, -8.9407e-07, -1.6518e-03, -3.9749e-03, -1.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.3479878306388855 tensor(98, device='cuda:0')\n",
            "lsy tensor(53.4688, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([-1.0000e+00, -1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00, -9.8348e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -6.9519e-02, -1.0931e-04, -8.9407e-07, -1.5259e-05, -7.1526e-06,\n",
            "        -1.7226e-05,  0.0000e+00,  0.0000e+00, -5.9605e-08, -5.9605e-08],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.3668963313102722 tensor(121, device='cuda:0')\n",
            "lsy tensor(53.8438, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07,\n",
            "        -5.7399e-05, -4.9472e-06,  0.0000e+00,  0.0000e+00, -5.9605e-08,\n",
            "        -5.1737e-04, -1.0000e+00, -1.0000e+00, -1.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "         0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
            "       device='cuda:0')\n",
            "clossl, wrong 0.345932275056839 tensor(93, device='cuda:0')\n",
            "lsy tensor(54.8750, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.36396709084510803 tensor(107, device='cuda:0')\n",
            "lsy tensor(48.3125, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00, -7.9870e-06,  0.0000e+00, -1.1921e-07,\n",
            "        -5.9605e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "        -2.3842e-07,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.9605e-08,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.38098761439323425 tensor(112, device='cuda:0')\n",
            "lsy tensor(60.3750, device='cuda:0', dtype=torch.float16, grad_fn=<MaxBackward1>) tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<MinBackward1>)\n",
            "pred tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07,\n",
            "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
            "         0.0000e+00,  0.0000e+00, -5.9605e-08,  0.0000e+00,  0.0000e+00],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
            "rwd tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.], device='cuda:0')\n",
            "clossl, wrong 0.35023748874664307 tensor(123, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "import torchvision.transforms.v2 as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "for i in range(30):\n",
        "    print(i)\n",
        "    train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "    agent.train_jepa(train_loader, optim)\n",
        "\n",
        "    # checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "    # torch.save(checkpoint, folder+'agentoptimargm.pkl')\n",
        "\n",
        "    # agentsd, _ = rename_sd(agent.state_dict())\n",
        "    # all_sd = store_sd(all_sd, agentsd)\n",
        "    # torch.save(all_sd, folder+'all_sd.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5-_pfGZTsip",
        "outputId": "dd2b4f22-31f1-4f12-c6f1-25ac68e565d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "# env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\", use_backgrounds=False, restrict_themes=True, use_monochrome_assets=True)\n",
        "\n",
        "# ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n",
        "\n",
        "\n",
        "# # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "# 2  5/11 8\n",
        "# 1/10 4 7/9\n",
        "# 0  3/12 6\n",
        "\n",
        "# 13 11 14\n",
        "# 10 12 9\n",
        "\n",
        "# from gymnasium.wrappers import TimeLimit\n",
        "from gym.wrappers import TimeLimit\n",
        "env = TimeLimit(env, max_episode_steps=600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PraFUAPB3j7v",
        "outputId": "691b40c7-6945-40d2-9226-cd21ededb4d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "<ipython-input-9-99d212b47ecb>:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-9-99d212b47ecb>:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dided\n",
            "time\n",
            "[0, 5, 6, 12, 6, 11, 2, 11, 4, 9, 11, 5, 9, 5, 4, 11, 8]\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# print(env.action_space) # 15\n",
        "\n",
        "def simulate(agent, buffer=[], k=4):\n",
        "    # agent.eval()\n",
        "    out=None\n",
        "    writer = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    # writer = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    act_list=[]\n",
        "    lstate=[]\n",
        "    # h0 = torch.randn((agent.jepa.pred.num_layers, agent.d_model), device=device)\n",
        "    while True:\n",
        "    # for i in range(400):\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "        # with torch.no_grad():\n",
        "        #     st = agent.jepa.enc(state)\n",
        "        #     # st_ = agent.jepa.pred(st)\n",
        "        #     stt = agent.tcost(st).squeeze(-1)\n",
        "        #     imshow(state.detach().cpu().squeeze(0))\n",
        "        #     print(stt)\n",
        "            # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        # if len(act)<=0: act = agent(state).cpu()[0,:4].tolist()\n",
        "        # print(act.shape, h0.shape) # [1, 6], [1, 256]\n",
        "        lstate.append(state)\n",
        "        if len(act)<=0:\n",
        "            # lact, lh0, lx, lz = agent(state, h0)\n",
        "            # act = lact.cpu()[0,:k].tolist()\n",
        "            # act = agent(state, k)\n",
        "            act = agent(lstate, k=k)\n",
        "            lstate=[]\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        # print(i, 'act: ',action, 'reward: ',reward)\n",
        "        act_list.append(action)\n",
        "        writer.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"dided\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    print(act_list)\n",
        "    env.close()\n",
        "    writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "_=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cm6KjvBrnNO",
        "outputId": "38db3cb7-fc99-44b0-ac57-5ceb0109f8ca",
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-47-d80e75a9d669>:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-47-d80e75a9d669>:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dided\n",
            "time\n",
            "[8, 8, 4, 5, 3, 5, 8, 8, 14, 8, 8, 5, 4, 8, 8, 13, 6, 8, 10, 8, 13, 8, 9, 3, 0, 4, 14, 0, 9, 4, 5, 4, 3, 3, 8, 5, 3, 8, 3, 8, 11, 8, 0, 8, 0, 8, 4, 6, 4, 8, 2, 14, 8, 14, 8, 8, 8, 8, 8, 3, 0, 14, 14, 8, 8, 8, 3, 2, 3, 0, 0, 4, 3, 3, 11, 7, 4, 5, 5, 8, 4, 8, 0, 8, 14, 6, 2, 9, 0, 8, 0, 5, 2, 8, 4, 6, 8, 2, 0, 6, 1, 2, 4, 14, 12, 8, 14, 2, 8, 8, 5, 8, 8, 0, 7, 8, 2, 8, 3, 5, 2, 13, 3, 8, 8, 4, 13, 8, 4, 2, 5, 8, 11, 3, 4, 3, 2, 14, 9, 8, 14, 8, 3, 8, 8, 3, 3, 13, 8, 2, 11, 4, 3, 3, 8, 2, 3, 5, 14, 4, 8, 4, 8, 8, 4, 5, 8, 4, 9, 3, 13, 5, 13, 8, 3, 5, 3, 8, 14, 9, 11, 4, 5, 4, 3, 14, 2, 8, 3, 8, 9, 8, 8, 8, 0, 3, 8, 4, 5, 8, 3, 8, 5, 14, 3, 13, 8, 8, 14, 13, 5, 8, 3, 11, 8, 8, 8, 4, 5, 8, 8, 8, 4, 3, 8, 2, 8, 3, 3, 14, 13, 2, 8, 2, 8, 8, 14, 2, 0, 11, 4, 3, 11, 13, 4, 8, 4, 8, 9, 8, 3, 5, 8, 9, 9, 6, 9, 3, 11, 11, 8, 11, 5, 2, 3, 4, 3, 11, 4, 3, 8, 3, 3, 8, 8, 4, 13, 3, 8, 6, 4, 4, 8, 9, 0, 8, 13, 8, 3, 3, 8, 0, 11, 2, 8, 2, 3, 8, 8, 8, 14, 14, 2, 8, 11, 2, 2, 6, 14, 8, 2, 11, 4, 8, 14, 5, 9, 3, 8, 9, 0, 8, 14, 3, 8, 13, 8, 3, 14, 8, 6, 0, 8, 8, 3, 8, 8, 3, 5, 14, 14, 9, 4, 14, 2, 3, 8, 11, 3, 3, 8, 8, 0, 5, 4, 2, 2, 13, 4, 13, 2, 2, 2, 5, 11, 9, 3, 5, 4, 8, 0, 2, 4, 5, 8, 4, 8, 8, 9, 2, 11, 13, 11, 3, 11, 4, 4, 8, 8, 8, 8, 4, 6, 4, 8, 2, 9, 3, 8, 3, 14, 11, 2, 8, 4, 4, 8, 8, 8, 2, 2, 4, 3, 14, 0, 5, 2, 8, 0, 0, 9, 8, 8, 4, 8, 4, 0, 11, 8, 9, 5, 5, 8, 5, 8, 13, 8, 2, 8, 8, 11, 14, 8, 0, 0, 8, 4, 2, 6, 3, 2, 6, 2, 8, 8, 14, 2, 14, 2, 4, 8, 8, 4, 4, 8, 14, 3, 14, 8, 8, 8, 4, 2, 0, 3, 5, 4, 2, 8, 14, 3, 3, 9, 14, 0, 5, 8, 6, 11, 8, 8, 2, 5, 14, 3, 11, 8, 0, 2, 13, 5, 8, 8, 8, 5, 3, 8, 13, 3, 8, 11, 0, 3, 5, 8, 5, 13, 11, 8, 3, 5, 0, 8, 3, 4, 3, 4, 0, 8, 14, 3, 8, 8, 4, 4, 3, 3, 5, 0, 3, 0, 0, 8, 11, 13, 8, 9, 11, 14, 8, 2, 13, 8, 11, 8, 4, 0, 11, 8, 8, 8, 14, 3, 5, 5, 2, 8, 3, 4, 0, 11, 9, 8, 3, 14, 8, 8, 14, 3, 8, 2, 11, 8, 8, 3, 11, 14, 5, 8, 8, 3, 11, 3, 3, 2, 0, 8, 14, 8, 3, 0, 8, 3, 3, 8, 3, 0, 3, 11, 8, 2, 3, 0, 3, 8, 8, 13, 8, 14]\n",
            "dided\n",
            "time\n",
            "[8, 14, 4, 4, 4, 2, 8, 11, 8, 8, 8, 3, 2, 9, 8, 0, 2, 8, 8, 5, 8, 0, 2, 14, 3, 8, 8, 8, 8, 2, 3, 8, 8, 8, 13]\n",
            "dided\n",
            "time\n",
            "[8, 13, 8, 6, 8, 8, 11, 6, 13, 8, 8, 4, 8, 8, 2, 3, 9, 13, 8, 11, 5, 5, 3, 4, 8, 3, 8, 3, 12, 8, 14, 8, 0, 3, 4, 2, 0, 6, 4, 2, 4, 14, 4, 3, 3, 11, 11, 8, 13, 8, 8, 4, 2, 8, 8, 13, 3, 8, 5, 4, 3, 14, 9, 8, 14, 11, 4, 8, 3, 6, 8, 5, 3, 8, 5, 0, 4, 2, 9, 2, 3, 0, 8, 8, 8, 8, 0, 9, 3, 3, 8, 14, 8, 8, 13, 8, 14, 3, 3, 8, 4, 8, 9, 14, 8, 3, 3, 8, 8, 8, 9, 11, 3, 14, 8, 9, 8, 4, 8, 3, 3, 6, 8, 8, 13, 14, 0, 4, 3, 5, 13, 4, 2, 0, 8, 2, 2, 13, 14, 0, 2, 4, 8, 5, 3, 6, 8, 3, 3, 0, 3, 8, 3, 3, 8, 8, 8, 4, 14, 14, 5, 14, 8, 0, 8, 4, 8, 3, 3, 3, 8, 8, 3, 3, 2, 14, 11, 3, 13, 5, 3, 3, 11, 13, 8, 8, 8, 8, 8, 8, 8, 3, 3, 2, 11, 2, 14, 14, 5, 8, 0, 4, 0, 14, 2, 4, 14, 8, 4, 14, 13, 5, 3, 8, 8, 4, 3, 3, 0]\n",
            "dided\n",
            "time\n",
            "[3, 0, 11, 4, 10, 3, 2, 8, 6, 8, 8, 8, 4, 3, 4, 8, 8, 3, 8, 14, 8, 8, 14, 8, 9, 11, 9, 8, 0, 14, 14, 8, 8, 5, 4, 11, 0, 9, 14, 0, 3, 4]\n",
            "dided\n",
            "time\n",
            "[4, 0, 8, 3, 5, 14, 3, 8, 5, 8, 4, 3, 8, 11, 8, 4, 8, 4, 8, 3, 8, 8, 5, 14, 11, 8, 8, 8, 8, 3, 9, 4, 4, 4, 13, 14, 6, 11, 5, 9, 2, 8, 8, 8, 9, 8, 5, 14, 8, 3, 4, 2, 8, 3, 8, 0, 0, 8, 9, 0, 0, 14, 8, 5, 3, 8, 8, 14, 14, 13, 14, 8, 0, 5, 3, 4, 4, 4, 11, 8, 5, 3, 13, 8, 8, 8, 13, 6, 2, 3]\n",
            "0 #### train ####\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-47-d80e75a9d669>:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-47-d80e75a9d669>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "repr, std, cov, clossl, z, norm 0.050199687480926514 0.2283935546875 3.4165687561035156 0.49619200825691223 1.3102490901947021 2.627256393432617\n",
            "repr, std, cov, clossl, z, norm 0.05339137092232704 0.2294921875 3.3471217155456543 0.4295380711555481 1.2595643997192383 2.339510202407837\n",
            "repr, std, cov, clossl, z, norm 0.04236960411071777 0.2298583984375 3.330925464630127 0.42237308621406555 1.278914213180542 2.836348533630371\n",
            "repr, std, cov, clossl, z, norm 0.05475496128201485 0.2296142578125 3.3375391960144043 0.4159761965274811 1.2828510999679565 2.350651264190674\n",
            "repr, std, cov, clossl, z, norm 0.04160648211836815 0.2293701171875 3.3557305335998535 0.45039573311805725 1.4270884990692139 2.6106982231140137\n",
            "repr, std, cov, clossl, z, norm 0.0648849755525589 0.2291259765625 3.3698673248291016 0.5053079724311829 1.3031196594238281 2.460367202758789\n",
            "repr, std, cov, clossl, z, norm 0.040094297379255295 0.2308349609375 3.2985994815826416 0.43139466643333435 1.2930095195770264 2.6224358081817627\n",
            "repr, std, cov, clossl, z, norm 0.05146126076579094 0.231201171875 3.274540424346924 0.4848814308643341 1.2869971990585327 2.209646701812744\n",
            "repr, std, cov, clossl, z, norm 0.040287576615810394 0.22607421875 3.504601001739502 0.4350287914276123 1.2827478647232056 2.573362350463867\n",
            "repr, std, cov, clossl, z, norm 0.04332108423113823 0.228515625 3.4079105854034424 0.4363424777984619 1.2271088361740112 2.102628231048584\n",
            "repr, std, cov, clossl, z, norm 0.030417054891586304 0.2265625 3.5241854190826416 0.40806660056114197 1.3688805103302002 2.1776013374328613\n",
            "repr, std, cov, clossl, z, norm 0.03740246966481209 0.2271728515625 3.4766132831573486 0.3913949728012085 1.2893091440200806 2.1527504920959473\n",
            "train_data.data 20482\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 4, 2, 2, 8, 4, 8, 14, 6, 8, 3, 3, 8, 8, 8, 4, 2, 8, 8, 0, 8, 9, 2, 2, 3, 8, 3, 8, 8, 14, 8, 2, 3, 3, 13, 4, 2, 3, 3, 0, 13, 8, 8, 4, 14, 2, 2, 6, 4, 8, 8, 8, 14, 6, 14, 2, 3, 3, 8, 3, 3, 3, 9, 3, 3, 3, 3, 4, 3, 5, 11, 13, 3, 3, 3, 3, 4, 2, 8, 5, 4, 3, 5, 11, 8, 4, 9, 13, 3, 8, 2, 3, 4, 8, 0, 14, 3, 5, 2, 14, 14, 4, 9, 8, 8, 3, 8, 0, 3, 3, 0, 6]\n",
            "dided\n",
            "time\n",
            "[6, 8, 2, 8, 14, 3, 3, 8, 4, 11, 9, 8, 3, 11, 6, 8, 3, 14, 8, 14]\n",
            "dided\n",
            "time\n",
            "[14, 8, 14, 8, 8, 13, 0, 8, 8, 9, 8, 6, 4, 3, 8, 2, 3, 8, 8, 8, 8, 13, 0, 2, 8, 11, 4, 8, 8, 3, 8, 8, 9, 8, 11, 14, 8, 3, 5, 8, 0, 8, 13, 11, 3, 8, 8, 14, 8, 3, 8, 2, 14, 8, 14, 4, 3, 8, 0, 13, 14, 3, 8, 3, 4, 4, 8, 8, 8, 14, 14, 11, 14, 0, 14, 3, 3, 11, 2, 9, 3, 4, 8, 11, 14, 2, 14, 4, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 4, 0, 8, 4, 3, 9, 5, 0, 4, 0, 3, 8, 10, 6, 14, 5, 11, 4, 4, 8, 0, 14, 2, 14, 13, 8, 2, 0, 8, 8, 8, 8, 14, 3, 8, 4, 8, 5, 8, 8, 3, 4, 4, 8, 14, 4, 8, 3]\n",
            "dided\n",
            "time\n",
            "[3, 6, 0, 8, 8, 4, 3, 2, 0, 2, 5, 8, 0, 3, 5, 5, 8, 8, 5, 2, 3, 8, 0, 8, 0, 14, 3, 8, 8, 8, 4, 13, 3, 4, 3, 2, 8, 5, 5, 8, 14, 8, 8, 8, 2, 5, 2, 8, 11, 13, 8, 14, 4, 8, 14, 8, 8, 11, 8, 11, 3, 8, 5, 13, 8, 3, 2, 3, 13, 8, 5, 5, 3, 13, 9, 8, 2, 14, 0, 13, 14, 4, 3, 2, 8, 2, 4, 0, 8, 4, 2, 5, 11, 3, 14, 3, 3, 4, 14, 8, 14, 3, 14, 3, 3, 14, 7, 8, 8, 4, 3, 8, 9, 14, 8, 8, 2, 8, 8, 8, 8, 3, 5, 4, 3, 3, 3, 9, 8, 9, 3, 4, 4, 8, 0, 8, 14, 8, 8, 13, 13, 14, 13, 14]\n",
            "1 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.030239468440413475 0.233642578125 3.158824920654297 0.37288448214530945 1.2757306098937988 2.152578115463257\n",
            "repr, std, cov, clossl, z, norm 0.03477417677640915 0.2314453125 3.265927791595459 0.46659520268440247 1.2961010932922363 1.7384819984436035\n",
            "repr, std, cov, clossl, z, norm 0.03046225570142269 0.2266845703125 3.5106117725372314 0.4347151815891266 1.3484346866607666 2.3537771701812744\n",
            "repr, std, cov, clossl, z, norm 0.04478132724761963 0.229736328125 3.3402657508850098 0.4005786180496216 1.2879937887191772 2.1635429859161377\n",
            "repr, std, cov, clossl, z, norm 0.029292959719896317 0.2308349609375 3.2981433868408203 0.4605409502983093 1.2740751504898071 1.8021148443222046\n",
            "repr, std, cov, clossl, z, norm 0.03841055557131767 0.22412109375 3.6301350593566895 0.5085026025772095 1.293576717376709 2.092989683151245\n",
            "repr, std, cov, clossl, z, norm 0.02933657541871071 0.2210693359375 3.804314613342285 0.41870516538619995 1.343421220779419 1.6218377351760864\n",
            "repr, std, cov, clossl, z, norm 0.03793816640973091 0.2293701171875 3.377295970916748 0.47742098569869995 1.2621781826019287 1.9240930080413818\n",
            "repr, std, cov, clossl, z, norm 0.031285952776670456 0.2401123046875 2.877171277999878 0.3724302649497986 1.2581950426101685 1.878142237663269\n",
            "repr, std, cov, clossl, z, norm 0.03870415687561035 0.2303466796875 3.3250019550323486 0.45123210549354553 1.2791039943695068 2.3636653423309326\n",
            "repr, std, cov, clossl, z, norm 0.027417577803134918 0.2257080078125 3.551722764968872 0.35597702860832214 1.2955002784729004 2.0038914680480957\n",
            "repr, std, cov, clossl, z, norm 0.03728378191590309 0.227294921875 3.4771690368652344 0.4645523726940155 1.319761037826538 2.1804027557373047\n",
            "train_data.data 20482\n",
            "dided\n",
            "time\n",
            "[14, 13, 14, 8, 14, 8, 3, 4, 8, 9, 3, 8, 5, 8, 4, 5, 4, 0, 13, 8, 4, 11, 8, 4, 8, 4, 4, 2, 0, 8, 3, 2, 8, 3, 0, 8, 11, 14, 14, 14, 14, 4, 3, 8, 11, 6, 3, 8, 8, 2, 11, 8, 8, 13, 5, 11, 8, 3, 14, 13, 8, 5, 14, 8, 13, 5, 8, 0, 6, 2, 8, 4, 0, 2, 14, 8, 8, 6, 8, 2, 14, 8, 8, 9, 4, 3, 3, 8, 8, 8, 3, 5, 0, 8, 8, 8, 9, 8, 5, 13, 3, 11, 4, 9, 5, 8, 9, 9, 3, 3, 2, 0, 4, 8, 8, 9, 13, 14, 14, 0, 0, 8, 8, 5, 8, 6, 8, 2, 8, 3, 8, 8, 5, 3, 8, 14, 14, 9, 3, 14, 5, 4, 0, 9, 2, 2, 8, 4, 3, 13, 8, 5, 6, 11, 13, 3, 8, 9, 4, 5, 14, 8, 8, 8, 8, 4, 2, 3, 3, 8, 0, 2, 8, 2, 3, 3, 14, 14, 2, 8, 8, 8, 0, 4, 11, 3, 0, 2, 4, 8, 8, 8, 8, 2, 3, 14, 14, 8, 2, 14, 8, 4, 14, 5, 2, 5, 14, 3, 0, 0, 3, 8, 14, 8, 3, 8, 8, 8, 14, 8, 8, 8, 4, 3, 2, 8, 14, 5, 4, 0, 0, 0, 3, 2, 8, 14, 5, 4, 8, 8, 14, 0, 14, 8, 8, 3, 6, 8, 2, 2, 10, 3, 0, 2, 8, 4, 4, 5, 4, 14, 8, 8, 5, 3, 0, 3, 8, 4, 11, 2, 8, 8, 8, 8, 8, 14, 14, 4, 8, 8, 3, 8, 3, 3, 4, 5, 8, 8, 0, 5, 4, 8, 8, 6, 13, 3, 5, 8, 14, 8, 9, 8, 4, 4, 8, 14, 5, 8, 13, 8, 14, 13, 7, 8, 2, 0, 0, 4, 5, 3, 4, 3, 8, 3, 2, 5, 8, 3, 14, 8, 4, 3, 2, 4, 14, 0, 3, 0, 4, 0, 6, 8, 9, 8, 8, 8, 2, 2, 0, 8, 3, 8, 3, 3, 14, 14, 2, 3, 13, 8, 5, 8, 8, 3, 8, 9, 11, 8, 8, 3, 13, 0, 3, 8, 3, 8, 2, 13, 2, 0, 4, 0, 8, 3, 4, 3, 3, 2, 14, 8, 0, 9, 3, 5, 8, 8, 8, 14, 4, 5, 10, 2, 2, 14, 8, 2, 14, 9, 5, 8, 8, 0, 8, 8, 8, 2, 8, 11, 8, 4, 2, 5, 3, 3, 8, 8, 8, 0, 4, 3, 11, 14, 4, 3, 3, 0, 6, 5, 4, 4, 4, 6, 4, 4, 3, 14, 14, 13, 4, 2, 14, 8, 0, 5, 11, 8, 3, 11, 8, 2, 13, 9, 14, 8, 5, 3, 8, 8, 9, 9, 4, 8, 8, 9, 8, 0, 3, 14, 13, 8, 11, 11, 5, 3, 5, 6, 8, 8, 2, 11, 8, 4, 13, 4, 3, 3, 6, 14, 3, 8, 8, 2, 4, 3, 14, 8, 11, 8, 4, 8, 14, 3, 8, 4, 9, 8, 5, 3, 13, 0, 3, 11, 5, 2, 2, 9, 3, 8, 5, 3, 8, 8, 8, 5, 8, 3, 4, 8, 8, 3, 8, 0, 3, 4, 3, 9, 13, 4, 14, 8, 2, 6, 4, 8, 9, 3, 3, 2, 2, 4, 2, 0, 14, 3, 14, 0, 8, 4, 2, 8, 3, 8, 8, 8, 4, 6, 11, 5, 3, 2, 14, 3, 3, 8, 3, 3, 8, 0, 3, 14, 3, 3, 8, 4, 3, 13, 4, 2, 3, 4, 9, 11, 2, 5, 11, 14, 8, 0, 0, 3, 4, 14, 14, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 14, 8, 8, 4, 3, 4, 8, 8, 9, 8, 8, 13, 13, 8, 14, 3, 2, 8, 2, 8, 3, 3, 11]\n",
            "dided\n",
            "time\n",
            "[9, 14, 5, 8, 0, 14, 5, 4, 4, 3, 8, 8, 8, 3, 8, 0, 9, 8, 3, 8, 8, 3, 8, 8, 13, 0, 2, 8, 9, 5, 14, 8, 2, 11, 14, 8, 8, 2, 8, 8, 14, 0, 2, 8, 8, 14, 0, 5, 8, 14, 5, 4, 2, 14, 0, 11, 3, 8, 8, 6, 0, 11, 3, 9, 3, 8, 9, 2, 8, 13, 4, 11, 3, 5, 8, 8, 8, 3, 0, 8, 14, 3, 8, 2, 6, 3, 13, 2, 8]\n",
            "dided\n",
            "time\n",
            "[8, 13, 4, 14, 2, 8, 11, 3, 2, 3, 8, 4, 3, 3, 3, 0, 0, 13, 2, 13, 2, 5, 8, 3, 14, 2, 3, 8, 2, 0, 12, 8, 4, 11, 3, 2, 8, 5, 8, 4, 9, 5]\n",
            "dided\n",
            "time\n",
            "[5, 2, 11, 3, 11, 14, 3, 3, 2, 3, 6, 11, 3, 8, 8, 8, 14, 3, 14, 14, 2, 8, 8, 8, 14, 2, 14, 2, 8, 3, 0, 3, 8, 2, 13, 4, 9, 14, 8, 2, 8, 3, 8, 14, 8, 4, 8, 3, 5, 0, 14, 5, 9, 8, 14, 8, 8, 0, 11, 14, 3, 4, 2, 8, 4, 5, 5, 10, 3, 3, 11, 8, 8, 8, 8, 13, 5, 14, 9, 1, 14, 3, 8, 2, 6, 0, 8, 8, 8, 8, 5, 4, 11, 3, 11, 8, 8, 8, 3, 8, 8, 13, 0, 3, 3, 8, 8, 0, 4, 8, 9, 8, 8, 4, 5, 14, 11, 5, 14, 8, 3, 8, 9, 4, 8, 9, 11, 2, 8, 9, 8, 8, 0, 2, 0, 0, 0, 14, 5, 14, 8, 8, 14, 3, 9, 0, 3, 8, 4, 11, 14, 5, 8, 8, 9, 14, 8, 0, 13, 8, 5, 4, 2, 3, 8, 0, 3, 0, 0, 0, 8, 8, 0, 0, 3, 8, 0, 2, 8, 9, 4, 8, 4, 8, 5, 14, 8, 14, 3, 3, 8, 5, 4, 8, 13, 0, 4, 2, 4, 2, 14, 0, 8, 14, 4, 0, 2, 8, 11, 0, 6, 5, 2, 8, 2, 8, 2, 3, 2, 8, 8, 13, 8, 3, 14, 14, 4, 14, 3, 0, 0, 14, 11, 8, 3, 0, 4, 3, 2, 3, 3, 8, 3, 2, 5, 2, 8, 3, 4, 8, 2, 2, 2, 8, 5, 8, 14, 2, 3, 5, 5, 8, 8, 2, 2, 14, 3, 3, 11, 3, 3, 4, 4, 13, 8, 11, 2, 6, 3, 14, 3, 9, 11, 3, 4, 8, 8, 3, 3, 14, 8, 8, 2, 11, 8, 4, 14, 8, 6, 8, 0, 11, 3, 14, 3, 8, 3, 8, 13, 0, 8, 8, 5, 6, 14, 4, 4, 3, 8, 0, 5, 14, 14, 14, 8, 2, 5, 14, 4, 9, 8, 8, 11, 14, 8, 8, 5, 4, 8, 8, 8, 8, 2, 4, 8, 14, 6, 0, 14, 9, 3, 8, 6, 3, 8, 2, 5, 8, 8, 8, 5, 14, 8, 8, 11, 2, 8, 0, 8, 3, 2, 14, 8, 8, 11, 8, 3, 3, 8, 2, 5, 4, 4, 8, 14, 0, 8, 13, 8, 2, 3, 2, 3, 13, 8, 2, 4, 5, 14, 0, 8, 8, 5, 8, 14, 2, 3, 8, 0, 5, 2, 8, 8, 8, 8, 4, 6, 3, 0, 8, 3, 3, 3, 4, 8, 4, 14, 8, 3, 3, 11, 8, 2, 0, 8, 0, 5, 8, 8, 14, 8, 3, 4, 14, 0, 8, 8, 4, 13, 2, 8, 9, 3, 8, 5, 4, 14, 4, 8, 0, 11, 14, 13, 8, 8, 6, 7, 4, 8, 8, 8, 8, 0, 3, 2, 8, 8, 3, 4, 14, 2, 8, 3, 8, 9, 6, 6, 14, 8, 3, 8, 3, 8, 14, 14, 8, 11, 11, 4, 8, 2, 5, 2, 8, 3, 14, 13, 8, 14, 14, 14, 2, 2, 13, 3, 4, 13, 14, 3, 5, 4, 9, 8, 0, 5, 4, 8, 3, 4, 3, 4, 3, 8, 2, 14, 3, 5, 2, 4, 8, 5, 2, 2, 5, 9, 0, 6, 13, 0, 2, 4, 3, 8, 4, 4, 4, 4, 9, 8, 0, 5, 3, 2, 8, 4, 8, 8, 3, 8, 14, 14, 3, 2, 8, 5, 3, 8, 4, 8, 8, 5, 8, 8, 3, 11, 4, 4, 3, 13, 0, 4, 8, 8, 8, 2, 4, 11, 2, 2, 8, 2, 8, 11, 8, 9, 3, 11, 8, 0, 8, 0, 14, 8, 8, 14, 8, 7, 2, 3, 8, 8, 0, 14, 5, 6, 4, 8, 3, 8, 4, 3, 11, 10, 8, 0, 8, 8, 8, 8, 8, 3, 5, 14, 4, 13, 8, 2, 14, 4, 8, 8, 8, 8, 8, 3, 4, 4, 11, 2, 8, 14, 8, 4, 0, 8, 14, 8, 4, 3, 14, 3, 8, 8, 4, 3, 0, 8, 3, 14, 4, 13, 0, 0]\n",
            "2 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02485060878098011 0.2252197265625 3.570621967315674 0.41615286469459534 1.2404062747955322 2.3992795944213867\n",
            "repr, std, cov, clossl, z, norm 0.034216366708278656 0.2286376953125 3.384977340698242 0.4774250388145447 1.2245571613311768 2.2174956798553467\n",
            "repr, std, cov, clossl, z, norm 0.026624483987689018 0.229248046875 3.360844612121582 0.5352165699005127 1.3333603143692017 2.3448619842529297\n",
            "repr, std, cov, clossl, z, norm 0.03183343634009361 0.2344970703125 3.1166858673095703 0.5451011657714844 1.242904782295227 2.495314121246338\n",
            "repr, std, cov, clossl, z, norm 0.02659958228468895 0.234375 3.123640298843384 0.4627368748188019 1.2942672967910767 1.9189338684082031\n",
            "repr, std, cov, clossl, z, norm 0.03252717852592468 0.2275390625 3.4733314514160156 0.4439626634120941 1.2708508968353271 1.9212883710861206\n",
            "repr, std, cov, clossl, z, norm 0.025751007720828056 0.2264404296875 3.5179672241210938 0.4248899221420288 1.2907782793045044 2.0556976795196533\n",
            "repr, std, cov, clossl, z, norm 0.03526955470442772 0.2283935546875 3.4046435356140137 0.45062950253486633 1.300011157989502 1.7587661743164062\n",
            "repr, std, cov, clossl, z, norm 0.024946225807070732 0.2294921875 3.3561367988586426 0.4758056700229645 1.2846043109893799 2.214874505996704\n",
            "repr, std, cov, clossl, z, norm 0.03324524313211441 0.229736328125 3.3520689010620117 0.44731205701828003 1.2911038398742676 2.0093228816986084\n",
            "repr, std, cov, clossl, z, norm 0.027985719963908195 0.229736328125 3.3436312675476074 0.40503257513046265 1.2876981496810913 2.3253231048583984\n",
            "repr, std, cov, clossl, z, norm 0.03943905979394913 0.225830078125 3.5496597290039062 0.47621434926986694 1.265323281288147 2.156860589981079\n",
            "train_data.data 20466\n",
            "dided\n",
            "time\n",
            "[0, 0, 2, 3, 8, 2, 3, 3, 8, 8, 8, 4, 3, 8, 3, 13, 2, 4, 2, 0, 9, 8, 3, 3, 9, 8, 8, 14, 14, 0, 3, 2, 4, 4, 6, 8, 2, 9, 3, 8, 9, 9, 12, 14, 0, 5, 3, 8, 2, 11, 2, 8, 0, 0, 4, 14, 4, 3, 3, 4, 6, 5, 1, 9, 9, 8, 5, 14, 4, 8, 3, 8, 8, 5, 2, 8, 9, 13, 4, 8, 3, 3, 8, 8, 4, 5, 8, 8, 9, 3, 4, 3, 9, 4, 8, 0, 9, 9, 8, 3, 0, 8, 3, 14, 4, 3, 2, 3, 9, 14, 8, 8, 6, 11, 10, 12, 8, 8, 5, 8, 9, 11, 11, 13, 9, 3, 5, 5, 6, 9, 12, 4, 8, 8, 3, 0, 0, 8, 4, 4, 9, 5, 4, 2, 8, 13, 0, 5, 6, 10, 12, 12, 9, 2, 5, 14, 9, 9, 12, 12, 3, 9, 9, 14, 9, 8]\n",
            "dided\n",
            "time\n",
            "[8, 4, 0, 2, 6, 4, 12, 12, 6, 11, 12, 11, 9, 4, 14, 14, 3, 9, 10, 1, 6, 12, 1, 12, 9, 3, 13, 2, 4, 3, 5, 11, 9, 8, 14, 9, 9, 8, 2]\n",
            "dided\n",
            "time\n",
            "[3, 9, 14, 12, 9, 9, 4, 14, 4, 9, 3, 8, 4, 8, 5, 8, 9, 4, 8]\n",
            "dided\n",
            "time\n",
            "[4, 8, 8, 4, 9, 3, 8, 14, 9, 8, 13, 14, 6, 12, 1, 10, 8, 8, 0, 8, 6, 11, 12, 12, 4, 14, 11, 3, 9, 8, 2, 8, 8, 3, 8, 8, 8, 8, 2, 13, 9, 3, 8, 5, 2, 3, 5, 14, 8, 8, 3, 14, 2, 4, 8, 0, 3, 9, 8, 3, 9, 14, 8, 5, 8, 13, 3, 3, 14, 9, 8, 0, 3, 8, 8, 9, 8, 11, 8, 13, 5, 6, 3, 3, 8, 8, 9, 8, 4, 9, 8, 2, 8, 8, 11, 3, 6, 12, 10, 12, 8, 8, 5, 14, 9, 9, 11, 0, 8, 3, 4, 3, 6, 0, 9, 3, 8, 2, 2, 2, 9, 9, 12, 12, 6, 9, 12, 12, 9, 4, 3, 4, 4, 5, 14, 3, 9, 2, 11, 2, 11, 8, 3, 3, 4, 4, 8, 8, 0, 8, 3, 3, 14, 3, 14, 9, 9, 3, 8, 6, 6, 11, 12, 8, 0, 8, 0, 8, 14, 8, 3, 3, 13, 9, 9, 5, 3, 3, 2, 14, 9, 14, 3, 8, 9, 9, 8, 8, 8, 3]\n",
            "dided\n",
            "time\n",
            "[4, 3, 3, 14, 9, 9, 8, 2, 4, 8, 8, 8, 4, 8, 8, 2, 2, 14, 8, 8, 3, 3, 8, 8, 9, 4, 9, 4, 9, 14, 2, 5, 9, 2, 3, 5, 6, 11, 12, 12, 8, 3, 8, 0, 9, 14, 4, 8, 8, 13, 4, 8, 0, 8, 14, 8, 6, 1, 12, 10, 11, 3, 8, 6, 0, 3, 0, 3, 6, 4, 12, 12, 4, 8, 11, 14, 13, 14, 9, 8, 9, 3, 2, 3, 6, 9, 12, 12, 2, 0, 8, 0, 4, 8, 9, 8, 6, 1, 1, 10, 9, 4, 12, 12, 9, 6, 2, 3, 3, 3, 4, 0, 0, 4, 0, 8, 8, 3, 8, 8, 4, 11, 8, 8, 0, 3, 8, 8, 9, 3, 13, 9, 6, 9, 11, 4, 9, 3, 8, 3, 8, 11, 13, 4, 9, 8, 8, 14, 14, 13, 8, 4, 0, 13, 11, 2, 4, 2, 8, 4, 3, 8, 9, 4, 9, 11, 5, 11, 2, 8, 3, 9, 9, 9, 11, 6, 3, 3, 4, 2, 8, 14, 4, 8, 3, 9, 5, 14, 4, 3, 0, 14, 8, 2, 4, 3, 9, 6, 14, 3, 4, 8, 8, 14, 4, 4, 11, 2, 9, 14, 8, 3, 9, 8, 11, 6, 9, 9, 8]\n",
            "3 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.028400344774127007 0.2269287109375 3.486931085586548 0.43777284026145935 1.3294031620025635 2.2941055297851562\n",
            "repr, std, cov, clossl, z, norm 0.03544149175286293 0.2296142578125 3.352665901184082 0.46403688192367554 1.2812387943267822 2.236327648162842\n",
            "repr, std, cov, clossl, z, norm 0.030852526426315308 0.23046875 3.2988719940185547 0.4497179687023163 1.2915910482406616 2.0718281269073486\n",
            "repr, std, cov, clossl, z, norm 0.044823843985795975 0.2337646484375 3.142090320587158 0.45316165685653687 1.2850888967514038 1.9388556480407715\n",
            "repr, std, cov, clossl, z, norm 0.030817998573184013 0.22998046875 3.32828426361084 0.4037620425224304 1.2815935611724854 2.031543254852295\n",
            "repr, std, cov, clossl, z, norm 0.04572347179055214 0.2249755859375 3.5834121704101562 0.49382033944129944 1.3481073379516602 1.7467930316925049\n",
            "repr, std, cov, clossl, z, norm 0.03056401200592518 0.2252197265625 3.571194648742676 0.5417723655700684 1.2204015254974365 1.8776277303695679\n",
            "repr, std, cov, clossl, z, norm 0.04324404522776604 0.231201171875 3.266833782196045 0.4957757592201233 1.3077532052993774 1.773760199546814\n",
            "repr, std, cov, clossl, z, norm 0.030661718919873238 0.228515625 3.413473129272461 0.439273864030838 1.335067868232727 2.3735575675964355\n",
            "repr, std, cov, clossl, z, norm 0.03359704092144966 0.23046875 3.2964115142822266 0.4832916855812073 1.292873501777649 2.089559316635132\n",
            "repr, std, cov, clossl, z, norm 0.028415238484740257 0.2281494140625 3.4093284606933594 0.48129555583000183 1.286649465560913 2.1321864128112793\n",
            "repr, std, cov, clossl, z, norm 0.033155038952827454 0.2286376953125 3.383457660675049 0.4053097069263458 1.3547136783599854 1.5568451881408691\n",
            "train_data.data 20630\n",
            "dided\n",
            "time\n",
            "[9, 9, 8, 13, 11, 8, 3, 4, 3, 0, 8, 3, 8, 3, 4, 7, 3, 3, 2, 8, 3, 2, 11, 2, 14, 8, 9, 3, 4, 6, 13, 14, 0, 3, 3, 8, 8, 2, 11, 14, 8, 8, 3, 8, 14, 14, 6, 4, 8, 0, 4, 6, 3, 3, 4, 3, 11, 5, 2, 3, 3, 4, 6, 2, 8, 8, 8, 3, 8, 11, 13, 14, 8, 3, 3, 3, 3, 3, 8, 8, 2, 13, 3, 14, 3]\n",
            "dided\n",
            "time\n",
            "[8, 8, 9, 0, 8, 8, 8, 9, 4, 2, 13, 3, 14, 14, 9, 8, 4, 2, 3, 9, 8, 11, 14, 3, 8, 11, 3, 3, 8, 11, 8, 3, 8, 2, 0, 2, 13, 2, 4, 2, 8, 14, 8, 8, 14, 8, 4, 8, 14, 3, 4, 3, 2, 8, 8, 4, 2, 3, 3, 8, 8, 8, 14, 5, 2, 8, 2, 5, 14, 2, 11, 4, 8, 8, 3, 3, 8, 2, 4, 11, 9, 4, 8, 11, 2, 14, 0, 8, 3, 5, 14, 11, 8, 9, 3, 5, 13, 2, 5, 0, 8, 4, 3, 8, 8, 14, 0, 3, 8, 14, 3, 3, 4, 3, 11, 2, 4, 14, 2, 8, 2, 11, 3, 14, 4, 4, 11, 8, 8, 9, 11, 5, 3, 8, 3, 3, 3, 5, 8, 2, 14, 8, 14, 14, 4, 14, 5, 5, 2, 8, 13, 8, 4, 9, 14, 3, 3, 14, 3, 3, 5, 4, 8, 4, 10, 0, 4, 5, 14, 4, 11, 0, 3, 3, 5, 5, 13, 14, 4, 13, 2, 3, 0, 3, 14, 8, 13, 5, 7, 3, 4, 8, 3, 0, 3, 2, 14, 3, 3, 5, 2, 14, 8, 8, 3, 2, 8, 2, 2, 14, 6, 3, 0, 5, 3, 13, 2, 3, 9, 8, 4, 3, 4, 8, 9, 3, 8, 8, 8, 0, 14, 9, 14, 0, 8, 4, 8, 4, 0, 8, 8, 8, 14, 2, 2, 5, 13, 0, 13, 14, 8, 0, 5, 11, 3, 2, 4, 9, 4, 2, 3, 8, 8, 8, 4, 3, 8, 8, 8, 14, 3, 8, 14, 14, 3, 8, 4, 6, 8, 8, 8, 6, 13, 0, 8, 3, 3, 14, 8, 14, 3, 2, 0, 8, 8, 14, 4, 8, 0, 11, 0, 14, 4, 14, 8, 9, 9, 8, 3, 8, 11, 8, 4, 3, 9, 2, 13, 3, 5, 14, 5, 5, 0, 0, 13, 5, 2, 3, 5, 4, 2, 0, 5, 3, 11, 8, 13, 0, 3, 4, 5, 13, 8, 2, 4, 3, 11, 14, 14, 9, 2, 8, 3, 5, 3, 9, 8, 14, 11, 3, 3, 6, 6, 3, 9, 5, 8, 8, 11, 8, 3, 11, 8, 0, 2, 8, 8, 14, 3, 4, 5, 8, 4, 8, 6, 0, 0, 13, 9, 8, 8, 5, 8, 8, 5, 13, 9, 8, 8, 13, 8, 8, 3, 8, 4, 14, 2, 9, 8, 14, 13, 2, 8, 0, 4, 11, 14, 14, 5, 5, 2, 0, 4, 2, 0, 13, 8, 5, 2, 3, 3, 8, 14, 9, 11, 8, 4, 5, 3, 8, 8, 3, 4, 11, 8, 2, 4, 5, 3, 11, 14, 8, 2, 2, 0, 8, 2, 6, 0, 8, 14, 14, 0, 2, 9, 4, 8, 0, 4, 4, 14, 8, 8, 8, 11, 14, 2, 10, 3, 8, 8, 14, 9, 14, 8, 8, 8, 8, 7, 2, 4, 8, 5, 8, 8, 14, 3, 8, 3, 8, 5, 0, 3, 14, 13, 3, 3, 3, 5, 11, 8, 4, 8, 8, 6, 2, 5, 8, 3, 8, 5, 13, 8]\n",
            "dided\n",
            "time\n",
            "[13, 8, 8, 9, 8, 5, 4, 4, 13, 3, 3, 3, 8, 2, 8, 5, 9, 5, 9, 0, 3, 3, 8, 11, 8, 8, 3, 3, 5, 3, 4, 3, 8, 11, 2, 8, 4, 8, 13, 8, 3, 8, 3, 3, 7, 14, 3, 3, 8, 0, 3, 8, 14, 8, 0, 8, 8, 3, 6, 3, 14, 14, 3, 3, 3, 3, 0, 4, 14, 0, 8, 14, 9, 13, 9, 8, 5, 4, 8, 0, 8]\n",
            "dided\n",
            "time\n",
            "[14, 11, 9, 14, 4, 2, 4, 3, 8, 3, 11, 9, 3, 14, 0, 5, 8, 13, 8, 0, 14, 13, 3, 8, 4, 8, 14, 9, 5, 5, 9, 5, 4, 8, 11, 8, 0, 8, 2, 8, 13, 8, 8, 8, 8, 8, 8, 8, 11, 3, 2, 3, 4, 13, 8, 8, 9, 3, 9, 14, 8, 0, 5, 8, 3, 2, 2, 14, 2, 8, 9, 8, 2, 4, 8, 8, 14, 11, 14, 14, 8, 3, 3, 4, 0, 4, 8, 8, 13, 3, 14, 8, 8, 14, 8]\n",
            "dided\n",
            "time\n",
            "[14, 8, 8, 2, 8, 13, 3, 3, 2, 8, 3, 2, 2, 8, 2, 8, 6, 2, 0, 0, 8, 3, 4, 2, 3, 8, 3, 8, 2, 3, 4, 8, 3, 3, 8, 13, 5, 4, 8, 5]\n",
            "4 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.0311942957341671 0.2257080078125 3.5400681495666504 0.3710521161556244 1.313453197479248 2.2909963130950928\n",
            "repr, std, cov, clossl, z, norm 0.034628208726644516 0.228271484375 3.4176981449127197 0.47709402441978455 1.406011939048767 1.9091883897781372\n",
            "repr, std, cov, clossl, z, norm 0.026986509561538696 0.2349853515625 3.0763564109802246 0.4736819863319397 1.3321717977523804 1.5894745588302612\n",
            "repr, std, cov, clossl, z, norm 0.02898566424846649 0.23046875 3.2977192401885986 0.44847533106803894 1.3652222156524658 1.703845500946045\n",
            "repr, std, cov, clossl, z, norm 0.023403799161314964 0.2257080078125 3.5274717807769775 0.5070616602897644 1.333919644355774 1.4394172430038452\n",
            "repr, std, cov, clossl, z, norm 0.02746584638953209 0.2275390625 3.4510254859924316 0.4651363790035248 1.3729664087295532 1.8376718759536743\n",
            "repr, std, cov, clossl, z, norm 0.031055893748998642 0.2303466796875 3.312014579772949 0.3948083221912384 1.2820547819137573 1.4970263242721558\n",
            "repr, std, cov, clossl, z, norm 0.0325605683028698 0.229248046875 3.3555150032043457 0.5294002890586853 1.302607774734497 1.6181520223617554\n",
            "repr, std, cov, clossl, z, norm 0.029630431905388832 0.223876953125 3.6282968521118164 0.46388307213783264 1.3106380701065063 1.807602047920227\n",
            "repr, std, cov, clossl, z, norm 0.039077166467905045 0.2232666015625 3.691403865814209 0.4701252579689026 1.3213098049163818 1.6997398138046265\n",
            "repr, std, cov, clossl, z, norm 0.027829106897115707 0.2257080078125 3.5676097869873047 0.4553568363189697 1.3319454193115234 1.880733609199524\n",
            "repr, std, cov, clossl, z, norm 0.0342973954975605 0.2310791015625 3.2725796699523926 0.395526260137558 1.2677297592163086 2.1700873374938965\n",
            "train_data.data 20276\n",
            "dided\n",
            "time\n",
            "[4, 8, 5, 8, 14, 4, 8, 4, 2, 8, 8, 4, 12, 3, 2, 3, 8, 5, 8, 7, 8, 8, 8, 6, 14, 11, 8, 5, 8, 14, 8, 0, 14, 8, 9, 6, 2]\n",
            "dided\n",
            "time\n",
            "[5, 9, 8, 14, 0, 11, 4, 9, 8, 3, 8, 13, 0, 8, 13, 13, 11, 3, 9, 8, 9, 8, 0, 4, 11, 9, 4, 9, 0, 2, 6, 3, 9, 4, 8, 9, 4, 5, 8, 8, 8, 2, 4, 8, 14, 8, 12, 5, 3, 3, 3, 8, 8, 14, 3, 3, 2, 4, 8, 4, 8, 8, 3, 3, 14, 2, 9, 8, 2, 8, 3, 8, 4, 2, 8, 8, 3, 4, 3, 5, 8, 0, 2, 11, 8, 8, 8, 0, 14, 8, 14, 0, 3, 14, 3, 3, 0, 0, 4, 5, 0, 8, 8, 8, 8, 4, 9, 8, 3, 3, 4, 11, 9, 4, 4, 14, 8, 13, 4, 3, 2, 4, 4, 8, 4, 6, 8, 2, 14, 8, 3, 5, 8, 14, 5, 8, 5, 8, 0, 3, 5, 8, 3, 8, 0, 2, 5, 4, 0, 8, 14, 3, 2, 9, 3, 3, 3, 13, 8, 13, 8, 14, 8, 2, 9]\n",
            "dided\n",
            "time\n",
            "[2, 8, 14, 0, 3, 8, 11, 0, 2, 2, 3, 3, 3, 2, 8, 4, 3, 3, 0, 8, 8, 8, 3, 0, 5, 8, 8, 4, 9, 14, 4, 4, 8, 8, 3, 2, 13, 0, 8, 2, 3, 4, 8, 4, 5, 8, 14, 13, 5, 8, 8, 0, 8, 0, 4, 2, 9, 2, 0, 8, 3, 8, 3, 8, 8, 8, 3, 11, 0, 3, 14, 5, 4, 2, 0, 8, 4, 2, 0, 14, 3, 14, 3, 3, 3, 2, 9, 0, 3, 14, 2, 3, 8, 0, 8, 5, 3, 2, 5, 14, 3, 5, 8, 3, 8, 0, 8, 4, 8, 8, 0, 3, 8, 8, 2, 14, 14, 2, 3, 3, 0, 14, 8, 2, 3, 3, 5, 3, 4, 2, 8, 5, 3, 0, 5, 5, 0, 11, 3, 0, 13, 0, 14, 8, 4, 0, 11, 5, 7, 14, 8, 13, 13, 9, 4, 14, 8, 14, 3, 3, 5, 8, 3, 14, 3, 5, 4, 3, 2, 8, 8, 14, 3, 2, 8, 4, 0, 3, 8, 13, 8, 14, 8, 4, 4, 8, 2, 8, 7, 13, 3, 12, 14, 3, 8, 4, 3, 0, 13, 3, 8, 14, 8, 3, 8, 3, 2, 0, 6, 8, 3, 8, 8, 14, 8, 14, 8, 3, 9, 4, 3, 8, 3, 3, 3, 8, 8, 8, 14, 14, 14, 4, 8, 8, 8, 8, 11, 3, 4, 4, 11, 8, 2, 9, 8, 4, 3, 11, 8, 4, 4, 8, 8, 8, 3, 4, 11, 3, 8, 8, 4, 8, 8, 5, 4, 10, 3, 8, 2, 14, 3, 8, 3, 8, 3, 8, 0, 5, 14, 0, 5, 9, 3, 2, 8, 3, 4, 8, 9, 14, 8, 11, 2, 8, 13, 14, 4, 0, 3, 5, 13, 0, 2, 11, 8, 8, 3, 0, 3, 2, 8, 8, 2, 8, 8, 3, 4, 3, 2, 13, 3, 8, 8, 0, 4, 13, 3, 5, 9, 6, 8, 8, 8, 8, 8, 3, 2, 13, 3, 14, 2, 14, 5, 4, 3, 4, 3, 13, 3, 14, 3, 9, 2, 3, 8, 3, 8, 8, 2, 3, 8, 11, 13, 5, 10, 8, 8, 0, 11, 8, 3, 5, 8, 3, 0, 8, 2, 8, 11, 2, 4, 4, 8, 8, 4, 3, 2, 3, 2, 9, 4, 5, 0, 4, 3, 8, 8, 8, 3, 3, 2, 2, 11, 14, 3, 2, 4, 2, 8, 3, 8, 2, 0, 9, 8, 4, 11, 11, 4, 8, 4, 8, 3, 8, 14, 8, 4, 8, 5, 8, 4, 8, 3, 13, 11, 8, 13, 5, 14, 14, 4, 4, 8, 2, 0, 8, 4, 8, 8, 8, 3, 9, 14, 0, 2, 2, 8, 3, 8, 8, 5, 8, 9, 4, 8, 4, 9, 3, 0, 8, 8, 8, 3, 8, 3, 6, 3, 8, 3, 0, 11, 9, 14, 2, 4, 14, 11, 8, 14, 2, 13, 8, 2, 0, 3, 0, 8, 3, 9, 14, 7, 4, 11, 5, 2, 2, 8, 0, 8, 5, 10, 14, 5, 8, 3, 8, 9, 3, 8, 3, 8, 8, 8, 6, 8, 8, 8, 2, 4, 3, 14, 8, 4, 5, 14, 0, 3, 3, 14, 4, 14, 8, 3, 14, 2, 8, 14, 2, 8, 3, 5, 5, 8, 2, 4, 8, 9, 14, 5, 6, 2, 9, 3, 8, 8, 5, 8, 13, 8, 8, 5, 3, 14, 3, 5, 14, 3, 3, 8, 8, 3, 2, 2, 3, 9, 4, 8, 9, 14, 8, 0, 13, 2, 8, 8, 6, 14, 3, 3, 14, 8, 8, 2, 4, 8, 11, 3, 13, 3, 8, 0, 3, 11, 8, 8, 8, 5, 8, 8, 14, 8, 4, 14, 9, 14, 3, 4, 8, 10, 3, 9, 4, 2]\n",
            "dided\n",
            "time\n",
            "[8, 3, 14, 3, 4, 0, 4, 14, 5, 14, 3, 5, 2, 14, 8, 8, 8, 8, 3, 0, 14, 0, 14, 8, 4, 9, 8, 5, 2, 3, 8, 0, 13, 8, 8, 6, 3, 8, 8, 6, 14, 6, 14, 8, 2, 5, 5, 5, 3, 0, 9, 6, 8, 8, 8, 8, 0, 0, 8, 3, 3, 9, 4, 11, 5, 11, 8, 14, 2, 8, 14, 14, 2, 3, 8, 4, 6, 9, 4, 8, 3, 3, 14]\n",
            "dided\n",
            "time\n",
            "[3, 14, 11, 4, 5, 11, 4, 8, 5, 3, 3, 8, 5, 4, 8, 4, 11, 3, 8, 3, 8, 5, 8, 3, 8, 0, 11, 6, 8, 8, 8, 8, 3, 3, 13, 11, 5, 8, 8, 4, 8, 9, 8, 4, 5, 4, 2, 13, 9, 3, 8, 6, 4, 4, 2, 5, 8, 2, 8, 8]\n",
            "5 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.025653153657913208 0.23046875 3.3082075119018555 0.4155210852622986 1.3001651763916016 2.130786657333374\n",
            "repr, std, cov, clossl, z, norm 0.027127452194690704 0.2308349609375 3.2883830070495605 0.4032193124294281 1.3106248378753662 2.2478461265563965\n",
            "repr, std, cov, clossl, z, norm 0.02475205808877945 0.227783203125 3.451923370361328 0.41745004057884216 1.3748835325241089 2.457810401916504\n",
            "repr, std, cov, clossl, z, norm 0.02816041372716427 0.2264404296875 3.5183775424957275 0.410548597574234 1.294135332107544 2.136397123336792\n",
            "repr, std, cov, clossl, z, norm 0.029010510072112083 0.226318359375 3.509275436401367 0.43223094940185547 1.30239999294281 2.283470630645752\n",
            "repr, std, cov, clossl, z, norm 0.02462456002831459 0.228515625 3.3926775455474854 0.4781641364097595 1.2572433948516846 2.2518720626831055\n",
            "repr, std, cov, clossl, z, norm 0.025441214442253113 0.2296142578125 3.3522233963012695 0.4748712182044983 1.2501469850540161 2.338423013687134\n",
            "repr, std, cov, clossl, z, norm 0.03299768269062042 0.22900390625 3.38329815864563 0.5019852519035339 1.285364031791687 2.3010568618774414\n",
            "repr, std, cov, clossl, z, norm 0.0250054020434618 0.2305908203125 3.32021427154541 0.4333431124687195 1.302261471748352 2.5980048179626465\n",
            "repr, std, cov, clossl, z, norm 0.02971210703253746 0.2296142578125 3.3685755729675293 0.5492796301841736 1.3319751024246216 1.7825355529785156\n",
            "repr, std, cov, clossl, z, norm 0.025565313175320625 0.2314453125 3.272334337234497 0.4548315703868866 1.3447856903076172 2.063746452331543\n",
            "repr, std, cov, clossl, z, norm 0.03279685229063034 0.2313232421875 3.2709717750549316 0.4525006413459778 1.2465431690216064 1.9043481349945068\n",
            "train_data.data 20459\n",
            "dided\n",
            "time\n",
            "[2, 8, 8, 4, 3, 13, 2, 8, 14, 5, 3, 9, 3, 13, 3, 4, 9, 3, 4, 4, 2, 8, 3, 8, 14, 8, 3, 2, 3, 8, 5, 5, 3, 3, 8, 9, 0, 14, 2, 0, 8, 0, 0, 4, 8, 3, 3, 9, 8, 4, 11, 5, 8, 11, 3, 11, 9, 0, 0, 3, 2, 3, 0, 5, 8, 9, 8, 3, 3, 13, 3, 4, 5, 3, 6, 11, 8, 8, 13, 8]\n",
            "dided\n",
            "time\n",
            "[8, 13, 8, 4, 8, 3, 5, 8, 3, 8, 4, 8, 5, 2, 11, 8, 2, 2, 0, 8, 3, 8, 3, 0, 3, 14, 14, 5, 2, 6, 3, 5, 8, 13, 8, 4, 2, 8, 11, 11, 8, 8, 6, 3, 2, 8, 0, 2, 0, 11, 4, 11, 3, 8, 8, 8, 8, 3, 3, 5, 6, 2, 11, 4, 4, 8, 8, 14, 9, 2, 8, 11, 2, 2, 8, 8, 5, 0, 0, 8, 5, 4, 8, 4, 4, 0, 5, 3, 8, 3, 2, 8, 2, 13, 14, 9, 9, 2, 11, 11, 3, 8, 14, 2, 13, 8, 8, 14, 11, 8, 8, 6, 14, 0, 4, 0, 8, 14, 8, 10, 3, 9, 3, 9, 8, 8, 2, 3, 6, 2, 8, 2, 8, 5, 14, 14, 14, 4, 2, 3, 8, 2, 4, 5, 0]\n",
            "dided\n",
            "time\n",
            "[8, 3, 4, 12, 9, 2, 13, 3, 8, 13, 2, 2, 5, 8, 0, 3, 2, 11, 4, 0, 8, 3, 4, 0, 14, 3, 3, 9, 5, 14, 3, 3, 8, 8, 5, 2, 13, 5, 5, 5, 14, 3, 6, 4, 8, 8, 4, 9, 8, 9, 4, 14, 2, 11, 8, 8, 14, 8, 8, 3, 8, 9, 2, 3, 3, 3, 8, 4, 9, 11, 0, 13, 8]\n",
            "dided\n",
            "time\n",
            "[13, 3, 8, 9, 8, 14, 8, 2, 8, 0, 4, 8, 3, 3, 3, 5, 7, 8, 3, 2, 11, 5, 2, 8, 4, 13, 4, 14, 0, 3, 8, 3, 10, 3, 3, 4, 3, 14, 14, 3, 8, 8, 8, 2, 8, 0, 8, 4, 8, 4, 14, 8, 3, 5, 2, 2, 11, 9, 2, 2, 3, 8, 9, 8, 5, 14, 6, 8, 5, 4, 2, 4, 14, 8, 14, 14, 4, 8]\n",
            "dided\n",
            "time\n",
            "[8, 14, 8, 4, 8, 8, 8, 2, 9, 11, 3, 13, 5, 3, 8, 6, 8, 4, 9, 2, 14, 14, 2, 11, 4, 3, 4, 2, 2, 0, 4, 4, 6, 14, 0, 8, 13, 14, 8, 3, 5, 6, 8, 8, 14, 8, 8, 0, 8, 14, 8, 8, 5, 9, 8, 2, 8, 3, 3, 4, 4, 8, 5, 13, 8, 8, 12, 13, 8, 8, 14, 4, 8, 5, 9, 0, 8, 3, 9, 8, 5, 8, 2, 8, 14, 3, 5, 3, 8, 13, 3, 3, 2, 4, 8, 4, 8, 14, 14, 3, 3, 4, 14, 8, 2, 14, 3, 3, 4, 8, 8, 9, 8, 3, 9, 0, 8, 3, 8, 3, 4, 7, 8, 0, 8, 8, 5, 8, 8, 14, 14, 5, 2, 8, 13, 13, 13, 6, 8, 4, 14, 14, 8, 8, 8, 14, 3, 5, 9, 4, 8, 5, 4, 0, 4, 3, 14, 14, 8, 4, 4, 8, 3, 4, 5, 11, 9, 3, 3, 2, 3, 8, 2, 3, 0, 2, 8, 8, 8, 2, 3, 8, 14, 2, 5, 4, 8, 8, 8, 8, 8, 8, 8, 5, 4, 4, 9, 8, 5, 4, 8, 11, 9, 5, 4, 4, 3, 8, 14, 0, 3, 8, 0, 0, 3, 2, 8, 14, 8, 13, 2, 14, 8, 5, 11, 5, 8, 8, 4, 8, 3, 4, 3, 8, 8, 3, 4, 8, 3, 4, 11, 2, 3, 11, 4, 11, 8, 11, 4, 4, 9, 3, 4, 5, 8, 2, 3, 8, 0, 3, 4, 14, 3, 14, 8, 14, 4, 14, 2, 8, 4, 5, 5, 3, 13, 8, 0, 3, 4, 2, 8, 11, 3, 14, 14, 14, 3, 2, 11, 5, 3, 2, 3, 9, 9, 9, 8, 8, 3, 4, 8, 8, 14, 8, 4, 0, 3, 13, 0, 11, 5, 3, 9, 14, 0, 8, 6, 8, 3, 14, 4, 3, 3, 8, 6, 11, 8, 14, 2, 3, 8, 14, 9, 4, 0, 14, 4, 14, 8, 4, 13, 8, 5, 8, 13, 3, 3, 3, 6, 2, 8, 3, 14, 14, 8, 3, 8, 8, 8, 2, 0, 8, 0, 3, 8, 8, 8, 4, 8, 5, 3, 14, 2, 8, 8, 0, 3, 8, 2, 8, 5, 9, 8, 9, 3, 5, 0, 8, 3, 2, 8, 9, 8, 11, 3, 0, 0, 8, 9, 4, 4, 2, 2, 5, 4, 0, 3, 2, 11, 3, 3, 3, 4, 2, 8, 3, 5, 4, 14, 8, 3, 3, 8, 3, 2, 5, 2, 14, 2, 14, 4, 8, 4, 14, 4, 8, 3, 2, 13, 8, 3, 8, 0, 3, 14, 8, 3, 3, 8, 14, 3, 14, 4, 8, 11, 2, 13, 3, 8, 5, 0, 8, 11, 5, 0, 8, 14, 14, 3, 8, 8, 5, 11, 3, 0, 8, 14, 8, 3, 8, 5, 8, 5, 2, 3, 8, 14, 8, 4, 5, 8, 6, 9, 2, 8, 8, 3, 11, 3, 2, 14, 14, 4, 3, 8, 0, 8, 14, 14, 14, 1, 3, 11, 3, 2, 11, 8, 0, 8, 8, 0, 2, 3, 3, 3, 8, 0, 11, 5, 0, 3, 6, 2, 8, 3, 8, 8, 8, 6, 9, 8, 8, 8, 14, 3, 3, 13, 3, 8, 14, 8, 13, 8, 3, 8, 8, 3, 4, 8, 0, 8, 6, 8, 3, 2, 11, 11, 3, 8, 8, 4, 8, 0, 8, 14, 2, 5, 13, 3, 14, 8, 5, 14, 2, 8, 14, 8, 14, 0, 2, 13, 3, 8, 2, 2, 13, 8, 2, 6, 8, 8, 6, 4, 13, 8, 2, 3, 11, 3, 3, 10, 2, 9, 8, 2, 8, 0, 4, 8, 14, 5, 8, 2, 4, 3, 3, 8, 4, 11, 4, 4, 11, 14, 8, 2, 3, 9, 5, 8, 3, 3, 11, 3, 8, 4, 2, 11, 6, 2, 8, 9, 14, 3, 8, 3, 3, 8, 8, 8, 9, 3, 11, 5, 8, 3, 3, 8, 4, 3, 5, 3, 8, 8, 2, 2, 14, 3, 8, 8, 8, 4, 9, 0, 14, 9, 13, 5, 3, 5, 8, 8, 11, 8, 9, 14, 8, 3, 13, 3, 3, 8, 5, 14, 3, 0, 3, 14, 3, 8, 8, 9, 8, 8, 8, 2, 3, 13, 8, 3, 0, 3, 2, 5, 11, 8, 14, 4, 5, 11, 9, 8, 3, 5, 0, 2, 3, 11, 8, 11, 4, 3, 13, 4, 2, 4, 4, 14, 3, 0, 5, 13, 8, 8, 14, 8, 8, 8, 13, 8, 8, 3, 8, 14, 9, 4, 4, 8, 0, 2, 8, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 6, 5, 8, 8, 3, 3, 14, 4, 8, 3, 14, 8, 9, 11, 11, 8, 8, 8, 5, 14, 11, 8, 3, 8, 14, 14, 2, 8, 3, 9, 3, 2, 14, 3, 8, 8, 8, 4, 11, 4, 3, 3, 8, 3, 13, 3, 11, 0, 0]\n",
            "6 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02697022818028927 0.231689453125 3.2644808292388916 0.4925228953361511 1.359649419784546 2.1612114906311035\n",
            "repr, std, cov, clossl, z, norm 0.03276905044913292 0.2276611328125 3.460308074951172 0.5408653020858765 1.2971036434173584 2.255065679550171\n",
            "repr, std, cov, clossl, z, norm 0.03154568746685982 0.220947265625 3.826835870742798 0.45222553610801697 1.27215576171875 2.63155460357666\n",
            "repr, std, cov, clossl, z, norm 0.04745533689856529 0.2276611328125 3.449700355529785 0.47329533100128174 1.2379229068756104 1.8823297023773193\n",
            "repr, std, cov, clossl, z, norm 0.03267407417297363 0.2340087890625 3.144285202026367 0.521720826625824 1.204567313194275 1.8904998302459717\n",
            "repr, std, cov, clossl, z, norm 0.046236805617809296 0.233642578125 3.1448073387145996 0.4995023310184479 1.2746907472610474 2.156045436859131\n",
            "repr, std, cov, clossl, z, norm 0.027550216764211655 0.22900390625 3.3801212310791016 0.49986574053764343 1.3610988855361938 2.3447418212890625\n",
            "repr, std, cov, clossl, z, norm 0.037958987057209015 0.225341796875 3.580761671066284 0.427501380443573 1.252273678779602 1.7984979152679443\n",
            "repr, std, cov, clossl, z, norm 0.027440419420599937 0.22705078125 3.5003225803375244 0.4202691912651062 1.320556402206421 1.937322974205017\n",
            "repr, std, cov, clossl, z, norm 0.03774913400411606 0.2281494140625 3.426877975463867 0.4869043529033661 1.2336136102676392 2.47320818901062\n",
            "repr, std, cov, clossl, z, norm 0.02843860723078251 0.230224609375 3.336027145385742 0.44254642724990845 1.2718772888183594 2.1140024662017822\n",
            "repr, std, cov, clossl, z, norm 0.03367422893643379 0.23046875 3.3074960708618164 0.43532735109329224 1.3143543004989624 2.0123331546783447\n",
            "train_data.data 20472\n",
            "dided\n",
            "time\n",
            "[0, 11, 8, 5, 8, 8, 2, 11, 0, 6, 8, 11, 2, 8, 0, 5, 2, 14, 3, 8, 3, 4, 0, 0, 5, 3, 8, 4, 13, 14, 14, 8, 3, 4, 4, 8, 9, 14, 2, 8, 8, 5, 8, 3, 5, 4, 8, 4, 8, 13, 3, 14, 0, 11, 2, 8, 3, 6, 8, 0, 8, 8, 8, 8, 8, 8, 14, 8, 0, 14, 8, 6, 4, 8, 8, 5, 8, 3, 3, 3, 11, 0, 2, 8, 14, 5, 11, 9, 2, 6, 9, 2, 8, 4, 8, 5, 6, 8, 9, 8, 11, 2, 5, 0, 8, 13, 8, 9, 9, 4, 0, 14, 6, 14, 4, 0, 2, 8, 3, 11, 8, 3, 13, 14, 3, 2, 4, 4, 3, 8, 14, 8, 8, 8, 5, 8, 8, 9, 0, 0]\n",
            "dided\n",
            "time\n",
            "[9, 0, 0, 3, 8, 8, 8, 8, 8, 5, 3, 8, 8, 8, 8, 3, 0, 3, 3, 11, 3, 6, 11, 3, 13, 8, 3, 8, 8, 8, 8, 3, 8, 9, 5, 8, 8, 2, 2, 14, 8, 4, 8, 14, 8, 3, 5, 3, 13, 8, 8, 4, 3, 4, 8, 8, 3, 8, 2, 8, 8, 0, 8, 14, 13, 5, 0, 8, 8, 5, 8, 8, 14, 8, 0, 3, 3, 11, 0, 8, 3, 2, 13, 11, 6, 8, 3, 8, 0, 14, 8, 0, 0, 8, 2, 2, 0, 8, 9, 6, 0, 8, 9, 4, 3, 3, 8, 4, 8, 3, 14, 11, 3, 2, 8, 8, 6, 3, 3, 3, 2, 11, 14, 14, 11, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 8, 4, 0, 8, 8, 3, 2, 6, 3, 13, 8, 4, 14, 13, 3, 0, 4, 0, 11, 8, 8, 3, 8, 3, 13, 4, 13, 8, 2, 0, 5, 4, 2, 3, 8, 8, 13, 8, 4, 6, 8, 14, 8, 8, 3, 3, 9, 4, 2, 4, 8, 2, 0, 11, 9, 8, 0, 3, 2, 8, 4, 0, 8, 8, 3, 4, 8, 8, 2, 8, 9, 5, 8, 3, 4, 8, 2, 2, 9, 8, 8, 13, 8, 8, 8, 4]\n",
            "dided\n",
            "time\n",
            "[8, 8, 4, 3, 6, 2, 13, 8, 8, 14, 2, 14, 2, 3, 8, 3, 0, 8, 3, 8, 2, 2, 6, 0, 3, 0, 8, 8, 3, 3, 5, 9, 11, 3, 8, 8, 8, 9, 4, 11, 8, 9, 5, 11, 8, 8, 11, 13, 4, 0, 8, 8, 2, 3, 3, 2, 14, 3, 14, 4, 8, 4, 2, 9, 3, 8, 0, 8, 9, 9, 8, 4, 8, 8, 2, 8, 0, 9, 3, 2, 3, 0, 5, 4, 9, 8, 2, 8, 2, 8, 8, 5, 8, 8, 5, 8, 0, 8, 8, 0, 13, 6, 3, 11, 3, 14, 8, 2, 3, 5, 2, 2, 9, 8, 13, 4, 3, 3, 4, 8, 8, 8, 3, 3, 3, 3, 3, 2, 0, 11, 3, 11, 8, 3, 8, 0, 5, 3, 2, 13, 4, 6, 4, 6, 11, 3, 8, 8, 9, 5, 2, 3, 5, 14, 8, 8, 14, 2, 8, 4, 11, 8, 5, 14, 2, 3, 8, 4, 2, 2, 3, 13, 11, 5, 4, 14, 8, 3, 0, 8, 2, 3, 2, 9, 8, 8, 2, 0, 11, 4, 2, 0, 3, 8, 3, 2, 4, 8, 0, 2, 9, 0, 8, 2, 9, 9, 3, 5, 8, 11, 0, 3, 8, 4, 3, 6]\n",
            "dided\n",
            "time\n",
            "[4, 3, 6, 13, 3, 13, 8, 8, 8, 11, 3, 5, 8, 8, 0, 0, 11, 9, 0, 3, 8, 6, 3, 13, 4, 9, 14, 11, 3, 8, 6, 3, 14, 8, 14, 3, 14, 8, 3, 9, 3, 8, 13, 8, 3, 3, 8, 2, 8, 2, 3, 9, 5, 8, 14, 14, 11, 4, 2, 11, 3, 13, 14, 8, 5, 0, 5, 14, 2, 8, 3, 13, 3, 11, 8, 0, 3, 5, 8, 3, 13]\n",
            "7 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02529161609709263 0.230224609375 3.293968677520752 0.4067527949810028 1.2936476469039917 2.290499687194824\n",
            "repr, std, cov, clossl, z, norm 0.02853996679186821 0.228271484375 3.407203197479248 0.48104020953178406 1.3312346935272217 2.292938470840454\n",
            "repr, std, cov, clossl, z, norm 0.026361674070358276 0.225341796875 3.5615830421447754 0.46963992714881897 1.3557754755020142 2.665540933609009\n",
            "repr, std, cov, clossl, z, norm 0.03563272953033447 0.2293701171875 3.354449987411499 0.4139714241027832 1.2849993705749512 2.190217971801758\n",
            "repr, std, cov, clossl, z, norm 0.032700929790735245 0.2301025390625 3.3230786323547363 0.46217742562294006 1.2461278438568115 2.0056896209716797\n",
            "repr, std, cov, clossl, z, norm 0.04110687971115112 0.22607421875 3.5243024826049805 0.4124694764614105 1.3766555786132812 2.0813512802124023\n",
            "repr, std, cov, clossl, z, norm 0.03246361017227173 0.231689453125 3.268521547317505 0.44153982400894165 1.2649328708648682 2.123563766479492\n",
            "repr, std, cov, clossl, z, norm 0.042338915169239044 0.23193359375 3.2406258583068848 0.40385377407073975 1.265582799911499 2.4556877613067627\n",
            "repr, std, cov, clossl, z, norm 0.0317966602742672 0.227294921875 3.471864700317383 0.43255615234375 1.3836562633514404 2.122664451599121\n",
            "repr, std, cov, clossl, z, norm 0.043594684451818466 0.2293701171875 3.3553714752197266 0.46285712718963623 1.3038065433502197 1.827567219734192\n",
            "repr, std, cov, clossl, z, norm 0.028892196714878082 0.2254638671875 3.5413172245025635 0.44284680485725403 1.2382973432540894 2.1728177070617676\n",
            "repr, std, cov, clossl, z, norm 0.03858564794063568 0.2291259765625 3.35714054107666 0.48350459337234497 1.2579731941223145 1.9115179777145386\n",
            "train_data.data 20437\n",
            "dided\n",
            "time\n",
            "[13, 5, 3, 11, 2, 3, 3, 4, 8, 14, 8, 5, 2, 5, 3, 3, 2, 8, 8, 5, 8, 5, 5, 0, 9, 0, 0, 14, 8, 11, 5, 5, 3, 6, 2, 8, 3, 8, 3, 8, 5, 14, 8, 0, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 8, 8, 11, 8, 0, 8, 2, 5, 8, 8, 0, 11, 3, 3, 0, 4, 0, 4, 14, 5, 8, 8, 4, 8, 8, 0, 8, 11, 9, 14, 3, 8, 0, 0, 5, 13, 3, 5, 4, 9, 3, 13]\n",
            "dided\n",
            "time\n",
            "[9, 3, 13, 8, 8, 2, 14, 4, 2, 8, 8, 4, 8, 8, 9, 8, 14, 0, 8, 8, 8, 8, 8, 8, 8, 5, 8, 8, 8, 11, 4, 14, 8, 5, 8, 3, 8, 3, 11, 8, 8, 3, 13, 11, 3, 4, 2, 3, 14, 9, 14, 5, 8, 0, 5, 14, 8, 8, 14, 8, 8, 3, 9]\n",
            "dided\n",
            "time\n",
            "[3, 9, 14, 2, 3, 8, 2, 0, 14, 8, 13, 3, 8, 9, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[3, 13, 4, 9, 3, 8, 8, 3, 14, 13, 3, 11, 3, 0, 8, 8, 11, 2, 2, 8, 8, 0, 13, 8, 4, 8, 0, 4, 8, 0, 14, 2, 3, 2, 8, 4, 14, 3, 8, 4, 3, 8, 8, 4, 3, 8, 8, 2, 8, 14, 5, 9, 3, 11, 14, 6, 4, 8, 14, 2, 4, 8, 8, 8, 14, 2, 8, 8, 3, 2, 3, 4, 3, 13, 8, 13, 14, 8, 3, 9, 4, 3]\n",
            "8 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.027535337954759598 0.2318115234375 3.253821849822998 0.4935227334499359 1.3559530973434448 2.0962300300598145\n",
            "repr, std, cov, clossl, z, norm 0.03542451933026314 0.231689453125 3.25034499168396 0.40910160541534424 1.2663933038711548 2.156846046447754\n",
            "repr, std, cov, clossl, z, norm 0.027811046689748764 0.228271484375 3.4312307834625244 0.4875829815864563 1.3317739963531494 2.032907009124756\n",
            "repr, std, cov, clossl, z, norm 0.03236374631524086 0.228271484375 3.422123432159424 0.39517977833747864 1.3277982473373413 2.3153598308563232\n",
            "repr, std, cov, clossl, z, norm 0.024814920499920845 0.2315673828125 3.247011184692383 0.3973684012889862 1.3370606899261475 2.2721199989318848\n",
            "repr, std, cov, clossl, z, norm 0.03095058910548687 0.228759765625 3.39389705657959 0.42764031887054443 1.3120304346084595 1.8533393144607544\n",
            "repr, std, cov, clossl, z, norm 0.026144327595829964 0.228515625 3.3976991176605225 0.3933686912059784 1.2324497699737549 1.657750129699707\n",
            "repr, std, cov, clossl, z, norm 0.029161935672163963 0.226806640625 3.4892876148223877 0.45284825563430786 1.2874681949615479 2.031932830810547\n",
            "repr, std, cov, clossl, z, norm 0.027773873880505562 0.2294921875 3.3525028228759766 0.4065997302532196 1.3440889120101929 2.0134143829345703\n",
            "repr, std, cov, clossl, z, norm 0.03323023393750191 0.2320556640625 3.2193610668182373 0.3845211863517761 1.2537084817886353 1.7776256799697876\n",
            "repr, std, cov, clossl, z, norm 0.02911260724067688 0.22998046875 3.3366453647613525 0.35964760184288025 1.2630667686462402 1.452840805053711\n",
            "repr, std, cov, clossl, z, norm 0.039123669266700745 0.2298583984375 3.322871685028076 0.4407927393913269 1.2950830459594727 1.7353590726852417\n",
            "train_data.data 20367\n",
            "dided\n",
            "time\n",
            "[3, 3, 8, 9, 13, 0, 2, 14, 8, 5, 8, 4, 2, 0, 14, 4, 6, 3, 4, 8, 8, 8, 4, 4, 3, 3, 0, 3, 3, 11, 9, 2, 8]\n",
            "dided\n",
            "time\n",
            "[4, 11, 2, 10, 8, 2, 0, 2, 8, 9, 0, 8, 11, 14, 0, 8, 2, 3, 13, 8, 3, 0, 3, 9, 4, 8, 9, 3, 2, 8, 3, 4, 5, 8, 11, 4, 14, 8, 13, 13, 11, 8, 3, 8, 13, 8, 8, 5, 3, 8, 2, 9, 8, 7, 11, 3, 8]\n",
            "dided\n",
            "time\n",
            "[5, 5, 2, 14, 9, 8, 9, 13, 0, 5, 8, 14, 5, 0, 11, 2, 14, 0, 8, 11, 8, 4, 8, 5, 6, 8, 2, 3, 8, 14, 5, 0, 3, 2, 4, 8, 5, 4, 4, 3, 8, 8, 5, 8, 3, 0, 3, 4, 4, 3, 11, 8, 2, 13, 3, 8, 3, 2, 13, 4, 13, 3, 2, 8, 4, 9, 8, 3, 9, 8, 11, 3, 8, 8, 11, 3, 8, 3, 3, 5, 3, 4, 5, 8, 4, 3, 11, 2, 8, 8, 8, 5, 4, 3, 3, 5, 0, 8, 13, 8, 3, 0, 8, 14, 9, 11, 9, 4, 8, 8, 8, 4, 9, 8, 6, 14, 2, 8, 0, 9]\n",
            "dided\n",
            "time\n",
            "[8, 0, 9, 13, 3, 4, 2, 3, 3, 8, 14, 14, 3, 4, 0, 2, 4, 8, 8, 9, 14, 2, 4, 2, 11, 8, 5, 3, 14, 8, 8, 6, 9, 11, 3, 14, 8, 14, 6, 8, 5, 2, 0, 9, 13, 11, 5, 9, 11, 13, 9, 13, 2, 3, 13, 8, 8, 8, 6, 8, 13, 4, 5, 13, 8, 4, 3, 9, 2, 5, 8, 0, 5, 8, 8, 13, 8, 3]\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 4, 14, 14, 5, 13, 11, 3, 14, 8, 14, 5, 5, 2, 3, 4, 3, 4, 2, 2, 5, 11, 5, 9, 8, 2, 13, 0, 0, 8, 8, 2, 9, 9, 5, 14, 3, 8, 0, 4, 8, 8, 3, 4, 11, 3, 2, 8, 0, 4, 3, 13, 4, 5, 8, 3, 2, 2, 0, 2, 14, 8, 11, 5, 8, 2, 4, 3, 3, 9, 8, 4, 8, 3, 0, 4, 4, 0, 3, 3, 4, 4]\n",
            "9 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.03285463899374008 0.225341796875 3.548342227935791 0.4537407457828522 1.2645691633224487 1.8226011991500854\n",
            "repr, std, cov, clossl, z, norm 0.05688956007361412 0.2279052734375 3.417818546295166 0.6210587024688721 1.302134394645691 1.9298256635665894\n",
            "repr, std, cov, clossl, z, norm 0.050459764897823334 0.228515625 3.4082279205322266 0.5347555875778198 1.3444961309432983 1.505632996559143\n",
            "repr, std, cov, clossl, z, norm 0.06199505180120468 0.2276611328125 3.442476749420166 0.556046724319458 1.3320510387420654 1.767141342163086\n",
            "repr, std, cov, clossl, z, norm 0.034959398210048676 0.229736328125 3.3292887210845947 0.4771645665168762 1.2901837825775146 1.8243484497070312\n",
            "repr, std, cov, clossl, z, norm 0.03577859327197075 0.2291259765625 3.373295307159424 0.5231065154075623 1.2766019105911255 1.4206011295318604\n",
            "repr, std, cov, clossl, z, norm 0.023236390203237534 0.2264404296875 3.50321626663208 0.43756356835365295 1.3103573322296143 2.142808437347412\n",
            "repr, std, cov, clossl, z, norm 0.02874409779906273 0.2242431640625 3.6166112422943115 0.40265458822250366 1.3412151336669922 1.7538363933563232\n",
            "repr, std, cov, clossl, z, norm 0.02087746560573578 0.229248046875 3.3692805767059326 0.4540559649467468 1.3028069734573364 1.6774609088897705\n",
            "repr, std, cov, clossl, z, norm 0.02320466935634613 0.234130859375 3.149522542953491 0.40547430515289307 1.3552206754684448 1.6417490243911743\n",
            "repr, std, cov, clossl, z, norm 0.018481500446796417 0.228759765625 3.4026637077331543 0.37409916520118713 1.3152979612350464 1.2702903747558594\n",
            "repr, std, cov, clossl, z, norm 0.020538538694381714 0.2281494140625 3.419999599456787 0.44452807307243347 1.4025516510009766 1.5230441093444824\n",
            "train_data.data 20438\n",
            "dided\n",
            "time\n",
            "[3, 4, 4, 3, 0, 8, 3, 14, 0, 3, 3, 2, 3, 3, 8, 4, 3, 3, 8, 9, 2, 4, 2, 14, 14, 3, 14, 2, 11, 0, 8, 13, 3, 5, 4, 8, 8, 8, 4, 13, 3, 4, 8, 8, 3, 9, 3]\n",
            "dided\n",
            "time\n",
            "[9, 3, 3, 8, 13, 3, 5, 9, 0, 13, 8, 14, 3, 2, 3, 8, 3, 4, 9, 8, 13, 9, 2, 2, 8, 4, 4, 14, 8, 8, 4, 2, 4, 8, 8, 14, 14, 8, 8, 8, 5, 8, 2, 3, 5, 8, 1, 2, 3, 0, 5, 8, 2, 13, 3, 3, 4, 3, 8, 3, 11, 8, 0, 8, 4, 3, 8, 0, 5, 2, 4, 3, 8, 2, 3, 8, 13, 2, 7, 8, 4, 14, 8, 8, 3, 5, 3, 3, 13, 13, 8, 0, 9, 8, 5, 14, 4, 8, 8, 8, 8, 5, 8, 2, 14, 8, 5, 5, 11, 8, 13, 8, 11, 8, 11, 0, 5, 4, 11]\n",
            "dided\n",
            "time\n",
            "[4, 11, 3, 3, 11, 3, 4, 8, 5, 14, 5, 3, 8, 14, 2, 2, 13, 3, 9, 8, 2, 14, 13, 4, 8, 2, 0, 3, 14, 3, 8, 3, 5, 13, 14, 13, 8, 14, 8, 2, 14, 8, 8, 8, 4, 8, 5, 2, 0, 14, 8, 13, 3, 8, 8, 2, 8, 4, 8, 8, 8, 3, 8, 8, 8, 8, 9, 8, 13, 14, 4, 9, 2, 14, 3, 5, 0, 2, 14, 4, 8, 8, 2, 8, 2, 3, 8, 9, 13, 9, 3, 9, 14, 0, 8, 5, 8, 5, 11, 2, 5, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 14, 8, 3, 2, 3, 8, 5, 4, 8, 6, 14, 13, 3, 8, 3, 5, 5, 8, 3, 14, 2, 0, 8]\n",
            "dided\n",
            "time\n",
            "[2, 6, 8, 8, 0, 9, 8, 8, 8, 9, 4, 8, 2, 3, 8, 6, 9, 5, 11, 0, 14, 8, 8, 4, 11, 8]\n",
            "10 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.019288092851638794 0.228515625 3.42250919342041 0.40786629915237427 1.277739405632019 1.8817790746688843\n",
            "repr, std, cov, clossl, z, norm 0.025751683861017227 0.229736328125 3.3578295707702637 0.39026567339897156 1.2673442363739014 1.0805763006210327\n",
            "repr, std, cov, clossl, z, norm 0.024631526321172714 0.232177734375 3.224140167236328 0.41401633620262146 1.287062168121338 1.5086588859558105\n",
            "repr, std, cov, clossl, z, norm 0.030223293229937553 0.2347412109375 3.095271110534668 0.42309099435806274 1.2339106798171997 1.9256068468093872\n",
            "repr, std, cov, clossl, z, norm 0.026220722123980522 0.230712890625 3.295884370803833 0.5028176307678223 1.2349964380264282 1.805087924003601\n",
            "repr, std, cov, clossl, z, norm 0.0392114594578743 0.2247314453125 3.580421209335327 0.41025614738464355 1.3073540925979614 1.8063111305236816\n",
            "repr, std, cov, clossl, z, norm 0.027116145938634872 0.2220458984375 3.7339706420898438 0.46508556604385376 1.2537026405334473 1.8443785905838013\n",
            "repr, std, cov, clossl, z, norm 0.037259720265865326 0.226318359375 3.505466938018799 0.49323156476020813 1.2690428495407104 2.08884859085083\n",
            "repr, std, cov, clossl, z, norm 0.03228272497653961 0.2305908203125 3.3099758625030518 0.5064743161201477 1.3354697227478027 2.0891852378845215\n",
            "repr, std, cov, clossl, z, norm 0.036937348544597626 0.2371826171875 3.0199060440063477 0.5306951403617859 1.3403384685516357 2.1062183380126953\n",
            "repr, std, cov, clossl, z, norm 0.028434786945581436 0.2337646484375 3.16462779045105 0.48779428005218506 1.2838062047958374 2.0845956802368164\n",
            "repr, std, cov, clossl, z, norm 0.03737659007310867 0.228271484375 3.4386417865753174 0.37281301617622375 1.313633680343628 2.2090351581573486\n",
            "train_data.data 20887\n",
            "dided\n",
            "time\n",
            "[8, 13, 2, 8, 0, 5, 5, 14, 8, 8, 6, 2, 14, 9, 2, 8, 8, 11, 4, 8, 14, 14, 14, 2, 8, 3, 9, 9, 8, 8, 8, 14, 3, 3, 8, 2, 8, 8, 5, 8, 0, 13, 2, 3, 3, 8, 3, 3, 4, 14, 3, 8, 14, 9, 4, 3, 6, 4, 2, 8, 6, 0, 0, 4, 0, 4, 4, 14, 8, 14, 8, 0, 11, 4, 8, 3, 4, 3, 8, 8, 5, 11]\n",
            "dided\n",
            "time\n",
            "[11, 3, 11, 2, 0, 4, 3, 0, 14, 3, 3, 8, 5, 8, 3, 3, 8, 3, 14, 11, 3, 5, 4, 8, 13, 3, 14, 14, 2, 11, 5, 3, 3, 3, 0, 13, 0, 8, 9, 2, 4, 0, 3, 8, 5, 8, 3, 8, 3, 8, 5, 14, 14, 4, 14, 5, 8, 3, 5, 3, 8, 0, 0, 3, 14, 3, 0, 11, 8, 2, 8]\n",
            "dided\n",
            "time\n",
            "[2, 8, 0, 5, 3, 8, 3, 5, 6, 13, 3, 3, 4, 13, 8, 8, 9, 8, 8, 5, 8, 8, 3, 8, 8, 8, 3, 3, 2, 14, 8, 4, 13, 3, 3, 8, 8, 8, 0, 3, 0, 8, 3, 8, 2, 2, 8, 11, 6, 13, 2, 3, 5, 4, 4, 8, 9, 13, 2, 14, 2, 11, 8]\n",
            "dided\n",
            "time\n",
            "[11, 8, 14, 4, 5, 3, 5, 9, 8, 5, 11, 13, 8, 3, 3, 3, 3, 8, 11, 3, 8, 5, 13, 8, 8, 8, 8, 11, 3, 13, 8, 11, 5, 8, 8, 8, 3, 2, 8, 8, 5, 8, 11, 8, 13, 8, 2, 0, 3, 8, 14, 8, 6, 3, 14, 8, 5, 8, 5]\n",
            "dided\n",
            "time\n",
            "[8, 5, 8, 8, 13, 5, 2, 5, 8, 2, 9, 2, 2, 8, 3, 4, 8, 3, 3, 8, 3, 11, 3, 14, 4, 5, 11, 8, 3, 8, 13, 4, 14, 0, 14, 0, 3, 3, 6, 3, 9, 5, 13, 8, 3, 3, 4, 5, 2, 8, 3, 3, 14, 8, 3, 14, 13, 0, 3, 9, 4, 6, 3, 13, 14, 3, 8, 2, 0, 11, 8, 4, 5, 8, 11, 8, 3, 5, 11, 0, 3, 0, 13, 4, 3, 5, 14, 8, 14, 13, 8, 13, 14, 8]\n",
            "11 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.025879941880702972 0.226318359375 3.5534629821777344 0.44258517026901245 1.3106061220169067 1.8573557138442993\n",
            "repr, std, cov, clossl, z, norm 0.03880420699715614 0.2293701171875 3.3777365684509277 0.45318588614463806 1.2641345262527466 1.6532984972000122\n",
            "repr, std, cov, clossl, z, norm 0.026705309748649597 0.22705078125 3.4821314811706543 0.43028679490089417 1.2184116840362549 2.088620185852051\n",
            "repr, std, cov, clossl, z, norm 0.03282112628221512 0.2257080078125 3.5484251976013184 0.41128799319267273 1.2395641803741455 2.193774938583374\n",
            "repr, std, cov, clossl, z, norm 0.02338799461722374 0.2340087890625 3.127133369445801 0.4133242964744568 1.3184369802474976 2.397143840789795\n",
            "repr, std, cov, clossl, z, norm 0.03119928576052189 0.2310791015625 3.263500213623047 0.4401763677597046 1.3142106533050537 2.065401554107666\n",
            "repr, std, cov, clossl, z, norm 0.022632351145148277 0.2291259765625 3.3516180515289307 0.42313435673713684 1.3154141902923584 2.2273969650268555\n",
            "repr, std, cov, clossl, z, norm 0.03159492090344429 0.2266845703125 3.5005102157592773 0.43147698044776917 1.3123064041137695 1.9186749458312988\n",
            "repr, std, cov, clossl, z, norm 0.033055905252695084 0.2257080078125 3.560577154159546 0.45142820477485657 1.2710825204849243 2.2102763652801514\n",
            "repr, std, cov, clossl, z, norm 0.034277692437171936 0.226318359375 3.497537612915039 0.5019078850746155 1.2720979452133179 2.123542308807373\n",
            "repr, std, cov, clossl, z, norm 0.031199177727103233 0.22705078125 3.485637664794922 0.4231764078140259 1.2453856468200684 2.3008506298065186\n",
            "repr, std, cov, clossl, z, norm 0.03920168802142143 0.23095703125 3.272953987121582 0.5411730408668518 1.2210584878921509 2.0102438926696777\n",
            "train_data.data 20358\n",
            "dided\n",
            "time\n",
            "[8, 14, 3, 8, 8, 14, 8, 9, 0, 2, 3, 9, 4, 8, 3, 3, 3, 8, 8, 8, 3, 8, 9, 8, 3, 14, 0, 0, 3, 5, 2, 8, 9, 14, 11, 2, 4, 5, 1, 4, 13, 2, 0, 8, 4, 2, 8, 8, 8, 8, 14, 3, 5, 2, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[10, 3, 8, 8, 2, 0, 3, 3, 4, 3, 5, 3, 3, 8, 14, 2, 8, 8, 3, 6, 3, 6, 5, 2, 11, 3, 3, 8, 5, 3, 5, 2, 4, 13, 9, 3, 8, 8, 8, 3, 14, 9, 8, 0, 3, 2, 3, 0, 4, 8, 5, 13, 8, 8, 8, 0, 0, 8, 2, 8, 2, 3, 11, 8, 8, 8, 8, 5, 4, 0, 2, 3]\n",
            "dided\n",
            "time\n",
            "[0, 2, 3, 13, 14, 3, 11, 0, 2, 2, 3, 11, 2, 13, 3, 3, 5, 0, 5, 8, 4, 8, 8, 14, 14, 8, 8, 3, 0, 0, 2, 3, 8, 13, 3, 5, 9, 4, 13, 8, 4, 9, 4, 3, 9, 2, 3, 4, 0, 9, 4, 14, 2, 3, 3, 5, 2, 2, 4, 2, 4, 3, 5, 14, 8, 4, 4, 2, 14, 11, 14, 8, 3, 11, 14, 3, 3, 11, 14, 8, 2, 4, 0, 3, 8, 8, 3, 7]\n",
            "dided\n",
            "time\n",
            "[8, 3, 7, 14, 11, 2, 8, 3, 13, 9, 2, 8, 8, 3, 13, 8, 9, 8, 8, 8, 8, 3, 8, 0, 9, 10, 11, 4, 0, 4, 8, 3, 8, 11, 8, 8, 8, 0, 8, 8, 3, 13, 8, 8, 2, 2, 0, 8, 3, 9, 14, 0, 8, 8, 8, 8, 2, 8, 8, 3, 8, 11, 11, 3, 8, 9, 14, 2, 3, 4, 8, 13, 0, 13, 13, 8, 5, 11, 0, 3, 3, 5, 2, 8, 4, 8, 8, 14, 8, 2, 8, 5, 4, 3]\n",
            "dided\n",
            "time\n",
            "[3, 5, 8, 8, 3, 8, 8, 13, 11, 9, 3, 8, 8, 3, 3, 13, 2, 5, 0, 4, 8, 13, 8, 5, 4, 3, 9, 0, 8, 3, 14, 2, 14, 0, 2, 3, 8, 8, 2, 14, 5, 11, 0, 14, 8, 4, 6]\n",
            "12 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.03225032612681389 0.228759765625 3.405928611755371 0.43773147463798523 1.2447561025619507 2.1359410285949707\n",
            "repr, std, cov, clossl, z, norm 0.03105231188237667 0.2281494140625 3.409276008605957 0.5922204256057739 1.2769789695739746 1.9330388307571411\n",
            "repr, std, cov, clossl, z, norm 0.029794203117489815 0.22998046875 3.311598300933838 0.35890060663223267 1.2261682748794556 1.920636773109436\n",
            "repr, std, cov, clossl, z, norm 0.02655383199453354 0.2333984375 3.1672844886779785 0.5363924503326416 1.3083597421646118 2.0056028366088867\n",
            "repr, std, cov, clossl, z, norm 0.025296581909060478 0.2291259765625 3.3878302574157715 0.4147712290287018 1.2600289583206177 2.264611005783081\n",
            "repr, std, cov, clossl, z, norm 0.02273322083055973 0.2301025390625 3.329771041870117 0.4610450565814972 1.2969169616699219 2.173818588256836\n",
            "repr, std, cov, clossl, z, norm 0.024185264483094215 0.2333984375 3.1608011722564697 0.46052029728889465 1.268160343170166 2.191113233566284\n",
            "repr, std, cov, clossl, z, norm 0.026441466063261032 0.2301025390625 3.312941074371338 0.41941896080970764 1.3000435829162598 2.1260852813720703\n",
            "repr, std, cov, clossl, z, norm 0.022723941132426262 0.2249755859375 3.5804691314697266 0.4905291199684143 1.2317447662353516 2.0518798828125\n",
            "repr, std, cov, clossl, z, norm 0.024076083675026894 0.2232666015625 3.6644861698150635 0.48506179451942444 1.265976905822754 2.2575573921203613\n",
            "repr, std, cov, clossl, z, norm 0.0208742655813694 0.2293701171875 3.3530988693237305 0.4145384728908539 1.2544831037521362 1.9956964254379272\n",
            "repr, std, cov, clossl, z, norm 0.026194920763373375 0.2291259765625 3.3827171325683594 0.4584912955760956 1.2787845134735107 2.473085641860962\n",
            "train_data.data 20411\n",
            "dided\n",
            "time\n",
            "[4, 6, 3, 3, 5, 5, 14, 0, 0, 8, 3, 11, 3, 8, 11, 5, 8, 5, 0, 6, 0, 14, 8, 8, 14, 3, 6, 8, 4, 3, 14, 8, 8, 3, 14, 8, 14, 11, 5, 3, 3, 3, 8, 4, 3, 3, 3, 0, 6, 8, 13, 3, 2, 8, 8, 3, 9, 8, 5, 2]\n",
            "dided\n",
            "time\n",
            "[8, 5, 2, 3, 4, 8, 5, 14, 5, 9, 3, 3, 8, 5, 4, 8, 8, 4, 2, 5, 13, 2, 8, 14, 11, 2, 11, 0, 13, 9, 3, 3, 8, 4, 3, 5, 5, 3, 8, 4, 2, 14, 8, 9, 14, 2, 3, 8, 0, 2, 0, 8, 8, 8, 9, 4, 11, 3, 8, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 2, 4, 14, 8, 8, 3, 8, 3, 2, 8, 2, 8, 2, 9, 2, 11, 3, 11, 3, 14, 8, 13, 3, 2, 13, 4, 8, 8, 8, 0, 4, 0, 2, 5, 4, 3, 2, 4, 6, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 3, 0, 0, 8, 0, 8, 9, 2, 9, 8, 8, 5, 9, 8, 3, 3, 3, 14, 2, 9, 8, 4, 11, 5, 8, 3, 2, 8, 0, 8, 14, 4, 2, 4, 3, 2, 6, 5, 5, 8, 8, 5, 8, 4, 8, 2, 8]\n",
            "dided\n",
            "time\n",
            "[8, 2, 8, 2, 8, 2, 3, 11, 3, 13, 8, 9, 0, 2, 8, 0, 14, 14, 8, 0, 3, 8, 13, 0, 13, 13, 0, 3, 0, 3, 3, 11, 2, 5, 3, 3, 6, 3, 11, 8, 0, 3, 8, 8, 11, 3, 8, 3, 3, 8, 13, 13, 8, 11, 2, 3, 8, 0, 2, 8, 13, 13, 5, 8, 14, 0, 11, 2, 8, 13, 5, 8, 3, 14, 2, 4, 11, 2, 8, 8, 14, 3, 6, 9, 3, 4, 8, 8, 8, 4, 3, 8, 0, 3, 8, 8, 6, 14, 5, 3, 4, 2, 3, 2, 13, 11, 9, 11, 9, 8, 0, 8, 13, 8, 8, 0, 4, 8, 8, 9, 8, 2, 4, 9, 2, 14, 0, 5]\n",
            "13 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02187514677643776 0.228759765625 3.396859645843506 0.3839642107486725 1.2615669965744019 2.0930469036102295\n",
            "repr, std, cov, clossl, z, norm 0.031162079423666 0.2310791015625 3.272585868835449 0.4363974630832672 1.3027626276016235 1.741248369216919\n",
            "repr, std, cov, clossl, z, norm 0.021702919155359268 0.232666015625 3.219752311706543 0.46014463901519775 1.2653924226760864 2.0654659271240234\n",
            "repr, std, cov, clossl, z, norm 0.03773898631334305 0.226318359375 3.5038247108459473 0.502004086971283 1.2092418670654297 1.7271286249160767\n",
            "repr, std, cov, clossl, z, norm 0.0205391738563776 0.228271484375 3.4037842750549316 0.5209517478942871 1.4299256801605225 1.5948947668075562\n",
            "repr, std, cov, clossl, z, norm 0.03666379675269127 0.2259521484375 3.5287351608276367 0.5154902338981628 1.2781010866165161 2.0456554889678955\n",
            "repr, std, cov, clossl, z, norm 0.020000604912638664 0.2266845703125 3.52177095413208 0.5232344269752502 1.2954944372177124 1.6748274564743042\n",
            "repr, std, cov, clossl, z, norm 0.034434109926223755 0.2275390625 3.4617509841918945 0.45190560817718506 1.2726932764053345 2.144838333129883\n",
            "repr, std, cov, clossl, z, norm 0.02553306519985199 0.2266845703125 3.4875741004943848 0.44280681014060974 1.2670040130615234 2.1852550506591797\n",
            "repr, std, cov, clossl, z, norm 0.036681707948446274 0.2237548828125 3.6485509872436523 0.45783674716949463 1.276994228363037 2.2472198009490967\n",
            "repr, std, cov, clossl, z, norm 0.02001381665468216 0.2274169921875 3.4502451419830322 0.43978986144065857 1.3065952062606812 2.1573169231414795\n",
            "repr, std, cov, clossl, z, norm 0.028085479512810707 0.2269287109375 3.4677700996398926 0.47941991686820984 1.296722650527954 1.7442176342010498\n",
            "train_data.data 21118\n",
            "dided\n",
            "time\n",
            "[14, 0, 5, 3, 11, 0, 8, 8, 4, 8, 8, 5, 8, 9, 3, 2, 3, 8, 14, 8, 13, 6, 14, 3, 8, 14, 3, 8, 8, 0, 2, 8, 11, 3, 5, 4, 0, 2, 3, 3, 3, 14, 0, 8, 14, 4, 2, 0, 4, 5, 4, 8, 8, 6, 8, 6, 3, 9, 6, 8, 11, 4, 8, 3, 8, 4, 8, 8, 14, 2, 14, 3, 8, 4, 3, 4, 3, 0, 3, 2, 11, 4, 8, 3, 14, 11, 0, 4, 2, 0, 3, 2, 4, 3, 4, 2, 0, 3, 3, 2, 2, 11, 4, 2]\n",
            "dided\n",
            "time\n",
            "[11, 4, 2, 3, 3, 8, 4, 3, 3, 5, 9, 14, 13, 3, 3, 0, 8, 3, 14, 4, 9, 8, 8, 14, 3, 3, 3, 9, 8, 2, 4, 0, 8, 8, 5, 3, 5, 8, 4, 5, 11, 8, 4, 8, 5, 8, 2, 2, 13, 8, 5, 14, 14, 3, 9, 5, 11, 4, 13, 3, 4, 8, 3, 9, 5, 8, 8, 3, 5, 8, 8, 13, 8, 4, 4, 3, 0, 4, 8, 3, 0, 3, 3, 8, 8, 11, 0, 4, 3, 8, 8, 0, 11, 4, 13, 14, 5, 5, 8, 4, 11, 5, 3, 8, 3, 3, 11, 8, 8, 8, 8, 8, 3, 11, 14, 5, 4, 8, 4, 13, 0, 13, 8, 8, 5, 14, 2, 2, 6, 8, 2]\n",
            "dided\n",
            "time\n",
            "[8, 2, 8, 8, 8, 3, 3, 4, 3, 14, 8, 9, 11, 11, 5, 8, 8, 13, 8, 11, 4, 3, 9, 3, 8, 4, 8, 13, 3, 8, 0, 4, 3, 9, 8, 13, 3, 14, 3, 13, 0, 8, 9, 3, 8, 13, 3, 4, 0, 5, 4, 5, 0, 8, 8, 5, 3, 8, 8, 9, 9, 5, 0, 8, 8, 3, 0, 11, 9, 2, 14, 4, 13, 14, 8, 8, 14, 3, 9, 3, 3, 3, 3, 2, 8, 8, 4, 2, 14, 5, 0, 2, 8, 0, 4, 8, 8, 8, 5, 3, 8, 3, 9, 0, 5, 8, 8, 11, 14, 8, 13, 8, 8, 2, 4, 0, 8, 3]\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 3, 3, 2, 3, 0, 13, 3, 9, 4, 9, 4, 8, 14, 8, 8, 5, 3, 6, 11, 2, 2, 4, 14, 4, 9, 3, 2, 8, 14, 3, 8, 6, 5, 14, 8, 0, 3, 5, 3, 2, 0, 6, 9, 14, 2, 5, 14, 0, 3, 2, 14, 3, 14, 8, 14, 4, 3, 9, 14, 13, 11, 8, 5, 2, 8, 5, 9, 4, 8, 3, 5, 9, 9, 3, 8, 8, 0, 8, 0, 3, 13, 8, 3, 5, 2, 8, 4, 8, 2, 8, 2, 13, 8, 8, 4, 3, 3, 5, 8, 8, 8, 4, 14, 14, 0, 4, 4, 5, 8, 3, 4, 13, 8, 3, 0, 11, 5, 3, 8, 13, 5]\n",
            "dided\n",
            "time\n",
            "[8, 13, 5, 3, 0, 8, 3, 3, 2, 4, 14, 8, 0, 3, 8, 8, 0, 3, 8, 8, 5, 4]\n",
            "14 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02437947690486908 0.231201171875 3.2809979915618896 0.4301820397377014 1.247361660003662 1.7831627130508423\n",
            "repr, std, cov, clossl, z, norm 0.03531284257769585 0.2294921875 3.354597568511963 0.39313244819641113 1.3660346269607544 1.8947935104370117\n",
            "repr, std, cov, clossl, z, norm 0.020517107099294662 0.2265625 3.499293804168701 0.3787553608417511 1.2747420072555542 1.9595407247543335\n",
            "repr, std, cov, clossl, z, norm 0.02752295508980751 0.2347412109375 3.0961108207702637 0.4459184408187866 1.3064440488815308 1.8070266246795654\n",
            "repr, std, cov, clossl, z, norm 0.02444656565785408 0.23095703125 3.275557279586792 0.4454728364944458 1.2629624605178833 1.8854553699493408\n",
            "repr, std, cov, clossl, z, norm 0.02910182811319828 0.2230224609375 3.6681504249572754 0.46763041615486145 1.2850626707077026 1.6914260387420654\n",
            "repr, std, cov, clossl, z, norm 0.022898567840456963 0.2269287109375 3.488372802734375 0.5159032344818115 1.2373052835464478 2.0106794834136963\n",
            "repr, std, cov, clossl, z, norm 0.036140237003564835 0.2239990234375 3.6511638164520264 0.47886189818382263 1.28842294216156 2.145960807800293\n",
            "repr, std, cov, clossl, z, norm 0.02291843295097351 0.22705078125 3.4806106090545654 0.5037558078765869 1.3356167078018188 2.0981903076171875\n",
            "repr, std, cov, clossl, z, norm 0.02942216955125332 0.2303466796875 3.297726631164551 0.49155572056770325 1.356189250946045 1.813692331314087\n",
            "repr, std, cov, clossl, z, norm 0.023903867229819298 0.230712890625 3.3025388717651367 0.4151118993759155 1.2687759399414062 2.151236057281494\n",
            "repr, std, cov, clossl, z, norm 0.02791311964392662 0.23046875 3.3067235946655273 0.43619030714035034 1.3231369256973267 2.125526189804077\n",
            "train_data.data 20400\n",
            "dided\n",
            "time\n",
            "[4, 2, 8, 3, 11, 8, 11, 4, 4, 8, 9, 8, 8, 6, 3, 5, 9, 14, 11, 8, 3, 8, 6, 14, 3, 2, 8, 11, 8, 9, 11, 5, 8, 8, 9, 7, 2, 3, 9, 3, 0, 3, 13, 11, 8, 3, 3, 8, 11, 9, 11, 8, 3, 2, 8, 0, 5, 8, 9, 0, 4, 4, 8, 14, 3, 14, 0, 9, 11, 11, 5, 0, 0, 0, 8, 14, 14, 4, 8, 3, 8, 3, 8, 4, 6, 4, 3, 0, 9, 14, 2, 3, 8, 2, 8, 3, 8, 8, 0, 9, 8, 0, 8, 9, 8, 14, 4, 8, 3, 8]\n",
            "dided\n",
            "time\n",
            "[8, 0, 8, 8, 3, 3, 2, 8, 2, 2, 8, 0, 4, 9, 9, 3, 14, 14, 0, 11, 5, 9, 2, 11, 2, 5, 3, 3, 13, 9, 5, 4, 8, 8, 8, 3, 13, 11, 3, 3, 4, 8, 3, 2, 3, 4, 8, 14, 11, 4, 11, 14, 11, 14, 14, 14, 0, 0, 8, 8, 2, 11, 3, 8, 14, 1, 5, 6, 0, 4, 0, 8, 5, 4, 0, 8, 8, 3, 4, 5, 3, 4, 14, 4, 3, 3, 2, 3, 2, 3, 3, 8]\n",
            "dided\n",
            "time\n",
            "[3, 3, 8, 4, 4, 8, 3, 9, 5, 6, 3, 11, 8, 14, 2, 8, 9, 14, 14, 9, 4, 13, 3, 8, 3, 8, 9, 8, 3, 11, 3, 3, 8, 0, 4, 11, 4, 14, 8, 8, 4, 3, 3, 14, 2, 2, 6, 3, 8, 8, 14, 2, 3, 8, 0, 4, 5, 0, 9, 0, 8, 14, 11, 14, 11, 14, 11, 3, 3, 2, 8, 5, 3, 0, 8, 3, 0, 3, 8, 2, 11, 0, 11, 8, 14, 2, 4, 13, 14, 8, 9, 4, 2, 8, 14, 5, 3, 8, 3, 3, 3, 8, 8, 8, 8, 6, 2, 4, 8, 5, 13, 8, 0, 3, 8, 0, 11, 3, 0, 5, 13, 8, 8, 3, 0, 3, 3, 5, 2, 5, 9, 8, 0, 8, 8, 11, 8, 3, 0, 6, 3, 0, 8, 2, 13, 3, 8, 0, 8, 3, 3, 3, 8, 4, 14, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 4, 3, 11, 4, 8, 4, 4, 0, 3, 2, 2, 8, 2, 0, 8, 4, 14, 14, 3, 2, 14, 5, 8, 8, 13, 9, 8, 8, 2, 5, 2, 9, 8, 14, 4, 0, 3, 11, 9, 8, 8, 11, 8, 8, 3, 3, 0, 8, 8, 0, 2, 0, 5, 14]\n",
            "dided\n",
            "time\n",
            "[5, 14, 3, 14, 11, 8, 5, 8, 8, 8, 3, 5, 0, 9, 3, 11, 3, 9, 3, 3, 2, 14, 8, 8, 11, 2, 8, 3, 4, 14, 11, 8, 2, 5, 5, 3, 8, 8, 11, 4, 3, 3, 3, 11, 3, 8, 0, 4, 2, 8, 13, 13, 13, 9, 3, 9, 8, 5, 2, 3, 13, 0, 3, 8, 8, 13, 8, 2, 4, 8, 0, 9, 0, 8, 14, 5, 9, 0, 8, 4, 3, 11, 2, 4, 2, 3, 4, 5, 2, 3, 8, 0, 3, 2, 3, 2, 3, 11, 11, 5, 4, 3, 14, 11, 8, 8, 14, 14, 8, 3, 3, 8, 8, 2, 8, 0, 8, 8, 11, 14, 8, 13, 3, 14, 8, 3, 14, 8, 5, 8, 14, 3, 4, 2, 0, 3, 3, 2, 0, 13, 8, 0, 8, 8, 11, 4, 3, 0, 0, 14, 4, 8, 8, 9, 14, 9, 0, 9, 9, 4, 3, 14, 3, 9, 4, 3]\n",
            "15 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.020977236330509186 0.2298583984375 3.3232827186584473 0.36448484659194946 1.301919937133789 1.987878441810608\n",
            "repr, std, cov, clossl, z, norm 0.02747534215450287 0.232421875 3.2218880653381348 0.4181075692176819 1.2439197301864624 2.283031463623047\n",
            "repr, std, cov, clossl, z, norm 0.02402099035680294 0.23046875 3.2869863510131836 0.43465039134025574 1.286439061164856 2.1509432792663574\n",
            "repr, std, cov, clossl, z, norm 0.026652196422219276 0.228759765625 3.385120391845703 0.36790454387664795 1.371635913848877 2.078566789627075\n",
            "repr, std, cov, clossl, z, norm 0.021263260394334793 0.2269287109375 3.4835879802703857 0.48935577273368835 1.3052496910095215 2.1578361988067627\n",
            "repr, std, cov, clossl, z, norm 0.027790173888206482 0.22216796875 3.7202839851379395 0.45131513476371765 1.318672776222229 2.0589406490325928\n",
            "repr, std, cov, clossl, z, norm 0.02371656894683838 0.22705078125 3.4748783111572266 0.367085337638855 1.3010472059249878 1.9193986654281616\n",
            "repr, std, cov, clossl, z, norm 0.027812592685222626 0.2318115234375 3.264193058013916 0.4336718022823334 1.3512810468673706 1.9602322578430176\n",
            "repr, std, cov, clossl, z, norm 0.022719761356711388 0.2313232421875 3.2524633407592773 0.3619276285171509 1.2709378004074097 1.7862149477005005\n",
            "repr, std, cov, clossl, z, norm 0.039839424192905426 0.2308349609375 3.2813141345977783 0.45170801877975464 1.3562549352645874 1.7275642156600952\n",
            "repr, std, cov, clossl, z, norm 0.02965380996465683 0.2230224609375 3.7004494667053223 0.4716455340385437 1.3278156518936157 1.837412714958191\n",
            "repr, std, cov, clossl, z, norm 0.032107360661029816 0.2254638671875 3.5868406295776367 0.47098198533058167 1.3311599493026733 1.8693937063217163\n",
            "train_data.data 20758\n",
            "dided\n",
            "time\n",
            "[3, 6, 8, 3, 0, 8, 8, 8, 4, 9, 5, 8, 9, 3, 4, 3, 8, 8, 0, 11, 3, 8, 0, 3, 14, 0, 14, 3, 3, 3, 8, 8, 8, 8, 4, 4, 8, 2, 2, 8, 8, 9, 11, 14, 3, 8, 8, 4, 6, 13, 11, 14, 4, 14, 3, 4, 0, 8, 8, 2, 4, 4, 9, 8, 8, 4, 8, 14, 0, 0, 8, 8, 8, 14, 14, 6, 14, 11, 3, 0, 0, 6, 3, 3, 2, 14, 8, 3, 8, 3, 8, 8, 3, 8, 4, 3, 8, 4, 14, 14]\n",
            "dided\n",
            "time\n",
            "[4, 14, 14, 11, 3, 3, 14, 5, 3, 5, 0, 8, 9, 3, 4, 9, 0, 11, 3, 9, 14, 6, 8, 3, 8, 8, 8, 8, 5, 4, 2, 14, 2, 9, 8, 8, 14, 8, 3, 13, 11, 8, 4, 8, 8, 9, 14, 8, 8, 11, 3, 8, 14, 8, 3, 0, 0, 3, 0]\n",
            "dided\n",
            "time\n",
            "[3, 0, 4, 9, 4, 2, 13, 14, 13, 8, 4, 4, 8, 14, 5, 8, 8, 8, 8, 2, 8, 3, 14, 14, 8, 8, 8, 2, 0, 5, 2, 8, 11, 8, 3, 3, 11, 14, 3, 8, 8, 4, 2, 9, 8, 8, 4, 2, 8, 6, 0, 4, 3, 14, 8, 5, 5, 3, 3, 5, 8, 2, 8, 14, 8, 2, 3, 5, 3, 8, 8, 8, 8, 13, 4, 8, 0, 3, 3, 5, 4, 8, 9, 2, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 14, 5, 5, 8, 13, 3, 11, 0, 8, 0, 11, 14, 14, 11, 0, 0, 4, 8, 8, 0, 5, 3, 3, 5, 8, 14, 8, 8, 8, 9, 3, 3, 4, 8, 8, 3, 3, 8, 7, 9, 9, 8, 3, 8, 3, 5, 14, 8, 8, 2, 8, 14, 2, 4, 8, 4, 4, 2, 8, 3, 13, 4, 5, 8]\n",
            "dided\n",
            "time\n",
            "[4, 13, 3, 2, 3, 3, 8, 5, 3, 4, 8, 14, 8, 3, 8, 13, 5, 11, 2, 0, 8, 5, 14, 8, 8, 8, 8, 9, 0, 0, 2, 3, 8, 0, 2, 9, 3, 3, 2, 14, 3, 3, 2]\n",
            "16 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.024294691160321236 0.225341796875 3.569840669631958 0.5277284383773804 1.2646580934524536 2.042667865753174\n",
            "repr, std, cov, clossl, z, norm 0.026289023458957672 0.227783203125 3.4497556686401367 0.47850605845451355 1.3417924642562866 1.9674650430679321\n",
            "repr, std, cov, clossl, z, norm 0.022961463779211044 0.2322998046875 3.2270543575286865 0.46260130405426025 1.3597140312194824 1.7836543321609497\n",
            "repr, std, cov, clossl, z, norm 0.030287496745586395 0.233642578125 3.15311861038208 0.39152660965919495 1.3361769914627075 2.006941556930542\n",
            "repr, std, cov, clossl, z, norm 0.024870017543435097 0.2332763671875 3.1824541091918945 0.3868386149406433 1.3773373365402222 1.8010441064834595\n",
            "repr, std, cov, clossl, z, norm 0.0380508117377758 0.228759765625 3.4139461517333984 0.4021066725254059 1.259276270866394 1.4389879703521729\n",
            "repr, std, cov, clossl, z, norm 0.025079570710659027 0.227294921875 3.4711952209472656 0.42464399337768555 1.2834886312484741 1.415871262550354\n",
            "repr, std, cov, clossl, z, norm 0.040034521371126175 0.228271484375 3.4127354621887207 0.3849217891693115 1.351056694984436 1.845922589302063\n",
            "repr, std, cov, clossl, z, norm 0.027253607288002968 0.2281494140625 3.421943187713623 0.45948362350463867 1.3817365169525146 1.4622697830200195\n",
            "repr, std, cov, clossl, z, norm 0.0711519718170166 0.2283935546875 3.4075145721435547 0.423287957906723 1.1947643756866455 1.538454532623291\n",
            "repr, std, cov, clossl, z, norm 0.02456655539572239 0.231201171875 3.265091896057129 0.4486384689807892 1.2921463251113892 2.089559316635132\n",
            "repr, std, cov, clossl, z, norm 0.04561242833733559 0.2281494140625 3.431511163711548 0.491487056016922 1.2590749263763428 1.6870824098587036\n",
            "train_data.data 20906\n",
            "dided\n",
            "time\n",
            "[3, 2, 3, 2, 11, 14, 3, 8, 3, 8, 8, 14, 9, 8, 11, 8, 8, 8, 4, 8, 8, 8, 9, 3, 14, 7, 8, 3, 11, 8, 9, 11, 3, 3, 13, 11, 3, 0, 3, 6, 4, 0, 8, 6, 3, 11, 2, 4, 14, 3, 11, 8, 2, 8, 0, 3, 2, 4, 8, 3, 14, 3, 11, 5, 5, 2, 8, 0]\n",
            "dided\n",
            "time\n",
            "[2, 8, 0, 0, 4, 5, 8, 14, 4, 3, 9, 3, 0, 3, 11, 5, 8, 3, 5, 3, 3, 4, 9, 14, 8, 13, 5, 14, 2]\n",
            "dided\n",
            "time\n",
            "[0, 3, 8, 11, 4, 14, 8, 2, 0, 0, 6, 0, 11, 9, 11, 3, 9, 0, 8, 5, 13, 4, 14, 3, 8, 14, 0, 4, 11, 8, 8, 3, 13, 2, 8, 0, 3, 0, 5, 3, 3, 9, 2, 3, 9, 3, 13, 8, 0, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 8, 3, 7, 2, 8, 8, 13, 3, 3, 8, 8, 8, 0, 3, 3, 14, 8, 0, 7, 8, 14, 9, 8, 5, 11, 6, 2, 3, 3, 0, 8, 13, 8, 8, 8, 8, 8, 4, 6, 3, 8, 14, 8, 8, 2, 3, 8, 3, 11, 3, 14, 8, 4, 8, 0, 9, 8, 13, 9]\n",
            "dided\n",
            "time\n",
            "[9, 6, 8, 8, 8, 3, 8, 4, 8, 2, 0, 8, 14, 8, 3, 3, 8, 3, 14, 8, 4, 8, 2, 3, 8, 3, 0, 8, 14, 8, 2, 8, 3, 8, 14, 14, 5, 6, 8, 8, 3, 8, 0, 8, 3, 8, 5, 0, 4, 11, 8, 9, 8, 5, 8, 13, 8]\n",
            "17 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.023466838523745537 0.22900390625 3.366807222366333 0.515995442867279 1.2225959300994873 1.5330109596252441\n",
            "repr, std, cov, clossl, z, norm 0.04107869043946266 0.234130859375 3.1402230262756348 0.483138769865036 1.2821487188339233 1.973456621170044\n",
            "repr, std, cov, clossl, z, norm 0.023654531687498093 0.2340087890625 3.122159004211426 0.4752632975578308 1.2608734369277954 1.8046022653579712\n",
            "repr, std, cov, clossl, z, norm 0.03086654655635357 0.2305908203125 3.3024635314941406 0.43918848037719727 1.370351791381836 1.6662547588348389\n",
            "repr, std, cov, clossl, z, norm 0.026233749464154243 0.225830078125 3.5465087890625 0.43216514587402344 1.315356731414795 2.0865516662597656\n",
            "repr, std, cov, clossl, z, norm 0.036562371999025345 0.2269287109375 3.4650001525878906 0.5337997674942017 1.2842727899551392 1.789681315422058\n",
            "repr, std, cov, clossl, z, norm 0.02032703533768654 0.2261962890625 3.515479326248169 0.44217243790626526 1.273699164390564 1.4879751205444336\n",
            "repr, std, cov, clossl, z, norm 0.023647738620638847 0.2293701171875 3.374129295349121 0.4228993356227875 1.3182744979858398 1.9930247068405151\n",
            "repr, std, cov, clossl, z, norm 0.01877673901617527 0.228759765625 3.3915042877197266 0.40035468339920044 1.2813011407852173 2.026860237121582\n",
            "repr, std, cov, clossl, z, norm 0.025161854922771454 0.2274169921875 3.455472946166992 0.41651949286460876 1.25357985496521 1.6905580759048462\n",
            "repr, std, cov, clossl, z, norm 0.01693815551698208 0.2269287109375 3.476558208465576 0.40641582012176514 1.292720913887024 1.7340694665908813\n",
            "repr, std, cov, clossl, z, norm 0.021028095856308937 0.2281494140625 3.4382641315460205 0.4072420001029968 1.3802651166915894 1.8028185367584229\n",
            "train_data.data 20019\n",
            "dided\n",
            "time\n",
            "[3, 3, 3, 0, 2, 2, 0, 9, 5, 8, 14, 14, 8, 8, 0, 5, 2, 3, 3, 0, 2, 14, 5, 0, 2, 9]\n",
            "dided\n",
            "time\n",
            "[9, 3, 8, 8, 3, 4, 8, 9, 0, 9, 3, 4, 8, 13, 8, 0, 4, 5, 5, 3, 11, 6, 5, 14, 8, 0, 14, 0, 8, 8, 11, 9, 8, 11, 5, 13, 8, 8, 6, 0, 8, 2, 8, 5, 8, 3, 11, 14, 3, 8, 8, 3, 11, 2, 5, 8, 9, 9, 0, 8, 4, 4, 5, 5, 8, 3, 11, 8, 3, 14, 3, 8, 0, 8, 14, 8, 8, 14, 3, 8, 2, 9, 11, 8, 11, 4, 5, 0, 8, 3, 4, 2, 8, 8, 7, 14, 6, 3, 14, 3, 14, 8, 8, 0, 2, 13, 3, 3, 11, 5, 8, 8, 8]\n",
            "dided\n",
            "time\n",
            "[2, 8, 8, 14, 8, 11, 8, 5, 8, 13, 11, 3, 8, 9, 8, 0, 14, 2, 9, 3, 7, 3, 0, 8, 11, 14, 3, 14, 9, 3, 3, 8, 8, 0, 8, 13, 0, 8, 4, 0, 8, 13, 8, 8, 8, 5, 4, 4, 8, 2, 0, 0, 8, 4, 9, 2, 2, 2, 3, 3, 8, 9, 0, 0, 8, 3, 3, 14, 3, 4, 14, 4, 14, 8, 9, 8, 0, 5, 9, 8, 8, 3, 14, 4, 3, 0, 9, 4, 0, 13, 8, 3, 8, 4, 2, 3, 8, 4, 3, 8, 3, 3, 3, 0, 4, 3, 8, 8, 8, 2, 0, 9, 0, 3, 2, 14, 13, 8, 8, 9, 5, 8, 8, 8, 9, 14, 4, 4, 3, 4, 3, 4]\n",
            "dided\n",
            "time\n",
            "[4, 3, 4, 8, 14, 4, 6, 8, 13, 14, 4, 8, 0, 3, 11, 5, 14, 8, 0, 8, 3, 2, 4, 14, 11, 14, 14, 5, 5, 8, 8, 8, 4, 2, 2, 8, 4, 2, 8, 13, 11, 11, 8, 4, 14, 4, 11, 3, 4, 8, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 5, 0, 8, 0, 4, 8, 0, 11, 2, 2, 3, 8, 3, 8, 5, 5, 5, 8, 8, 3, 4, 0, 2, 3, 5, 8, 3, 8, 3, 5, 8, 3, 5, 2, 9, 13, 4, 8, 6, 8, 8, 5, 0, 6, 8, 11, 3, 8, 3, 4, 2, 9, 5, 5, 8, 8, 11, 8, 14, 6, 3, 0, 0, 3, 8, 8, 8, 8, 8, 8, 0, 2, 9, 0, 13, 3, 14, 8, 0, 2, 3, 8, 4, 0, 8, 2, 9, 8, 0, 8, 14, 8, 6, 3, 9, 8, 8, 8, 6, 8, 8, 2, 8, 7, 5, 0, 3, 13, 5]\n",
            "18 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.017246661707758904 0.2303466796875 3.328307628631592 0.4835886061191559 1.3987395763397217 1.7570000886917114\n",
            "repr, std, cov, clossl, z, norm 0.024950847029685974 0.229248046875 3.3590779304504395 0.40031901001930237 1.292405366897583 1.5847060680389404\n",
            "repr, std, cov, clossl, z, norm 0.02466520667076111 0.2249755859375 3.5738823413848877 0.41676703095436096 1.2633843421936035 1.7478103637695312\n",
            "repr, std, cov, clossl, z, norm 0.028408493846654892 0.2335205078125 3.1347603797912598 0.468270480632782 1.3021049499511719 1.9980974197387695\n",
            "repr, std, cov, clossl, z, norm 0.020517826080322266 0.232666015625 3.1898202896118164 0.42578232288360596 1.3599793910980225 2.020515203475952\n",
            "repr, std, cov, clossl, z, norm 0.027846364304423332 0.2293701171875 3.366260290145874 0.4385825991630554 1.2541158199310303 2.0198841094970703\n",
            "repr, std, cov, clossl, z, norm 0.023566145449876785 0.2283935546875 3.4151759147644043 0.4691988527774811 1.3691104650497437 2.2562737464904785\n",
            "repr, std, cov, clossl, z, norm 0.03453052043914795 0.2264404296875 3.5332231521606445 0.5341334342956543 1.2803033590316772 2.36316180229187\n",
            "repr, std, cov, clossl, z, norm 0.027334587648510933 0.228759765625 3.392683267593384 0.46608951687812805 1.3313984870910645 2.1342358589172363\n",
            "repr, std, cov, clossl, z, norm 0.02851269580423832 0.2340087890625 3.134028434753418 0.4676482677459717 1.2697921991348267 1.9373341798782349\n",
            "repr, std, cov, clossl, z, norm 0.028001166880130768 0.2344970703125 3.1253061294555664 0.4434750974178314 1.3311618566513062 2.017575263977051\n",
            "repr, std, cov, clossl, z, norm 0.03205769881606102 0.22998046875 3.3373899459838867 0.376984566450119 1.349541187286377 2.515272617340088\n",
            "train_data.data 20926\n",
            "dided\n",
            "time\n",
            "[13, 5, 8, 11, 8, 3, 5, 9, 11, 8, 6, 8, 0, 3, 9, 3, 3, 14, 14, 14, 5, 8, 4, 3, 3, 11, 2, 8, 7, 3, 8, 14, 3, 2, 14, 5, 3, 3, 14, 8, 8, 0, 8, 5, 0, 8, 11, 8, 0, 8, 3, 9, 8, 3, 0, 8, 4, 6, 8, 3, 14, 0, 0, 8, 4, 8, 8, 8, 8, 5, 9, 14, 3, 14, 3, 5, 8, 0, 8, 14, 14, 3, 8, 9, 7, 0, 2, 3, 8, 8, 4, 11, 3, 11, 13, 9, 8, 8, 8, 8, 2, 3, 4, 13, 6, 4, 8, 14, 3, 3, 3, 8, 8, 9, 8, 8, 2, 0, 4, 3, 6, 9, 8, 4, 0, 8, 8, 4, 5, 8, 8, 3, 5, 8, 2, 3, 4, 8, 4, 14, 5, 4, 3, 4, 11, 8, 8, 4, 3, 14, 14, 14, 8, 5, 0, 0, 0, 8, 0, 4, 3, 2, 4, 8, 13, 8, 13, 8, 13, 8, 8, 3, 8, 13, 9, 4, 2, 2, 3, 8, 9, 0, 13, 0, 8, 9, 8, 8, 2, 3, 11, 11, 14, 3, 4, 4, 13, 4, 14, 5, 9, 11]\n",
            "dided\n",
            "time\n",
            "[11, 8, 2, 14, 14, 8, 5, 3, 14, 11, 0, 8, 5, 3, 1, 8, 13, 4, 14, 8, 2, 8, 0, 8, 14, 4, 6, 3, 8, 5, 2, 13, 9, 3, 11, 8, 8]\n",
            "dided\n",
            "time\n",
            "[9, 4, 2, 8, 14, 0, 5, 3, 3, 5, 3, 3, 3, 4, 5, 14, 3, 8, 8, 8, 3, 8, 4, 13, 5, 9, 8, 13, 0, 13, 8, 4, 2, 3, 8, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 4, 5, 14, 8, 13, 8, 8, 5, 4, 8, 3, 8, 11, 0, 11, 0, 5, 4, 14, 13, 9, 9, 8, 2, 7, 4, 8, 8, 14, 8, 8, 4, 12, 14, 9, 8, 8, 11, 0, 9, 3, 2, 8, 8, 13, 8, 3, 5, 8, 9, 8, 11, 0, 5, 14, 8, 4, 0, 3, 0, 8, 3, 8, 14, 9, 9, 9, 14, 8, 13, 8, 11, 0, 8, 8, 14, 3, 11, 4, 3, 3, 2, 2, 0, 5, 3, 3, 11, 3, 3, 3, 8, 9, 3, 2, 3, 0, 4, 0, 4, 4, 8, 8, 4, 8, 6, 8, 5, 8, 13, 0, 8, 13]\n",
            "dided\n",
            "time\n",
            "[13, 3, 3, 4, 3, 8, 5, 8, 3, 3, 8, 14, 4, 0, 3, 5, 14, 8, 14, 8, 8, 4, 3, 2, 8, 13, 9, 0, 3, 2, 3, 8, 5, 8, 4, 9, 3, 3, 2, 14, 4, 0, 8, 0, 8, 3, 5, 5, 3, 8, 14, 9, 4, 0, 2, 5, 14, 13, 13, 4, 8, 8, 9, 6, 8]\n",
            "19 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.024933762848377228 0.2255859375 3.565504550933838 0.5004581212997437 1.2164186239242554 2.114684820175171\n",
            "repr, std, cov, clossl, z, norm 0.03034784458577633 0.2276611328125 3.4396204948425293 0.4123971164226532 1.2977079153060913 2.0305938720703125\n",
            "repr, std, cov, clossl, z, norm 0.023089248687028885 0.23388671875 3.1452174186706543 0.47649627923965454 1.2881978750228882 2.094391345977783\n",
            "repr, std, cov, clossl, z, norm 0.030095521360635757 0.2332763671875 3.17612886428833 0.4541093409061432 1.2339366674423218 1.609165072441101\n",
            "repr, std, cov, clossl, z, norm 0.025256173685193062 0.232177734375 3.217586040496826 0.4029996693134308 1.3611725568771362 2.3058602809906006\n",
            "repr, std, cov, clossl, z, norm 0.03661514073610306 0.2271728515625 3.469517707824707 0.47426488995552063 1.3455173969268799 1.9371910095214844\n",
            "repr, std, cov, clossl, z, norm 0.021340884268283844 0.222412109375 3.7425897121429443 0.4768086373806 1.3596042394638062 2.1334705352783203\n",
            "repr, std, cov, clossl, z, norm 0.03191789984703064 0.22705078125 3.4732842445373535 0.3830142617225647 1.2853831052780151 2.1155343055725098\n",
            "repr, std, cov, clossl, z, norm 0.024222053587436676 0.2318115234375 3.2401041984558105 0.42102402448654175 1.3167425394058228 2.068986177444458\n",
            "repr, std, cov, clossl, z, norm 0.0344453863799572 0.2314453125 3.2547049522399902 0.40722358226776123 1.4051547050476074 1.8861316442489624\n",
            "repr, std, cov, clossl, z, norm 0.02276887558400631 0.226318359375 3.5096869468688965 0.3725924789905548 1.3491655588150024 1.9576566219329834\n",
            "repr, std, cov, clossl, z, norm 0.038470882922410965 0.224609375 3.6012871265411377 0.5065472722053528 1.3450250625610352 1.9645743370056152\n",
            "train_data.data 20071\n",
            "dided\n",
            "time\n",
            "[8, 8, 8, 8, 11, 4, 8, 8, 11, 3, 8, 0, 3, 3, 14, 0, 4, 5, 3, 9, 9, 8, 8, 13, 2, 2, 3, 14, 3, 5, 8, 10, 3, 8, 9, 14, 4, 8, 8, 8, 0, 1, 13, 2, 14, 3, 14, 4, 13, 0, 9, 2, 13, 8, 14, 4]\n",
            "dided\n",
            "time\n",
            "[8, 14, 4, 0, 8, 13, 8, 14, 14, 14, 3, 5, 8, 0, 3, 0, 14, 4, 5, 14, 8, 4, 8, 3, 8, 6, 8]\n",
            "dided\n",
            "time\n",
            "[6, 8, 5, 0, 3, 11, 10, 8, 4, 3, 6, 8, 8, 9, 0, 14, 5, 8, 0, 11, 4, 3, 8, 0, 3, 14, 13, 14, 8, 0, 8, 8, 13, 3, 3, 8, 4, 8, 3, 0, 8, 11, 0, 4, 3, 5, 5, 0, 4, 0, 0, 8, 9, 3, 3, 14, 2, 3, 0, 14, 8, 3, 8, 3, 0, 8, 2, 9, 0, 5, 8, 11, 8, 5, 8, 8, 8, 14]\n",
            "dided\n",
            "time\n",
            "[14, 8, 8, 8, 2, 2, 8, 8, 4, 5, 14, 3, 8, 3, 5, 0, 2, 2, 5, 5, 5, 5, 0, 14, 14, 9, 14, 3, 4, 8, 3, 0, 8, 8, 3, 13, 8, 8, 3, 4, 9, 5, 0, 3, 2, 14, 13, 5, 14, 13, 3, 0, 11, 5, 2, 0, 3, 14, 3, 8, 8, 9, 13, 14, 3, 9, 8, 2]\n",
            "dided\n",
            "time\n",
            "[9, 8, 2, 2, 8, 8, 8, 14, 5, 4, 9, 3, 3, 8, 5, 8, 8, 3, 5, 13, 3, 13, 3, 8, 8, 0, 5, 8, 3, 0, 8, 5, 3, 5, 8, 3, 8, 9, 14, 2, 3, 8, 0, 0, 8, 8, 8, 5, 13, 13, 11, 11, 9, 8, 4, 14, 9, 3, 5, 2, 7, 13, 6, 3, 14, 2, 3, 14, 0, 8, 5, 13, 8, 4, 8, 4, 13, 8, 0, 8, 0, 4, 8, 5, 8, 8, 14, 9, 8, 4, 3, 2, 8, 13, 2, 14, 2, 11, 14, 5, 2, 8, 8, 3, 0, 3, 3]\n",
            "20 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02728155627846718 0.2232666015625 3.688267230987549 0.46608516573905945 1.2211158275604248 1.7544885873794556\n",
            "repr, std, cov, clossl, z, norm 0.03384743258357048 0.2333984375 3.173039436340332 0.5269307494163513 1.389276385307312 2.2289750576019287\n",
            "repr, std, cov, clossl, z, norm 0.023262662813067436 0.239990234375 2.8769888877868652 0.5003501772880554 1.3654299974441528 2.407559633255005\n",
            "repr, std, cov, clossl, z, norm 0.037291985005140305 0.229248046875 3.3747520446777344 0.47995778918266296 1.25258207321167 2.4777474403381348\n",
            "repr, std, cov, clossl, z, norm 0.023737534880638123 0.2218017578125 3.7849512100219727 0.41109898686408997 1.3681731224060059 2.4242453575134277\n",
            "repr, std, cov, clossl, z, norm 0.030081788077950478 0.2230224609375 3.6858887672424316 0.4313943088054657 1.3383785486221313 2.052652597427368\n",
            "repr, std, cov, clossl, z, norm 0.022475847974419594 0.22900390625 3.368110418319702 0.4138678312301636 1.3609281778335571 1.8419240713119507\n",
            "repr, std, cov, clossl, z, norm 0.035794246941804886 0.236083984375 3.026798963546753 0.4680648446083069 1.3347749710083008 2.258965015411377\n",
            "repr, std, cov, clossl, z, norm 0.023970140144228935 0.226806640625 3.4996232986450195 0.48758265376091003 1.3172321319580078 2.3088417053222656\n",
            "repr, std, cov, clossl, z, norm 0.03288242593407631 0.223876953125 3.6492974758148193 0.5425607562065125 1.3156743049621582 1.8763271570205688\n",
            "repr, std, cov, clossl, z, norm 0.02571525238454342 0.22265625 3.7068729400634766 0.4410010278224945 1.443307876586914 1.5562124252319336\n",
            "repr, std, cov, clossl, z, norm 0.03074602596461773 0.2261962890625 3.5165281295776367 0.5097915530204773 1.2995706796646118 2.2290518283843994\n",
            "train_data.data 20775\n",
            "dided\n",
            "time\n",
            "[3, 3, 9, 8, 9, 3, 0, 0, 4, 3, 11, 4, 5, 4, 14, 8, 2, 13, 9, 13, 8, 0, 8, 8, 9, 2, 8, 8, 3, 4, 8, 3, 2, 6, 8, 8, 4, 8, 8, 9, 4, 3, 14, 8, 9, 8, 3, 3, 4, 13, 8, 8, 4, 13, 0, 14, 14, 3, 4, 8, 9, 8, 14, 5, 4, 8, 3, 2, 14, 11, 3, 8, 2, 11, 4, 9, 8, 2, 3, 8, 4, 8, 8, 8, 13, 8, 14, 8, 4, 11, 10, 11, 4, 9, 11, 8, 4, 2, 8, 9, 8, 8, 2, 8, 12, 4, 8, 8, 14, 4, 14, 8, 13, 9, 3, 6, 14, 8]\n",
            "dided\n",
            "time\n",
            "[4, 9, 11, 14, 4, 4, 14, 13, 4, 8, 8, 2, 13, 3, 8, 8, 2, 14, 11, 4, 7, 3, 2, 8, 4, 3, 6, 0, 4, 2, 8, 3, 8, 8, 8, 3, 3, 6, 8, 5, 2, 8, 3, 5, 2, 3, 4, 8, 14, 5, 0, 0, 5, 0, 8, 3, 4, 5, 3, 8, 8, 3, 3, 14, 9, 4, 8, 0, 8, 4, 8, 8, 9, 0, 9, 3, 4, 2, 2, 8, 6, 9, 3, 8, 9, 3, 8, 2, 0, 8, 4, 4, 2, 3, 8, 8, 4, 11, 5, 2, 13, 14, 2, 8, 8]\n",
            "dided\n",
            "time\n",
            "[3, 2, 8, 2, 9, 8, 3, 9, 2, 5, 3, 6, 4, 11, 5, 11, 9, 11, 2, 8, 8, 8, 13, 13, 9, 11, 0, 8, 4, 4, 13, 9, 2, 0, 14, 13, 8, 8, 8, 8, 4, 8, 13, 3, 9, 0, 8, 8, 11, 9, 12, 8, 4, 11, 14, 0, 4]\n",
            "dided\n",
            "time\n",
            "[8, 3, 8, 8, 0, 4, 3, 2, 9, 14, 8, 8, 4, 14, 8, 6, 4, 6, 8, 9, 9, 8, 0, 3, 9, 13, 4, 3, 14, 13, 0, 4, 9, 4, 5, 4, 8, 2, 3, 3, 9, 2, 13, 3, 2, 8, 0, 14, 9, 8, 14, 3, 3, 4, 0, 5, 3, 8, 13, 5, 0, 4, 9, 4, 4, 4, 8, 0, 5, 8, 8, 4]\n",
            "dided\n",
            "time\n",
            "[8, 8, 4, 4, 4, 0, 3, 4, 3, 3, 9, 3, 3, 3, 3, 8, 14, 3, 2, 0, 3, 11, 3, 0, 9, 0, 5, 3, 3, 0, 3, 0, 9, 0, 5, 14, 13, 8, 3, 5, 8, 13, 8, 8, 3, 8, 14, 8, 3, 8, 13, 2, 8, 0, 2, 14, 8, 9, 14, 2, 9, 8, 0, 14, 3, 9, 14, 3, 6, 2, 3, 3, 9, 5, 3, 3, 0, 3, 3, 0, 4, 9, 11, 12, 3, 8, 8, 8, 5]\n",
            "21 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.023045295849442482 0.23486328125 3.1099088191986084 0.5126065611839294 1.2998348474502563 1.7309684753417969\n",
            "repr, std, cov, clossl, z, norm 0.025246523320674896 0.23583984375 3.0608558654785156 0.4837666153907776 1.246376872062683 1.669614553451538\n",
            "repr, std, cov, clossl, z, norm 0.02153889089822769 0.237548828125 2.9864211082458496 0.42868679761886597 1.315906047821045 1.3042830228805542\n",
            "repr, std, cov, clossl, z, norm 0.02366502769291401 0.2303466796875 3.321394443511963 0.4394482970237732 1.2784290313720703 2.0489280223846436\n",
            "repr, std, cov, clossl, z, norm 0.022049764171242714 0.2235107421875 3.6534669399261475 0.4309993386268616 1.2101138830184937 1.74065363407135\n",
            "repr, std, cov, clossl, z, norm 0.027891777455806732 0.2252197265625 3.55330753326416 0.40661248564720154 1.3313133716583252 1.6604773998260498\n",
            "repr, std, cov, clossl, z, norm 0.022057494148612022 0.22607421875 3.5475869178771973 0.3654165267944336 1.3471630811691284 1.6217163801193237\n",
            "repr, std, cov, clossl, z, norm 0.03017408773303032 0.227294921875 3.4715704917907715 0.42498278617858887 1.3242844343185425 1.783899188041687\n",
            "repr, std, cov, clossl, z, norm 0.020509976893663406 0.2320556640625 3.245546340942383 0.3810482919216156 1.3411835432052612 1.5959025621414185\n",
            "repr, std, cov, clossl, z, norm 0.026557328179478645 0.23095703125 3.279473066329956 0.43807947635650635 1.2417871952056885 1.6035734415054321\n",
            "repr, std, cov, clossl, z, norm 0.022357039153575897 0.2293701171875 3.3595263957977295 0.4418973922729492 1.2719016075134277 1.8073726892471313\n",
            "repr, std, cov, clossl, z, norm 0.02832426317036152 0.2325439453125 3.2003979682922363 0.5413550138473511 1.3072071075439453 1.7887438535690308\n",
            "train_data.data 21201\n",
            "dided\n",
            "time\n",
            "[4, 8, 11, 0, 3, 3, 2, 0, 4, 8, 8, 8, 3, 6, 4, 3, 8, 4, 5, 13, 5, 13, 3, 5, 4, 8, 2, 8, 13, 2, 0, 0, 0, 14, 2, 7, 8, 4, 14, 4, 8, 3, 8, 3, 14, 0, 8, 9, 0, 13, 4, 5, 0, 13, 9, 2, 14, 8, 3, 14, 11, 14, 3]\n",
            "dided\n",
            "time\n",
            "[14, 3, 6, 2, 3, 2, 8, 5, 4, 3, 8, 13, 8, 4, 8, 2, 0, 14, 14, 2, 3, 11, 8, 3, 3, 4, 4, 3, 13, 4, 8, 8, 8, 0, 3, 9, 3, 14, 2, 8, 3, 5, 14, 13, 14, 8, 3, 8, 0, 3, 3, 4, 8, 8, 3, 3, 11, 2, 2, 2, 8, 3, 2, 8, 0, 6, 0, 0, 3, 0, 8, 3, 3, 8, 8, 8, 0, 3, 0, 8, 9, 14, 8, 3, 3, 5, 3, 8, 5, 8, 8, 5, 0, 8, 4, 11, 14, 8, 8, 14, 3, 14, 5, 3, 0, 11, 5, 5, 5, 11, 5]\n",
            "dided\n",
            "time\n",
            "[11, 5, 13, 8, 0, 8, 2, 8, 0, 8, 5, 8, 14, 8, 3, 3, 8, 13, 3, 14, 0, 14, 4, 14, 8, 8]\n",
            "dided\n",
            "time\n",
            "[8, 0, 3, 3, 14, 11, 3, 14, 8, 0, 8, 3, 8, 4, 0, 5, 14, 14, 0, 3, 14, 8, 4, 14, 0, 8, 8, 2, 8, 8, 8, 0, 8, 0, 8, 3, 8, 13, 3, 5, 3, 8, 5, 0, 8, 14, 0, 3, 2, 4, 4, 11, 3, 14, 8, 13, 3, 8, 13, 8, 8, 8, 0, 8, 8, 0, 8, 5, 8, 0, 3, 8, 3, 3, 8, 0, 0, 8, 3, 0, 13, 14, 6, 8, 3, 4, 5, 4, 8, 0, 8, 9, 8, 8, 1, 14, 8, 3, 8, 8, 14, 4, 5, 2, 0, 8, 14, 8, 5, 2, 3, 2, 8, 6, 8, 14, 3, 4, 0, 0, 5, 13, 11, 2, 8, 8, 3, 13, 14, 9, 11, 4, 8, 8, 5, 0, 2, 3, 8, 5, 8, 8, 8, 3]\n",
            "dided\n",
            "time\n",
            "[8, 8, 3, 4, 5, 0, 0, 8, 2, 4, 3, 5, 3, 3, 8, 3, 3, 8, 8, 3, 13, 1, 3, 4, 8, 14, 9, 3, 11, 0, 2, 5, 13, 4, 3, 8, 8, 4, 9, 3, 3, 14, 1, 5, 0, 0, 9, 9, 5, 3, 13, 0, 14, 13, 2, 4, 8, 8, 13, 8, 4, 8, 3, 3, 8, 8, 4, 8, 3, 6, 3, 5, 3, 3, 8, 5, 8, 14, 9, 8, 8, 8, 3, 3, 0]\n",
            "22 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02372095361351967 0.2337646484375 3.158536911010742 0.4487737715244293 1.3295519351959229 1.9668608903884888\n",
            "repr, std, cov, clossl, z, norm 0.03644609451293945 0.232177734375 3.2391180992126465 0.46034348011016846 1.2835816144943237 1.731926679611206\n",
            "repr, std, cov, clossl, z, norm 0.024783451110124588 0.224609375 3.6262259483337402 0.4048144221305847 1.33124840259552 1.7884809970855713\n",
            "repr, std, cov, clossl, z, norm 0.03362453728914261 0.2265625 3.4930009841918945 0.487104594707489 1.3465477228164673 2.17256760597229\n",
            "repr, std, cov, clossl, z, norm 0.023741120472550392 0.2298583984375 3.342524528503418 0.4357489049434662 1.288879156112671 2.3531131744384766\n",
            "repr, std, cov, clossl, z, norm 0.02626609057188034 0.2247314453125 3.60383939743042 0.4742712676525116 1.3734444379806519 2.109625816345215\n",
            "repr, std, cov, clossl, z, norm 0.022400101646780968 0.2230224609375 3.6712653636932373 0.36849290132522583 1.3203048706054688 2.1400489807128906\n",
            "repr, std, cov, clossl, z, norm 0.024556845426559448 0.2275390625 3.446584463119507 0.401516318321228 1.2459361553192139 1.9040441513061523\n",
            "repr, std, cov, clossl, z, norm 0.021040014922618866 0.2305908203125 3.2880921363830566 0.42639145255088806 1.3925672769546509 2.077164888381958\n",
            "repr, std, cov, clossl, z, norm 0.024976953864097595 0.22998046875 3.315284013748169 0.4119904041290283 1.220847249031067 1.946879267692566\n",
            "repr, std, cov, clossl, z, norm 0.026847243309020996 0.2286376953125 3.389211654663086 0.5536951422691345 1.2730265855789185 2.1911585330963135\n",
            "repr, std, cov, clossl, z, norm 0.02426161803305149 0.2301025390625 3.3182902336120605 0.4066815674304962 1.2859920263290405 2.455853223800659\n",
            "train_data.data 20651\n",
            "dided\n",
            "time\n",
            "[8, 3, 8, 3, 0, 11, 8, 2, 9, 8, 5, 8, 5, 3, 11, 8, 5, 5, 8, 11, 3, 8, 3, 3, 8, 5, 2, 5, 8, 4, 8, 8, 3, 0, 8, 0, 8, 9, 4, 3, 13, 13, 3, 8, 3, 9, 13, 4, 0, 8, 0, 8, 4, 3, 3, 13, 9, 3, 8, 0, 3, 11, 9, 8, 0, 4, 5, 8, 0, 8, 8, 3, 0, 2, 5, 11, 8, 4, 4, 3, 3, 4, 14, 8, 6, 2, 0, 13, 8, 4, 3, 14, 8, 3, 11, 8, 3, 14, 8, 3, 3, 8, 3, 0, 4, 13, 13, 8, 3, 14, 8, 0, 8, 8, 8, 14, 8, 0, 3, 8, 8, 5, 4, 8, 0, 3, 5, 9, 11, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 0, 11, 0, 3, 8, 14, 0, 8, 9, 11, 0, 8, 0, 0, 2, 3, 8, 2, 8]\n",
            "dided\n",
            "time\n",
            "[8, 8, 3, 14, 3, 8, 3, 8, 0, 0, 8, 4, 3, 8, 0, 3, 5, 3, 9, 8, 8, 6, 0, 8, 8, 5, 3, 8, 8, 8, 8, 14, 14, 14, 8, 3, 14, 0, 3, 13, 3, 2, 8, 14, 11, 8, 14, 14, 4, 4, 8, 3, 2, 7, 5, 2, 11, 8]\n",
            "dided\n",
            "time\n",
            "[8, 14, 0, 8, 8, 0, 13, 3, 8, 4, 13, 8, 13, 8, 3, 8, 3, 5, 13, 0, 0, 5, 2, 8, 9, 11, 5, 5, 2, 11, 3, 8, 8, 8, 8, 0, 8, 3, 8, 14, 8, 4, 3, 14, 0, 3, 2, 5, 2, 13, 8, 13, 14, 8, 8, 5, 0, 8, 3, 0, 4, 14, 11, 3, 5, 13, 3, 5, 8, 13, 8, 8, 8, 2, 3, 5, 0, 5, 3, 11, 6, 8, 3, 8, 2, 2, 4, 8, 8, 8, 0, 8, 13, 8, 9]\n",
            "dided\n",
            "time\n",
            "[8, 9, 2, 0, 4, 14, 2, 4, 8, 11, 8, 8, 5, 13, 3, 8, 13, 8, 0, 9, 4, 0, 5, 8, 8, 14, 3, 5, 8, 8, 3, 8, 0, 2, 4, 14, 8, 8, 8, 0, 14, 8, 8, 5, 6, 7, 8, 3, 3, 8, 9, 9, 4, 0, 8, 8, 8, 4, 3, 5, 3, 0, 8, 2, 8, 5, 0, 3, 0, 3, 14, 11, 4, 3, 8, 8, 0, 8, 8, 11, 8, 4, 8, 3, 3, 12, 3, 4, 8, 8, 3, 2, 8, 5, 4, 8, 4, 4, 8, 3, 8, 3, 2, 8, 3, 4, 8, 5, 3, 8, 11, 0, 0, 11, 3, 8, 8, 8, 11, 8, 0, 3, 0, 4, 0, 3, 6, 5, 8, 3, 13, 2, 8, 5, 6, 14, 1, 5, 8, 8, 3, 3, 0, 14, 14, 11, 0, 4, 11, 3]\n",
            "dided\n",
            "time\n",
            "[3, 4, 14, 0, 0, 5, 13, 3, 9, 8, 3, 3, 3, 2, 4, 8, 6, 5, 3, 3, 0, 3, 14, 5, 9, 8, 0, 0, 2, 3, 2, 2, 3, 8, 3, 0, 3, 14, 8, 8, 11, 8, 4, 13, 4, 14, 14, 3, 8, 3, 11, 0, 6, 3, 14, 5, 8, 3, 11, 6, 3, 3, 14, 3, 8, 8, 8, 4, 14, 8, 0, 3, 8, 8]\n",
            "23 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.026243416592478752 0.224365234375 3.6151230335235596 0.402402400970459 1.2675752639770508 1.8843278884887695\n",
            "repr, std, cov, clossl, z, norm 0.025536593049764633 0.23388671875 3.161813259124756 0.4784758388996124 1.3600014448165894 1.85051429271698\n",
            "repr, std, cov, clossl, z, norm 0.02613375149667263 0.232421875 3.2312934398651123 0.419685035943985 1.3177595138549805 1.6833291053771973\n",
            "repr, std, cov, clossl, z, norm 0.024857496842741966 0.2305908203125 3.2942514419555664 0.40796786546707153 1.3456127643585205 2.020928144454956\n",
            "repr, std, cov, clossl, z, norm 0.025885455310344696 0.2288818359375 3.383279323577881 0.40228739380836487 1.2058104276657104 1.4542815685272217\n",
            "repr, std, cov, clossl, z, norm 0.02898908592760563 0.2230224609375 3.688650369644165 0.4211835563182831 1.3423256874084473 1.5003700256347656\n",
            "repr, std, cov, clossl, z, norm 0.028950223699212074 0.2279052734375 3.4316306114196777 0.4584487974643707 1.2955317497253418 1.5271599292755127\n",
            "repr, std, cov, clossl, z, norm 0.03010626509785652 0.22802734375 3.433180332183838 0.5244534611701965 1.2972042560577393 1.2842460870742798\n",
            "repr, std, cov, clossl, z, norm 0.027153365314006805 0.2369384765625 2.9944472312927246 0.4685797691345215 1.4204896688461304 1.2989107370376587\n",
            "repr, std, cov, clossl, z, norm 0.03460448980331421 0.2325439453125 3.1953787803649902 0.5523991584777832 1.3945318460464478 1.8061696290969849\n",
            "repr, std, cov, clossl, z, norm 0.026112746447324753 0.225341796875 3.56815242767334 0.49178484082221985 1.3000528812408447 2.0129032135009766\n",
            "repr, std, cov, clossl, z, norm 0.028097935020923615 0.223388671875 3.6813526153564453 0.46233931183815 1.283265471458435 1.5510749816894531\n",
            "train_data.data 21391\n",
            "dided\n",
            "time\n",
            "[8, 3, 4, 3, 3, 8, 13, 14, 8, 8, 11, 3, 5, 4, 3, 8, 9, 8, 3, 3, 4, 5, 13, 8, 13, 3, 14, 3, 11, 6, 5, 3, 14, 11, 14, 14, 8, 4, 8, 3, 11, 8, 3, 8, 2, 11, 3, 0, 9, 8, 11, 2, 13, 3, 3]\n",
            "dided\n",
            "time\n",
            "[3, 3, 4, 8, 11, 3, 8, 8, 11, 14, 8, 2, 14, 5, 4, 6, 0, 8, 5, 8, 8, 8, 3, 3, 3, 14, 14, 4, 4, 5, 2, 8, 5, 8, 9, 8, 11, 0, 3, 5, 8, 3, 8, 3, 8, 8, 5, 11, 14, 4, 14, 8, 0, 8, 0, 2, 12, 8, 3, 5, 11, 8, 8, 8, 4, 8, 8, 8, 3, 5, 12, 0, 8, 14, 14, 11, 8, 3, 8, 3, 8, 0, 5, 8, 8, 14, 3, 3, 2, 3, 5, 5, 4, 9, 0, 9, 8, 11, 4, 8, 8, 14, 3, 8, 8, 11, 4, 4, 9, 0, 14, 0, 5, 0, 3, 0, 3, 8, 3]\n",
            "dided\n",
            "time\n",
            "[8, 3, 5, 11, 0, 13, 3, 6, 0, 0, 5, 8, 3, 11, 3, 8, 0, 5, 8, 8, 3, 13, 3, 8, 3, 5, 14, 0, 3, 9, 8, 11, 8, 13, 5, 0, 4, 5, 8, 3, 11, 8, 8, 8, 14, 8, 8, 8, 9, 8, 8, 2, 2, 14, 8, 4, 3, 3, 2, 5, 8, 8, 8, 3, 14, 14, 8, 0, 0, 2, 14, 14, 4, 8, 8, 8, 4, 8, 9, 11, 3, 8, 14, 2, 4, 11, 13, 8, 8, 3, 9, 4, 4]\n",
            "dided\n",
            "time\n",
            "[8, 8, 2, 8, 0, 5, 5, 2, 8, 3, 3, 2, 4, 8, 14, 13, 8, 3, 8, 3, 8, 8, 13, 3, 0, 12, 4, 3, 0, 4, 8, 14, 4, 8, 3, 3, 8, 2, 14, 8, 3, 3, 8, 4, 4, 4, 8, 8, 2, 0, 14, 8, 8, 4, 3, 13, 11, 5, 4, 9, 13, 3, 2, 14, 2, 0, 8, 8, 2, 5, 8, 14, 2, 2, 11, 8, 0, 4, 11, 5, 4, 11, 3, 4, 5, 4, 14, 2, 3, 3, 14, 14, 2, 2, 3, 13, 8, 0, 3, 2, 4, 8, 3, 2, 14, 8, 8, 8, 2]\n",
            "dided\n",
            "time\n",
            "[8, 11, 8, 4, 4, 11, 0, 5, 5, 4, 8, 8, 11, 13, 2, 3, 8, 3, 3, 8, 0, 13, 8, 8, 8, 8, 4, 3, 0, 8, 8, 9, 0, 8, 3, 5, 14, 9, 0, 13, 8, 3, 1, 6, 0, 8, 8, 9, 3, 0, 11, 3, 9, 14, 13, 3, 8, 8, 13, 3, 9, 3, 3, 3, 9, 5, 14, 8, 3, 8, 8, 0, 3, 3, 9, 4, 9, 5, 9, 2, 11, 4, 0, 4, 13, 11, 5, 9, 11, 4, 0, 4, 0, 8, 0, 6, 2, 3, 8, 5, 8, 2, 8, 8, 6, 8, 8, 8, 12, 8, 5, 5, 5, 14, 13, 4, 2, 3, 9, 8]\n",
            "24 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.024647904559969902 0.2230224609375 3.7013540267944336 0.41108518838882446 1.2157845497131348 1.618037223815918\n",
            "repr, std, cov, clossl, z, norm 0.028036437928676605 0.228271484375 3.408921241760254 0.4354279935359955 1.3527742624282837 1.803377628326416\n",
            "repr, std, cov, clossl, z, norm 0.025847002863883972 0.23046875 3.316978693008423 0.4124022126197815 1.3277186155319214 1.7546449899673462\n",
            "repr, std, cov, clossl, z, norm 0.025975791737437248 0.2315673828125 3.2379136085510254 0.4409123957157135 1.3312314748764038 1.6378402709960938\n",
            "repr, std, cov, clossl, z, norm 0.0256448183208704 0.232177734375 3.217319965362549 0.4064624011516571 1.2857848405838013 1.8995352983474731\n",
            "repr, std, cov, clossl, z, norm 0.03494228422641754 0.2298583984375 3.341538429260254 0.38790246844291687 1.2743734121322632 2.028505563735962\n",
            "repr, std, cov, clossl, z, norm 0.02635328657925129 0.2230224609375 3.700814962387085 0.4468190670013428 1.3448596000671387 2.2826340198516846\n",
            "repr, std, cov, clossl, z, norm 0.033584173768758774 0.230712890625 3.2909796237945557 0.44645991921424866 1.2903645038604736 1.9476808309555054\n",
            "repr, std, cov, clossl, z, norm 0.03052740916609764 0.23095703125 3.2785284519195557 0.5454992651939392 1.3312662839889526 2.016977071762085\n",
            "repr, std, cov, clossl, z, norm 0.03999946266412735 0.23388671875 3.126436710357666 0.4804595410823822 1.2762075662612915 2.0871541500091553\n",
            "repr, std, cov, clossl, z, norm 0.03518514707684517 0.224853515625 3.591188430786133 0.47744834423065186 1.2834596633911133 2.1123359203338623\n",
            "repr, std, cov, clossl, z, norm 0.046341657638549805 0.226318359375 3.5178070068359375 0.48355501890182495 1.279280424118042 2.2077512741088867\n",
            "train_data.data 20390\n",
            "dided\n",
            "time\n",
            "[3, 9, 8, 5, 0, 4, 9, 8, 3, 3, 2, 8, 8, 3, 8, 8, 0, 3, 4, 8, 3, 14, 2, 5, 0, 0, 8, 14, 2, 6, 3, 9, 14, 8, 0, 9, 6, 8, 3, 5, 8, 11, 3, 2, 14]\n",
            "dided\n",
            "time\n",
            "[0, 4, 14, 4, 9, 8, 8, 14, 13, 3, 14, 9, 2, 1, 8, 8, 8, 8, 14, 13, 3, 4, 8, 3, 8, 14, 11, 3, 13, 11, 0, 8, 8, 5, 8, 3, 2, 0, 3, 3, 0, 8, 0, 8, 5, 8, 8, 0, 5, 11, 4, 14, 5, 2]\n",
            "dided\n",
            "time\n",
            "[2, 3, 11, 9, 8, 5, 4, 4, 4, 8, 0, 0, 0, 12, 2, 10, 3, 8, 5, 8, 8, 14, 13, 8, 8, 2, 8, 4, 0, 13, 0, 4, 4, 3, 14, 4, 8, 2, 8, 4, 8, 14, 8, 5, 3, 4, 3, 8, 2, 8, 8, 4, 2, 0, 3, 8, 8, 14, 14, 8, 8, 8, 0, 0, 11, 9, 4, 14, 2, 3, 3, 8, 3, 8, 2, 7, 8, 8, 3, 2, 8, 14, 8, 8, 13, 7, 14, 8, 0, 8, 8, 8, 4]\n",
            "dided\n",
            "time\n",
            "[3, 3, 8, 5, 4, 13, 5, 14, 3, 5, 8, 3, 5, 11, 8, 8, 4, 14, 9, 9, 0, 13, 8, 3, 13, 4, 0]\n",
            "dided\n",
            "time\n",
            "[4, 0, 3, 2, 13, 0, 6, 3, 6, 9, 8, 14, 0, 3, 5, 2, 0, 3, 14, 8, 8, 8, 9, 8, 3, 0, 2, 8, 8, 5, 14, 6, 8, 3, 8, 3, 0, 9, 2, 8, 8, 14, 9, 3, 14, 3, 3, 8, 8, 11, 0, 4, 8, 8, 8, 4, 9, 3, 8, 8, 5, 3, 5, 0, 4, 8, 8, 0, 11, 3, 5, 4, 5, 8, 2, 3, 0, 8, 8, 9, 14, 13, 8, 3, 8, 13, 8, 8, 3, 4, 8, 8, 9, 4, 0, 14]\n",
            "25 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.03701063245534897 0.231201171875 3.258319616317749 0.47457388043403625 1.3107072114944458 2.1034998893737793\n",
            "repr, std, cov, clossl, z, norm 0.04949948191642761 0.2294921875 3.3582370281219482 0.4508177638053894 1.3784891366958618 2.0436275005340576\n",
            "repr, std, cov, clossl, z, norm 0.03761470317840576 0.2296142578125 3.336409330368042 0.38842567801475525 1.3172645568847656 1.8408739566802979\n",
            "repr, std, cov, clossl, z, norm 0.05767747759819031 0.22802734375 3.418154716491699 0.5718715190887451 1.386889934539795 2.2220218181610107\n",
            "repr, std, cov, clossl, z, norm 0.02805946208536625 0.222900390625 3.7168068885803223 0.4837803244590759 1.3507007360458374 1.5681427717208862\n",
            "repr, std, cov, clossl, z, norm 0.0341489240527153 0.228271484375 3.4089431762695312 0.46100446581840515 1.3156962394714355 1.9124231338500977\n",
            "repr, std, cov, clossl, z, norm 0.022987820208072662 0.2322998046875 3.207927703857422 0.3994975686073303 1.2899467945098877 1.882367730140686\n",
            "repr, std, cov, clossl, z, norm 0.0509551502764225 0.2381591796875 2.966928243637085 0.4171815812587738 1.3347669839859009 1.669110894203186\n",
            "repr, std, cov, clossl, z, norm 0.020398693159222603 0.236328125 3.044895887374878 0.4126386344432831 1.2615079879760742 1.7760692834854126\n",
            "repr, std, cov, clossl, z, norm 0.02934098243713379 0.2354736328125 3.090085506439209 0.4841810166835785 1.2994786500930786 1.7976096868515015\n",
            "repr, std, cov, clossl, z, norm 0.027197277173399925 0.2294921875 3.36256742477417 0.47042885422706604 1.3279019594192505 1.7772778272628784\n",
            "repr, std, cov, clossl, z, norm 0.03984888643026352 0.2215576171875 3.7678065299987793 0.47373566031455994 1.3473737239837646 1.7732572555541992\n",
            "train_data.data 20494\n",
            "dided\n",
            "time\n",
            "[4, 0, 14, 8, 14, 8, 3, 9, 8, 2, 4, 11, 8, 0, 0, 8, 3, 8, 2, 14, 8, 8, 14, 14, 4, 8, 14, 2, 13, 8, 8, 3, 3, 3, 9, 4, 5, 5, 8, 8, 8, 0, 3, 14, 8, 8, 3, 9, 2, 2, 3, 8, 3, 8, 12, 13, 8, 5, 2, 4, 8, 14, 4, 5, 5, 0, 8, 2, 0, 0, 4, 0, 13, 5, 8]\n",
            "dided\n",
            "time\n",
            "[5, 8, 8, 8, 0, 4, 2, 8, 3, 9, 8, 5, 0, 3, 8, 3, 4, 11, 8, 2, 0, 8, 8, 2, 4, 6, 4, 3, 11, 4, 14, 0, 2, 11, 3, 3, 8, 0, 4, 3, 8, 8, 9, 8]\n",
            "dided\n",
            "time\n",
            "[8, 9, 8, 8, 5, 3, 13, 0, 3, 0, 8, 8, 5, 14, 2, 0, 2, 11, 2, 8, 8, 8, 3, 8, 13, 11, 8, 5, 8, 4, 2, 5, 4, 4, 8, 11, 3, 9, 8, 2, 0, 8, 8, 8, 8, 13, 8, 0, 3, 0, 3, 0, 8, 3, 0, 5, 8, 3, 13, 8, 11, 4, 8, 13, 4, 0, 5, 13, 5, 3, 3, 0, 4, 4, 11, 14, 11, 5, 13, 0]\n",
            "dided\n",
            "time\n",
            "[5, 13, 0, 8, 8, 3, 8, 14, 4, 4, 8, 2, 3, 3, 5, 12, 14, 4, 13, 3, 13, 5, 2, 0, 6, 3, 13, 8, 3, 8, 14, 8, 8, 0, 2, 11, 0, 11, 3, 0, 3, 3, 14, 3, 14, 8, 5, 0, 3, 3, 5, 3, 4, 5, 14, 5, 4, 3, 8, 2, 3, 11, 9, 3, 0, 9, 3, 3, 8, 0, 3, 9, 8, 13, 5, 3, 11, 0, 3, 3, 14, 5, 3, 0, 8, 8, 3, 0, 13, 3, 0, 3, 11]\n",
            "dided\n",
            "time\n",
            "[8, 8, 8, 14, 8, 9, 13, 3, 8, 8, 3, 5, 5, 6, 9, 3, 14, 3, 3, 5, 13, 3, 2, 2, 0, 4, 5, 14, 8, 0, 11, 8, 2, 4, 8, 8, 3, 3, 9, 3, 14, 5, 13, 8, 5, 3, 4, 4, 11, 5, 3, 8, 3, 0, 14, 9, 14, 9, 5, 13, 3, 0, 8, 0, 8, 13, 8, 3, 3, 0, 5, 11, 11, 0, 0, 4, 7, 8, 6, 8, 5, 14, 8, 5, 2, 4, 3, 9, 8, 3, 8, 8, 8, 0, 0, 3, 8, 2, 5, 8, 3, 6, 9, 0, 8, 3, 9, 2, 3, 2, 8, 3, 11, 8, 3, 8, 8, 9, 3, 0, 9, 4, 8, 13, 8, 11, 8, 8, 8, 9, 0, 6, 6, 2, 3, 13, 8, 8, 4, 11, 3, 4, 2, 8, 0, 14, 9, 8, 2, 8, 3, 9, 0, 4, 6, 3, 3, 6, 8, 8, 3, 8, 8, 14, 0, 2, 8, 14, 8, 3, 8, 8, 4, 8, 8, 4, 7, 14, 3, 8, 4, 8, 8, 6, 3, 0]\n",
            "26 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.02379721589386463 0.2225341796875 3.688401222229004 0.5267829895019531 1.3613427877426147 1.7369550466537476\n",
            "repr, std, cov, clossl, z, norm 0.026794495061039925 0.229736328125 3.3504395484924316 0.4616785943508148 1.3183425664901733 1.6978027820587158\n",
            "repr, std, cov, clossl, z, norm 0.022178998216986656 0.227294921875 3.4558608531951904 0.5152245759963989 1.2883515357971191 1.7255586385726929\n",
            "repr, std, cov, clossl, z, norm 0.02344086579978466 0.229248046875 3.360377073287964 0.5211493372917175 1.279618263244629 1.6993705034255981\n",
            "repr, std, cov, clossl, z, norm 0.019063856452703476 0.22705078125 3.4887828826904297 0.5355604887008667 1.2219146490097046 1.623456597328186\n",
            "repr, std, cov, clossl, z, norm 0.021260101348161697 0.22607421875 3.546919822692871 0.4451504051685333 1.3063459396362305 1.6334339380264282\n",
            "repr, std, cov, clossl, z, norm 0.017973577603697777 0.2344970703125 3.1132800579071045 0.42259863018989563 1.3108819723129272 1.574385404586792\n",
            "repr, std, cov, clossl, z, norm 0.018595919013023376 0.23193359375 3.2337851524353027 0.4191119968891144 1.3803056478500366 1.5161817073822021\n",
            "repr, std, cov, clossl, z, norm 0.01769280433654785 0.2332763671875 3.178919792175293 0.4383815824985504 1.3683942556381226 1.5456457138061523\n",
            "repr, std, cov, clossl, z, norm 0.019029326736927032 0.230224609375 3.319291830062866 0.3825113773345947 1.3334290981292725 1.564587116241455\n",
            "repr, std, cov, clossl, z, norm 0.017018292099237442 0.2265625 3.4934940338134766 0.45227551460266113 1.2359315156936646 2.0317182540893555\n",
            "repr, std, cov, clossl, z, norm 0.020038437098264694 0.2255859375 3.5531718730926514 0.4943931996822357 1.3553874492645264 1.5574604272842407\n",
            "train_data.data 20649\n",
            "dided\n",
            "time\n",
            "[0, 11, 13, 8, 4, 8, 2, 8, 3, 0, 3, 0, 8, 11, 0, 8, 3, 2, 8, 4, 11, 14, 14, 3, 3, 0, 8, 3, 8, 5, 11, 9, 3, 3, 2, 8, 9, 8, 8, 0, 14, 8, 3, 3, 8, 9, 3, 8, 8, 2, 8, 9, 3, 8, 3, 0, 14, 14, 14, 3, 8, 8, 8, 5, 8, 3, 8, 8, 3, 14, 8, 9, 8, 11, 8, 0, 3, 5, 11, 3, 4, 3, 8, 9, 0, 8, 8, 3, 0, 11, 11, 4, 8, 13, 13, 8, 8, 8, 0, 3, 0, 8, 3, 8, 8, 0, 4]\n",
            "dided\n",
            "time\n",
            "[0, 4, 13, 8, 13, 9, 8, 9, 6, 8, 8, 8, 5, 5, 8, 8, 8, 14, 8, 3, 0, 5, 8, 14, 14, 5, 3, 8, 11, 8, 0, 3, 9, 8, 9, 4, 0, 14, 3, 8, 4, 3, 5, 6, 3, 3, 0, 14, 4, 4, 11, 2, 4, 4, 9, 10, 3, 3, 8, 4, 4, 9, 0, 2, 9, 8, 2, 13]\n",
            "dided\n",
            "time\n",
            "[8, 2, 13, 3, 2, 14, 13, 11, 9, 4, 8, 8, 4, 9, 9, 4, 12, 4, 12, 6, 9, 0, 14, 11, 9, 6, 14, 8, 4, 6, 6, 14, 13, 14, 0, 8, 4, 4, 8, 11, 9, 8, 6, 8, 5, 8, 8, 3, 14, 8, 8, 8, 3, 8, 8, 8, 9, 8, 4, 0, 9, 4, 5, 4, 4, 14, 3, 11, 3, 5, 0, 8, 5, 8, 2, 3, 3, 3, 13, 8, 8, 3, 2, 2, 1, 3, 4, 14, 8, 5, 4, 8, 4, 4, 8, 8, 8, 8, 8, 3, 9, 4, 8, 9, 5, 11, 8, 3, 4, 14, 3, 4]\n",
            "dided\n",
            "time\n",
            "[9, 3, 4, 3, 9, 9, 4, 8, 9, 8, 5, 8, 4, 8, 14, 11, 5, 8, 4, 8, 3, 11, 8, 8, 4, 8, 4, 11, 3, 3, 8, 5, 9, 8, 6, 13, 8, 0, 3, 13, 3, 14, 8, 9, 14, 2, 8, 11, 2, 8, 3, 3, 9, 4, 8, 14, 9, 6, 8, 2, 8, 7, 4, 11, 6, 14, 14, 3, 3, 8, 4, 14, 9, 14, 0, 3, 4, 5, 9, 8, 4, 8, 5, 13, 8, 9, 4, 3, 6, 5, 2, 8, 8, 8, 9, 0, 4, 11, 14, 2, 14, 8]\n",
            "dided\n",
            "time\n",
            "[8, 4, 11, 5, 0, 8, 2, 0, 0, 5, 5, 3, 9, 4, 13, 5, 9, 0, 14, 13, 9, 13, 3, 4, 9, 9, 0, 8, 3, 8, 5, 2, 3, 14, 3, 5, 9, 6, 0, 2, 5, 5, 9, 4, 9, 3, 9, 3, 4, 8, 9, 4, 9, 8, 11, 2, 8, 8, 14, 5, 13, 4, 3, 14, 9, 14, 2, 4, 8, 3, 6, 11, 8, 4, 3, 5, 5, 11, 3, 0, 4, 11, 4, 11, 13, 3, 3, 4, 9, 11, 3, 0, 9, 3, 8, 6, 8, 3, 14, 8, 8, 4, 4, 3, 4, 4, 3, 8, 3, 5, 8, 8, 8, 3, 9, 3, 8]\n",
            "27 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.01841879077255726 0.2296142578125 3.360733985900879 0.4649631679058075 1.3014483451843262 1.4700942039489746\n",
            "repr, std, cov, clossl, z, norm 0.023475833237171173 0.2279052734375 3.4356181621551514 0.45710527896881104 1.3181370496749878 1.3696155548095703\n",
            "repr, std, cov, clossl, z, norm 0.018583066761493683 0.2276611328125 3.4823856353759766 0.4592522978782654 1.4001048803329468 1.8748183250427246\n",
            "repr, std, cov, clossl, z, norm 0.02406950853765011 0.2279052734375 3.434276580810547 0.4483586549758911 1.3131276369094849 1.8840276002883911\n",
            "repr, std, cov, clossl, z, norm 0.020343512296676636 0.232421875 3.218306541442871 0.42588332295417786 1.2464425563812256 1.478369951248169\n",
            "repr, std, cov, clossl, z, norm 0.02409624680876732 0.2315673828125 3.255371570587158 0.4492309093475342 1.286125898361206 1.8212605714797974\n",
            "repr, std, cov, clossl, z, norm 0.020694930106401443 0.22705078125 3.4794044494628906 0.5341665744781494 1.2191118001937866 1.6515188217163086\n",
            "repr, std, cov, clossl, z, norm 0.024300817400217056 0.22509765625 3.5793356895446777 0.5911680459976196 1.3896896839141846 2.07415771484375\n",
            "repr, std, cov, clossl, z, norm 0.02170434407889843 0.225341796875 3.569383144378662 0.503913938999176 1.272733211517334 1.9937058687210083\n",
            "repr, std, cov, clossl, z, norm 0.027342259883880615 0.2275390625 3.4800519943237305 0.43107810616493225 1.4307961463928223 1.9183398485183716\n",
            "repr, std, cov, clossl, z, norm 0.021681467071175575 0.236572265625 3.043321132659912 0.41569647192955017 1.3643990755081177 1.7460455894470215\n",
            "repr, std, cov, clossl, z, norm 0.027275847271084785 0.2279052734375 3.4508190155029297 0.46312201023101807 1.2852996587753296 1.9193638563156128\n",
            "train_data.data 21111\n",
            "dided\n",
            "time\n",
            "[3, 9, 5, 0, 5, 3, 14, 8, 13, 5, 8, 11, 5, 8, 8, 3, 5, 0, 3, 8, 5, 8, 11, 3, 3, 3, 8, 11, 14, 5, 3, 13, 8, 8, 5, 3, 14, 9, 5, 4, 3, 2, 3, 8, 5, 5, 0, 2, 13, 8, 4, 3, 3, 5, 2, 3, 14, 3, 3, 3, 14, 14, 3, 8, 5, 3, 8, 8, 5, 3, 8, 14, 14, 3, 8, 3, 8, 8, 14, 5, 5, 8, 0, 4, 4, 5, 5, 5, 2]\n",
            "dided\n",
            "time\n",
            "[3, 0, 14, 8, 8, 2, 2, 4, 6, 0, 0, 5, 2, 8, 8, 3, 9, 8, 8, 6, 8, 14, 8, 4, 8, 8, 8, 8, 3, 3, 11, 2, 11, 6, 8, 2, 8, 8, 8, 8, 2, 14, 13, 5, 2, 3, 3, 2, 14, 4, 8, 14, 14, 5, 8, 8, 4, 0, 2, 3, 0, 3, 3, 8, 3, 0, 2, 8, 3, 8, 4, 13, 8, 2, 5, 14, 2, 13, 5, 8, 0, 4, 0, 0, 8, 3, 8, 4, 2, 0, 8, 8, 8, 8, 14, 5, 3, 3, 8, 3, 3, 14, 3, 14, 4, 3, 5, 3, 13, 3, 8, 3, 5, 3, 14, 3, 13, 14, 13, 10, 8, 3, 3, 3, 11, 5, 11, 14, 0, 11, 8, 8, 4, 4, 2, 2, 8, 14, 3, 4, 0, 5, 11, 3, 2, 13, 8, 11, 11, 9, 3, 8, 13, 13, 0, 5, 3, 5, 4, 8, 13, 13, 3, 11, 13, 11, 0, 8, 5, 3, 4, 0, 8, 8, 14, 3, 5, 11, 3, 9, 11, 0, 0, 3, 14, 11, 8, 0, 14, 8, 8, 5, 8, 8, 2, 4, 4, 3, 8, 8, 9, 8, 2, 8, 13, 8]\n",
            "dided\n",
            "time\n",
            "[8, 5, 8, 5, 3, 3, 3, 8, 3, 2, 9, 14, 14, 3, 3, 8, 0, 14, 3, 14, 8, 11, 4, 4, 13, 5, 8, 8, 8, 14, 5, 14, 4, 8, 3, 3, 11, 5, 13, 5, 9, 3, 8, 3, 3, 3, 0, 0, 0, 8, 4, 3, 2, 3, 2, 8, 5, 4, 0, 0, 3, 3, 14, 5, 8, 14, 8, 5, 8]\n",
            "dided\n",
            "time\n",
            "[5, 3, 3, 8, 3, 3, 4, 0, 3, 2, 14, 3, 8, 8, 3, 8, 8, 8, 13, 3, 5, 4, 13, 8, 6, 8, 14, 8, 6, 9, 14, 14, 5, 3, 14, 8, 13, 3, 11, 6, 8, 5, 8, 14, 0, 8, 13, 8, 0, 13, 8, 11, 0, 8, 2]\n",
            "dided\n",
            "time\n",
            "[8, 2, 14, 3, 4, 11, 8, 8, 8, 4, 2, 7, 5, 8, 13, 8, 13, 0, 8, 9, 14, 6, 14, 8, 2, 8, 0, 11, 13, 8, 0, 3, 8, 14, 3, 8, 11, 0, 3, 11, 14, 0, 5, 2, 0, 2, 11, 11, 8, 8, 5, 8, 8, 8, 4, 8, 2]\n",
            "28 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.020987696945667267 0.2266845703125 3.508538007736206 0.4265114963054657 1.3396061658859253 2.3164641857147217\n",
            "repr, std, cov, clossl, z, norm 0.024953018873929977 0.23193359375 3.237173318862915 0.4317145347595215 1.3731818199157715 2.1052756309509277\n",
            "repr, std, cov, clossl, z, norm 0.02079167775809765 0.2288818359375 3.3971800804138184 0.4212593138217926 1.26012122631073 1.9103931188583374\n",
            "repr, std, cov, clossl, z, norm 0.022995855659246445 0.2310791015625 3.2669320106506348 0.4050820767879486 1.3431206941604614 1.8580576181411743\n",
            "repr, std, cov, clossl, z, norm 0.02038145810365677 0.2294921875 3.3403725624084473 0.48781904578208923 1.2739883661270142 1.9503915309906006\n",
            "repr, std, cov, clossl, z, norm 0.024179605767130852 0.22607421875 3.5315918922424316 0.44509801268577576 1.4157648086547852 2.0605506896972656\n",
            "repr, std, cov, clossl, z, norm 0.021964630112051964 0.2220458984375 3.761442184448242 0.4633539915084839 1.3328526020050049 1.4658037424087524\n",
            "repr, std, cov, clossl, z, norm 0.027426524087786674 0.2264404296875 3.537684202194214 0.4313427209854126 1.3060301542282104 1.8141334056854248\n",
            "repr, std, cov, clossl, z, norm 0.024817006662487984 0.2303466796875 3.3631296157836914 0.4349774122238159 1.249772548675537 1.662079095840454\n",
            "repr, std, cov, clossl, z, norm 0.026666175574064255 0.2314453125 3.2828564643859863 0.470552533864975 1.2648862600326538 1.4886819124221802\n",
            "repr, std, cov, clossl, z, norm 0.020515108481049538 0.2342529296875 3.1390364170074463 0.4788002371788025 1.3686423301696777 1.3659600019454956\n",
            "repr, std, cov, clossl, z, norm 0.02667139284312725 0.2291259765625 3.384145498275757 0.38581734895706177 1.3348289728164673 1.5239684581756592\n",
            "train_data.data 21323\n",
            "dided\n",
            "time\n",
            "[5, 8, 8, 6, 11, 3, 8, 2, 4, 0, 13, 8, 3, 3, 3, 13, 11, 8, 5, 2, 2, 14, 14, 8, 9, 3, 14, 14, 3, 14, 9, 11, 2]\n",
            "dided\n",
            "time\n",
            "[8, 2, 14, 5, 8, 0, 8, 8, 8, 3, 4, 8, 5, 2, 2, 0, 3, 8, 3, 8, 8, 3, 8, 8]\n",
            "dided\n",
            "time\n",
            "[3, 8, 8, 0, 8, 9, 3, 8, 8, 11, 13, 8, 9, 11, 11, 13, 3, 11, 3, 3, 13, 2, 8, 4, 2, 0, 5, 2, 4, 5, 2, 8]\n",
            "dided\n",
            "time\n",
            "[5, 2, 8, 3, 6, 13, 4, 13, 3, 8, 4, 8, 14, 13, 8, 8, 8, 8, 4, 14, 4, 4, 13, 0, 8, 8, 8, 8, 2, 14, 8, 14, 3, 9, 3, 14, 3, 9, 3, 8, 8, 4, 9, 8, 2, 3, 4, 14, 2, 14, 2, 8, 11, 8, 1, 4, 13, 5, 9, 4, 0, 4, 2, 9]\n",
            "dided\n",
            "time\n",
            "[4, 2, 9, 11, 14, 10, 8, 14, 14, 0, 3, 3, 4, 8, 3, 8, 8, 4, 2, 8, 5, 8, 3, 2, 2, 8, 8, 3, 13, 8, 14, 5, 3, 8, 3, 2, 9, 8, 4, 8, 3, 13, 8, 13, 3, 0, 0, 8, 8, 2, 5, 14, 11, 8, 5, 11, 14, 8, 3, 9, 3, 2, 8, 8, 2, 2, 8, 3, 3, 8, 6, 2, 8, 5, 8, 5, 14, 5, 5, 11, 8, 3, 9, 3, 13, 8, 4, 14, 3, 3, 3, 0, 3, 8, 0, 8, 2, 13, 8, 3]\n",
            "29 #### train ####\n",
            "repr, std, cov, clossl, z, norm 0.021770944818854332 0.2264404296875 3.5072193145751953 0.43344029784202576 1.310856819152832 1.369931697845459\n",
            "repr, std, cov, clossl, z, norm 0.027471289038658142 0.224609375 3.612905502319336 0.5068073272705078 1.1580207347869873 1.5093032121658325\n",
            "repr, std, cov, clossl, z, norm 0.0220493171364069 0.2332763671875 3.204843044281006 0.4027479588985443 1.2936636209487915 1.6034127473831177\n",
            "repr, std, cov, clossl, z, norm 0.025327539071440697 0.2340087890625 3.1614251136779785 0.40424466133117676 1.3781753778457642 1.3302680253982544\n",
            "repr, std, cov, clossl, z, norm 0.021977560594677925 0.2335205078125 3.155837059020996 0.4709432125091553 1.3158124685287476 1.625071406364441\n",
            "repr, std, cov, clossl, z, norm 0.023118607699871063 0.2313232421875 3.269291400909424 0.42773446440696716 1.256413459777832 1.2356858253479004\n",
            "repr, std, cov, clossl, z, norm 0.019976038485765457 0.2294921875 3.3560123443603516 0.4385949671268463 1.3552229404449463 1.44846510887146\n",
            "repr, std, cov, clossl, z, norm 0.03140540048480034 0.2261962890625 3.5214619636535645 0.4365968406200409 1.4420056343078613 1.183523416519165\n",
            "repr, std, cov, clossl, z, norm 0.02232544869184494 0.224853515625 3.592114210128784 0.44299647212028503 1.4148402214050293 1.4849271774291992\n",
            "repr, std, cov, clossl, z, norm 0.03191094845533371 0.224853515625 3.575329303741455 0.4512440264225006 1.342373251914978 1.5122851133346558\n",
            "repr, std, cov, clossl, z, norm 0.020772689953446388 0.22998046875 3.316129207611084 0.4315887987613678 1.3275035619735718 1.5180906057357788\n",
            "repr, std, cov, clossl, z, norm 0.031139571219682693 0.2271728515625 3.461928367614746 0.5656669735908508 1.3641165494918823 1.483857274055481\n",
            "train_data.data 20384\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(30):\n",
        "    # print(\"#### simulate ####\")\n",
        "    # buffer_=[]\n",
        "    for _ in range(5):\n",
        "        buffer = simulate(agent, buffer)\n",
        "        # buffer_ = simulate(agent, buffer_)\n",
        "\n",
        "    train_data = BufferDataset(buffer, seq_len)\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    agent.train_jepa(train_loader, optim)\n",
        "\n",
        "    # checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "    # torch.save(checkpoint, folder+'agentoptim1.pkl')\n",
        "\n",
        "    # buffer = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "    # with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "    # agentsd, _ = rename_sd(agent.state_dict())\n",
        "    # all_sd = store_sd(all_sd, agentsd)\n",
        "    # torch.save(all_sd, folder+'all_sd.pkl')\n",
        "\n",
        "    print(\"train_data.data\",len(train_data.data))\n",
        "    while len(train_data.data)>20000: # 10000:6.9gb, 20000:5.5gb\n",
        "        buffer.pop(random.randrange(len(buffer)))\n",
        "        train_data = BufferDataset(buffer, seq_len)\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8zxYU9jpE8K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "e580aeef-d488-4777-baf1-d32bd19e1d41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video width=400 controls autoplay><source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"></video>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "##save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "023eacc5-020d-411a-b37b-e03e0c584eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240925_131251-egd22e59</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/egd22e59' target=\"_blank\">grateful-disco-68</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/egd22e59' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/egd22e59</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(project=\"procgen\",\n",
        "    config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RCD647ZpPrGf"
      },
      "outputs": [],
      "source": [
        "# @title agent save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xLh80kPvEzwX"
      },
      "outputs": [],
      "source": [
        "# @title agent pixel save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.emb = torch.nn.Embedding(15, dim_a) # env.action_space # 15\n",
        "        self.deconv = Deconv(d_model)\n",
        "        self.jepa.sim_coeff=2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 20.0 # 1.0 # ν cov Covariance\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        for i in range(20): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "\n",
        "            dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "\n",
        "            # loss, sx_ = self.rnn_pred(sx_, x)\n",
        "            loss, sx_ = self.rnn_pred(sx_, x_)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.5*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # loss=torch.tensor(0, dtype=torch.float)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # # # ae loss\n",
        "                    # state_ = self.deconv(sy.detach()) # not self.deconv(sy)\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "\n",
        "                    # cost loss\n",
        "                    # reward_ = self.tcost(sy).squeeze(-1) # [batch_size]\n",
        "                    # clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb #+ clossl\n",
        "\n",
        "                    loss = loss + jloss + closs #+ conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    # print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sy_ = sy_.detach()\n",
        "                    loss=0\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29O1eyvhnRSD",
        "outputId": "c470e601-d5da-4b94-bb69-928dd9d823af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-77-bbc83a6aed37>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent combine\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        optim_z = torch.optim.SGD([z], lr=1e2, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.95)) #\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.data)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i,x.data, z.squeeze(), loss.item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        # print(\"search\",loss.item())\n",
        "        return lact#, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        _,T,_ = sx.shape\n",
        "        batch = 1\n",
        "        lr = 1e-1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                x[:,:self.lx.shape[1]] = self.lx[:,:T]\n",
        "                z[:,:self.lz.shape[1]] = self.lz[:,:T]\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            # print(i,x.data, z.squeeze(), loss.item())\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        return lact, lh0, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, gamma=0.9): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sx = self.jepa.pred(sxaz)\n",
        "                sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        # if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(syaz)\n",
        "                    sy_ = sy_ + self.jepa.pred(syaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy_).squeeze(-1) # [batch_size]\n",
        "                    clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    _, st = self.get(st, world_state=world_zero)\n",
        "                    # print(\"stt\",st.shape)\n",
        "                    stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb + clossl\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mY7BITKjSKC",
        "outputId": "cbad0ffc-e8e9-4a4d-bd3c-d6d9ddaf6e21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2336301\n",
            "1278976\n",
            "399360\n",
            "1024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-76-e20d23bca149>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=3, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v, drop=0.2)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=10. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=50. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=1. # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # e = d_model**-0.5\n",
        "        # self.h0 = torch.empty((self.jepa.pred.num_layers, 1, d_model), device=device).uniform_(-e, e) # [num_layers, batch, d_model]\n",
        "        # self.h0 = torch.normal(mean=0, std=e, size=(self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # torch.nn.init.xavier_uniform_(self.h0) # xavier_uniform_, kaiming_normal_\n",
        "\n",
        "        # self.lx, self.lz = torch.empty(1,0,dim_a), torch.empty(1,0,dim_z)\n",
        "        self.lx, self.lz = None, None\n",
        "        state = torch.zeros((1, 3,64,64), device=device)\n",
        "        self.sx = self.jepa.enc(state)\n",
        "\n",
        "    # def forward(self, state, k=1): # live run in env # np (64, 64, 3)\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        self.update_h0(lstate, laction)\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "            # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(sx, T=6, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.h0=lh0[k].unsqueeze(1) # [num_layers, 1, d_model]\n",
        "        # self.lx, self.lz = lx[:,k:], lz[:,k:] # [batch, T, dim_a], [batch, T, dim_z]\n",
        "        self.lx, self.lz = lx[k:], lz[k:] # [T, dim_a], [T, dim_z]\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "\n",
        "                # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "                # print(torch.cat(lstate, dim=0).shape)\n",
        "                # lsx = self.jepa.enc(torch.stack(lstate, dim=0))#.unsqueeze(0)\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))#.unsqueeze(0)\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx-torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                # batch, seq_len, _ = lstate.shape\n",
        "                # seq_len, _ = lstate.shape\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    try: la = self.emb(self.la[:seq_len])\n",
        "                    except:\n",
        "                        print(\"err self.la\")\n",
        "                        # la = self.emb([0]*seq_len)\n",
        "                        la = self.emb(torch.zeros(seq_len, dtype=int, device=device))\n",
        "\n",
        "        # lz = nn.Parameter(torch.zeros((batch, seq_len, self.dim_z),device=device))\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        # optim_z = torch.optim.SGD([lz], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([lz], 1e0, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "\n",
        "        for i in range(20): # num epochs\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                # print(sxaz.shape, self.h0.shape)\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                # sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                loss = F.mse_loss(out_, out)\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            print(lz.data)\n",
        "            with torch.no_grad(): lz.clamp_(min=-1, max=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1]\n",
        "        self.la = la[k:]\n",
        "        return h0\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 16\n",
        "        lr = 1e1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                # x[:,:self.lx.shape[1]], z[:,:self.lz.shape[1]] = self.lx[:,:T], self.lz[:,:T]\n",
        "                x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].repeat(batch,1,1), self.lz[:T].repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            # print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            # print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i, \"loss\", loss.squeeze().data)\n",
        "            # print(x.shape,torch.argmax(-dist,dim=-1).shape,z.shape,loss.shape) # [16, 6, 3], [16, 6], [16, 6, 1], [16, 1]\n",
        "            # print(i, torch.cat([x,torch.argmax(-dist,dim=-1),z],dim=-1).squeeze().data)\n",
        "            print(i, \"x act z\", torch.cat([x[0],torch.argmax(-dist,dim=-1)[0].unsqueeze(-1),z[0]],dim=-1).squeeze().data)\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, t, dim_a], [batch, t, dim_z]\n",
        "\n",
        "\n",
        "    def search_optimxz(self, sx, T=6, h0=None):\n",
        "        self.eval()\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 4 # 16\n",
        "        lr = 1e1 # adamw 1e-1, 3e-1 ; sgd\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        # optim_x = torch.optim.SGD([x], lr=lr)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_uniform_(z) # xavier_normal_, xavier_uniform_\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        # optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e1, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "        optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].unsqueeze(0).repeat(batch,1,1), self.lz[:T].unsqueeze(0).repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        print(\"search\", z[0].squeeze())\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        for i in range(10): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, lsx, lh0,c = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.sum().backward()\n",
        "            # optim_x.step(); optim_z.step()\n",
        "            # optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            print(i, \"search loss\", loss.squeeze().data)\n",
        "            # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "            print(i, \"search z\", z[0].squeeze().data)\n",
        "            # print(torch.argmin(dist,dim=-1).int())\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        print(\"c\",torch.stack(c)[:,idx])\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, T, dim_a], [batch, T, dim_z]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # cost += tcost + icost\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "        c=[]\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            # print(sx.shape, a.shape, z.shape)\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            c.append(tcost)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0, c\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    # imshow(state[0].cpu())\n",
        "                    # print(\"norm\", torch.norm(sy[0]-sy_[0], dim=-1))\n",
        "                    # # if torch.norm(sy[0]-sy_[0], dim=-1) > 1:\n",
        "                    # print(i, reward[0])\n",
        "                    # print(sy)\n",
        "                    # print(sy_)\n",
        "                    # print(sy[0]-sy_[0])\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "\n",
        "                    # cost loss\n",
        "                    # syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    syh0 = torch.cat([sy.flatten(1),F.dropout(h0, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                    closs = 100*clossl\n",
        "\n",
        "                    loss = loss + jloss + closs # for no retain_graph\n",
        "                    # loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    h0 = h0.detach()\n",
        "                    loss=0 # no retain_graph\n",
        "                # else:\n",
        "                #     scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batc h_size, d_model]\n",
        "            # sx=sy_\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "\n",
        "                    # z = self.jepa.argm(sy_, a, sy)\n",
        "                    z = self.argm(sy, sy_, h0, a, reward)\n",
        "                    with torch.no_grad(): z.mul_(torch.rand_like(z)).mul_((torch.rand_like(z)>0.5).bool()) # dropout without scailing\n",
        "\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "\n",
        "                    # cost loss\n",
        "                    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    # syh0 = torch.cat([sy.flatten(1),F.dropout(h0, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                    closs = self.closs_coeff * clossl\n",
        "\n",
        "                    # print(h0.requires_grad)\n",
        "                    # pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "                    # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "                    # print(\"reward, pred, clossl\", reward[mask].data, pred[mask].data, clossl.item())\n",
        "                    # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "                    # torch.norm(sy-sx, dim=-1)\n",
        "                    # sx=sy\n",
        "\n",
        "                    loss = loss + jloss + closs # for no retain_graph\n",
        "                    # loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    norm = torch.norm(sy, dim=-1)[0].item()\n",
        "                    z_norm = torch.norm(z)\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    h0 = h0.detach()\n",
        "                    loss=0 # no retain_graph\n",
        "                # else:\n",
        "                #     scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "\n",
        "print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cg0BI2TwY9-p"
      },
      "outputs": [],
      "source": [
        "# @title z.grad.data = -z.grad.data\n",
        "\n",
        "# self.eval()\n",
        "batch = 4 # 16\n",
        "x = nn.Parameter(torch.empty((batch, T, agent.dim_a),device=device))\n",
        "torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "z = nn.Parameter(torch.zeros((batch, T, agent.dim_z),device=device))\n",
        "torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "# optim_ = torch.optim.SGD([x,z], lr=1e1) # 3e3\n",
        "optim_ = torch.optim.AdamW([x,z], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "print(\"search z\", z[0].squeeze())\n",
        "print(\"search x\", x[0].squeeze())\n",
        "sx, h0 = sx.detach(), h0.detach()\n",
        "for i in range(10): # num epochs\n",
        "    dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    x_ = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "    # print(sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [4, 1, 3], [4, 1, 8], [1, 1, 256]\n",
        "    loss, lsx, lh0,c = agent.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "    loss.sum().backward()\n",
        "    z.grad.data = -z.grad.data\n",
        "    optim_.step()\n",
        "    optim_.zero_grad()\n",
        "    with torch.no_grad():\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "    print(i, \"search loss\", loss.squeeze().data)\n",
        "    print(i, \"search z\", z[0].squeeze().data)\n",
        "    print(i, \"search x\", x[0].squeeze().data)\n",
        "dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "print(\"c\",torch.stack(c)[:,idx])\n",
        "# return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, T, dim_a], [batch, T, dim_z]\n",
        "# print(lact[idx], lh0[:,:,idx,:], x[idx], z[idx])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5VMebkQ1mJtD"
      },
      "outputs": [],
      "source": [
        "# @title argm agent.rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def argm(sx, x,h0, lr=3e3): # 3e3\n",
        "    # agent.eval()\n",
        "    # batch_size, T, _ = sx.shape\n",
        "    batch = 16 # 16\n",
        "    z = nn.Parameter(torch.zeros((batch, T, agent.dim_z),device=device))\n",
        "    torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "    optim_z = torch.optim.SGD([z], lr=1e3, maximize=True) # 3e3\n",
        "    # optim_z = torch.optim.AdamW([z], 1e-2, (0.9, 0.999), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "    # optim_z = torch.optim.AdamW([z], 1e-0, (0.9, 0.95), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "    # optim_z = torch.optim.LBFGS([z], max_iter=5, lr=1)\n",
        "\n",
        "    # print(\"argm\", z[0].squeeze())\n",
        "    sx, h0 = sx.detach(), h0.detach()\n",
        "    x = x.detach().repeat(batch,1,1)\n",
        "    for i in range(5): # num epochs\n",
        "        # print(sx.shape, x.shape, z.shape, h0.shape) # [1, 256], [4, 1, 3], [4, 1, 8], [1, 1, 256]\n",
        "        loss, lsx, lh0,c = agent.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "        loss.sum().backward()\n",
        "        optim_z.step()\n",
        "        optim_z.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "        # print(i, \"argm loss\", loss.squeeze().data)\n",
        "        # print(i, \"argm z\", z[0].squeeze().data)\n",
        "    idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "    return z[idx].unsqueeze(0)\n",
        "\n",
        "\n",
        "T=1\n",
        "xx = torch.empty((1, T, agent.dim_a))\n",
        "torch.nn.init.xavier_uniform_(xx)\n",
        "x = nn.Parameter(xx.clone())#.repeat(batch,1,1))\n",
        "# print(x.shape)\n",
        "optim_x = torch.optim.SGD([x], lr=1e1) # 1e-1,1e-0,1e4 ; 1e2\n",
        "# optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "# optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "\n",
        "state = torch.zeros((1, 3,64,64))\n",
        "with torch.no_grad():\n",
        "    sx = agent.jepa.enc(state).detach()\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "print(time.time()-start)\n",
        "\n",
        "print(\"search\",x.squeeze().data)\n",
        "for i in range(20): # 5\n",
        "    dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    x_ = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "    z = argm(sx, x_,h0)\n",
        "    # print(sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [1, 1, 3], [1, 1, 8], [1, 1, 256]\n",
        "    loss, lsx, lh0,c = agent.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "    loss.sum().backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "    print(i, \"search loss\", x.squeeze().data, loss.item())\n",
        "    # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "\n",
        "# z sgd 1e3\n",
        "# 9 search loss tensor([0.0142, 0.0142, 0.0142, 0.0142])\n",
        "# 9 search z tensor([-0.3381, -0.7005, -0.5877, -0.0664, -0.1439,  0.0283,  0.0541, -0.1439])\n",
        "\n",
        "# x sgd 1e2\n",
        "# 1 tensor([0.3561, 0.3059, 0.8830]) 0.014148875139653683\n",
        "# 9 tensor([0.3560, 0.3064, 0.8828]) 2.328815611463142e-07\n",
        "\n",
        "# 1e0\n",
        "# 19 tensor([-0.5768,  0.5778,  0.5774]) 6.543130552927323e-07\n",
        "# 19 tensor([0.3570, 0.6689, 0.6521]) 2.474381801675918e-07\n",
        "# 19 tensor([0.5783, 0.5765, 0.5772]) 1.519319567933053e-07\n",
        "# 19 tensor([0.3427, 0.6795, 0.6487]) 4.220427456402831e-07\n",
        "#\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gcvgdCB1h1_E"
      },
      "outputs": [],
      "source": [
        "# @title torch.optim.LBFGS\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# Example of a deep nonlinear model f(x)\n",
        "class DeepNonlinearModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNonlinearModel, self).__init__()\n",
        "        self.lin = nn.Sequential(\n",
        "            nn.Linear(10, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "f = DeepNonlinearModel()\n",
        "# x = torch.randn(1, 10, requires_grad=True)\n",
        "# xx = torch.randn((1,10))\n",
        "x = nn.Parameter(xx.clone())#.repeat(batch,1,1))\n",
        "\n",
        "# Define loss function (mean squared error for this example)\n",
        "target = torch.tensor([[0.0]])  # Target output\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "def closure():\n",
        "    optimizer.zero_grad()  # Zero out the gradients\n",
        "    output = f(x)          # Forward pass through the model\n",
        "    loss = loss_fn(output, target)  # Calculate the loss\n",
        "    loss.backward()         # Backpropagate\n",
        "    return loss\n",
        "\n",
        "optimizer = torch.optim.LBFGS([x], lr=1.0, max_iter=5)  # Limit to 2-3 iterations for speed\n",
        "start_time = time.time()\n",
        "for _ in range(2):  # LBFGS does multiple iterations internally\n",
        "    loss = optimizer.step(closure)  # Perform a step of optimisation\n",
        "    print(loss.item())\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimisation completed in {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Final loss: {loss.item()}\")\n",
        "print(f\"Optimised x: {x.detach().numpy()}\")\n",
        "\n",
        "start_time = time.time()\n",
        "optimizer = torch.optim.SGD([x], lr=1e1, maximize=True) # 3e3\n",
        "for _ in range(5):  # LBFGS does multiple iterations internally\n",
        "    loss = optimizer.step()  # Perform a step of optimisation\n",
        "    output = f(x)          # Forward pass through the model\n",
        "    loss = loss_fn(output, target)  # Calculate the loss\n",
        "    loss.backward()         # Backpropagate\n",
        "    optimizer.zero_grad()  # Zero out the gradients\n",
        "    print(loss.item())\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimisation completed in {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Final loss: {loss.item()}\")\n",
        "print(f\"Optimised x: {x.detach().numpy()}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "# batch=sx.size(dim=0)\n",
        "batch=32\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "# optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone()) # argm 0.38188403844833374 3.86767578125\n",
        "# torch.nn.init.zeros_(z)\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e4)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 100\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "print(z.squeeze())\n",
        "\n",
        "want z around [-1,1], large lr, few steps, punish large z\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "    # def argm(self, sx, a, lr=3e3): # 3e3\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "    #     optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    #     # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    #     sx, a = sx.detach(), a.detach()\n",
        "    #     for i in range(5): # 10\n",
        "    #         sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #         with torch.amp.autocast('cuda'):\n",
        "    #             # sx_ = self.jepa.pred(sxaz)\n",
        "    #             sx_ = sx + self.jepa.pred(sxaz)\n",
        "    #             cost = -self.tcost(sx_)\n",
        "\n",
        "    #         cost.backward()\n",
        "    #         optim.step()\n",
        "    #         # scaler.scale(cost).backward()\n",
        "    #         # scaler.step(optim)\n",
        "    #         # scaler.update()\n",
        "    #         optim.zero_grad()\n",
        "    #         with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "    #         print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     return z.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def argm(self, sx, lr=3e3): # 3e3\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch_size, T, _ = sx.shape\n",
        "        batch = 16\n",
        "        # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "        z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "        optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "        sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "        # sx = sx.detach()\n",
        "        for i in range(20): # 10\n",
        "            # print(sx.shape,z.shape)\n",
        "            sxz = torch.cat([sx, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                cost = model(sxz)\n",
        "            cost.sum().backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "            # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "            # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        # return z.detach()\n",
        "        # print(\"argm z\",z.squeeze().data)\n",
        "        # print(\"cost\",cost.squeeze())\n",
        "        idx = torch.argmax(loss)\n",
        "        # return z[idx].detach().unsqueeze(0)\n",
        "        return z[:,idx].detach()\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        batch=1\n",
        "        T=1\n",
        "        x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "\n",
        "        lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "        # ratio = 6e0\n",
        "        lr = 1e-1 # adamw 1e-1\n",
        "        ratio = 4\n",
        "        # optim_x = torch.optim.SGD([x], lr=lr)\n",
        "        # optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # print(x.shape)\n",
        "\n",
        "\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze())\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "        for i in range(50):\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            z = argm(x)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "            # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "            # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad():\n",
        "                # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                x.clamp_(min=-1, max=1)\n",
        "            # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "        idx = torch.argmax(loss)\n",
        "        print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F8nNzai_b-G5"
      },
      "outputs": [],
      "source": [
        "# @title test quant icost search rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "d_model=16\n",
        "sicost = ICost(d_model, n=4)\n",
        "stcost=nn.Sequential(nn.Linear(d_model, 1)).to(device)\n",
        "dim_z=1\n",
        "jepa_pred=nn.Sequential(nn.Linear(d_model+dim_z+3, d_model)).to(device)\n",
        "\n",
        "\n",
        "def search(sx, T=None, bptt=None):\n",
        "    if T==None: T = 256\n",
        "    if bptt==None: bptt = min(T,32)\n",
        "    batch=sx.size(dim=0)\n",
        "    # with torch.amp.autocast('cuda'):\n",
        "    x = nn.Parameter(torch.zeros((batch, T, 3),device=device))\n",
        "    torch.nn.init.xavier_uniform_(x)\n",
        "    # optim = torch.optim.SGD([x], lr=1e5, maximize=True)\n",
        "    optim = torch.optim.SGD([x], lr=1e5)\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    for _ in range(3): # num epochs\n",
        "        sx_ = sx.detach()\n",
        "        for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "            loss, sx_ = rnn_pred(sx_, la)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            print(loss)\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "\n",
        "            with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "            sx_ = sx_.detach()\n",
        "            # print(loss.item(), lact)\n",
        "    # print(\"search\",loss.item())\n",
        "    # return la, lact # [batch_size, T]\n",
        "    return la, lact, x # [batch_size, T]\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    if z is None: z=torch.zeros((batch,dim_z),device=device) # average case?\n",
        "    # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    # for t in range(seq_len): # simple single layer\n",
        "    t=0\n",
        "    a = la[:,t] # [1, dim_a]\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # sx = sx + jepa_pred(sxaz)\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        sx = jepa_pred(sxaz)\n",
        "    print(lsx)\n",
        "    lsx = torch.cat([lsx, sx], dim=0)\n",
        "    print(lsx)\n",
        "    # print(lsx.requires_grad, sx.requires_grad)\n",
        "    # icost = 0.5*sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    icost = sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    # print(icost.requires_grad)\n",
        "    tcost = -stcost(sx.squeeze(0)).squeeze(0)\n",
        "    cost += (tcost + icost)*gamma**t\n",
        "    print(\"tcost, icost\", tcost, icost)\n",
        "    # cost=icost\n",
        "    # print(cost)\n",
        "    return cost, sx#, z\n",
        "\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "\n",
        "batch=1\n",
        "sx=torch.rand((batch,d_model), device=device)\n",
        "la, lact, x = search(sx, T=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uivwksBdwVH"
      },
      "outputs": [],
      "source": [
        "state = buffer[7][80][0]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = transform(state).unsqueeze(0).to(device)[0]\n",
        "sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "out= agent.deconv(sx_).squeeze(0)\n",
        "print(out.shape)\n",
        "imshow(state.detach().cpu())\n",
        "imshow(out.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjm2kV3H7ZVR"
      },
      "outputs": [],
      "source": [
        "for name, p in agent.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())\n",
        "\n",
        "\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mhTHWmEjI0JO"
      },
      "outputs": [],
      "source": [
        "# @title gym\n",
        "# https://gymnasium.farama.org/\n",
        "# https://github.com/Farama-Foundation/Gymnasium\n",
        "import gymnasium as gym\n",
        "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"Pendulum-v1\") # https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HcOidvtW9KAH"
      },
      "outputs": [],
      "source": [
        "# @title from RNN2\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None): # [batch_size, seq_len, input_size]\n",
        "        if h0 is None: h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        if c0 is None: c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        out, h0 = self.rnn(x, h0)\n",
        "        # out, (h0,c0) = self.lstm(x, (h0,c0))\n",
        "        # out:(batch_size, seq_length, hidden_size) (n, 28, 128)\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "d_model,dim_a,dim_z = 256,3,1\n",
        "pred = nn.Sequential(\n",
        "    nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model),\n",
        "    )\n",
        "gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "\n",
        "print(sum(p.numel() for p in pred.parameters() if p.requires_grad)) # 264192\n",
        "print(sum(p.numel() for p in gru.parameters() if p.requires_grad)) # 397824\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "6c79ab20-46bb-4299-c26b-0a27e138c717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2775104\n",
            "2362625\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzzjgoXCnhT7"
      },
      "outputs": [],
      "source": [
        "# @title train autoencoder\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 3e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    train(train_loader,model,optim)\n",
        "    state = buffer[7][80][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = model.encode(state.unsqueeze(0))\n",
        "    out= model.decode(sx_)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AodVas3L4ZS",
        "outputId": "f1940ab6-b72d-4c8d-f97d-6d876f1b92e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256])\n",
            "136960\n",
            "136960\n"
          ]
        }
      ],
      "source": [
        "# @title efficientnet\n",
        "# https://arxiv.org/pdf/2207.10318 # visualise kernal\n",
        "\n",
        "# https://pytorch.org/hub/research-models\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py\n",
        "\n",
        "import torch\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py\n",
        "from torchvision.models.efficientnet import *\n",
        "from torchvision.models import efficientnet\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # https://arxiv.org/pdf/2104.00298\n",
        "# Stage Operator Stride #Channels #Layers\n",
        "# 0 Conv3x3 2 24 1\n",
        "# 1 Fused-MBConv1, k3x3 1 24 2\n",
        "# 2 Fused-MBConv4, k3x3 2 48 4\n",
        "# 3 Fused-MBConv4, k3x3 2 64 4\n",
        "# 4 MBConv4, k3x3, SE0.25 2 128 6\n",
        "# 5 MBConv6, k3x3, SE0.25 1 160 9\n",
        "# 6 MBConv6, k3x3, SE0.25 2 256 15\n",
        "# 7 Conv1x1 & Pooling & FC - 1280 1\n",
        "\n",
        "# # elif arch.startswith(\"efficientnet_v2_s\"):\n",
        "# inverted_residual_setting = [\n",
        "#     FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n",
        "#     FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n",
        "#     FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n",
        "#     MBConvConfig(4, 3, 2, 64, 128, 6),\n",
        "#     MBConvConfig(6, 3, 1, 128, 160, 9),\n",
        "#     MBConvConfig(6, 3, 2, 160, 256, 15),\n",
        "# ]\n",
        "# last_channel = 1280\n",
        "\n",
        "# d_list=[24, 48, 64, 128, 160, 256] #\n",
        "d_list=[16, 32, 48, 96, 108, 172] #\n",
        "inverted_residual_setting = [\n",
        "    efficientnet.FusedMBConvConfig(1, 3, 1, d_list[0], d_list[0], 2),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[0], d_list[1], 4),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[1], d_list[2], 4),\n",
        "    efficientnet.MBConvConfig(4, 3, 2, d_list[2], d_list[3], 6),\n",
        "    efficientnet.MBConvConfig(6, 3, 1, d_list[3], d_list[4], 9),\n",
        "    efficientnet.MBConvConfig(6, 3, 2, d_list[4], d_list[5], 15),\n",
        "]\n",
        "last_channel = 512\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "effnet = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "effnet.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "\n",
        "#   (features): Sequential(\n",
        "#     (0): Conv2dNormActivation(\n",
        "#       (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "#       (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "#       (2): SiLU(inplace=True)\n",
        "\n",
        "input = torch.rand((1,1,256,256), device=device)\n",
        "out = effnet(input)\n",
        "print(out.shape)\n",
        "# print(effnet)\n",
        "print(sum(p.numel() for p in effnet.parameters() if p.requires_grad)) #\n",
        "print(sum(p.numel() for p in effnet.parameters())) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-nT5j864BIn",
        "outputId": "ac676107-a22d-4315-a3c7-785e3c6456c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "# !pip install -qq vector-quantize-pytorch\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LK5u500Vad2P"
      },
      "outputs": [],
      "source": [
        "# @title FSQ jax\n",
        "# https://github.com/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "import itertools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "Codeword = jax.Array\n",
        "Indices = jax.Array\n",
        "\n",
        "def round_ste(z):\n",
        "  \"\"\"Round with straight through gradients.\"\"\"\n",
        "  zhat = jnp.round(z)\n",
        "  return z + jax.lax.stop_gradient(zhat - z)\n",
        "\n",
        "class FSQ:\n",
        "  \"\"\"Quantizer.\"\"\"\n",
        "  def __init__(self, levels: list[int], eps: float = 1e-3):\n",
        "    self._levels = levels\n",
        "    self._eps = eps\n",
        "    self._levels_np = np.asarray(levels)\n",
        "    self._basis = np.concatenate(([1], np.cumprod(self._levels_np[:-1]))).astype(np.uint32)\n",
        "    self._implicit_codebook = self.indexes_to_codes(np.arange(self.codebook_size))\n",
        "    print(\"self._basis\",self._basis)\n",
        "    print(\"self._implicit_codebook\",self._implicit_codebook)\n",
        "\n",
        "  @property\n",
        "  def num_dimensions(self):\n",
        "    \"\"\"Number of dimensions expected from inputs.\"\"\"\n",
        "    return len(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook_size(self):\n",
        "    \"\"\"Size of the codebook.\"\"\"\n",
        "    return np.prod(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook(self):\n",
        "    \"\"\"Returns the implicit codebook. Shape (prod(levels), num_dimensions).\"\"\"\n",
        "    return self._implicit_codebook\n",
        "\n",
        "  def bound(self, z: jax.Array) -> jax.Array:\n",
        "    \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "    half_l = (self._levels_np - 1) * (1 - self._eps) / 2\n",
        "    offset = jnp.where(self._levels_np % 2 == 1, 0.0, 0.5)\n",
        "    shift = jnp.tan(offset / half_l)\n",
        "    return jnp.tanh(z + shift) * half_l - offset\n",
        "\n",
        "  def quantize(self, z: jax.Array) -> Codeword:\n",
        "    \"\"\"Quanitzes z, returns quantized zhat, same shape as z.\"\"\"\n",
        "    quantized = round_ste(self.bound(z))\n",
        "\n",
        "    # Renormalize to [-1, 1].\n",
        "    half_width = self._levels_np // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "  def _scale_and_shift(self, zhat_normalized):\n",
        "    # Scale and shift to range [0, ..., L-1]\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "  def _scale_and_shift_inverse(self, zhat):\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat - half_width) / half_width\n",
        "\n",
        "  def codes_to_indexes(self, zhat: Codeword) -> Indices:\n",
        "    \"\"\"Converts a `code` to an index in the codebook.\"\"\"\n",
        "    assert zhat.shape[-1] == self.num_dimensions\n",
        "    zhat = self._scale_and_shift(zhat)\n",
        "    return (zhat * self._basis).sum(axis=-1).astype(jnp.uint32)\n",
        "\n",
        "  def indexes_to_codes(self, indices: Indices) -> Codeword:\n",
        "    \"\"\"Inverse of `indexes_to_codes`.\"\"\"\n",
        "    indices = indices[..., jnp.newaxis]\n",
        "    print(indices, self._basis, self._levels_np)\n",
        "    print(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    codes_non_centered = np.mod(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    return self._scale_and_shift_inverse(codes_non_centered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xHxv7ptuwVHX"
      },
      "outputs": [],
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "        # self.mean = self.codebook.mean(dim=0)\n",
        "        # self.max = self.codebook.max(dim=0).values\n",
        "        # self.min = self.codebook.min(dim=0).values\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2 # [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "        # half_l = (self.levels-1)/2 # me ?\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5) # [0.0000, 0.0000, 0.5000] mean?\n",
        "        # print(\"half_l\", half_l)\n",
        "        # shift = torch.tan(offset / half_l) # [0.0000, 0.0000, 1.5608] < tan(1)\n",
        "\n",
        "        # print(\"shift\", shift)\n",
        "        # print(\"bound\", torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # print(f'half_l {half_l}, shift {shift}, bound {torch.tanh(z + shift) * half_l - offset}')\n",
        "        # return torch.tanh(z + shift) * half_l - offset\n",
        "        # return torch.tanh(z - shift) * half_l + offset\n",
        "        return torch.tanh(z) * half_l + offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        # print(\"quantized\", quantized)\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "# print(fsq.codebook)\n",
        "\n",
        "# batch_size, seq_len = 1, 1\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SnfcKPses5X",
        "outputId": "7c50a3e3-281a-4375-b86f-ece58f6775c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "half_l tensor([0.9990, 0.9990, 0.4995]), shift tensor([0.0000, 0.0000, 1.5608]), bound tensor([-0.4617,  0.5365, -0.0515])\n",
            "quantized tensor([0., 1., 0.])\n",
            "tensor([0., 1., 0.])\n"
          ]
        }
      ],
      "source": [
        "# @title test fsq\n",
        "fsq = FSQ(levels = [4])\n",
        "\n",
        "# 2: 1.6 half_l tensor([0.4995]), shift tensor([1.5608]), bound tensor([-0.5195])\n",
        "# 3: 0.6 # half_l tensor([0.9990]), shift tensor([0.]), bound tensor([-0.9207])\n",
        "# 4: 0.4, 1.3 # half_l tensor([1.4985]), shift tensor([0.3466]), bound tensor([-1.7726])\n",
        "# 5: 0.5, 1 # half_l [1.9980], shift [0.], bound [-1.8415]\n",
        "x = torch.tensor([.9],device=device)\n",
        "# x = torch.tensor([-1.6],device=device)\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "\n",
        "\n",
        "x = torch.tensor([-0.6,0.6,-1.6],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,-1.5],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,1.6],device=device)\n",
        "x = torch.tensor([-0.5,0.6,-0.1],device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "round emb\n",
        "\n",
        "# half_l [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "# offset [0.0000, 0.0000, 0.5000] mean?\n",
        "# shift [0.0000, 0.0000, 1.5608] torch.tan(offset / half_l)\n",
        "# bound [-0.5365,  0.5365, -0.4696] tanh(z + shift) * half_l - offset\n",
        "\n",
        "\n",
        "\n",
        "levels = torch.tensor([3,3,2])\n",
        "eps = 1e-3\n",
        "\n",
        "half_l = (levels - 1) * (1 - eps) / 2\n",
        "offset = torch.where(levels % 2 == 1, 0.0, 0.5)\n",
        "# print(\"half_l\", half_l)\n",
        "shift = torch.tan(offset / half_l)\n",
        "# print(\"shift\", shift)\n",
        "# print(\"bound\", torch.tanh(x + shift) * half_l - offset)\n",
        "# return torch.tanh(x + shift) * half_l - offset\n",
        "out = torch.tanh(x) * half_l + offset\n",
        "print(out)\n",
        "\n",
        "shift=torch.tan(torch.tensor([1.]))\n",
        "print(shift)\n",
        "bound = torch.tanh(x - shift)\n",
        "print(bound)\n",
        "\n",
        "print(torch.tanh(torch.tensor([0.])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z_VgsenYLpM",
        "outputId": "8c7b23ae-8cdb-4846-dae3-32fd046a4d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[0.0437, 0.3097, 0.4537]]], requires_grad=True)\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n"
          ]
        }
      ],
      "source": [
        "# @title test rnn_pred symlog\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "# fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # xx = fsq(x)\n",
        "    # xx = fsq(x.clone())\n",
        "    # print(xx)\n",
        "    # x = torch.tanh(x)\n",
        "    # loss = x.sum()\n",
        "    # loss = model(xx)\n",
        "    loss = model(x)\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    # x = torch.clamp(x, min=-1, max=1)\n",
        "    # x = torch.clamp(x.clone(), min=-1, max=1)\n",
        "    with torch.no_grad():\n",
        "        # x.clamp_(min=-1, max=1)\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# # model = nn.Sequential(nn.Linear(3,1))\n",
        "# model = nn.Sequential(nn.Linear(3*2,1))\n",
        "# device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# batch_size = 1\n",
        "# seq_len = 3\n",
        "# x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# # torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "\n",
        "# def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "#     batch, seq_len, dim_a = la.shape\n",
        "#     cost = 0\n",
        "#     lsx=sx\n",
        "#     for t in range(seq_len): # simple single layer\n",
        "#         a = la[:,t] # [1, dim_a]\n",
        "#         sxaz = torch.cat([sx, a], dim=-1)\n",
        "#         # with torch.amp.autocast('cuda'):\n",
        "#         cost = cost + model(sxaz)\n",
        "#         lsx = torch.cat([lsx, sx], dim=0)\n",
        "#     return cost, sx\n",
        "\n",
        "\n",
        "# # def ste_clamp(input, min=-1, max=1):\n",
        "# #     clamped_output = torch.clamp(input, min, max)\n",
        "# #     clamp_mask = (input < min) | (input > max)\n",
        "# #     return torch.where(clamp_mask, input, clamped_output)\n",
        "\n",
        "# def ste_clamp(x, min=-1, max=1):\n",
        "#     return torch.clamp(x, min, max).detach() + x - x.detach()\n",
        "\n",
        "# def ste_abs(x): return x.sign() * x\n",
        "# def symlog(x): return torch.sign(x) * torch.log(ste_abs(x) + 1.0)\n",
        "# def symexp(x): return torch.sign(x) * torch.exp(ste_abs(x) - 1.0)\n",
        "\n",
        "\n",
        "# sx = torch.rand((batch_size,3),device=device)\n",
        "# sx_ = sx.detach()\n",
        "# for i in range(10): # num epochs\n",
        "#     # la = fsq(x.clone())\n",
        "#     la = fsq(x)\n",
        "#     print(i)\n",
        "#     print(x,x.requires_grad)\n",
        "#     print(la,la.requires_grad)\n",
        "#     loss, sx_ = rnn_pred(sx_, la)\n",
        "#     # loss.backward()\n",
        "#     loss.backward(retain_graph=True) # retain_graph bec fsq got tanh that creates new graph?\n",
        "#     optim.step()\n",
        "#     optim.zero_grad()\n",
        "#     # x = torch.tanh(x)\n",
        "#     # x = torch.clamp(x, min=-1, max=1)\n",
        "#     # x = ste_clamp(x.clone(), min=-1, max=1)\n",
        "#     # x = symlog(x.clone())\n",
        "#     # sx_ = sx_.detach()\n",
        "\n",
        "\n",
        "# # print(xx)\n",
        "# print(x)\n",
        "# # print(xhat)\n",
        "# print(la)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YWmwVYhVVh5R"
      },
      "outputs": [],
      "source": [
        "# @title test ste_argmin\n",
        "import torch\n",
        "emb = torch.nn.Embedding(15, 3) # env.action_space # 15\n",
        "x = torch.rand(1,3)\n",
        "\n",
        "# def ste_argmin(x, dim=-1):\n",
        "#     idx = torch.argmin(x, dim)\n",
        "#     # out = torch.zeros_like(x)\n",
        "#     out = torch.zeros_like(idx).unsqueeze(-1)\n",
        "#     print(idx.shape, out.shape)\n",
        "#     out.scatter_(1, idx, 1)\n",
        "#     return out\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# def softargmin(x, beta=10):\n",
        "#     # Apply softmax to the negative of the input to approximate argmin\n",
        "#     weights = F.softmax(-x * beta, dim=-1)\n",
        "#     indices = torch.arange(x.size(-1), dtype=x.dtype, device=x.device)\n",
        "#     soft_argmin = torch.sum(weights * indices, dim=-1)\n",
        "#     return soft_argmin\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "# out = differentiable_argmax(-x)\n",
        "# print(out)\n",
        "\n",
        "\n",
        "# def softargmax1d(input, beta=100): # https://github.com/david-wb/softargmax/blob/master/softargmax.py\n",
        "#     *_, n = input.shape\n",
        "#     input = nn.functional.softmin(beta * input, dim=-1)\n",
        "#     indices = torch.linspace(0, 1, n)\n",
        "#     result = torch.sum((n - 1) * input * indices, dim=-1)\n",
        "#     return result\n",
        "\n",
        "# ste_round\n",
        "\n",
        "# # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "# dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "# print(lact)\n",
        "\n",
        "device='cpu'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "    dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    A=differentiable_argmax(-dist)\n",
        "    # print(A.shape)\n",
        "    print(torch.argmax(A))\n",
        "    x_=A@emb.weight.data\n",
        "    # print(\"dist\", dist.shape)\n",
        "    # lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = ste_argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = softargmin(dist)\n",
        "    # print(lact)\n",
        "    # x = emb.weight.data[lact]\n",
        "\n",
        "    # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "    print(\"x_\",x_)\n",
        "    # x = emb(x_)\n",
        "\n",
        "    loss = model(x_).sum()\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "x_ = torch.tensor([14])\n",
        "x = emb(x_)\n",
        "# print(x)\n",
        "# # print(emb.weight)\n",
        "# pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "pseudo_inverse_weight = torch.linalg.pinv(emb.weight)\n",
        "# weight_inv = torch.pinverse(emb.weight.T)\n",
        "\n",
        "dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "# print(x@pseudo_inverse_weight)\n",
        "# A=differentiable_argmax(-x@pseudo_inverse_weight)\n",
        "A=differentiable_argmax(-dist)\n",
        "print(A)\n",
        "\n",
        "# print(pseudo_inverse_weight.shape, pseudo_inverse_weight)\n",
        "# # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "# x_ = x@ pseudo_inverse_weight\n",
        "# print(\"x_\",x_)\n",
        "\n",
        "# print(emb.weight@ pseudo_inverse_weight)\n",
        "# dist=torch.dist(emb.weight@ pseudo_inverse_weight, torch.eye(15))\n",
        "# print(dist)\n",
        "# print(pseudo_inverse_weight@ emb.weight)\n",
        "\n",
        "# print(emb.weight@ weight_inv.T)\n",
        "# print(weight_inv.T@ emb.weight)\n",
        "\n",
        "# torch.linalg.lstsq(A, B).solution\n",
        "\n",
        "\n",
        "x_ = torch.tensor([4])\n",
        "embx = emb(x_) # emb.weight[x_,:]\n",
        "print(embx)\n",
        "\n",
        "Apinv = torch.linalg.pinv(A)\n",
        "x = embx@Apinv\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pGZld_gLH1RA"
      },
      "outputs": [],
      "source": [
        "# @title test bptt\n",
        "\n",
        "x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    loss=0\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        # loss = loss -stcost(xxx.clone()).sum()\n",
        "        loss = loss -stcost(xxx).sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "# RuntimeError: Output 1 of SplitBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size, T = 1,6\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "# optim = torch.optim.SGD([x], lr=1e-3)\n",
        "# # xx = torch.split(x, bptt, dim=1)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    loss=0\n",
        "    # xx = torch.split(x, bptt, dim=1)\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        loss = loss -stcost(xxx.clone()).sum()\n",
        "        # loss = loss -stcost(xxx).sum()\n",
        "        # loss.backward()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    print(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "jepapred = nn.Sequential(nn.Linear(3*2,3))\n",
        "stcost = nn.Sequential(nn.Linear(3,1))\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    for t in range(seq_len): # simple single layer\n",
        "        # print(la.shape)\n",
        "        a = la[:,t,:].clone() # [1, dim_a]\n",
        "        # sxaz = torch.cat([sx, a], dim=-1)\n",
        "        sxaz = torch.cat([sx.clone(), a.clone()], dim=-1)\n",
        "        # sxaz = torch.cat([sx.clone(), a], dim=-1)\n",
        "        sx = jepapred(sxaz)\n",
        "        tcost = -stcost(sx).sum()\n",
        "        lsx = torch.cat([lsx, sx], dim=0)\n",
        "        # print(lsx.requires_grad, sx.requires_grad)\n",
        "        # icost = 0.5*icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(icost.requires_grad)\n",
        "        cost += tcost# + icost\n",
        "    return cost, sx#, z\n",
        "\n",
        "\n",
        "\n",
        "batch_size=4\n",
        "sx = torch.rand((batch_size,3),device=device)\n",
        "T = 6\n",
        "bptt = 3\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device)) # FSQ 3 levels\n",
        "x = torch.empty((batch_size, T, 3),device=device) # FSQ 3 levels\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "# optim = torch.optim.SGD([x], lr=1e-3) #, maximize=True)\n",
        "# print(x.shape)\n",
        "# print(len(xx))\n",
        "# print(xx[0].shape)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    sx_ = sx.detach()\n",
        "    for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "\n",
        "        # xxx=x\n",
        "        # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "        la = fsq(xxx)\n",
        "        # la = xxx\n",
        "        # print(x,x.requires_grad)\n",
        "        # print(la,la.requires_grad)\n",
        "        # loss, sx_ = rnn_pred(sx_, la)\n",
        "        loss = -stcost(la).sum()\n",
        "\n",
        "        print(\"loss\",loss)\n",
        "        loss.backward()\n",
        "        # loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # sx_ = sx_.detach()\n",
        "        # print(loss.item(), lact)\n",
        "\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    x = torch.tanh(x) # clamp\n",
        "    print(x)\n",
        "    # print(x)\n",
        "print(\"search\",loss.item())\n",
        "# print(lact)\n",
        "# return la, lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # # def search(self, sx, T=256, bptt=32):\n",
        "    # def search(self, sx, T=None, bptt=None):\n",
        "    #     if T==None: T = 256\n",
        "    #     if bptt==None: bptt = min(T,3)\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "    #     torch.nn.init.xavier_uniform_(x)\n",
        "    #     # optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "    #     # xx = torch.split(x, bptt, dim=1)\n",
        "    #     xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "    #     optim = torch.optim.SGD(xx, lr=1e7) #, maximize=True)\n",
        "\n",
        "    #     for _ in range(10): # num epochs\n",
        "    #         sx_ = sx.detach()\n",
        "    #         for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "    #             # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "    #             la = fsq(xxx.clone())\n",
        "    #             # print(x,x.requires_grad)\n",
        "    #             # print(la,la.requires_grad)\n",
        "    #             loss, sx_ = self.rnn_pred(sx_, la)\n",
        "    #             loss.backward(retain_graph=True)\n",
        "    #             optim.step()\n",
        "    #             optim.zero_grad()\n",
        "    #             sx_ = sx_.detach()\n",
        "    #             # print(loss.item(), lact)\n",
        "    #             # xx = torch.tanh(xx) # clamp\n",
        "    #         xx = [torch.tanh(xxx) for xxx in xx]\n",
        "    #         x = torch.cat(xx,dim=1)\n",
        "    #         # x = torch.tanh(x) # clamp\n",
        "    #         print(x)\n",
        "    #     print(\"search\",loss.item())\n",
        "    #     # print(lact)\n",
        "    #     return la, lact # [batch_size, T]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx0k_ndHOEMe"
      },
      "outputs": [],
      "source": [
        "# @title visualise kernels\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import utils\n",
        "# https://stackoverflow.com/questions/55594969/how-to-visualise-filters-in-a-cnn-with-pytorch\n",
        "\n",
        "layers = [0,3,6,9]\n",
        "layers = [0,3,6,9,12]\n",
        "layer = 9\n",
        "\n",
        "def visualise(model,layer):\n",
        "    kernels = model.cnn[layer].weight.data.clone()\n",
        "    n,c,w,h = kernels.shape\n",
        "    print(kernels.shape)\n",
        "    if c not in [1,3]:\n",
        "        # kernels = kernels.mean(dim=1, keepdim=True)\n",
        "        kernels = kernels[:,2,:,:].unsqueeze(dim=1)\n",
        "    nrow=10\n",
        "    rows = np.min((kernels.shape[0]//nrow + 1, 64))\n",
        "    grid = utils.make_grid(kernels, nrow=nrow, normalize=True, padding=1)\n",
        "    plt.figure(figsize=(nrow,rows))\n",
        "\n",
        "    kernels = kernels - kernels.min()\n",
        "    kernels = kernels / kernels.max()\n",
        "    filter_img = utils.make_grid(kernels, nrow = 12)\n",
        "    # change ordering since matplotlib requires images to\n",
        "    # be (H, W, C)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0))\n",
        "\n",
        "    # plt.imshow(grid.cpu().numpy().transpose((1, 2, 0)))\n",
        "\n",
        "# visualise(agent.sense,layer)\n",
        "visualise(agent.jepa.enc,layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "N2TGs69fnrZo",
        "outputId": "e7624553-e17a-4a9f-85a4-512720ed329a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tcost.1.weight torch.Size([2, 512])\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1920x1440 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRU(267, 256, batch_first=True, dropout=0.2)\n"
          ]
        }
      ],
      "source": [
        "# @title visualise lin\n",
        "# https://matplotlib.org/stable/plot_types/index.html\n",
        "from torchvision import utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for name, param in agent.jepa.pred.named_parameters(): # for param in model.parameters():\n",
        "# for name, param in agent.emb.named_parameters():\n",
        "for name, param in agent.tcost.named_parameters():\n",
        "    print(name, param.shape)\n",
        "    if len(param.shape)==1: param=param.unsqueeze(0)\n",
        "    Z=param.detach()#.numpy()\n",
        "\n",
        "    filter_img = utils.make_grid(Z, nrow = 12, normalize=True, padding=1)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0)) # (H, W, C)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "    # pos=ax.imshow(Z)\n",
        "    # fig.colorbar(pos)\n",
        "    plt.show()\n",
        "\n",
        "print(agent.jepa.pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjEWGq2WGi9a",
        "outputId": "649e3612-f156-496e-d8d5-fc576110e2ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0015, -0.0132,  0.0280,  ...,  0.0297,  0.0289,  0.0152],\n",
            "        [ 0.0168,  0.0031, -0.0288,  ..., -0.0064, -0.0137, -0.0085]])\n"
          ]
        }
      ],
      "source": [
        "# print(vars(agent.jepa.pred.))\n",
        "# print(vars(agent.tcost.state_dict()))\n",
        "# print(agent.jepa.pred._parameters.keys())\n",
        "# print(agent.jepa.pred._parameters['weight_ih_l0'])\n",
        "# print(agent.jepa.pred._parameters['weight_hh_l2']) # weight_hh_l0, weight_hh_l2\n",
        "# print(agent.tcost.state_dict().keys())\n",
        "print(agent.tcost.state_dict()['tcost.1.weight']) # tcost.2.bias, tcost.4.bias\n",
        "# print(agent.tcost.named_parameters())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A90BTTw0Lr-t",
        "outputId": "a95870e2-bc89-43ba-d40b-febea4ce2382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n",
            "690080\n"
          ]
        }
      ],
      "source": [
        "# @title ConvEnc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ConvEnc(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[5], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[5], d_list[5], 2, 2, 0), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # 2457024\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # 685248\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 4, 2, 2), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 4, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 4, 2, 2), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 4, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # # #\n",
        "\n",
        "\n",
        "            nn.Conv2d(1, d_list[0], 4, 4, 0), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 4, 4, 0), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 4, 4, 0), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 4, 4, 0), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            #\n",
        "\n",
        "\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[5],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "\n",
        "convenc = ConvEnc(256).to(device)\n",
        "input = torch.rand((4,1,256,256), device=device)\n",
        "out = convenc(input)\n",
        "print(out.shape)\n",
        "print(sum(p.numel() for p in convenc.parameters() if p.requires_grad)) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "S_xnBFjXVxgz"
      },
      "outputs": [],
      "source": [
        "# @title transfer weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "d = 10  # size of the first dimension\n",
        "a = 5   # size of the extra nodes to omit\n",
        "m = 8   # output dimension\n",
        "\n",
        "source_layer = nn.Linear(d+a, m)\n",
        "target_layer = nn.Linear(d, m)\n",
        "# source_layer = nn.Linear(d, m)\n",
        "# target_layer = nn.Linear(d+a, m)\n",
        "\n",
        "def transfer(tgt,src):\n",
        "    with torch.no_grad():\n",
        "        tgt.weight[:, :src.weight.shape[1]].copy_(src.weight[:, :tgt.weight.shape[1]])\n",
        "        tgt.bias.copy_(src.bias)\n",
        "    return tgt,src\n",
        "\n",
        "target_layer, source_layer = transfer(target_layer, source_layer)\n",
        "\n",
        "\n",
        "src_sd = source_layer.state_dict()\n",
        "tgt_sd = target_layer.state_dict()\n",
        "\n",
        "def transfersd(tgt,src):\n",
        "    with torch.no_grad():\n",
        "        tgt['weight'][:, :src['weight'].shape[1]].copy_(src['weight'][:, :tgt['weight'].shape[1]])\n",
        "        tgt['bias'].copy_(src['bias'])\n",
        "    return tgt\n",
        "\n",
        "tgt_sd = transfersd(tgt_sd, src_sd)\n",
        "target_layer.load_state_dict(tgt_sd)\n",
        "\n",
        "\n",
        "agent_src = Agent(d_model=256, dim_a=3, dim_z=1, dim_v=512).to(device)\n",
        "\n",
        "# agent.tcost = TCost((1+agent.jepa.pred.num_layers)*agent.d_model) # replace tcost\n",
        "\n",
        "agent = Agent(d_model=256, dim_a=3, dim_z=3, dim_v=512).to(device)\n",
        "\n",
        "# agent.jepa.pred\n",
        "# target_layer, source_layer = transfer(target_layer, source_layer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(vars(agent.jepa.pred))\n",
        "# gru = agent.jepa.pred\n",
        "# gru = agent_src.jepa.pred\n",
        "# for wht_name in gru._all_weights[0]: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "#     print(wht_name, gru._parameters[wht_name].shape)\n",
        "\n",
        "# weight_ih_l0 dim_z=3: [768, 262] , dim_z=1: [768, 260]\n",
        "# weight_hh_l0 torch.Size([768, 256])\n",
        "# bias_ih_l0 torch.Size([768])\n",
        "# bias_hh_l0 torch.Size([768])\n",
        "\n",
        "# tgt_gru = agent.jepa.pred\n",
        "# src_gru = agent_src.jepa.pred\n",
        "tgt_gru = torch.nn.GRU(d_model+dim_a+dim_z+2, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "src_gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "\n",
        "tgt_gru[]\n",
        "def transfer_gru(tgt_gru, src_gru): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for i in range(min(len(tgt_gru._all_weights), len(src_gru._all_weights))):\n",
        "        # for lyr in tgt_gru._all_weights:\n",
        "            lyr = tgt_gru._all_weights[i]\n",
        "            for wht_name in lyr: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "                # print(wht_name, tgt_gru._parameters[wht_name].shape)\n",
        "                tgt_wht, src_wht = tgt_gru._parameters[wht_name], src_gru._parameters[wht_name]\n",
        "                if len(tgt_wht.shape)==2:\n",
        "                    tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "                elif len(tgt_wht.shape)==1:\n",
        "                    tgt_gru._parameters[wht_name] = src_wht\n",
        "    return tgt_gru\n",
        "tgt_gru = transfer_gru(tgt_gru, src_gru)\n",
        "\n",
        "# for wht_name in tgt_gru._all_weights[0]: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "#     print(wht_name, tgt_gru._parameters[wht_name].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "d_model=256; dim_a=3; dim_z=1; dim_v=512\n",
        "\n",
        "pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "# pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "print(pred._all_weights)\n",
        "for lyr in pred._all_weights:\n",
        "    for wht_name in lyr: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "        print(wht_name, pred._parameters[wht_name].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(pred.state_dict().keys())\n",
        "\n",
        "tgt_gru = torch.nn.GRU(d_model+dim_a+dim_z+2, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "src_gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "print(tgt_gru.state_dict()['weight_ih_l0'][0][:10])\n",
        "print(src_gru.state_dict()['weight_ih_l0'][0][:10])\n",
        "\n",
        "print(tgt_gru.state_dict()['bias_ih_l0'][:10])\n",
        "print(src_gru.state_dict()['bias_ih_l0'][:10])\n",
        "tgt_gru.state_dict().keys()\n",
        "src_gru.state_dict().keys()\n",
        "\n",
        "# tgt_gru\n",
        "# src_gru\n",
        "for wht_name in tgt_gru.state_dict().keys():\n",
        "    if not wht_name in src_gru.state_dict().keys(): continue\n",
        "    print(wht_name)\n",
        "    # print(tgt_gru.state_dict()[wht_name])\n",
        "    # tgt_gru.state_dict()[wht_name].copy_(src_gru.state_dict()[wht_name])\n",
        "\n",
        "tgt_sd = tgt_gru.state_dict()\n",
        "src_sd = src_gru.state_dict()\n",
        "def transfer_sd(tgt_sd, src_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in src_sd.keys(): continue\n",
        "            # print(wht_name)\n",
        "            tgt_wht, src_wht = tgt_sd[wht_name], src_sd[wht_name]\n",
        "            if len(tgt_wht.shape)==2:\n",
        "                tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "            elif len(tgt_wht.shape)==1:\n",
        "                tgt_wht.copy_(src_wht)\n",
        "    return tgt_sd\n",
        "tgt_sd = transfer_sd(tgt_sd, src_sd)\n",
        "print(tgt_sd['weight_ih_l0'][0][:10])\n",
        "print(tgt_sd['bias_ih_l0'][:10])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CwApoQMMKzB",
        "outputId": "98f67f91-ef5b-406f-b852-5a93130f9e58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0012018680572509766\n",
            "tensor([0.2797, 0.2218, 0.2731, 0.3268, 0.2632, 0.2914, 0.3217, 0.2845])\n"
          ]
        }
      ],
      "source": [
        "# @title test init norm\n",
        "print(agent.emb.state_dict()['weight'].norm(dim=-1))\n",
        "\n",
        "# x = torch.rand(16)\n",
        "x = torch.rand(8,16)\n",
        "# print(x)\n",
        "# torch.nn.init.normal_(x, mean=0.0, std=1.0)\n",
        "# torch.nn.init.xavier_normal_(x)\n",
        "import time\n",
        "start = time.time()\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# with torch.no_grad(): x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # 0.00966, 0.000602, 0.0004\n",
        "# torch.nn.init.normal_(x, mean=0.0, std=1./x.shape[-1]**0.5)\n",
        "torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "print(time.time()-start)\n",
        "# std = ((Sum (xi-mean)^2)/ N)^(1/2)\n",
        "# print(x)\n",
        "# print(((x**2).sum())**(0.5))\n",
        "print(torch.norm(x, dim=-1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B1yvJkX89C_o"
      },
      "outputs": [],
      "source": [
        "# @title wasserstein\n",
        "import torch\n",
        "\n",
        "def wasserstein(x, y, weight=1):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "    cdf_x, cdf_y = x.cumsum(dim=-1), y.cumsum(dim=-1)\n",
        "    dist = weight * torch.abs(cdf_x - cdf_y) # Wasserstein dist = L1 norm between CDFs\n",
        "    # dist = weight * (cdf_x - cdf_y)**2 # me\n",
        "    return dist.sum()\n",
        "\n",
        "def wasserstein(x, y, weight=1):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "    # cdf_x, cdf_y = x.cumsum(dim=-1), y.cumsum(dim=-1)\n",
        "    # dist = weight * torch.abs(cdf_x - cdf_y) # Wasserstein dist = L1 norm between CDFs\n",
        "    # cs = (x-y).cumsum(dim=-1)\n",
        "    cs = (x-y) @ torch.tril(torch.ones(x.shape[0], x.shape[0]))\n",
        "    # dist = weight * torch.abs(cs)\n",
        "    dist = weight * cs**2\n",
        "    # dist = weight * (cdf_x - cdf_y)**2 # me\n",
        "    return dist.sum()\n",
        "\n",
        "\n",
        "def soft_wasserstein_loss(x, y, smoothing=0.1):\n",
        "    # Normalise distributions\n",
        "    x = x / x.sum()\n",
        "    y = y / y.sum()\n",
        "    # Compute the cumulative distributions (CDFs) with a small smoothing factor\n",
        "    cdf_x = torch.cumsum(x, dim=-1) + smoothing\n",
        "    cdf_y = torch.cumsum(y, dim=-1) + smoothing\n",
        "    # Compute smooth Wasserstein distance (L2 distance between CDFs)\n",
        "    distance = torch.norm(cdf_x - cdf_y, p=2)  # L2 distance instead of L1 for smoother gradients\n",
        "    return distance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float32)\n",
        "# x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5], dtype=float))\n",
        "x = nn.Parameter(torch.tensor([-0.01, -0.0, -0.99], dtype=torch.float))\n",
        "y = torch.tensor([0.0, 0.0, -1.0], dtype=torch.float)\n",
        "\n",
        "# x = nn.Parameter(torch.rand(1024, dtype=float))\n",
        "# y = torch.rand(1024, dtype=float)\n",
        "# a = len(train_data.buffer)/len(train_data.data) # ratio dided/tt steps\n",
        "a=1/45\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "weight = torch.where(y < -0.5, 1/a, 1/(1-a))\n",
        "print(weight)\n",
        "dist = wasserstein(x, y, weight=weight)\n",
        "print(time.time() - start)\n",
        "print(dist)  # Should output 0.7\n",
        "# dist.backward()\n",
        "\n",
        "# 0.0004496574401855469\n",
        "# 0.000331878662109375\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3nfZRhVc9Ssp"
      },
      "outputs": [],
      "source": [
        "# @title wasserstein sinkhorn train\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# agent.eval()\n",
        "# batch_size, T, _ = sx.shape\n",
        "x = nn.Parameter(torch.tensor([0,0,-1,0,0,0,-0.1, 0], device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3) # 3e3\n",
        "# optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.999)) # ? 1e0 ; 3e-2 1e-1\n",
        "# optim = torch.optim.AdamW([x], 1e-0, (0.9, 0.95)) # ? 1e0 ; 3e-2 1e-1\n",
        "y = torch.tensor([0,0,0,0,0,0,-1,0], dtype=torch.float)\n",
        "a=1/45\n",
        "weight = torch.where(y < -0.5, 1/a, 1/(1-a))\n",
        "# print(weight)\n",
        "\n",
        "# loss = wasserstein(x, y, weight=weight)\n",
        "# loss = wasserstein(x, y)\n",
        "# loss = sinkhorn(x, y)\n",
        "# loss.backward()\n",
        "# print(x.grad)\n",
        "\n",
        "\n",
        "for i in range(50): # num epochs\n",
        "    loss = wasserstein(x, y, weight=weight)\n",
        "    # loss = sinkhorn(x, y)\n",
        "    # loss = sinkhorn(x, y,0.05,80)\n",
        "    loss.sum().backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(x.data, loss.item())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def sinkhorn(x, y, epsilon=0.05, max_iters=100):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "\n",
        "    # Compute the cost matrix: here the cost is the squared distance between indices\n",
        "    # (|i-j|^2 for each position i, j)\n",
        "    posx = torch.arange(x.shape[-1], dtype=torch.float).unsqueeze(1)\n",
        "    posy = torch.arange(y.shape[-1], dtype=torch.float).unsqueeze(0)\n",
        "    cost_matrix = (posx - posy).pow(2)  # squared distance\n",
        "\n",
        "    # Initialize the dual variables\n",
        "    u = torch.zeros_like(x)\n",
        "    v = torch.zeros_like(y)\n",
        "\n",
        "    # Sinkhorn iterations\n",
        "    K = torch.exp(-cost_matrix / epsilon)  # Kernel matrix, regularised with epsilon\n",
        "    for _ in range(max_iters):\n",
        "        u = x / (K @ (y / (K.t() @ u + 1e-8)) + 1e-8)\n",
        "        v = y / (K.t() @ (x / (K @ v + 1e-8)) + 1e-8)\n",
        "    # print(K,u.data,v.data)\n",
        "    plan = torch.diag(u) @ K @ torch.diag(v)\n",
        "    dist = torch.sum(plan * cost_matrix)\n",
        "    return dist\n",
        "\n",
        "# Example\n",
        "x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float, requires_grad=True)\n",
        "y = torch.tensor([0.0, 0.0, 1.0], dtype=torch.float)\n",
        "# x = nn.Parameter(torch.tensor([0,0,-1,0,0,0,-0.1, 0], device=device))\n",
        "# y = torch.tensor([0,0,0,0,0,0,-1,0], dtype=float)\n",
        "\n",
        "# dist = sinkhorn(x, y)\n",
        "dist = sinkhorn(x, y, 0.05,80)\n",
        "dist.backward()  # To compute gradients with respect to x\n",
        "\n",
        "print(dist.item())\n",
        "print(x.grad)\n",
        "\n",
        "# [2.0000e+07, 3.0000e+07, 1.0000e-08]) tensor([       0.,        0., 49999996.] episodes>=80\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s1_GgDzoDyYB"
      },
      "outputs": [],
      "source": [
        "# @title torchrl.data.PrioritizedReplayBuffer\n",
        "from torchrl.data import LazyMemmapStorage, LazyTensorStorage, ListStorage\n",
        "buffer_lazytensor = ReplayBuffer(storage=LazyTensorStorage(size))\n",
        "\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "buffer_lazymemmap = ReplayBuffer(storage=LazyMemmapStorage(size), batch_size=32, sampler=SamplerWithoutReplacement())\n",
        "\n",
        "\n",
        "from torchrl.data import ListStorage, PrioritizedReplayBuffer\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "rb = PrioritizedReplayBuffer(alpha=0.7, beta=0.9, storage=ListStorage(10))\n",
        "data = range(10)\n",
        "rb.extend(data)\n",
        "# rb.extend(buffer)\n",
        "\n",
        "\n",
        "sample = rb.sample(3)\n",
        "print(sample)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt_UlGz6Xoq3"
      },
      "source": [
        "## plot 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VX5IExbRriwm"
      },
      "outputs": [],
      "source": [
        "# @title sklearn RBF\n",
        "# https://gist.github.com/eljost/2c4e1af652ef02b2989da341c5569af7\n",
        "# from nn_plot.ipynb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import scipy.stats as st\n",
        "\n",
        "# np.random.seed(1)\n",
        "def func(x):\n",
        "    # print(x.shape)\n",
        "    # x= np.sum(x**2, axis=-1)\n",
        "    x=np.random.rand(x.shape[0])\n",
        "    print(x.shape)\n",
        "    return x\n",
        "\n",
        "res = 50\n",
        "num_pts=15\n",
        "X=np.random.rand(num_pts,2)*res\n",
        "# Y = func(X)\n",
        "Y=np.random.rand(num_pts)\n",
        "# print(X);print(Y)\n",
        "\n",
        "lim = 1\n",
        "# lin = np.linspace(-lim, lim, res)\n",
        "lin = np.linspace(0, res, res)\n",
        "x1, x2 = np.meshgrid(lin, lin)\n",
        "xx = np.vstack((x1.flatten(), x2.flatten())).T\n",
        "\n",
        "kernel = RBF()\n",
        "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "gp.fit(X, Y)\n",
        "# print(\"Learned kernel\", gp.kernel_)\n",
        "y_mean, y_cov = gp.predict(xx, return_cov=True)\n",
        "\n",
        "posteriors = st.multivariate_normal.rvs(mean=y_mean, cov=y_cov, size=1)\n",
        "\n",
        "ax = plt.figure().add_subplot(projection='3d')\n",
        "Z=posteriors.reshape(-1, res)\n",
        "# ax.plot_surface(x1, x2, Z)\n",
        "ax.plot_surface(x1, x2, Z, cmap='rainbow', alpha=0.7)\n",
        "\n",
        "# ax.plot_surface(x1, x2, posteriors.reshape(-1, res))\n",
        "ax.contour(x1, x2, Z, zdir='z', offset=-1, cmap='coolwarm') # https://matplotlib.org/stable/gallery/mplot3d/contour3d_3.html#sphx-glr-gallery-mplot3d-contour3d-3-py\n",
        "# ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-0.4, 0.5))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-1, 2))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, c=zdata, cmap='Greens');\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, cmap='Greens');\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SusX7gpzxFNL",
        "outputId": "9f14a9da-e188-49ba-f5f5-70192ff33134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-2.6321], grad_fn=<AddBackward0>)\n",
            "tensor([2.7358]) tensor([-4.7358])\n"
          ]
        }
      ],
      "source": [
        "# @title chatgpt RBFKernelLayer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RBFKernelLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(RBFKernelLayer, self).__init__()\n",
        "        self.centres = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        dists = torch.cdist(x, self.centres, p=2) ** 2\n",
        "        return torch.exp(-dists / (2 * self.sigma ** 2))\n",
        "\n",
        "class SaddlePointNetwork(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(SaddlePointNetwork, self).__init__()\n",
        "        self.rbf_layer = RBFKernelLayer(in_features, out_features, sigma)\n",
        "        self.linear = nn.Linear(out_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rbf_output = self.rbf_layer(x)\n",
        "        # Introduce a saddle point structure\n",
        "        linear_output = self.linear(rbf_output)\n",
        "        # Example saddle function: x^2 - y^2\n",
        "        saddle_output = torch.sum(linear_output[:, :1]**2 - linear_output[:, 1:]**2, dim=1, keepdim=True)\n",
        "        return saddle_output\n",
        "\n",
        "# sin(ax)sin(bx)\n",
        "# (x^2 - y^2)\n",
        "import torch\n",
        "\n",
        "def rbf_saddle(x, y, gamma=1.0, a=1.0, b=1.0):\n",
        "    # RBF-like term\n",
        "    rbf_term = torch.exp(-gamma * torch.norm(x - y, p=2)**2)\n",
        "    # Saddle point term\n",
        "    saddle_term = (a * x)**2 - (b * y)**2\n",
        "    return rbf_term + saddle_term\n",
        "\n",
        "# Example usage\n",
        "x = torch.tensor([1.0], requires_grad=True)\n",
        "y = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "output = rbf_saddle(x, y)\n",
        "print(output)\n",
        "\n",
        "# Compute gradients\n",
        "output.backward()\n",
        "print(x.grad, y.grad)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rTmCo7pm0NxL"
      },
      "outputs": [],
      "source": [
        "# @title plot 3d\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "x = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "y = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "X, Y = torch.meshgrid(x, y)\n",
        "Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7fWZaQTDFg1",
        "outputId": "4c5ced88-54f1-436e-89f9-66f1c8396373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000, -0.7231,  0.3792,  0.0000]]) tensor([0.3362])\n",
            "tensor(0.0035, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# @title shape NN\n",
        "num_pts=1\n",
        "\n",
        "# X=torch.rand(num_pts,4)*2-1\n",
        "# X=torch.cat([torch.tensor([0,0]).unsqueeze(0),torch.rand(num_pts,2)*2-1], dim=-1)\n",
        "X=torch.cat([torch.zeros(1,1),torch.rand(num_pts,2)*2-1,torch.zeros(1,1)], dim=-1)\n",
        "Y=torch.rand(num_pts)\n",
        "print(X,Y)\n",
        "optim = torch.optim.SGD(model.parameters(), 1e-1)\n",
        "\n",
        "# model.train()\n",
        "pred = model(X)\n",
        "# print(Y.shape,pred.shape)\n",
        "# loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "loss = F.mse_loss(Y, pred.squeeze(-1))\n",
        "loss.backward()\n",
        "optim.step()\n",
        "optim.zero_grad()\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "boDd__PE2sGy"
      },
      "outputs": [],
      "source": [
        "# @title plot NN\n",
        "\n",
        "xx = torch.linspace(-1, 1, 100)\n",
        "yy = torch.linspace(-1, 1, 100)\n",
        "X, Y = torch.meshgrid(xx, yy) # [100,100]\n",
        "xy = torch.cat([X.unsqueeze(-1), torch.zeros(X.shape+(2,)), Y.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "with torch.no_grad(): Z = model(xy).squeeze(-1)\n",
        "# print(Z)\n",
        "# print(Z.shape)\n",
        "\n",
        "# Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qW6BYoXsX57o"
      },
      "outputs": [],
      "source": [
        "# @title test optim saddle same time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def f(x, y):\n",
        "    return x ** 2 - y ** 2 + x * y\n",
        "# (x-y)(x+y)+xy\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "dim_x, dim_z = 3, 8\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "xx = torch.empty((1, T, dim_x))\n",
        "torch.nn.init.xavier_uniform_(xx)\n",
        "# x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "\n",
        "# tensor([[0.6478, 0.0531, 0.0861]]) tensor([[-1.,  1.]]) 0.2974517047405243\n",
        "# tensor([-0.9419, -1.0000,  0.4416, -1.0000,  1.0000,  0.2963])\n",
        "\n",
        "# x = nn.Parameter(torch.tensor([[0.6478, 0.0531, 0.0861]]))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "z = nn.Parameter(torch.empty((batch, T, dim_z)))\n",
        "# z = torch.empty((1, T, 1))\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# z = nn.Parameter(z.repeat(batch,1,1))\n",
        "# z = nn.Parameter(torch.tensor([[-1.,  1.]]))\n",
        "# optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "# optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.95), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "# .95,1e-1,3e-1\n",
        "# .99,\n",
        "\n",
        "d_model = 4\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(10): # num epochs\n",
        "    # loss = f(x,z)\n",
        "    # loss = f(x.sum(-1),z)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step(); optim_z.step()\n",
        "    optim_x.zero_grad(); optim_z.zero_grad()\n",
        "    # print(i,x.squeeze(), z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    print(i,x.squeeze()[0].data, z[0].squeeze().data, loss[0].squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "        z.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "print(loss.squeeze())\n",
        "idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "# idx = torch.argmax(loss)\n",
        "# print(x[idx],z[idx],loss[idx])\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GJdFpDr2wIMT"
      },
      "outputs": [],
      "source": [
        "# @title test optim saddle argm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def argm(sx, lr=3e3): # 3e3\n",
        "    # batch=sx.size(dim=0)\n",
        "    batch_size, T, _ = sx.shape\n",
        "    batch = 16\n",
        "    # z = nn.Parameter(torch.zeros((batch,1),device=device))\n",
        "    # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "    z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "    torch.nn.init.xavier_uniform_(z)\n",
        "    # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "    # sx = sx.detach()\n",
        "    for i in range(20): # 10\n",
        "        # print(sx.shape,z.shape)\n",
        "        sxz = torch.cat([sx, z], dim=-1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            cost = model(sxz)\n",
        "        cost.sum().backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "        # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    # return z.detach()\n",
        "    print(cost.squeeze().data)\n",
        "    idx = torch.argmax(cost.squeeze(), dim=1)\n",
        "    return z[torch.arange(z.shape[0]),idx].detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "# x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# x = nn.Parameter(xx.clone())\n",
        "x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "# ratio = 6e0\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "# print(x.shape)\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(50):\n",
        "    z = argm(x)\n",
        "    # print(x.shape,z.shape)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "# print(\"z\",z)\n",
        "# print(loss.squeeze())\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvjIJP6RlEv2",
        "outputId": "447fdefd-452b-437d-c228-1847492b36f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 10])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x = torch.randn(16, 16)\n",
        "# print((b==torch.max(b)).nonzero())\n",
        "x = torch.randn(10, 3)\n",
        "idx = torch.randint(3,(10,))\n",
        "# print(x[:,idx].shape)\n",
        "print(x[torch.arange(x.shape[0]),idx].shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gJ3X_hQelW2x"
      },
      "outputs": [],
      "source": [
        "# @title train\n",
        "\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[3],d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model,10),\n",
        "        )\n",
        "\n",
        "        mul=4\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, 10),\n",
        "            )\n",
        "    # def forward(self, x): return self.cnn(x)\n",
        "\n",
        "model = Agent(d_model=256).to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.no_grad(): sx = model.cnn(image)\n",
        "        # print(sx.shape, r.shape)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(model.tcost(sx), r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        # except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wi4ODp-XlZoU"
      },
      "outputs": [],
      "source": [
        "# @title mnist data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 64 # 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QYbOgNoZn6JL"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# model = get_res(10).to(device)\n",
        "# model = get_res(2).to(device)\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model,1),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "model=Agent().to(device)\n",
        "\n",
        "\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            pred = model(image).squeeze(-1) # squeeze impt for regression!!!\n",
        "            # print(pred.shape, r.shape)\n",
        "            loss = loss_function(pred, r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n",
        "\n",
        "    images,r = next(iter(train_loader))\n",
        "    with torch.no_grad():\n",
        "        # pred = model(images.to(device)).argmax(-1).cpu()\n",
        "        pred = model(images.to(device)).squeeze(-1).cpu()\n",
        "        print(r)\n",
        "        print(pred)\n",
        "        print((r==pred).sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_kcajtpjr7Io"
      },
      "outputs": [],
      "source": [
        "# @title bin clss\n",
        "# def train(model, train_loader, loss_function, optimizer):\n",
        "#     model.train()\n",
        "#     for image, _ in train_loader:\n",
        "#         image = image.to(device)#.reshape(-1, 28*28)\n",
        "#         reconstructed = model(image)\n",
        "#         loss = loss_function(reconstructed, image)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "# class Agent(torch.nn.Module):\n",
        "#     def __init__(self, d_model = 256):\n",
        "#         super().__init__()\n",
        "#         # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "#         d_list=[32, 64, 128, 256, 256, 256] #\n",
        "#         # d_list = [min(d, d_model) for d in d_list]\n",
        "#         self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "#             nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "#             # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "#             nn.Flatten(start_dim=1),\n",
        "#             # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "#             nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "#             nn.Linear(d_model,1),\n",
        "#         )\n",
        "#     def forward(self, x): return self.cnn(x)\n",
        "\n",
        "d_model = 256\n",
        "# tcost = nn.Sequential( # trained cost\n",
        "#     # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, 1),\n",
        "#     ).to(device)\n",
        "mul=4\n",
        "tcost = nn.Sequential( # trained cost\n",
        "    # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "    nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, 2),\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-2, (0.9, 0.95))\n",
        "optim = torch.optim.AdamW(tcost.parameters(), 1e-3, (0.9, 0.95))\n",
        "# optim.param_groups[0][\"lr\"] = 1e-1\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# loss_function = torch.nn.MSELoss()\n",
        "# loss_function = torch.nn.L1Loss()\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "def train_cost(model, dataloader, optim, loss_function=loss_function):\n",
        "    model.train()\n",
        "    tcost.train()\n",
        "    for batch, (st, r) in enumerate(dataloader):\n",
        "        st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "        # st.requires_grad=True; r.requires_grad=True\n",
        "        # print(st.requires_grad, r.requires_grad)\n",
        "        # loss = F.mse_loss(model.tcost(model.jepa.enc(st)), r)\n",
        "        # print(model.jepa.enc(st))\n",
        "        # loss = loss_function(model.tcost(model.jepa.enc(st)), r)\n",
        "        with torch.no_grad(): sx = model.jepa.enc(st)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(tcost(sx), r)\n",
        "        # print(tcost(sx).squeeze(-1))\n",
        "        # loss = loss_function(model(st), r)\n",
        "        # print(next(model.tcost[0].parameters()).grad)\n",
        "        # print(next(model.jepa.enc.parameters()).grad)\n",
        "        # print(model.tcost.parameters()[0].grad)\n",
        "        # print(loss)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        optim.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "\n",
        "# for i in range(30):\n",
        "#     train_cost(agent, c_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Su8Op3bw0OIT"
      },
      "outputs": [],
      "source": [
        "# @title train_ae\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # jloss = std_loss + cov_loss\n",
        "\n",
        "                    # state_ = self.deconv(sy.detach()) # pure jepa\n",
        "                    state_ = self.deconv(sy) # ae\n",
        "                    # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "                    loss = conv_loss\n",
        "\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "    def get_down(self, state, world_state=None): # update world_state and mem from state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        return world_state\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    world_state = self.get_down(state, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "                    sy = self.effnet(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "\n",
        "                    world_state_ = self.deconvenc(sy).squeeze(1) # ae\n",
        "                    # loss = F.mse_loss(world_state_, world_state)\n",
        "                    loss = F.mse_loss(world_state_, world_state.detach())\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0u9XYJvdIf6p"
      },
      "outputs": [],
      "source": [
        "# @title dataloader from transformer\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return state, action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, buffer, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.data = [step for episode in buffer for step in episode]\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "        seq_len = data.size(0) // batch_size\n",
        "        data = data[:seq_len * batch_size]\n",
        "        # data = data.view(bsz, seq_len).t().contiguous()\n",
        "        data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "        # self.bptt = 35\n",
        "        # self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i = self.ind[index]\n",
        "        seq_len = min(self.bptt, len(self.data) - i)\n",
        "        data = self.data[i:i+seq_len]\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        # state, action, reward = zip(*sar)\n",
        "        # state = [self.transform(s) for s in state]\n",
        "        state, action, reward = self.data[idx]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return self.transform(state), action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "\n",
        "def collate_fn(sar):\n",
        "    # x,y=zip(*data)\n",
        "    state, action, reward = zip(*sar)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    # x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "    state=torch.stack(list(state), dim=0)\n",
        "    action=torch.stack(list(action), dim=0)\n",
        "    reward=torch.stack(list(reward), dim=0)\n",
        "    # y=torch.stack(list(y)).T.flatten()\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title Datasetme\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data, batch_size):\n",
        "#         data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.batch_size = batch_size\n",
        "\n",
        "#         seq_len = data.size(0) // batch_size\n",
        "#         data = data[:seq_len * batch_size]\n",
        "#         # data = data.view(bsz, seq_len).t().contiguous()\n",
        "#         data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.data.size(0) // self.batch_size\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         i = self.ind[index]\n",
        "#         seq_len = min(self.bptt, len(self.data) - i)\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         return data\n",
        "\n",
        "\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data):\n",
        "#         self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data) // self.bptt\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         i=idx*self.bptt\n",
        "#         seq_len = self.bptt\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         target = self.data[i+1:i+1+seq_len].reshape(-1)\n",
        "#         return data, target\n",
        "\n",
        "# train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "# batch_size=128\n",
        "# train_iter = Datasetme(train_iter)\n",
        "# # train_loader = Datasetme(train_iter, batch_size)\n",
        "\n",
        "\n",
        "# def collate_fn(data):\n",
        "#     x,y=zip(*data)\n",
        "#     # print(\"collate\",len(x),len(y))\n",
        "#     x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "#     y=torch.stack(list(y)).T.flatten()\n",
        "#     return x, y\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# seq_len = 50 # 50\n",
        "batch_size = 64 #512\n",
        "train_data = BufferDataset(buffer, batch_size)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True,collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cX71EprCMSNG"
      },
      "outputs": [],
      "source": [
        "# @title transfer_optim bad?\n",
        "\n",
        "import torch\n",
        "\n",
        "def transfer_optim(src_optim, tgt_optim, param_mapping):\n",
        "    src_sd = src_optim.state_dict()\n",
        "    tgt_sd = tgt_optim.state_dict()\n",
        "\n",
        "    # Iterate over each parameter in the target optimizer\n",
        "    for (tgt_idx, target_param) in enumerate(tgt_optim.param_groups[0]['params']):\n",
        "        target_id = id(target_param)\n",
        "\n",
        "        # Find the corresponding source parameter using param_mapping\n",
        "        if target_id in param_mapping:\n",
        "            source_param = param_mapping[target_id]\n",
        "            source_id = id(source_param)\n",
        "\n",
        "            # If there's an existing state for the source parameter, transfer it\n",
        "            if source_id in src_sd['state']:\n",
        "                source_state = src_sd['state'][source_id]\n",
        "                target_state = {}\n",
        "\n",
        "                # Handle momentum/first and second moments (e.g., `exp_avg`, `exp_avg_sq` in Adam)\n",
        "                for key in source_state.keys():\n",
        "                    if source_state[key].shape == target_param.shape: target_state[key] = source_state[key].clone()\n",
        "                    # If size doesn't match, either copy what you can or initialise new values\n",
        "                    elif key in ['exp_avg', 'exp_avg_sq']:  # Momentums (specific to Adam-like optimizers)\n",
        "                        target_state[key] = torch.zeros_like(target_param)\n",
        "                        target_state[key][:source_param.numel()] = source_state[key].flatten()[:target_param.numel()]\n",
        "                    else: target_state[key] = torch.zeros_like(target_param) # init\n",
        "                tgt_sd['state'][target_id] = target_state\n",
        "\n",
        "    # Load the updated state dict back into the target optimizer\n",
        "    tgt_optim.load_state_dict(tgt_sd)\n",
        "    return tgt_optim\n",
        "# {'state': {0: {'step': tensor(1.), 'exp_avg': tensor, 'exp_avg_sq': tensor}, 1: }}\n",
        "\n",
        "\n",
        "\n",
        "model_src = torch.nn.Linear(10, 5)  # Example source model\n",
        "model_tgt = torch.nn.Linear(20, 5)  # Example target model (with more input dimensions)\n",
        "\n",
        "# model_src = nn.Sequential( # trained cost\n",
        "#     nn.Linear(10, 5, bias=False), nn.Softmax(),\n",
        "#     )\n",
        "# d_model=4\n",
        "# model_tgt = nn.Sequential( # trained cost\n",
        "#     nn.Linear(20, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, 5), nn.Softmax(),\n",
        "#     )\n",
        "\n",
        "source_optimizer = optim.AdamW(model_src.parameters())\n",
        "target_optimizer = optim.AdamW(model_tgt.parameters())\n",
        "\n",
        "dummy_input = torch.randn(3, 10)\n",
        "dummy_target = torch.randn(3, 5)\n",
        "criterion = torch.nn.MSELoss()\n",
        "output = model_src(dummy_input)\n",
        "loss = criterion(output, dummy_target)\n",
        "loss.backward()\n",
        "source_optimizer.step()\n",
        "\n",
        "param_mapping = {id(tgt_param): src_param for src_param, tgt_param in zip(model_src.parameters(), model_tgt.parameters())}\n",
        "target_optimizer = transfer_optim(source_optimizer, target_optimizer, param_mapping)\n",
        "\n",
        "print(source_optimizer.state_dict())\n",
        "print(target_optimizer.state_dict())\n",
        "\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl').values()\n",
        "\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title transfer_optim bad? 2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def transfer_optimizer_state(source_layer, target_layer, optimizer):\n",
        "    opt_state_dict = optimizer.state_dict()\n",
        "    for group in opt_state_dict['param_groups']:\n",
        "        # For each parameter index (p in param group refers to the layer parameters)\n",
        "        for param_idx, p in enumerate(group['params']):\n",
        "            print(p,source_layer.weight)\n",
        "            if p == source_layer.weight:\n",
        "                # Find the corresponding target layer parameter (in this case, target_layer.weight)\n",
        "                target_param = target_layer.weight\n",
        "                source_state = optimizer.state[p]  # Get the state for the source parameter\n",
        "\n",
        "                # If the parameter is found in the optimizer's state dict\n",
        "                if 'exp_avg' in source_state and 'exp_avg_sq' in source_state:\n",
        "                    exp_avg = source_state['exp_avg']  # First moment (momentum)\n",
        "                    exp_avg_sq = source_state['exp_avg_sq']  # Second moment (variance)\n",
        "\n",
        "                    # Handle input dimension mismatch (copy/truncate or pad)\n",
        "                    source_in_dim = source_layer.weight.shape[1]\n",
        "                    target_in_dim = target_layer.weight.shape[1]\n",
        "\n",
        "                    # Copy optimizer state (exp_avg and exp_avg_sq) accordingly\n",
        "                    with torch.no_grad():\n",
        "                        # Copy the available part and initialize new dimensions to zero\n",
        "                        new_exp_avg = torch.zeros_like(target_param)\n",
        "                        new_exp_avg_sq = torch.zeros_like(target_param)\n",
        "                        # new_exp_avg[:, source_in_dim:] = 0  # Initialize extra dimensions\n",
        "                        # new_exp_avg_sq[:, source_in_dim:] = 0  # Initialize extra dimensions\n",
        "                        new_exp_avg[:, :source_in_dim] = exp_avg[:, :target_in_dim]\n",
        "                        new_exp_avg_sq[:, :source_in_dim] = exp_avg_sq[:, :target_in_dim]\n",
        "\n",
        "                    # Update the target layer's optimizer state\n",
        "                    optimizer.state[target_param] = {\n",
        "                        'exp_avg': new_exp_avg,\n",
        "                        'exp_avg_sq': new_exp_avg_sq,\n",
        "                        'step': source_state['step']  # Keep the same step count\n",
        "                    }\n",
        "\n",
        "                # Handle the bias (if it exists)\n",
        "                if hasattr(source_layer, 'bias') and hasattr(target_layer, 'bias'):\n",
        "                    source_bias = optimizer.state[source_layer.bias]\n",
        "                    target_bias = target_layer.bias\n",
        "\n",
        "                    optimizer.state[target_bias] = source_bias\n",
        "    return optimizer\n",
        "\n",
        "# Example usage:\n",
        "d = 10  # Input dimension of the source layer\n",
        "a = 5   # Extra nodes to be omitted or added in the target layer\n",
        "m = 8   # Output dimension (same for both)\n",
        "\n",
        "# Source layer (input dimension d+a)\n",
        "source_layer = nn.Linear(d+a, m)\n",
        "\n",
        "# Target layer (input dimension d, or d+a, or arbitrary)\n",
        "target_layer = nn.Linear(d, m)\n",
        "\n",
        "# Optimizer (using AdamW in this case)\n",
        "optimizer = torch.optim.AdamW(source_layer.parameters())\n",
        "\n",
        "# Perform weight transfer (from d+a to d or vice versa) here (assumed done already)\n",
        "\n",
        "print(optimizer.state_dict())\n",
        "# Transfer optimizer states\n",
        "optimizer = transfer_optimizer_state(source_layer, target_layer, optimizer)\n",
        "print(optimizer.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_optimizer_state(source_layer, target_layer, optimizer):\n",
        "    state_dict = optimizer.state_dict()\n",
        "    for old_param, new_param in zip(source_layer.parameters(), target_layer.parameters()):\n",
        "        # If old_param exists in optimizer state\n",
        "        if old_param in state_dict['state']:\n",
        "            # Get the state for the old parameter\n",
        "            old_state = state_dict['state'][old_param]\n",
        "            new_state = {}\n",
        "\n",
        "            for key, value in old_state.items():\n",
        "                if key in ['exp_avg', 'exp_avg_sq']:  # for Adam or AdamW momentum estimates\n",
        "                    # Handle the shape adjustment (copy, shrink, or randomly initialise the extra nodes)\n",
        "                    new_state[key] = torch.zeros_like(new_param)  # Initialise with zeros\n",
        "                    new_state[key][:old_param.shape[0]] = value[:new_param.shape[0]]  # Copy old values\n",
        "                    # else:\n",
        "                    #     new_state[key] = value.clone()  # Copy directly if shapes match\n",
        "                else:\n",
        "                    new_state[key] = value  # Copy other states directly if they exist\n",
        "\n",
        "            # Set the new parameter in optimizer state\n",
        "            state_dict['state'][new_param] = new_state\n",
        "            # Remove the old parameter from the optimizer state\n",
        "            del state_dict['state'][old_param]\n",
        "\n",
        "    # Load the updated state dict into the optimizer\n",
        "    optimizer.load_state_dict(state_dict)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_optim(src_model, tgt_model, src_optim, tgt_optim):\n",
        "    src_sd = src_optim.state_dict()\n",
        "    for src_param, tgt_param in zip(src_model.parameters(), tgt_model.parameters()):\n",
        "        # If src_param exists in optimizer state\n",
        "        if src_param in src_sd['state']:\n",
        "            # Get the state for the old parameter\n",
        "            old_state = src_sd['state'][src_param]\n",
        "            new_state = {}\n",
        "\n",
        "            for key, value in old_state.items():\n",
        "                new_state[key] = torch.zeros_like(tgt_param)  # Initialise with zeros\n",
        "                new_state[key][:src_param.shape[0]] = value[:tgt_param.shape[0]]  # Copy old values\n",
        "\n",
        "            src_sd['state'][tgt_param] = new_state\n",
        "            del src_sd['state'][src_param]\n",
        "\n",
        "    # Load the updated state dict into the optimizer\n",
        "    optimizer.load_state_dict(src_sd)\n",
        "    return optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "LKUSzmYLLuRh",
        "outputId": "07ca4b89-257b-4205-c5c8-6a96474ae82a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-186620617543>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# j=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mwht_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwht_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# print(o)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "# @title rename wht_name\n",
        "# wht_name='jepa.enc.cnn.0.weight'\n",
        "wht_name='jepa.pred.weight_ih_l0'\n",
        "# wht_name='emb.weight'\n",
        "# print(o.isnumeric())\n",
        "# mask = [x.isnumeric() for x in o]\n",
        "# print(o[mask])\n",
        "na_=''\n",
        "# j=0\n",
        "\n",
        "for wht_name in agent.state_dict().keys():\n",
        "    o=wht_name.split('.')\n",
        "    # print(o)\n",
        "    name=wht_name\n",
        "    print(\"####\", wht_name)\n",
        "    for i in range(len(o)):\n",
        "        c = o[i]\n",
        "        if c.isnumeric():\n",
        "            na = '.'.join(o[:i])\n",
        "            me = '.'.join(o[i+1:])\n",
        "            # print(c_,c, c_<c, )\n",
        "            c=int(c)\n",
        "            if na!=na_: # param name diff\n",
        "                j=0 # reset num\n",
        "                c_=c # track wht_name num\n",
        "                na_=na # track param name\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('1', name)\n",
        "            elif c_<c: # same param name, diff num\n",
        "                j+=1\n",
        "                c_=c\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('2', name)\n",
        "            else: # same param name, same num\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('3', name)\n",
        "    print('4', name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CACQCCaxA_Y",
        "outputId": "b5d127cd-18ce-49e5-b1e2-d883cb34125a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.1746836772511624\n"
          ]
        }
      ],
      "source": [
        "# @title geomloss, Python Optimal Transport\n",
        "# !pip install geomloss[full]\n",
        "\n",
        "import torch\n",
        "from geomloss import SamplesLoss  # See also ImagesLoss, VolumesLoss\n",
        "\n",
        "# # Create some large point clouds in 3D\n",
        "# x = torch.randn(100000, 3, requires_grad=True).cuda()\n",
        "# y = torch.randn(200000, 3).cuda()\n",
        "\n",
        "# x = torch.rand(1000, 1)\n",
        "# y = torch.rand(1000, 1)\n",
        "x = torch.tensor([0, 0, 1]).float().unsqueeze(-1)\n",
        "y = torch.tensor([0, 1, 0]).float().unsqueeze(-1)\n",
        "# k=1.\n",
        "# y = torch.tensor([k, k, k]).float().unsqueeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "# loss_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.01) # 0.05, quadratic, Wasserstein-2. low blur => closer to true Wasserstein dist but slower compute\n",
        "\n",
        "loss = loss_fn(x, y)  # By default, use constant weights = 1/number of samples\n",
        "print(loss)\n",
        "# g_x, = torch.autograd.grad(L, [x])\n",
        "\n",
        "# [0, 1, 0]: 2.4253e-12, 2.4253e-12\n",
        "# [0, 0, 0.1]: 0.1350; [0, 0, 0.5]: 0.0417; [0, 0, 1]: 0\n",
        "# k=0.: 0.1666; k=0.1: 0.1383; k=0.333: 0.1111; k=0.5: 0.1250; k=1.: 0.3333\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from geomloss import SamplesLoss\n",
        "\n",
        "# Define x and y as n-dimensional tensors representing mass distributions\n",
        "# x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float32, requires_grad=True).cuda()\n",
        "# y = torch.tensor([0, 0, 1], dtype=torch.float32, requires_grad=True).cuda()\n",
        "# x = torch.tensor([0.2, 0.3, 0.5]).float().unsqueeze(-1)\n",
        "# x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5]).float().unsqueeze(-1))\n",
        "x = nn.Parameter(torch.tensor([0,1.5,0]).float().unsqueeze(-1))\n",
        "y = torch.tensor([0, 0, 1]).float().unsqueeze(-1)\n",
        "\n",
        "# Create a position tensor representing the index of each element\n",
        "positions_x = torch.arange(x.shape[0], dtype=float).unsqueeze(1)\n",
        "positions_y = torch.arange(y.shape[0], dtype=float).unsqueeze(1)\n",
        "\n",
        "# Sinkhorn loss using GeomLoss\n",
        "loss_fn = SamplesLoss(\"sinkhorn\", p=1, blur=0.05)  # p=1 for Wasserstein-1\n",
        "# loss_fn = SamplesLoss(loss=\"sinkhorn\", p=1, blur=0.05, scaling=0.9, debias=True)\n",
        "\n",
        "transport_cost = loss_fn(positions_x, x, positions_y, y)\n",
        "\n",
        "print(transport_cost.item())\n",
        "# 1.298424361328248\n",
        "\n",
        "transport_cost.backward()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pip install POT\n",
        "\n",
        "import ot\n",
        "import numpy as np\n",
        "\n",
        "def sinkhorn_distance(x, y, reg=0.01):\n",
        "    # x = x / np.sum(x)\n",
        "    # y = y / np.sum(y)\n",
        "    # Create the cost matrix (1D example, Euclidean distances between positions)\n",
        "    n = len(x)\n",
        "    cost_matrix = np.abs(np.arange(n)[:, None] - np.arange(n)[None, :])\n",
        "    # print(cost_matrix)\n",
        "    # # Compute Sinkhorn distance using POT's Sinkhorn algorithm\n",
        "    print(x, y, cost_matrix, reg)\n",
        "    transport_plan = ot.sinkhorn(x, y, cost_matrix, reg)\n",
        "    print(transport_plan)\n",
        "    distance = np.sum(transport_plan * cost_matrix)\n",
        "    return distance\n",
        "\n",
        "x = np.array([0.2, 0.3, 0.5])\n",
        "y = np.array([0, 0, 1])\n",
        "distance = sinkhorn_distance(x, y)\n",
        "print(f'Sinkhorn distance: {distance}')\n",
        "# distance.backward()\n",
        "\n",
        "def sinkhorn_distance(x, y, reg=0.01):\n",
        "    # x = x / torch.sum(x)\n",
        "    # y = y / torch.sum(y)\n",
        "    # Create the cost matrix (1D example, Euclidean distances between positions)\n",
        "    n = len(x)\n",
        "    cost_matrix = torch.abs(torch.arange(n)[:, None] - torch.arange(n)[None, :])\n",
        "    # print(cost_matrix)\n",
        "    # Compute Sinkhorn distance using POT's Sinkhorn algorithm\n",
        "    print(x, y, cost_matrix, reg)\n",
        "    transport_plan = ot.sinkhorn(x, y, cost_matrix, reg)\n",
        "    print(transport_plan)\n",
        "    distance = torch.sum(transport_plan * cost_matrix)\n",
        "    return distance\n",
        "\n",
        "# x = np.array([0.2, 0.3, 0.5])\n",
        "# y = np.array([0, 0, 1])\n",
        "x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5]).float())#.unsqueeze(-1))\n",
        "y = torch.tensor([0, 0, 1]).float()#.unsqueeze(-1)\n",
        "\n",
        "distance = sinkhorn_distance(x, y)\n",
        "print(f'Sinkhorn distance: {distance}')\n",
        "distance.backward()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MqBL9hljvW-5"
      },
      "outputs": [],
      "source": [
        "# @title batchify argm train\n",
        "\n",
        "def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "    self.jepa.pred.train()\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "    lsx=sx.unsqueeze(1)\n",
        "    h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "    lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "    # print(lsx.shape, la.shape, lz.shape)\n",
        "    c=[]\n",
        "    for t in range(seq_len):\n",
        "        a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "        # print(sx.shape, a.shape, z.shape)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "            sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "            syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            tcost = -self.tcost(syh0)\n",
        "        c.append(tcost)\n",
        "        lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "        lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "        icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        cost += (tcost + icost)*gamma**t\n",
        "    return cost, lsx, lh0, c\n",
        "\n",
        "\n",
        "\n",
        "def argm(self, sy, sy_, h0, a, reward, lr=3e3): # 3e3\n",
        "    self.tcost.eval()\n",
        "    batch_size = sy.shape[0] # [batch_size, d_model]\n",
        "    z = nn.Parameter(torch.zeros((batch_size, self.dim_z), device=device))\n",
        "    # torch.nn.init.xavier_uniform_(z)\n",
        "    torch.nn.init.normal_(z, mean=0., std=.3/z.shape[-1]**0.5)\n",
        "    # optim = torch.optim.SGD([z], lr=1e-2) # 1e-2\n",
        "    # optim = torch.optim.AdamW([z], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "    optim = torch.optim.AdamW([z], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "    sy, sy_ = sy.detach(), sy_.detach()\n",
        "    out = sy - sy_\n",
        "    h0, a, reward = h0.detach(), a.detach(), reward.detach()\n",
        "    for i in range(10): # 10\n",
        "        with torch.amp.autocast('cuda'):\n",
        "\n",
        "\n",
        "\n",
        "            syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "            out_, h0_ = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "            repr_loss = F.mse_loss(out, out_[:, -1, :])\n",
        "            # syh0 = torch.cat([sy.flatten(1),F.dropout(h0_, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            syh0 = torch.cat([sy.flatten(1),h0_.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "            z_loss = torch.abs(z).sum() # z_loss = torch.norm(z)\n",
        "            print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl + self.zloss_coeff * z_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        cost.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        with torch.no_grad(): z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    return z.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def argm(lsy, sy, h0, la, rwd):\n",
        "    # lz = agent.argm(out, h0, la, reward)\n",
        "    agent.tcost.eval()\n",
        "    batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "    lz = nn.Parameter(torch.zeros((batch_size, bptt, agent.dim_z), device=device))\n",
        "    # torch.nn.init.xavier_uniform_(lz)\n",
        "    torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "    # optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "    # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "    optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "    lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "    for i in range(3): # 10\n",
        "        sy_, h0_ = sy.detach(), h0.detach()\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        lsy_ = torch.empty((batch_size, 0, agent.d_model), device=device) # [batch_size, T, d_model]\n",
        "        with torch.cuda.amp.autocast():\n",
        "            for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                out_, h0_ = agent.jepa.pred(syaz.unsqueeze(1), h0_) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                lh0 = torch.cat((lh0, h0_.unsqueeze(0)), dim=0)\n",
        "            repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "            clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "            z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "            # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = agent.jepa.sim_coeff * repr_loss + agent.closs_coeff * clossl + agent.zloss_coeff * z_loss\n",
        "        cost.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    agent.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    return lz.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# closs_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.01)\n",
        "bptt = 25\n",
        "for batch, Sar in enumerate(train_loader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "    h0 = torch.zeros((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "    state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "    sy_ = agent.jepa.enc(state).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    # sx=sy_\n",
        "    state, action, reward = Sar # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "    state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "\n",
        "    for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        lsy_ = torch.empty((batch_size, 0, agent.d_model), device=device) # [batch_size, T, d_model]\n",
        "\n",
        "        with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n",
        "            lsy = agent.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "            la = agent.emb(act) # [batch_size, bptt, dim_a]\n",
        "            out = lsy - torch.cat([sy_, lsy[:,:-1]], dim=1)\n",
        "            # lz = agent.argm(out, h0, la, reward)\n",
        "            lz = argm(lsy, sy_, h0, la, rwd)\n",
        "            # lz = torch.zeros((batch_size, bptt, agent.dim_z), device=device)\n",
        "\n",
        "            for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                out_, h0 = agent.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                lh0 = torch.cat((lh0, h0.unsqueeze(0)), dim=0)\n",
        "\n",
        "            repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            std_loss, cov_loss = agent.jepa.v_creg(agent.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "            jloss = agent.jepa.sim_coeff * repr_loss + agent.jepa.std_coeff * std_loss + agent.jepa.cov_coeff * cov_loss\n",
        "\n",
        "            syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "            # print(\"syh0, rwd\",syh0.shape,rwd.shape)\n",
        "            clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "            # reward_ = agent.tcost(syh0)\n",
        "            # clossl = wasserstein(rwd, reward_)#.squeeze(-1)\n",
        "            closs = agent.closs_coeff * clossl\n",
        "\n",
        "            # print(h0.requires_grad)\n",
        "            # pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "            # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "            # print(\"reward, pred, clossl\", reward[mask].data, pred[mask].data, clossl.item())\n",
        "            # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "            # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "            loss = jloss + closs\n",
        "\n",
        "            # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "            norm = torch.norm(lsy, dim=-1)[0][0].item()\n",
        "            z_norm = torch.norm(z)\n",
        "            # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "            # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "            print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "            scaler.scale(loss).backward()\n",
        "            # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "            scaler.step(optim)\n",
        "            scaler.update()\n",
        "            optim.zero_grad()\n",
        "            sy_, h0 = sy_.detach(), h0.detach()\n",
        "    break\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "mhkK_9AQm8_q",
        "Jt_UlGz6Xoq3",
        "wUhKd009Qvk3"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}