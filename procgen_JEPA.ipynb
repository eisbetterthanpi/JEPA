{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/procgen_JEPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7WkwnVjJTrW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00453a2a-90f6-40e7-985a-ae6efe7859dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -qq procgen\n",
        "# !pip install -qq procgen faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKlOoBh8yHXA"
      },
      "outputs": [],
      "source": [
        "# @title faiss\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# d = 256 # dimension\n",
        "# res = faiss.StandardGpuResources()  # use a single GPU\n",
        "# nlist = 100\n",
        "# m = 8\n",
        "# index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "# index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "# # index = faiss.IndexIVFPQ(index, d, nlist, m, 8) # each sub-vector is encoded as 8 bits # 3-IVFPQ.py\n",
        "# # index = faiss.index_cpu_to_gpu(res, 0, index) # 4-GPU.py\n",
        "# # index = faiss.index_cpu_to_all_gpus(index) # 5-Multiple-GPUs.py\n",
        "\n",
        "\n",
        "# import torch\n",
        "# ltmk = torch.rand(1000,d)\n",
        "# ltmv = torch.rand(1000,d)\n",
        "\n",
        "def makefaissindex(vert_store):\n",
        "    d = vert_store.shape[-1]\n",
        "    nlist = 100\n",
        "    index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "    index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "    if not index.is_trained: index.train(vert_store)\n",
        "    index.add(vert_store)\n",
        "    return index\n",
        "# index = makefaissindex(ltmk)\n",
        "\n",
        "\n",
        "def vecsearch(query, index, k=5, treshold=36): # k nearest neighbors\n",
        "    # index.nprobe = 5 # 1\n",
        "    D, I = index.search(query, k) # dist, idx\n",
        "    D, I = D[0], I[0]\n",
        "    mask = I[D<treshold]\n",
        "    return mask\n",
        "\n",
        "# import torch\n",
        "# query = torch.rand(1,d)\n",
        "\n",
        "# mask = vecsearch(query, index, k=5, treshold=37)\n",
        "# print(mask)\n",
        "# rag = ltmk[mask]\n",
        "# print(rag)\n",
        "\n",
        "\n",
        "# removing = torch.tensor([998, 769, 643])\n",
        "# index.remove_ids(removing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WXm1sGiK1oQS"
      },
      "outputs": [],
      "source": [
        "# @title mem\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self): # [batch_size, len_ltm, d_model]\n",
        "        self.stmk, self.stmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "\n",
        "    def __call__(self, query): # [batch_size, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, 1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, 1, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        return x.squeeze(1) # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, 1, d_model]\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "\n",
        "\n",
        "class Mem():\n",
        "    def __init__(self, batch_size=1):\n",
        "        self.index = None\n",
        "        self.ltmk, self.ltmv = torch.tensor([]), torch.tensor([])\n",
        "        # self.stmk, self.stmv, self.meta = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
        "        # self.ltmk, self.ltmv = torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.stmk, self.stmv, self.meta = torch.tensor([], device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __call__(self, query, a=0.5):\n",
        "        return a*self.Stm(query) + (1-a)*self.Ltm(query.cpu()).to(device)\n",
        "\n",
        "    def Stm(self, query): # [1, d_model]\n",
        "        if len(self.stmk)==0: return torch.zeros((1), device=device)\n",
        "        attn = query @ self.stmk.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ self.stmv # [1, len_ltm] @ [len_ltm, d_model] = [1, d_model]\n",
        "        self.meta = self.meta + attn.squeeze(0) # attention # [len_ltm]\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def Ltm(self, query, k=5, treshold=36): # [batch_size, d_model] or [d_model]\n",
        "        if self.index: rag = self.vecsearch(query, k, treshold)\n",
        "        else: rag = self.ltmk\n",
        "        if len(rag)==0: return torch.zeros(1)\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        attn = query @ rag.T # [1, d_model] @ [d_model, len_ltm] = [1, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [1, len_ltm]\n",
        "        x = attention @ rag\n",
        "        return x # [1, d_model]\n",
        "\n",
        "    def add(self, k, v): # [batch_size, d_model] or [d_model]\n",
        "        # print(\"add\", k.shape,self.stmk.shape)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=0)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=0)\n",
        "        self.meta = torch.cat([self.meta, torch.ones((1), device=device)], dim=-1)\n",
        "        if torch.rand(1)<0.1:\n",
        "            self.pop()\n",
        "            self.decay()\n",
        "\n",
        "    def decay(self, g=0.9, k=256): # remove unimportant mem in stm\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        if len(self.meta)>k:\n",
        "            topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "            self.meta = topk.values # cap stm size\n",
        "            self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5): # transfer from stm to ltm\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        k, v = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask] # remove from stm\n",
        "        self.meta = self.meta[~mask]\n",
        "        # print(\"pop\", k.shape, self.ltmk.shape, k)\n",
        "        k, v = k.cpu(), v.cpu()\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        self.ltmk = torch.cat([self.ltmk, k], dim=0) # add to ltm\n",
        "        self.ltmv = torch.cat([self.ltmv, v], dim=0)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.01:\n",
        "                self.index.train(self.ltmk)\n",
        "        else:\n",
        "            if len(self.ltmk)>=100:\n",
        "                self.index = makefaissindex(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        return rag\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(self, file='mem.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(self, file='mem.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv, self.stmk, self.stmv, self.meta = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "7DTSlle0RaQY"
      },
      "outputs": [],
      "source": [
        "# @title intrinsic cost\n",
        "# import faiss\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ICost():\n",
        "    def __init__(self, d_model, n=100):\n",
        "        self.recent=[]\n",
        "        # self.linmul = torch.linspace(0,1/n,n).unsqueeze(-1) # 1/n so that sum to 1\n",
        "        self.linsx = torch.zeros((n, d_model), device=device)\n",
        "        self.n = n\n",
        "        self.p=(n-1)/n\n",
        "\n",
        "    def boredom(self, lsx, linsx=None): # lsx: [len_seq, d_model]; for simulate only\n",
        "        if linsx==None: linsx = self.linsx.clone()\n",
        "        lsx, linsx = F.normalize(lsx, dim=-1), F.normalize(linsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        linsx = torch.cat([linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        linsx = linsx[mask]\n",
        "        bore = (linsx[:-1]@lsx[-1].T).sum()/(self.n-1)\n",
        "        return bore#.squeeze()\n",
        "\n",
        "    def update(self, lsx): # lsx: []\n",
        "        # self.linsx = torch.cat([lsx, self.linsx[:-lsx.shape[0]]], dim=0)\n",
        "        lsx = F.normalize(lsx, dim=-1)\n",
        "        len_seq = lsx.shape[0]\n",
        "        # print(\"update\", self.linsx.shape, lsx.shape)\n",
        "        linsx = torch.cat([self.linsx, lsx], dim=0)\n",
        "        weights = 1-self.p**torch.cat([torch.ones(self.n)*len_seq, torch.linspace(len_seq-1, 0, len_seq)], dim=0).float()\n",
        "        idx = torch.multinomial(weights, len_seq)\n",
        "        mask = torch.ones(self.n+len_seq, dtype=bool)\n",
        "        mask[idx] = False\n",
        "        self.linsx = linsx[mask]\n",
        "\n",
        "\n",
        "    # def curiousity(self, sx):\n",
        "    #     lin= nn.Linear(d_model, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "\n",
        "    #         n,d=10,2\n",
        "    #         data=torch.rand(n,d)\n",
        "\n",
        "    #         index = faiss.IndexFlatIP(d) # IndexFlatL2, IndexFlatIP\n",
        "    #         index = faiss.IndexIDMap(index)\n",
        "    #         ids=torch.arange(n)\n",
        "    #         index.add_with_ids(data,ids)\n",
        "    #         a=torch.rand(1,2)\n",
        "    #         id=torch.tensor([0])\n",
        "    #         index.remove_ids(id) # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#supported-operations\n",
        "    #         index.add_with_ids(a,id)\n",
        "\n",
        "    #         D, I = index.search(a, 20)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         curious = 1-torch.clamp(priority, 0, 1)\n",
        "    #         D.sum(-1)\n",
        "    #         curious = 1-torch.clamp(, max=1) # IP\n",
        "\n",
        "\n",
        "    # def __call__(self, st, a): # [batch_size, d_model]\n",
        "    def __call__(self, x): # [batch_size, d_model**2]\n",
        "        return 0\n",
        "\n",
        "# pain, death, boredom, empathy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEY9MmwZhA8a",
        "outputId": "3b8086d8-9feb-4e14-f9a3-21240ae4dba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1278976\n",
            "torch.Size([4, 256])\n",
            "1278979\n",
            "torch.Size([4, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(torch.nn.Module):\n",
        "    def __init__(self, d_model=256, drop=0.5):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278976\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Dropout(p=drop),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "class Deconv(torch.nn.Module):\n",
        "    def __init__(self, d_model = 1024):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] # 1278979\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d_model,4*d_list[4]), nn.ReLU(),\n",
        "            # nn.Linear(d_list[5],4*d_list[4]), nn.ReLU(),\n",
        "            nn.Unflatten(-1, (d_list[4],2,2)),\n",
        "            # nn.Unflatten(-1, (d_list[5],1,1)),\n",
        "            # nn.ConvTranspose2d(d_list[5], d_list[4], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[4], d_list[3], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[3], d_list[2], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[2], d_list[1], 3, 2, 1, output_padding=1), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[1], d_list[0], 5, 2, 2, output_padding=1), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(d_list[0], 3, 7, 2, 3, output_padding=1),\n",
        "        )\n",
        "    def forward(self, x): return self.decoder(x)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "conv = Conv().to(device)\n",
        "print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = conv(input)\n",
        "print(out.shape)\n",
        "\n",
        "deconv = Deconv(256).to(device)\n",
        "print(sum(p.numel() for p in deconv.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,256), device=device)\n",
        "out = deconv(input)\n",
        "print(out.shape)\n",
        "\n",
        "# print(conv)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title transfer_sd store_sd load_sd\n",
        "\n",
        "def transfer_sd(tgt_sd, src_sd): #\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in src_sd.keys(): continue\n",
        "            tgt_wht, src_wht = tgt_sd[wht_name], src_sd[wht_name]\n",
        "            # print(wht_name, tgt_wht.shape, src_wht.shape)\n",
        "            if tgt_wht.shape==src_wht.shape:\n",
        "                tgt_wht.copy_(src_wht)\n",
        "                continue\n",
        "            if tgt_wht.shape[0] != src_wht.shape[0]: continue # output dim diff\n",
        "            if len(tgt_wht.shape)==2: tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "    return tgt_sd\n",
        "\n",
        "def store_sd(all_sd, new_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in new_sd.keys():\n",
        "            if not wht_name in all_sd.keys():\n",
        "                # print(wht_name, new_sd[wht_name].shape)\n",
        "                all_sd[wht_name] = (new_sd[wht_name],)\n",
        "                continue\n",
        "            all_tpl, new_wht = all_sd[wht_name], new_sd[wht_name]\n",
        "            for all_wht in all_tpl:\n",
        "                print(wht_name, all_wht.shape, new_wht.shape)\n",
        "                if all_wht.shape==new_wht.shape:\n",
        "                    all_wht = new_wht\n",
        "                    break\n",
        "                if all_wht.shape[0] != new_wht.shape[0]: continue # diff output shape\n",
        "                if len(all_wht.shape)==2: all_wht[:, :new_wht.shape[1]] = new_wht[:, :all_wht.shape[1]]\n",
        "                break\n",
        "            if len(all_wht.shape)>=2 and len(all_wht.shape)>=2:\n",
        "                if all_wht.shape[0] != new_wht.shape[0]: all_tpl = all_tpl + (new_wht,) # wht not in all_wht\n",
        "    return all_sd\n",
        "\n",
        "def load_sd(tgt_sd, all_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in all_sd.keys(): continue\n",
        "            tgt_wht, all_tpl = tgt_sd[wht_name], all_sd[wht_name]\n",
        "            for all_wht in all_tpl:\n",
        "                # try: print(wht_name, tgt_wht.shape, all_wht.shape)\n",
        "                # except: print(wht_name, tgt_wht, all_wht)\n",
        "                if tgt_wht.shape==all_wht.shape:\n",
        "                    tgt_wht.copy_(all_wht)\n",
        "                    break\n",
        "                if tgt_wht.shape[0] != all_wht.shape[0]: continue # output dim diff\n",
        "                if len(tgt_wht.shape)==2: tgt_wht[:, :all_wht.shape[1]].copy_(all_wht[:, :tgt_wht.shape[1]])\n",
        "                break\n",
        "    return tgt_sd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256, dim_a=3, dim_z=3, dim_v=512).to(device)\n",
        "\n",
        "# modelsd = torch.load('agent.pkl', map_location=device).values()\n",
        "# tgt_sd = transfer_sd(agent.state_dict(), modelsd)\n",
        "# agent.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# all_sd = {}\n",
        "# all_sd = store_sd(all_sd, agent1.state_dict())\n",
        "# print(all_sd.keys())\n",
        "# checkpoint = {'model': all_sd}\n",
        "# torch.save(checkpoint, 'all_sd.pkl')\n",
        "\n",
        "# agent3 = Agent(d_model=256, dim_a=3, dim_z=1, dim_v=512).to(device)\n",
        "# agent3.tcost = tcost3\n",
        "# tgt_sd = load_sd(agent3.state_dict(), all_sd)\n",
        "# agent3.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "# for x,y in zip(agent1.state_dict().values(), agent3.state_dict().values()):\n",
        "#     print((x==y).all())\n",
        "\n",
        "# print(agent1.jepa.enc.cnn[1].num_batches_tracked)\n",
        "# jepa.enc.cnn.0.weight\n",
        "# print(agent1.jepa.enc.cnn[0].weight.shape)\n",
        "# print(agent1.jepa.enc.cnn[0].weight[0][0])\n",
        "# print(agent3.jepa.enc.cnn[0].weight[0][0])\n"
      ],
      "metadata": {
        "id": "Bos81kQf1dwh",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title rename_sd\n",
        "def rename_sd(agent_sd):\n",
        "    sd_={}\n",
        "    convert={}\n",
        "    na_=''\n",
        "    for wht_name, wht in agent_sd.items():\n",
        "        o=wht_name.split('.')\n",
        "        # print(\"####\", wht_name)\n",
        "        name=wht_name\n",
        "        for i in range(len(o)):\n",
        "            c = o[i]\n",
        "            if c.isnumeric():\n",
        "                na, me = '.'.join(o[:i]), '.'.join(o[i+1:])\n",
        "                c=int(c)\n",
        "                if na!=na_: # param name diff\n",
        "                    j=0 # reset num\n",
        "                    c_=c # track wht_name num\n",
        "                    na_=na # track param name\n",
        "                elif c_<c: # same param name, diff num\n",
        "                    j+=1\n",
        "                    c_=c\n",
        "                name = f'{na}.{j}.{me}'\n",
        "        # print(name)\n",
        "        sd_[name] = wht\n",
        "        convert[name] = wht_name\n",
        "    return sd_, convert\n",
        "\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "# # modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# modelsd, _ = rename_sd(modelsd)\n",
        "\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "\n",
        "# modelsd = transfer_sd(agentsd, modelsd)\n",
        "# agent.load_state_dict(modelsd, strict=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "SFVbGqMDqcDR",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title transfer_optim me\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# def transfer_optim(tgt_sd, src_sd, tgt_optim, src_optim): #\n",
        "def transfer_optim(tgt_sd, src_sd, tgt_optim_sd, src_optim_sd): #\n",
        "    non_lst = ['running_mean', 'running_var', 'num_batches_tracked', 'num_batches_tracked', 'loss_fn']\n",
        "    tgt_lst, src_lst = [], []\n",
        "    for i, (k,v) in enumerate(tgt_sd.items()):\n",
        "        # print(i, k, v.shape, any(s in k for s in non_lst))\n",
        "        if not any(s in k for s in non_lst): tgt_lst.append(k)\n",
        "    for i, (k,v) in enumerate(src_sd.items()):\n",
        "        if not any(s in k for s in non_lst): src_lst.append(k)\n",
        "\n",
        "    # tgt_optim_st, src_optim_st = tgt_optim.state_dict()['state'], src_optim.state_dict()['state']\n",
        "    tgt_optim_st, src_optim_st = tgt_optim_sd['state'], src_optim_sd['state']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, wht_name in enumerate(tgt_lst):\n",
        "            if not wht_name in src_lst: continue\n",
        "            tgt_wht, src_wht = tgt_optim_st[tgt_lst.index(wht_name)], src_optim_st[src_lst.index(wht_name)]\n",
        "            # print(wht_name, tgt_wht, src_wht)\n",
        "            tgt_shp, src_shp = tgt_wht['exp_avg'].shape, src_wht['exp_avg'].shape\n",
        "            if tgt_shp==src_shp:\n",
        "                tgt_wht = src_wht\n",
        "                continue\n",
        "            if tgt_shp[0] != src_shp[0]: continue # output dim diff\n",
        "            if len(tgt_shp)==2:\n",
        "                tgt_wht['step'] = src_wht['step']\n",
        "                tgt_wht['exp_avg'][:, :src_shp[1]] = src_wht['exp_avg'][:, :tgt_shp[1]]\n",
        "                tgt_wht['exp_avg_sq'][:, :src_shp[1]] = src_wht['exp_avg_sq'][:, :tgt_shp[1]]\n",
        "    # return tgt_optim.state_dict()\n",
        "    return tgt_optim_sd\n",
        "\n",
        "# model_src = torch.nn.Linear(10, 5)  # Example source model\n",
        "# model_tgt = torch.nn.Linear(20, 5)  # Example target model (with more input dimensions)\n",
        "\n",
        "# model_src = nn.Sequential( # trained cost\n",
        "#     nn.Linear(10, 5, bias=False), nn.Softmax(),\n",
        "#     )\n",
        "# d_model=4\n",
        "# model_tgt = nn.Sequential( # trained cost\n",
        "#     nn.Linear(20, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, 5), nn.Softmax(),\n",
        "#     )\n",
        "\n",
        "# source_optimizer = optim.AdamW(model_src.parameters())\n",
        "# target_optimizer = optim.AdamW(model_tgt.parameters())\n",
        "\n",
        "# dummy_input = torch.randn(3, 10)\n",
        "# dummy_target = torch.randn(3, 5)\n",
        "# criterion = torch.nn.MSELoss()\n",
        "# output = model_src(dummy_input)\n",
        "# loss = criterion(output, dummy_target)\n",
        "# loss.backward()\n",
        "# source_optimizer.step()\n",
        "\n",
        "# dummy_input = torch.randn(3, 20)\n",
        "# output = model_tgt(dummy_input)\n",
        "# loss = criterion(output, dummy_target)\n",
        "# loss.backward()\n",
        "# target_optimizer.step()\n",
        "\n",
        "\n",
        "# print(source_optimizer.state_dict())\n",
        "# print(target_optimizer.state_dict())\n",
        "\n",
        "# optimsd = transfer_optim(model_tgt.state_dict(), model_src.state_dict(), target_optimizer, source_optimizer)\n",
        "# target_optimizer.load_state_dict(optimsd)\n",
        "# print(target_optimizer.state_dict())\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "riBHnAAkkzrd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AfjFbveH64Io",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title TCost\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TCost(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=256): # in_dim=(1+self.jepa.pred.num_layers)*d_model\n",
        "        super().__init__()\n",
        "        self.tc = torch.tensor([-1., 0.], device=device).unsqueeze(-1) # unsqueeze(0).T\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_dim, 2, bias=False), nn.Softmax(),\n",
        "            # nn.Linear(in_dim, d_model), nn.ReLU(),\n",
        "            # nn.Dropout(p=0.5),\n",
        "            # nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, 2), nn.Softmax(),\n",
        "            )\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def update_loss_weight(self, train_data):\n",
        "        a = len(train_data.buffer)/len(train_data.data) # ratio dided/tt steps\n",
        "        # self.data = [step for episode in buffer for step in episode]\n",
        "        self.loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)], device=device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.tcost(x)@self.tc\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        out = self.tcost(x)\n",
        "        # print(\"ctost loss\", out, y)\n",
        "        y = torch.where(y < -0.5, 0, 1)\n",
        "        return self.loss_fn(out, y)\n",
        "\n",
        "\n",
        "# tcost=TCost(1024)\n",
        "# x=torch.rand(256,1024)\n",
        "# import time\n",
        "# start = time.time()\n",
        "# out=tcost(x)\n",
        "# # out=F.gumbel_softmax(out)\n",
        "# print(time.time()-start)\n",
        "# # nn.AdaptiveLogSoftmaxWithLoss(in_features=2, n_classes=2, cutoffs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuA25qQknUAX",
        "outputId": "96665237-381d-49df-ba6c-4002afa1ac43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-5d08c402e0cb>:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title jepa\n",
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "class JEPA(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, dim_a, dim_z, dim_v, drop=0.2):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc = Conv(d_model) # pixel\n",
        "        # self.enc = ConvEnc(d_model) #\n",
        "        # self.enc = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.enc.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "        # self.pred = nn.Sequential(\n",
        "        #     nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=drop)\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v), nn.ReLU(),\n",
        "            nn.Linear(dim_v, dim_v),# nn.ReLU(),\n",
        "            )\n",
        "        self.d_model = d_model\n",
        "        self.dim_z = dim_z\n",
        "        self.sim_coeff=25. # 10.0 # 25.0 # λ\n",
        "        self.std_coeff=25. # 1.0 # 25.0 # µ\n",
        "        self.cov_coeff=1. # 25.0 # 1.0 # ν\n",
        "        self.z=torch.zeros((1,dim_z),device=device)\n",
        "        # self.enc_ema = AveragedModel(self.enc, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "        # self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    def v_creg(self, x): # vx [batch_size, d_model]\n",
        "        x = x - x.mean(dim=0)\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2\n",
        "        batch_size, num_features = x.shape\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\n",
        "        # return self.std_coeff * std_loss, self.cov_coeff * cov_loss\n",
        "        return std_loss, cov_loss\n",
        "\n",
        "    def argm(self, sx, a, sy, lr=3e3, h0=None): # 3e3\n",
        "        batch=sx.size(dim=0)\n",
        "        z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "        optim = torch.optim.SGD([z], lr=lr)\n",
        "        # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95))\n",
        "        lossfn = torch.nn.MSELoss()\n",
        "        sx, a, sy = sx.detach(), a.detach(), sy.detach()\n",
        "        for i in range(5): # 10\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sy_ = self.pred(sxaz)\n",
        "                sy_, _ = self.pred(sxaz, h0)\n",
        "                loss = lossfn(sy_, sy)# + self.z_coeff * torch.norm(z)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"argm\",i,loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        return z#.detach()\n",
        "\n",
        "# torch.norm(z, dim=-1)\n",
        "# -(z*torch.log(z)).sum(-1) # Shannon entropy archive.is/CaYrq\n",
        "# in RL, distribution of action, if certainty is high, entropy is low\n",
        "\n",
        "\n",
        "    # def loss(self, x, y, a, z=None):\n",
        "    #     sx, sy = self.enc(x), self.enc(y)\n",
        "    #     z = self.argm(sx, a, sy)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     repr_loss = self.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "    #     # v_c_loss = self.v_creg(self.exp(sx))\n",
        "    #     vx, vy = self.exp(sx), self.exp(sy)\n",
        "    #     v_c_loss = self.v_creg(vx) + self.v_creg(vy)\n",
        "    #     return repr_loss + v_c_loss\n",
        "\n",
        "    # def forward(self, sx, a): # state, ctrl\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z=torch.zeros((batch,self.dim_z),device=device)\n",
        "    #     sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #     sy_ = self.pred(sxaz)\n",
        "    #     return sy_ # state1\n",
        "\n",
        "\n",
        "# d_model=16\n",
        "# dim_z= 1#-5\n",
        "# dim_v=32\n",
        "# dim_a=4\n",
        "# model = JEPA(in_dim, d_model, dim_a, dim_z, dim_v).to(device)\n",
        "# x=torch.rand(1, in_dimx)\n",
        "# y=torch.rand(1, in_dimy)\n",
        "# loss = model.loss(x,y)\n",
        "# distance = torch.norm(embeddings.weight.data - my_sample, dim=-1)\n",
        "# nearest = torch.argmin(distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqkI44ygzfxu",
        "outputId": "28b79faa-0a24-4385-85fd-78dca3d5e246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-154606bd94b8>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=8, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v, drop=0.2)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=10. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=50. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=1. # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        self.closs_coeff=100.\n",
        "        self.zloss_coeff=1.\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        self.lx, self.lz = torch.empty((0,dim_a),device=device), torch.empty((0,dim_z),device=device) # [T,dim_az]\n",
        "        self.sx = self.jepa.enc(torch.zeros((1, 3,64,64)))\n",
        "        self.la = torch.empty(0,device=device)\n",
        "\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        if len(self.la)>1 or laction!=None:\n",
        "            self.update_h0(lstate, laction)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                self.sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "            #     # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(self.sx, T=8, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.la, self.lx, self.lz = lact, lx, lz\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx - torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    self.la = torch.cat([torch.tensor(laction, device=device), self.la[len(laction):]], dim=-1)\n",
        "                la = self.emb(self.la[:seq_len])\n",
        "\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5) # torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        optim_z = torch.optim.SGD([lz], lr=1e1) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e0 ; 3e-2 1e-1\n",
        "        lsx, la = lsx.detach(), la.detach() # [T, d_model], [T, dim_a]\n",
        "        # print(\"update_h0 lz\", lz.data)\n",
        "        self.jepa.pred.train()\n",
        "        for i in range(1): # 1?\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0.detach()) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                loss = F.mse_loss(out_, out.squeeze(0))\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            # print(\"update_h0 loss, lz\",i,loss.item(), lz.data)\n",
        "            # with torch.no_grad(): lz.clamp_(min=-1, max=1)\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1].unsqueeze(0)\n",
        "        # print(\"update_h0\", self.lx.data)\n",
        "        # print(self.la.shape, self.lx.shape, self.lz.shape, self.la[seq_len:].shape, self.lx[seq_len:].shape, self.lz[seq_len:].shape)\n",
        "        self.la, self.lx, self.lz = self.la[seq_len:], self.lx[seq_len:], self.lz[seq_len:] # [T, dim_a], [T, dim_z]\n",
        "        return h0\n",
        "\n",
        "    def argm_s(self, sx, x, h0, lr=3e3): # 3e3\n",
        "        T, _ = x.shape\n",
        "        batch = 64 # 16\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        optim_z = torch.optim.SGD([z], lr=1e4, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-2, (0.9, 0.999), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        # optim_z = torch.optim.AdamW([z], 1e-0, (0.9, 0.95), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "        with torch.no_grad():\n",
        "            z[:,:self.lz.shape[0]] = self.lz[:T].unsqueeze(0).repeat(batch,1,1) # [batch, seq_len, dim_z]\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        x = x.detach().repeat(batch,1,1) # [batch, seq_len, dim_a]\n",
        "        # print(\"argm\", z[0].squeeze())\n",
        "        for i in range(2): # 5\n",
        "            # print(\"argm_search\", sx.shape, x.shape, z.shape, h0.shape) # [1, 256], [batch_size, T, 3], [batch_size, T, 8], [1, 1, 256]\n",
        "            loss, lsx, lh0,c = self.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            # print(i, \"argm z loss\", z[0].squeeze().data, loss[0].squeeze().data)\n",
        "            # print(\"c\", c)\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        return z[idx]#.unsqueeze(0)\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        x = nn.Parameter(torch.empty((T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        optim_x = torch.optim.SGD([x], lr=1e3) # 1e-1,1e-0,1e4 ; 1e2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        with torch.no_grad(): x[:self.lx.shape[0]] = self.lx[:T] # [seq_len, dim_az]\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        # print(\"search\",x.squeeze().data)\n",
        "        for i in range(2): # 5\n",
        "            dist = torch.norm(self.emb.weight.data.unsqueeze(0) - x.unsqueeze(-2), dim=-1) # [1,act_space,emb_dim], [T,1,emb_dim] -> [T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            z = self.argm_s(sx, x_,h0)\n",
        "            # print(\"search\", sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [T, dim_a], [T, dim_z], [1, 1, 256]\n",
        "            loss, lsx, lh0, c = self.rnn_pred(sx, x_.unsqueeze(0), z.unsqueeze(0), h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i,x.squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "            # print(i, \"search loss\", x.squeeze().data, loss.item(), z.squeeze().data)\n",
        "            # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "            print(\"search c\", [f'{cc.item():g}' for cc in c.squeeze(0)]) # print(f'{cc:6f}')\n",
        "        dist = torch.norm(self.emb.weight.data.unsqueeze(0) - x.unsqueeze(-2), dim=-1) # [1,act_space,emb_dim], [T,1,emb_dim] -> [T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [T]\n",
        "        # print(lact.shape, lh0.shape, x.shape, z.shape) # [1]) torch.Size([1, 1, 1, 256]) torch.Size([1, 3]) torch.Size([1, 8])\n",
        "        return lact, lh0, x.data, z # [T], [T, num_layers, batch, d_model], [T, dim_a], [T, dim_z]\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, _ = la.shape\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        # lsx = torch.empty((batch, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            # print(sx.shape, a.shape, z.shape)\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "        icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        # syh0 = torch.cat([lsx.flatten(1),lh0.permute(2,0,1,3).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "        syh0 = torch.cat([lsx[:,1:], lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,T,d_model], [T,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "        tcost = -self.tcost(syh0).unflatten(0, (batch, seq_len)).squeeze(-1)\n",
        "        c = (tcost + icost)*gamma**torch.arange(seq_len, device=device)\n",
        "        return c.sum(), lsx, lh0, c\n",
        "        # return c.sum(), lh0\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def argm(self, lsy, sy, h0, la, rwd):\n",
        "        self.tcost.eval()\n",
        "        batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "        lz = nn.Parameter(torch.zeros((batch_size, bptt, self.dim_z), device=device))\n",
        "        # torch.nn.init.xavier_uniform_(lz)\n",
        "        torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "        # optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "        # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "        optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "        lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "        for i in range(3): # 10\n",
        "            sy_, h0_ = sy.detach(), h0.detach()\n",
        "            lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "            lsy_ = torch.empty((batch_size, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "                for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                    syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                    out_, h0_ = self.jepa.pred(syaz.unsqueeze(1), h0_) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                    lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                    lh0 = torch.cat((lh0, h0_.unsqueeze(0)), dim=0)\n",
        "                repr_loss = F.mse_loss(lsy, lsy_)\n",
        "                syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "                clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "                # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "                cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl + self.zloss_coeff * z_loss\n",
        "            cost.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "            # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "        return lz.detach()\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "            # sx=sy_\n",
        "            state, action, reward = Sar # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "            state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "\n",
        "            for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "                lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "                lsy_ = torch.empty((batch_size, 0, self.d_model), device=device) # [batch_size, T, d_model]\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    lsy = self.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "                    la = self.emb(act) # [batch_size, bptt, dim_a]\n",
        "                    out = lsy - torch.cat([sy_, lsy[:,:-1]], dim=1)\n",
        "                    lz = self.argm(lsy, sy_, h0, la, rwd)\n",
        "\n",
        "                    for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                        syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                        out_, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                        sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                        lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                        lh0 = torch.cat((lh0, h0.unsqueeze(0)), dim=0)\n",
        "\n",
        "                    repr_loss = F.mse_loss(lsy, lsy_)\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "\n",
        "                    syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "                    # print(\"syh0, rwd\",syh0.shape,rwd.shape)\n",
        "                    clossl = self.tcost.loss(syh0, rwd.flatten())\n",
        "                    closs = self.closs_coeff * clossl\n",
        "\n",
        "                    # print(h0.requires_grad)\n",
        "                    # pred = self.tcost(syh0).squeeze(-1).cpu()\n",
        "                    # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "                    # print(\"reward, pred, clossl\", reward[mask].data, pred[mask].data, clossl.item())\n",
        "                    # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                    # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    norm = torch.norm(lsy, dim=-1)[0][0].item()\n",
        "                    z_norm = torch.norm(z)\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sy_, h0 = sy_.detach(), h0.detach()\n",
        "\n",
        "                    try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                    except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "# !pip show torch triton\n",
        "# # !pip install --upgrade torch\n",
        "# !pip install --upgrade triton\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "# print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "# print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "# print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# dim_a, dim_z = 3, 8\n",
        "# batch, T = 4,6\n",
        "# x = nn.Parameter(torch.empty((batch, T, dim_a),device=device))\n",
        "# torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "# dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# x = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "# z = nn.Parameter(torch.zeros((batch, T, dim_z),device=device))\n",
        "# torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "# state = torch.zeros((1, 3,64,64))\n",
        "# # state = torch.rand((1, 3,64,64), device=device)\n",
        "# sx = agent.jepa.enc(state)\n",
        "\n",
        "act = agent([state], k=4)\n",
        "# h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "# lact, lh0, lx, lz = agent.search(sx, T=6, h0=h0)\n",
        "# loss, lsx, lh0,c = agent.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "# print(loss,c)\n",
        "# print(lact, lh0, lx, lz)\n"
      ],
      "metadata": {
        "id": "UEH1P802JkHU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "f4325567-3486-485e-aefb-5d42fbed1692"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'state' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2eab810ffe5c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# sx = agent.jepa.enc(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# lact, lh0, lx, lz = agent.search(sx, T=6, h0=h0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'state' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_b7ZSW6IF1-",
        "outputId": "c29c01bd-3f6c-4863-d827-b80f488bf747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY\n",
            "From (redirected): https://drive.google.com/uc?id=1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY&confirm=t&uuid=73bdaf6e-b55b-4dd4-bbfa-2a34a6df9c4e\n",
            "To: /content/buffer512.pkl\n",
            "100% 706M/706M [00:11<00:00, 59.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown 1bGWBbcKUgHESkbD3NfYt1WWikScVSFOj -O agentoptim.pkl # M1 gru3 tcost1\n",
        "# !gdown 1XBDhD2efIFW9lnewGRLrb362w47a8b1q -O agentoptim.pkl # B2 gru3 tcost1\n",
        "# !gdown 12Ez0fE8QtJ8b35zeuZQp85mrbHbWvhA_ -O agentoptim.pkl # S3\n",
        "# !gdown 1zoZ52jctM0jed6TgD7kAwrtnuDMeA5II -O agentoptim.pkl # T4 gru1 tcost1 drop\n",
        "# !gdown 1GlZxrzdH5f28Qo4olbOi0vmAK5WDV7jc -O agentoptim.pkl # A2\n",
        "# !gdown 1UDgNtFsWGAhvqR9lwA0QbMLhUtmip4ne -O agentoptim.pkl # M1 agentoptimgru3tcost1\n",
        "# !gdown 1-0oc6yucS5JXLHX1zqbYe3NTVMuhP_5r -O agentoptim.pkl # A2 agentoptim25251c25z3\n",
        "# !gdown 1U1CuCU1FugkrzPXsvTPpIX-wzWz6szl2 -O agentoptim.pkl # T4 agentoptimargm\n",
        "# !gdown 1CWZAtiEwSnglClJbq2LJTYlKhPN10gfo -O agentoptim.pkl # S3 agentoptimargm\n",
        "# !gdown 1XAbr6l1pCmcUCKR6kYlQ_dSDsOBqRg_j -O agentoptim.pkl # B2 argm2search2\n",
        "\n",
        "# !gdown 1UkQuf-IC2LYErSapkF6rZM1dv3svGI5P -O agentoptim.pkl # T4 gru3 argm offline\n",
        "# !gdown 1-4sNf6mINCiD5YsBdQvCrlyqzzfS64si -O agentoptim.pkl # T4 gru3 argm offline\n",
        "\n",
        "# !gdown 1sCW9uvcdCJkCH5HQDdISLws5rMvmkmFR -O all_sd.pkl # M1 all_sd\n",
        "\n",
        "import pickle\n",
        "# !gdown 1j9hOq8_752duPB0PMYUJqabNvYoGLysX -O buffer512down.pkl # S\n",
        "# with open('buffer512down.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "!gdown 1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY -O buffer512.pkl # S\n",
        "with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# !gdown 1tzlp_Yc_70XSFy2yiCliLd6Jlt1X78lB -O buffergo.pkl # S3\n",
        "# !gdown 1egXy0t_kn0M0oL6sbwixoVr7bqMfcB8j -O buffergo.pkl # T4\n",
        "# !gdown 1-34fhOMTdMvtuAeHuL28Y4taSINvOejQ -O buffergo.pkl # B2\n",
        "# with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHQ_ynlwoyJ",
        "outputId": "153aa685-dfe1-4703-9a25-31f624c25cbb",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-69-b2bb48a0ec3c>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  modelsd, optimsd = torch.load(folder+'agentoptimargm.pkl', map_location=device).values()\n"
          ]
        }
      ],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "import pickle\n",
        "\n",
        "# with open(folder+'buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "# with open('buffergo.pkl', 'rb') as f: buffer = pickle.load(f)\n",
        "\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'agentoptim.pkl', map_location=device).values()\n",
        "modelsd, optimsd = torch.load(folder+'agentoptimargm.pkl', map_location=device).values()\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl', map_location=device).values()\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "# modelsd = transfer_sd(agentsd, modelsd)\n",
        "modelsd = transfer_sd(agent.state_dict(), modelsd)\n",
        "agent.load_state_dict(modelsd, strict=False)\n",
        "# # optimsd = transfer_optim(agent.state_dict(), modelsd, optim.state_dict(), optimsd)\n",
        "optim.load_state_dict(optimsd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# all_sd = torch.load(folder+'all_sd.pkl', map_location=device)\n",
        "# # all_sd = torch.load('all_sd.pkl', map_location=device)\n",
        "# _, convert = rename_sd(agent.state_dict())\n",
        "# # agentsd = dict((convert[k], v) for (k, v) in all_sd.items())\n",
        "# allsd = {}\n",
        "# for (k, v) in all_sd.items():\n",
        "#     try: allsd[convert[k]] = v\n",
        "#     except Exception as e: print('dict err', e)\n",
        "# # agentsd = dict((convert[k], v) for (k, v) in modelsd.items())\n",
        "# tgt_sd = load_sd(agent.state_dict(), allsd)\n",
        "# agent.load_state_dict(tgt_sd, strict=False)\n",
        "\n",
        "\n",
        "\n",
        "# for i, (k,v) in enumerate(modelsd.items()):\n",
        "# for i, (k,v) in enumerate(agent.state_dict().items()):\n",
        "#     print(i,k,v.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBfBomEBnJu0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "24794c3e-bcec-47ae-9ae4-8117d8e90d70"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'folder' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-88619bc80a50>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mall_sd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_sd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magentsd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# torch.save(all_sd, 'all_sd.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'all_sd.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'folder' is not defined"
          ]
        }
      ],
      "source": [
        "# buffer = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "# with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "\n",
        "# # agentsd, _ = rename_sd(agent.state_dict())\n",
        "# # checkpoint = {'model': agentsd, 'optimizer': optim.state_dict(),}\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# # torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "# torch.save(checkpoint, folder+'agentoptimargm1.pkl')\n",
        "# # torch.save(checkpoint, 'agentoptim.pkl')\n",
        "\n",
        "all_sd = {}\n",
        "agentsd, _ = rename_sd(agent.state_dict())\n",
        "all_sd = store_sd(all_sd, agentsd)\n",
        "# torch.save(all_sd, 'all_sd.pkl')\n",
        "torch.save(all_sd, folder+'all_sd.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NVcknabHMxH6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        self.buffer = self.process(buffer)\n",
        "        self.data = [step for episode in self.buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state] # list\n",
        "        return torch.stack(state, dim=0), torch.tensor(action), torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def process(self, buffer):\n",
        "        cleaned = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "        cleaned = [episode[-random.randint(20, 80):] for episode in cleaned]\n",
        "        random.shuffle(cleaned)\n",
        "        return cleaned\n",
        "\n",
        "\n",
        "    # def add(self, episode):\n",
        "    #     self.data.append(episode)\n",
        "\n",
        "    # def pop(self, data, p=1, k=5, n=3): # p: num eps to pop; k: knn clustered; n: ave frames\n",
        "    #     lin= nn.Linear(3*64*64, 100)#, bias=False)\n",
        "    #     with torch.no_grad():\n",
        "    #         imgs = [[sample[0] for sample in random.sample(episode,n)] for episode in buffer] # [num_episodes, num_samples, 64, 64, 3]\n",
        "    #         data=torch.from_numpy(np.stack(imgs)).float().mean(1) # sum mean\n",
        "    #         # imshow(torchvision.utils.make_grid(data.int().permute(0,3,1,2),nrow=4))\n",
        "    #         data=data.flatten(start_dim=-3)\n",
        "    #         data=lin(data) # random projection\n",
        "    #         data = F.normalize(data, dim=-1)\n",
        "    #         idx = torch.randperm(len(data))[:100] # sample some episodes\n",
        "    #         sample = data[idx]\n",
        "    #         index = faiss.IndexFlatL2(data.shape[-1]) # 6.53 ms ± 1.23 ms\n",
        "    #         # index = faiss.IndexFlatIP(data.shape[-1]) #\n",
        "    #         index.add(data)\n",
        "    #         D, I = index.search(sample, k) # estimate clusteredness using k nearest neighbors # dist, idx\n",
        "    #         priority = (2**-D).sum(-1) # L2\n",
        "    #         # priority = -D.sum(-1) # IP\n",
        "    #         topk = torch.topk(priority, p)#, dim=None, largest=True, sorted=True\n",
        "    #         index_list = idx[topk.values] # most clustered\n",
        "    #         for i in reversed(index_list): data.pop(i)\n",
        "    #     return data\n",
        "\n",
        "def collate_fn(sar):\n",
        "    state, action, reward = zip(*sar)\n",
        "    state=torch.stack(state, dim=1) # batch first -> dim=0\n",
        "    action=torch.stack(action, dim=1)\n",
        "    reward=torch.stack(reward, dim=1)\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(30, 14))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "seq_len = 50 # 50\n",
        "train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True)\n",
        "# train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # # [3,T,batch]\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "labels = torch.tensor([0])\n",
        "# pred = torch.tensor([[.999,.001]])\n",
        "pred = torch.tensor([[1.,0.]])\n",
        "pred = torch.tensor([[5.,-5.]])\n",
        "# pred = torch.tensor([[.5,.5]])\n",
        "# pred = torch.tensor([[1/a, 1/(1-a)]])\n",
        "# pred = torch.tensor([[1/(1-a), 1/a]])\n",
        "print(F.mse_loss(labels, pred))\n",
        "a=10\n",
        "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1/a, 1/(1-a)], device=device))\n",
        "print(loss_fn(pred, labels))\n",
        "\n",
        "# print((pred@torch.log(pred).T).sum())\n",
        "# print(pred,torch.log(pred).T)\n",
        "\n",
        "# 0.6931\n"
      ],
      "metadata": {
        "id": "viimAIpYSJq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ade61b9-20c9-49e0-e396-68e2fd86261e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(25.)\n",
            "tensor(4.5418e-05)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-73-f7a9308f1678>:8: UserWarning: Using a target size (torch.Size([1, 2])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  print(F.mse_loss(labels, pred))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "OksdjCeJYpYh",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbed565c-bd96-4123-91a8-d04d9be8633d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n",
            "<ipython-input-36-154606bd94b8>:281: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-36-154606bd94b8>:243: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "repr, std, cov, clossl, z, norm 0.9297412037849426 0.37646484375 1.891221284866333 0.6054637432098389 1.2243430614471436 3.3295342922210693\n",
            "repr, std, cov, clossl, z, norm 3.497551441192627 0.36865234375 2.424592971801758 0.8121257424354553 1.2247841358184814 3.6832871437072754\n",
            "repr, std, cov, clossl, z, norm 0.8595515489578247 0.36669921875 2.5664000511169434 0.6102255582809448 1.2282544374465942 2.5097272396087646\n",
            "repr, std, cov, clossl, z, norm 3.2781245708465576 0.393310546875 1.055051565170288 0.7407954335212708 1.3390730619430542 2.764970302581787\n",
            "repr, std, cov, clossl, z, norm 0.8096311688423157 0.420166015625 0.3255023956298828 0.6973307728767395 1.2729495763778687 5.012582302093506\n",
            "repr, std, cov, clossl, z, norm 3.0858519077301025 0.42431640625 0.263655424118042 0.7650043964385986 1.2445905208587646 5.191949367523193\n",
            "repr, std, cov, clossl, z, norm 0.8088642954826355 0.408935546875 0.5558009147644043 0.5917489528656006 1.1728483438491821 5.357416152954102\n",
            "repr, std, cov, clossl, z, norm 3.5477676391601562 0.39599609375 0.9523143768310547 0.741400957107544 1.2440197467803955 6.101078033447266\n",
            "repr, std, cov, clossl, z, norm 0.8329138159751892 0.397705078125 0.885151743888855 0.6166439056396484 1.2233582735061646 6.05314826965332\n",
            "repr, std, cov, clossl, z, norm 5.974606037139893 0.361572265625 2.981107473373413 0.7964233756065369 1.354250192642212 6.499009609222412\n",
            "repr, std, cov, clossl, z, norm 0.8303135633468628 0.34326171875 4.91627311706543 0.5912773609161377 1.206339955329895 5.225601673126221\n",
            "repr, std, cov, clossl, z, norm 3.551504373550415 0.36474609375 2.7159552574157715 0.8180069327354431 1.2243348360061646 2.874190092086792\n",
            "repr, std, cov, clossl, z, norm 0.7374093532562256 0.376220703125 1.9045134782791138 0.6228963732719421 1.3197603225708008 5.490325450897217\n",
            "repr, std, cov, clossl, z, norm 3.1891772747039795 0.398193359375 0.8756459355354309 0.784822940826416 1.2763080596923828 3.948457956314087\n",
            "repr, std, cov, clossl, z, norm 0.7436668872833252 0.411865234375 0.4882143437862396 0.6342717409133911 1.2544646263122559 6.910581111907959\n",
            "repr, std, cov, clossl, z, norm 3.166924476623535 0.41748046875 0.37476325035095215 0.787825882434845 1.3009155988693237 5.205332279205322\n",
            "1\n",
            "repr, std, cov, clossl, z, norm 0.8985558748245239 0.4091796875 0.5500495433807373 0.7045425772666931 1.3192687034606934 5.646978378295898\n",
            "repr, std, cov, clossl, z, norm 4.580137729644775 0.4091796875 0.5538798570632935 0.8083640336990356 1.1960185766220093 3.0504865646362305\n",
            "repr, std, cov, clossl, z, norm 0.9058523178100586 0.396484375 0.9344519972801208 0.616999626159668 1.2912882566452026 5.9094157218933105\n",
            "repr, std, cov, clossl, z, norm 4.975984573364258 0.385498046875 1.401587963104248 0.7793482542037964 1.215254545211792 5.114155292510986\n",
            "repr, std, cov, clossl, z, norm 0.7727648615837097 0.39404296875 1.0262813568115234 0.6023626327514648 1.3193947076797485 4.869534969329834\n",
            "repr, std, cov, clossl, z, norm 4.798449516296387 0.384765625 1.433631181716919 0.7500767111778259 1.3102710247039795 5.384404182434082\n",
            "repr, std, cov, clossl, z, norm 0.7582945227622986 0.364501953125 2.7472941875457764 0.6107710599899292 1.2185606956481934 3.2503888607025146\n",
            "repr, std, cov, clossl, z, norm 4.314958572387695 0.36376953125 2.7917776107788086 0.7568241953849792 1.3023953437805176 3.1692304611206055\n",
            "repr, std, cov, clossl, z, norm 0.7424757480621338 0.362548828125 2.906310558319092 0.5914034247398376 1.3051400184631348 5.400520324707031\n",
            "repr, std, cov, clossl, z, norm 3.4572296142578125 0.388427734375 1.2540662288665771 0.7392785549163818 1.289965033531189 5.4829583168029785\n",
            "repr, std, cov, clossl, z, norm 0.7130328416824341 0.41064453125 0.5178713798522949 0.5728767514228821 1.302543044090271 5.042557716369629\n",
            "repr, std, cov, clossl, z, norm 2.9011523723602295 0.412841796875 0.4675901532173157 0.7585253119468689 1.264258623123169 3.392730951309204\n",
            "repr, std, cov, clossl, z, norm 0.7618273496627808 0.4130859375 0.4628680348396301 0.5710360407829285 1.2959445714950562 3.194507598876953\n",
            "repr, std, cov, clossl, z, norm 2.9668123722076416 0.402099609375 0.742344081401825 0.8098572492599487 1.261331558227539 2.7413501739501953\n",
            "repr, std, cov, clossl, z, norm 0.809965968132019 0.40185546875 0.7512545585632324 0.5892637372016907 1.3829714059829712 5.615733623504639\n",
            "repr, std, cov, clossl, z, norm 2.9553990364074707 0.38134765625 1.608694314956665 0.6978943943977356 1.2575538158416748 5.441093444824219\n",
            "2\n",
            "repr, std, cov, clossl, z, norm 0.7451049089431763 0.367919921875 2.482950210571289 0.5793696641921997 1.25404953956604 6.1441216468811035\n",
            "repr, std, cov, clossl, z, norm 3.2068331241607666 0.3642578125 2.772723913192749 0.7391160130500793 1.3759971857070923 6.332133769989014\n",
            "repr, std, cov, clossl, z, norm 0.6662958264350891 0.3740234375 2.0528481006622314 0.5646779537200928 1.2984395027160645 2.176443099975586\n",
            "repr, std, cov, clossl, z, norm 3.136503219604492 0.394775390625 0.9967514276504517 0.7559934854507446 1.273790717124939 2.6140997409820557\n",
            "repr, std, cov, clossl, z, norm 0.6601184010505676 0.41796875 0.36450836062431335 0.6232861876487732 1.265839695930481 3.545811414718628\n",
            "repr, std, cov, clossl, z, norm 3.454773426055908 0.42333984375 0.27799347043037415 0.715776264667511 1.316869854927063 5.6603922843933105\n",
            "repr, std, cov, clossl, z, norm 0.630957841873169 0.413818359375 0.44497209787368774 0.5708730816841125 1.2916768789291382 5.297504425048828\n",
            "repr, std, cov, clossl, z, norm 2.918435573577881 0.408935546875 0.5591075420379639 0.7455911040306091 1.260596513748169 5.923457622528076\n",
            "repr, std, cov, clossl, z, norm 0.611720621585846 0.399658203125 0.8190462589263916 0.6017318964004517 1.3323700428009033 5.106482028961182\n",
            "repr, std, cov, clossl, z, norm 2.4632325172424316 0.386474609375 1.35404634475708 0.8281487226486206 1.2824718952178955 7.155119895935059\n",
            "repr, std, cov, clossl, z, norm 0.6378315091133118 0.39208984375 1.0954511165618896 0.5989532470703125 1.2414841651916504 3.9809200763702393\n",
            "repr, std, cov, clossl, z, norm 4.097650527954102 0.3544921875 3.643225908279419 0.7712599039077759 1.266589641571045 4.000574588775635\n",
            "repr, std, cov, clossl, z, norm 0.6416378021240234 0.371337890625 2.2252109050750732 0.5727410912513733 1.2528440952301025 6.227963924407959\n",
            "repr, std, cov, clossl, z, norm 2.9389755725860596 0.36474609375 2.720398187637329 0.7742495536804199 1.335992693901062 6.5609450340271\n",
            "repr, std, cov, clossl, z, norm 0.6211619973182678 0.3876953125 1.2910432815551758 0.5666402578353882 1.2398982048034668 6.655491828918457\n",
            "repr, std, cov, clossl, z, norm 2.926564931869507 0.40185546875 0.7482677698135376 0.857945442199707 1.242920160293579 6.863936424255371\n",
            "3\n",
            "repr, std, cov, clossl, z, norm 0.7682674527168274 0.4130859375 0.4614641070365906 0.641829252243042 1.265753984451294 4.476639747619629\n",
            "repr, std, cov, clossl, z, norm 4.790809631347656 0.41552734375 0.4112306237220764 0.7845198512077332 1.2414939403533936 5.861520290374756\n",
            "repr, std, cov, clossl, z, norm 0.8600930571556091 0.40673828125 0.6098722219467163 0.5579261183738708 1.3026930093765259 3.734300374984741\n",
            "repr, std, cov, clossl, z, norm 4.956859588623047 0.40380859375 0.6923278570175171 0.7891148328781128 1.2597483396530151 4.205559253692627\n",
            "repr, std, cov, clossl, z, norm 0.8037844896316528 0.4033203125 0.709735095500946 0.6141000986099243 1.289876103401184 5.017557144165039\n",
            "repr, std, cov, clossl, z, norm 3.8668200969696045 0.39453125 1.0026661157608032 0.8362987041473389 1.3147523403167725 7.118324279785156\n",
            "repr, std, cov, clossl, z, norm 0.7272436618804932 0.36328125 2.8493731021881104 0.5782057046890259 1.259618878364563 2.609449863433838\n",
            "repr, std, cov, clossl, z, norm 3.0489022731781006 0.37744140625 1.8308817148208618 0.8530043959617615 1.2916288375854492 4.431835174560547\n",
            "repr, std, cov, clossl, z, norm 0.6619144082069397 0.384033203125 1.4743101596832275 0.5734974145889282 1.2978662252426147 4.4246344566345215\n",
            "repr, std, cov, clossl, z, norm 2.3072497844696045 0.398193359375 0.874823272228241 0.7784439921379089 1.316939353942871 4.378998279571533\n",
            "repr, std, cov, clossl, z, norm 0.6473797559738159 0.41015625 0.5279755592346191 0.540355384349823 1.2323133945465088 4.998619556427002\n",
            "repr, std, cov, clossl, z, norm 2.3167619705200195 0.406494140625 0.6202691793441772 0.7291777729988098 1.2642098665237427 4.4397077560424805\n",
            "repr, std, cov, clossl, z, norm 0.6388471722602844 0.40869140625 0.5608832836151123 0.5259764790534973 1.2731821537017822 4.747980117797852\n",
            "repr, std, cov, clossl, z, norm 2.409332036972046 0.39794921875 0.8766904473304749 0.776892900466919 1.3386752605438232 4.610111713409424\n",
            "repr, std, cov, clossl, z, norm 0.6304079294204712 0.3720703125 2.1769661903381348 0.5699185729026794 1.260428547859192 5.874415397644043\n",
            "repr, std, cov, clossl, z, norm 3.3268613815307617 0.362548828125 2.8940348625183105 0.7332221269607544 1.228554368019104 6.843139171600342\n",
            "4\n",
            "repr, std, cov, clossl, z, norm 0.532400906085968 0.37158203125 2.205474376678467 0.575172483921051 1.335384726524353 7.40632963180542\n",
            "repr, std, cov, clossl, z, norm 2.35593843460083 0.366455078125 2.5832557678222656 0.7359082698822021 1.2756563425064087 6.6432576179504395\n",
            "repr, std, cov, clossl, z, norm 0.556378185749054 0.36962890625 2.3473167419433594 0.5380651354789734 1.3508542776107788 3.911184787750244\n",
            "repr, std, cov, clossl, z, norm 2.5222575664520264 0.39013671875 1.1797549724578857 0.6756282448768616 1.2275627851486206 4.2679595947265625\n",
            "repr, std, cov, clossl, z, norm 0.5395159125328064 0.412841796875 0.4645039141178131 0.5434595346450806 1.2811733484268188 6.036059379577637\n",
            "repr, std, cov, clossl, z, norm 2.399132251739502 0.4150390625 0.42160749435424805 0.8222004771232605 1.3276305198669434 6.518459320068359\n",
            "repr, std, cov, clossl, z, norm 0.5730342268943787 0.42041015625 0.320324182510376 0.531593382358551 1.3082590103149414 6.163846969604492\n",
            "repr, std, cov, clossl, z, norm 2.7554261684417725 0.418701171875 0.3506753444671631 0.7600398659706116 1.2378369569778442 6.078754901885986\n",
            "repr, std, cov, clossl, z, norm 0.6217166781425476 0.4033203125 0.7059481143951416 0.5821573734283447 1.2924059629440308 5.349751949310303\n",
            "repr, std, cov, clossl, z, norm 3.4077770709991455 0.39404296875 1.0173702239990234 0.8094949126243591 1.2763934135437012 5.4546990394592285\n",
            "repr, std, cov, clossl, z, norm 0.5839027166366577 0.39404296875 1.0209697484970093 0.5453871488571167 1.239200472831726 7.094329833984375\n",
            "repr, std, cov, clossl, z, norm 2.4107799530029297 0.378173828125 1.7810462713241577 0.7908557653427124 1.2875497341156006 6.390866279602051\n",
            "repr, std, cov, clossl, z, norm 0.569573163986206 0.367431640625 2.512266159057617 0.5561563968658447 1.2972439527511597 5.499871730804443\n",
            "repr, std, cov, clossl, z, norm 2.3851637840270996 0.373779296875 2.055696964263916 0.7834722995758057 1.2216225862503052 7.018586158752441\n",
            "repr, std, cov, clossl, z, norm 0.5693008899688721 0.365966796875 2.615280866622925 0.5807059407234192 1.3183486461639404 6.021134853363037\n",
            "repr, std, cov, clossl, z, norm 2.577254056930542 0.395751953125 0.9513832330703735 0.805272102355957 1.2577426433563232 5.0399909019470215\n",
            "5\n",
            "repr, std, cov, clossl, z, norm 0.6017304062843323 0.398681640625 0.8555749654769897 0.5653479695320129 1.329049825668335 5.204567909240723\n",
            "repr, std, cov, clossl, z, norm 2.7129995822906494 0.40576171875 0.633141040802002 0.7411238551139832 1.2185535430908203 4.6787190437316895\n",
            "repr, std, cov, clossl, z, norm 0.6775376796722412 0.4140625 0.4366362988948822 0.6104310750961304 1.28605055809021 5.014735221862793\n",
            "repr, std, cov, clossl, z, norm 3.904557943344116 0.404296875 0.6773544549942017 0.7590154409408569 1.3242143392562866 3.2068631649017334\n",
            "repr, std, cov, clossl, z, norm 0.6802104115486145 0.398681640625 0.8528993129730225 0.5879399180412292 1.2947076559066772 5.522814750671387\n",
            "repr, std, cov, clossl, z, norm 3.0966832637786865 0.3837890625 1.472299337387085 0.7714446187019348 1.2980072498321533 6.4254889488220215\n",
            "repr, std, cov, clossl, z, norm 0.694854736328125 0.380615234375 1.6453430652618408 0.5676591992378235 1.2677924633026123 2.945899248123169\n",
            "repr, std, cov, clossl, z, norm 2.5419247150421143 0.3740234375 2.0430703163146973 0.8199456930160522 1.278698205947876 3.4071695804595947\n",
            "repr, std, cov, clossl, z, norm 0.7035565972328186 0.357666015625 3.3289499282836914 0.5476326942443848 1.2861816883087158 4.025769233703613\n",
            "repr, std, cov, clossl, z, norm 2.157320737838745 0.3818359375 1.5803112983703613 0.8191543817520142 1.3000178337097168 4.659904956817627\n",
            "repr, std, cov, clossl, z, norm 0.7097347974777222 0.403564453125 0.6936160326004028 0.5692744851112366 1.3036214113235474 7.293630599975586\n",
            "repr, std, cov, clossl, z, norm 2.736060619354248 0.40771484375 0.5817816853523254 0.7374021410942078 1.3114399909973145 6.305118083953857\n",
            "repr, std, cov, clossl, z, norm 0.6935485601425171 0.4208984375 0.31051021814346313 0.554663360118866 1.2750149965286255 6.435070037841797\n",
            "repr, std, cov, clossl, z, norm 2.6286141872406006 0.416748046875 0.3840034306049347 0.7874501943588257 1.2193630933761597 5.421718597412109\n",
            "repr, std, cov, clossl, z, norm 0.6711668372154236 0.41259765625 0.4662591814994812 0.5920449495315552 1.2744381427764893 5.860947132110596\n",
            "repr, std, cov, clossl, z, norm 2.30783748626709 0.3935546875 1.038482904434204 0.8117471933364868 1.3270124197006226 5.981719970703125\n",
            "6\n",
            "repr, std, cov, clossl, z, norm 0.6069760918617249 0.36669921875 2.562046527862549 0.5894650816917419 1.2781225442886353 5.628709316253662\n",
            "repr, std, cov, clossl, z, norm 2.065326690673828 0.375244140625 1.9542920589447021 0.756758451461792 1.2882273197174072 6.533737659454346\n",
            "repr, std, cov, clossl, z, norm 0.6467692255973816 0.387451171875 1.2952159643173218 0.6002340912818909 1.2944152355194092 5.5456318855285645\n",
            "repr, std, cov, clossl, z, norm 2.1623778343200684 0.383056640625 1.5048154592514038 0.825885534286499 1.2374992370605469 6.03282356262207\n",
            "repr, std, cov, clossl, z, norm 0.7064729332923889 0.38720703125 1.3030447959899902 0.527083158493042 1.2889305353164673 5.868651390075684\n",
            "repr, std, cov, clossl, z, norm 3.443603038787842 0.380859375 1.6150445938110352 0.7562565803527832 1.3079309463500977 6.019283294677734\n",
            "repr, std, cov, clossl, z, norm 0.7334362864494324 0.3857421875 1.3676844835281372 0.5785033106803894 1.2431188821792603 7.877966403961182\n",
            "repr, std, cov, clossl, z, norm 2.35353946685791 0.384033203125 1.451688528060913 0.7367649078369141 1.246925711631775 8.085465431213379\n",
            "repr, std, cov, clossl, z, norm 0.9529016017913818 0.383544921875 1.4769854545593262 0.6054741740226746 1.3263614177703857 5.828885078430176\n",
            "repr, std, cov, clossl, z, norm 3.227501392364502 0.388916015625 1.2235995531082153 0.8466158509254456 1.2886136770248413 5.277717113494873\n",
            "repr, std, cov, clossl, z, norm 0.855699360370636 0.3857421875 1.3663395643234253 0.5906259417533875 1.2447067499160767 6.0390543937683105\n",
            "repr, std, cov, clossl, z, norm 2.7230629920959473 0.38818359375 1.2520134449005127 0.7884370684623718 1.3159675598144531 4.381773471832275\n",
            "repr, std, cov, clossl, z, norm 0.6924736499786377 0.393798828125 1.0173771381378174 0.6195449829101562 1.3335949182510376 6.626204967498779\n",
            "repr, std, cov, clossl, z, norm 2.6839239597320557 0.3876953125 1.268916130065918 0.731622040271759 1.2402065992355347 5.6203179359436035\n",
            "repr, std, cov, clossl, z, norm 0.6534398794174194 0.38232421875 1.524479627609253 0.6522198915481567 1.2757800817489624 5.2940568923950195\n",
            "repr, std, cov, clossl, z, norm 2.5544896125793457 0.38427734375 1.4308604001998901 0.8060216903686523 1.287030577659607 6.116453647613525\n",
            "7\n",
            "repr, std, cov, clossl, z, norm 0.5901556611061096 0.378173828125 1.748305320739746 0.610940158367157 1.2681013345718384 6.318483829498291\n",
            "repr, std, cov, clossl, z, norm 2.1762712001800537 0.380126953125 1.6434355974197388 0.7959847450256348 1.3096529245376587 7.374284744262695\n",
            "repr, std, cov, clossl, z, norm 0.5439724326133728 0.38916015625 1.193108320236206 0.5339261293411255 1.2553350925445557 7.0443525314331055\n",
            "repr, std, cov, clossl, z, norm 2.4420297145843506 0.3798828125 1.6536662578582764 0.7167506814002991 1.2876626253128052 6.006540775299072\n",
            "repr, std, cov, clossl, z, norm 0.5090932846069336 0.38330078125 1.4707202911376953 0.5628148317337036 1.328952431678772 5.952499866485596\n",
            "repr, std, cov, clossl, z, norm 1.9523937702178955 0.3916015625 1.0784403085708618 0.7894578576087952 1.3048185110092163 5.966565132141113\n",
            "repr, std, cov, clossl, z, norm 0.5672813057899475 0.397705078125 0.8466823101043701 0.6232161521911621 1.2497230768203735 6.047156810760498\n",
            "repr, std, cov, clossl, z, norm 1.825892448425293 0.39208984375 1.047926902770996 0.8237940073013306 1.250219702720642 7.959992408752441\n",
            "repr, std, cov, clossl, z, norm 0.6257153749465942 0.376708984375 1.803958535194397 0.6133223176002502 1.2175486087799072 6.970654010772705\n",
            "repr, std, cov, clossl, z, norm 1.9907805919647217 0.380126953125 1.5824313163757324 0.7794275879859924 1.2787772417068481 5.655029296875\n",
            "repr, std, cov, clossl, z, norm 0.5439409017562866 0.3779296875 1.6693583726882935 0.6490579843521118 1.2757765054702759 5.780271053314209\n",
            "repr, std, cov, clossl, z, norm 1.8111517429351807 0.385009765625 1.2838096618652344 0.8310035467147827 1.243105411529541 6.044894695281982\n",
            "repr, std, cov, clossl, z, norm 0.4848480820655823 0.390380859375 1.018835186958313 0.6033857464790344 1.3495930433273315 5.832734107971191\n",
            "repr, std, cov, clossl, z, norm 1.4920278787612915 0.3935546875 0.8592236042022705 0.8111342191696167 1.2668893337249756 5.3157267570495605\n",
            "repr, std, cov, clossl, z, norm 0.4482879042625427 0.3857421875 1.0784804821014404 0.5889890193939209 1.3241201639175415 6.815567493438721\n",
            "repr, std, cov, clossl, z, norm 1.6470775604248047 0.3759765625 1.3721704483032227 0.8347876071929932 1.2560652494430542 4.847045421600342\n",
            "8\n",
            "repr, std, cov, clossl, z, norm 0.43001803755760193 0.365234375 1.6786980628967285 0.5259748697280884 1.2693419456481934 5.024155139923096\n",
            "repr, std, cov, clossl, z, norm 1.5995548963546753 0.35205078125 2.255044937133789 0.8048503398895264 1.3469617366790771 4.737967491149902\n",
            "repr, std, cov, clossl, z, norm 0.4301188588142395 0.35009765625 2.3455355167388916 0.5171933174133301 1.3297502994537354 7.383820056915283\n",
            "repr, std, cov, clossl, z, norm 1.7466747760772705 0.362060546875 1.666651964187622 0.8217831254005432 1.2641608715057373 5.214128494262695\n",
            "repr, std, cov, clossl, z, norm 0.5128588676452637 0.36669921875 1.42344069480896 0.5528742074966431 1.2543871402740479 5.194253921508789\n",
            "repr, std, cov, clossl, z, norm 4.3958868980407715 0.3681640625 1.342024564743042 0.780828058719635 1.318987250328064 4.519575119018555\n",
            "repr, std, cov, clossl, z, norm 0.508836567401886 0.37890625 0.951525092124939 0.627724826335907 1.2842708826065063 3.814077615737915\n",
            "repr, std, cov, clossl, z, norm 4.08128547668457 0.369384765625 1.3605283498764038 0.797138512134552 1.2087469100952148 4.0376482009887695\n",
            "repr, std, cov, clossl, z, norm 0.41962024569511414 0.358154296875 2.3832297325134277 0.6300927996635437 1.2770063877105713 4.548581123352051\n",
            "repr, std, cov, clossl, z, norm 2.108631134033203 0.3662109375 1.4641826152801514 0.8315773606300354 1.3129934072494507 4.112534523010254\n",
            "repr, std, cov, clossl, z, norm 0.38925284147262573 0.361083984375 1.6557071208953857 0.5683039426803589 1.3594132661819458 4.150701999664307\n",
            "repr, std, cov, clossl, z, norm 1.6450324058532715 0.359619140625 1.868148684501648 0.7780997157096863 1.2719827890396118 4.603979587554932\n",
            "repr, std, cov, clossl, z, norm 0.38012465834617615 0.359130859375 1.8017001152038574 0.6230698823928833 1.2592687606811523 4.603760719299316\n",
            "repr, std, cov, clossl, z, norm 1.5409008264541626 0.367431640625 1.3645477294921875 0.7717812657356262 1.2739571332931519 4.188060760498047\n",
            "repr, std, cov, clossl, z, norm 0.36393558979034424 0.36279296875 1.4696890115737915 0.512698769569397 1.2543855905532837 4.219520568847656\n",
            "repr, std, cov, clossl, z, norm 1.4527276754379272 0.358642578125 1.8152391910552979 0.7685165405273438 1.243752360343933 3.9918124675750732\n",
            "9\n",
            "repr, std, cov, clossl, z, norm 0.4119431972503662 0.353515625 2.182803153991699 0.5295148491859436 1.2759690284729004 3.6673948764801025\n",
            "repr, std, cov, clossl, z, norm 2.555804967880249 0.35498046875 1.9877928495407104 0.7989456057548523 1.3192485570907593 4.3317084312438965\n",
            "repr, std, cov, clossl, z, norm 0.48436400294303894 0.3583984375 1.636396050453186 0.6730228066444397 1.2629092931747437 5.827895164489746\n",
            "repr, std, cov, clossl, z, norm 2.5547258853912354 0.354248046875 1.934391975402832 0.8182070255279541 1.2303682565689087 7.419598579406738\n",
            "repr, std, cov, clossl, z, norm 0.46705564856529236 0.351318359375 2.409254550933838 0.6928290128707886 1.294257640838623 6.504745006561279\n",
            "repr, std, cov, clossl, z, norm 1.8329366445541382 0.361572265625 1.726623773574829 0.7862903475761414 1.2229574918746948 5.030542850494385\n",
            "repr, std, cov, clossl, z, norm 0.38950517773628235 0.374267578125 1.0195614099502563 0.6761432886123657 1.2364305257797241 6.41029691696167\n",
            "repr, std, cov, clossl, z, norm 1.6788690090179443 0.37109375 1.134227991104126 0.7658172249794006 1.2256532907485962 6.250451564788818\n",
            "repr, std, cov, clossl, z, norm 0.37711527943611145 0.365478515625 1.5056836605072021 0.579589307308197 1.2697954177856445 6.185756683349609\n",
            "repr, std, cov, clossl, z, norm 1.3372409343719482 0.356201171875 2.109328269958496 0.7910770177841187 1.2900874614715576 4.68447208404541\n",
            "repr, std, cov, clossl, z, norm 0.37246862053871155 0.349609375 2.1123759746551514 0.6261422038078308 1.2769347429275513 5.160528659820557\n",
            "repr, std, cov, clossl, z, norm 1.5912352800369263 0.353515625 1.8890061378479004 0.7381590604782104 1.249656081199646 4.968817710876465\n",
            "repr, std, cov, clossl, z, norm 0.4035188555717468 0.359130859375 1.7128232717514038 0.5896462798118591 1.2867342233657837 8.697747230529785\n",
            "repr, std, cov, clossl, z, norm 2.00138521194458 0.3603515625 1.7032405138015747 0.7223464250564575 1.2238121032714844 6.152091979980469\n",
            "repr, std, cov, clossl, z, norm 0.3811120390892029 0.3623046875 1.5064806938171387 0.5736151933670044 1.3220003843307495 5.557088375091553\n",
            "repr, std, cov, clossl, z, norm 2.9427783489227295 0.359130859375 1.621515154838562 0.7218912243843079 1.3497848510742188 6.370952606201172\n",
            "10\n",
            "repr, std, cov, clossl, z, norm 0.46298080682754517 0.35498046875 1.790045976638794 0.599145770072937 1.301037311553955 5.311916828155518\n",
            "repr, std, cov, clossl, z, norm 10.778857231140137 0.3544921875 1.8793087005615234 0.812315821647644 1.3255654573440552 3.4667162895202637\n",
            "repr, std, cov, clossl, z, norm 0.40176063776016235 0.351806640625 1.9875277280807495 0.5764304995536804 1.2574388980865479 7.021871089935303\n",
            "repr, std, cov, clossl, z, norm 4.759909152984619 0.357666015625 1.6954247951507568 0.7945417165756226 1.2480028867721558 4.260780334472656\n",
            "repr, std, cov, clossl, z, norm 0.365949809551239 0.3671875 1.2769138813018799 0.5443287491798401 1.2613743543624878 3.8544886112213135\n",
            "repr, std, cov, clossl, z, norm 3.315664291381836 0.3642578125 1.3801237344741821 0.7694430947303772 1.223576307296753 4.25027322769165\n",
            "repr, std, cov, clossl, z, norm 0.3214128315448761 0.35546875 1.7757054567337036 0.5824722647666931 1.2923332452774048 4.374415874481201\n",
            "repr, std, cov, clossl, z, norm 2.3694865703582764 0.35205078125 1.9717016220092773 0.7749379873275757 1.2793242931365967 3.4169304370880127\n",
            "repr, std, cov, clossl, z, norm 0.3090066909790039 0.35498046875 1.830602765083313 0.5726965069770813 1.2533433437347412 3.084186553955078\n",
            "repr, std, cov, clossl, z, norm 2.038865804672241 0.359375 1.6516661643981934 0.8233116865158081 1.230958104133606 5.856394290924072\n",
            "repr, std, cov, clossl, z, norm 0.29945966601371765 0.366943359375 1.3380427360534668 0.6005449891090393 1.3462347984313965 4.085116863250732\n",
            "repr, std, cov, clossl, z, norm 2.307199716567993 0.361572265625 1.6337591409683228 0.7723387479782104 1.2517260313034058 2.6077544689178467\n",
            "repr, std, cov, clossl, z, norm 0.2988441586494446 0.35595703125 1.7790160179138184 0.5519002676010132 1.2374236583709717 2.5032026767730713\n",
            "repr, std, cov, clossl, z, norm 1.8067224025726318 0.355224609375 1.7966490983963013 0.7545424103736877 1.2198152542114258 2.1994190216064453\n",
            "repr, std, cov, clossl, z, norm 0.34479349851608276 0.354248046875 1.8493504524230957 0.6151137948036194 1.257588505744934 3.3380255699157715\n",
            "repr, std, cov, clossl, z, norm 3.3965227603912354 0.34912109375 2.1227011680603027 0.7280323505401611 1.3083646297454834 2.095534563064575\n",
            "11\n",
            "repr, std, cov, clossl, z, norm 0.3485235869884491 0.343505859375 2.4500083923339844 0.6239112019538879 1.315024495124817 2.1179022789001465\n",
            "repr, std, cov, clossl, z, norm 2.1535496711730957 0.364013671875 1.4552706480026245 0.8317946195602417 1.2819417715072632 2.6221508979797363\n",
            "repr, std, cov, clossl, z, norm 0.34402671456336975 0.374267578125 1.0915511846542358 0.5971273183822632 1.3192534446716309 1.77045476436615\n",
            "repr, std, cov, clossl, z, norm 1.4202234745025635 0.37451171875 1.070797324180603 0.7357344627380371 1.3101431131362915 2.3679258823394775\n",
            "repr, std, cov, clossl, z, norm 0.3180477023124695 0.36181640625 1.5033988952636719 0.5390297770500183 1.279022216796875 1.9808810949325562\n",
            "repr, std, cov, clossl, z, norm 1.1768110990524292 0.34326171875 2.4810571670532227 0.7891375422477722 1.3507148027420044 2.3803374767303467\n",
            "repr, std, cov, clossl, z, norm 0.31724902987480164 0.330810546875 3.552330493927002 0.5254672169685364 1.2252177000045776 0.9996098875999451\n",
            "repr, std, cov, clossl, z, norm 5.406391620635986 0.361083984375 1.5854452848434448 0.7383166551589966 1.23357355594635 1.1178555488586426\n",
            "repr, std, cov, clossl, z, norm 0.337724894285202 0.375244140625 1.2632651329040527 0.5783804059028625 1.2646244764328003 1.90869140625\n",
            "repr, std, cov, clossl, z, norm 3.666653633117676 0.3818359375 1.114638090133667 0.7757117748260498 1.2687127590179443 4.181267261505127\n",
            "repr, std, cov, clossl, z, norm 0.3252803087234497 0.3974609375 0.6847060322761536 0.5124738216400146 1.2803819179534912 2.853337049484253\n",
            "repr, std, cov, clossl, z, norm 2.123297929763794 0.386962890625 1.2135183811187744 0.7469582557678223 1.3255935907363892 1.9889801740646362\n",
            "repr, std, cov, clossl, z, norm 0.3605785369873047 0.3671875 2.281322717666626 0.56258624792099 1.2164075374603271 2.8359525203704834\n",
            "repr, std, cov, clossl, z, norm 1.7740403413772583 0.372802734375 1.520580768585205 0.7892522215843201 1.3410617113113403 2.2641501426696777\n",
            "repr, std, cov, clossl, z, norm 0.38027653098106384 0.37353515625 1.5575852394104004 0.5634502172470093 1.2771258354187012 1.356499433517456\n",
            "repr, std, cov, clossl, z, norm 1.5090309381484985 0.37841796875 0.9307504892349243 0.8229753971099854 1.1933000087738037 1.9894906282424927\n",
            "12\n",
            "repr, std, cov, clossl, z, norm 0.3740476965904236 0.37548828125 1.1458301544189453 0.5582200884819031 1.2775862216949463 2.9639244079589844\n",
            "repr, std, cov, clossl, z, norm 1.3543834686279297 0.36328125 1.9101141691207886 0.8702014684677124 1.2916126251220703 3.8456242084503174\n",
            "repr, std, cov, clossl, z, norm 0.3738683760166168 0.35498046875 3.1137890815734863 0.5457814335823059 1.3634161949157715 3.243314504623413\n",
            "repr, std, cov, clossl, z, norm 1.2760684490203857 0.354736328125 2.580148935317993 0.8296034336090088 1.3452244997024536 3.789097547531128\n",
            "repr, std, cov, clossl, z, norm 0.37209352850914 0.377197265625 1.0306239128112793 0.5602479577064514 1.275877594947815 4.16431999206543\n",
            "repr, std, cov, clossl, z, norm 1.2951959371566772 0.377197265625 0.9634093046188354 0.7377286553382874 1.2283220291137695 3.9469048976898193\n",
            "repr, std, cov, clossl, z, norm 0.40780141949653625 0.37060546875 1.2686662673950195 0.6043509840965271 1.2572576999664307 5.6172308921813965\n",
            "repr, std, cov, clossl, z, norm 1.5840868949890137 0.361083984375 1.97415030002594 0.7836953401565552 1.279475450515747 2.3625712394714355\n",
            "repr, std, cov, clossl, z, norm 0.4032897353172302 0.348388671875 3.019233226776123 0.5648037195205688 1.3175383806228638 5.410429954528809\n",
            "repr, std, cov, clossl, z, norm 2.3659722805023193 0.36474609375 1.5253351926803589 0.7390723824501038 1.2696738243103027 4.8111371994018555\n",
            "repr, std, cov, clossl, z, norm 0.40497732162475586 0.3662109375 1.3592981100082397 0.5831906795501709 1.2401762008666992 6.036224842071533\n",
            "repr, std, cov, clossl, z, norm 3.921058177947998 0.37060546875 1.1212289333343506 0.8174570202827454 1.2038072347640991 5.294412136077881\n",
            "repr, std, cov, clossl, z, norm 0.5202732682228088 0.3779296875 1.082716941833496 0.5191419720649719 1.2680728435516357 4.473343849182129\n",
            "repr, std, cov, clossl, z, norm 5.542510032653809 0.381103515625 0.9040496349334717 0.7717832326889038 1.2580046653747559 4.3070759773254395\n",
            "repr, std, cov, clossl, z, norm 0.495691180229187 0.379638671875 0.9605257511138916 0.4973977208137512 1.3195056915283203 3.667901039123535\n",
            "repr, std, cov, clossl, z, norm 3.876648426055908 0.37353515625 1.289332628250122 0.6888441443443298 1.3046884536743164 5.039984703063965\n",
            "13\n",
            "repr, std, cov, clossl, z, norm 0.43576812744140625 0.356689453125 2.535842180252075 0.5641847252845764 1.27195405960083 4.412045478820801\n",
            "repr, std, cov, clossl, z, norm 2.9544243812561035 0.358154296875 2.1437106132507324 0.7954798936843872 1.2654078006744385 4.526449680328369\n",
            "repr, std, cov, clossl, z, norm 0.39674270153045654 0.36376953125 1.6933544874191284 0.6432554125785828 1.2825884819030762 3.337101936340332\n",
            "repr, std, cov, clossl, z, norm 1.9324098825454712 0.3720703125 1.1150126457214355 0.7982473373413086 1.332553744316101 5.450586318969727\n",
            "repr, std, cov, clossl, z, norm 0.3630222976207733 0.384765625 0.6373687982559204 0.6095084547996521 1.2680096626281738 4.565979957580566\n",
            "repr, std, cov, clossl, z, norm 1.7714216709136963 0.388427734375 0.5934133529663086 0.7652276158332825 1.2692519426345825 4.737739562988281\n",
            "repr, std, cov, clossl, z, norm 0.3458866477012634 0.367919921875 1.2651031017303467 0.554934561252594 1.2397671937942505 4.660089015960693\n",
            "repr, std, cov, clossl, z, norm 1.7951552867889404 0.374755859375 1.0923160314559937 0.7206518054008484 1.2446526288986206 3.967343330383301\n",
            "repr, std, cov, clossl, z, norm 0.33971765637397766 0.362548828125 1.5369107723236084 0.5489259362220764 1.2669559717178345 3.0696470737457275\n",
            "repr, std, cov, clossl, z, norm 1.7673782110214233 0.34326171875 2.6105422973632812 0.7556253671646118 1.3017160892486572 4.5737528800964355\n",
            "repr, std, cov, clossl, z, norm 0.36201947927474976 0.3427734375 2.310983419418335 0.6195654273033142 1.2720966339111328 3.782475709915161\n",
            "repr, std, cov, clossl, z, norm 1.639994502067566 0.345703125 1.8875724077224731 0.7794090509414673 1.2050615549087524 3.129398822784424\n",
            "repr, std, cov, clossl, z, norm 0.3770269751548767 0.352294921875 1.6160719394683838 0.5872675180435181 1.2173212766647339 3.2477235794067383\n",
            "repr, std, cov, clossl, z, norm 1.6432331800460815 0.3544921875 1.4053711891174316 0.788084089756012 1.3813079595565796 3.394456148147583\n",
            "repr, std, cov, clossl, z, norm 0.3982192575931549 0.36767578125 0.9957601428031921 0.5974557995796204 1.2735092639923096 5.0444746017456055\n",
            "repr, std, cov, clossl, z, norm 1.5809180736541748 0.349853515625 1.584770679473877 0.7880116105079651 1.2812168598175049 4.856307506561279\n",
            "14\n",
            "repr, std, cov, clossl, z, norm 0.4136672019958496 0.31005859375 4.666446685791016 0.6509774923324585 1.2953318357467651 1.6788736581802368\n",
            "repr, std, cov, clossl, z, norm 1.3389235734939575 0.31103515625 3.9427490234375 0.8158946633338928 1.231927752494812 5.328178882598877\n",
            "repr, std, cov, clossl, z, norm 0.47497087717056274 0.34375 1.8466943502426147 0.6192670464515686 1.231305718421936 6.251908302307129\n",
            "repr, std, cov, clossl, z, norm 1.2652668952941895 0.353759765625 1.3638603687286377 0.8095632791519165 1.2278621196746826 5.664268493652344\n",
            "repr, std, cov, clossl, z, norm 0.5538051724433899 0.3408203125 2.883408784866333 0.7021473050117493 1.23722505569458 4.4187116622924805\n",
            "repr, std, cov, clossl, z, norm 1.3149399757385254 0.35009765625 2.332165241241455 0.7739914655685425 1.2340590953826904 2.9495434761047363\n",
            "repr, std, cov, clossl, z, norm 0.47252020239830017 0.35986328125 1.5490946769714355 0.6421656608581543 1.2798550128936768 1.205685019493103\n",
            "repr, std, cov, clossl, z, norm 1.4601459503173828 0.367431640625 1.1449549198150635 0.8158249258995056 1.2666969299316406 1.1400508880615234\n",
            "repr, std, cov, clossl, z, norm 0.4128701984882355 0.361083984375 1.4120268821716309 0.603420615196228 1.2879109382629395 2.9633891582489014\n",
            "repr, std, cov, clossl, z, norm 1.4117730855941772 0.364990234375 1.2330901622772217 0.7812160849571228 1.4063526391983032 5.177752494812012\n",
            "repr, std, cov, clossl, z, norm 0.39160871505737305 0.353271484375 1.8568341732025146 0.5662150382995605 1.3459625244140625 4.341135501861572\n",
            "repr, std, cov, clossl, z, norm 1.3449747562408447 0.34912109375 1.9794797897338867 0.7732840776443481 1.2902015447616577 4.127831935882568\n",
            "repr, std, cov, clossl, z, norm 0.3217073082923889 0.36083984375 1.2564542293548584 0.621344804763794 1.3241913318634033 3.504554510116577\n",
            "repr, std, cov, clossl, z, norm 1.1040632724761963 0.354248046875 1.4797499179840088 0.7777530550956726 1.2783570289611816 1.130596399307251\n",
            "repr, std, cov, clossl, z, norm 0.2946758568286896 0.35546875 1.2255148887634277 0.5517301559448242 1.277753472328186 4.510646820068359\n",
            "repr, std, cov, clossl, z, norm 1.2805663347244263 0.345703125 1.5778207778930664 0.767960250377655 1.2963581085205078 4.976919174194336\n",
            "15\n",
            "repr, std, cov, clossl, z, norm 0.3003701865673065 0.33203125 2.5218446254730225 0.5486814379692078 1.3018304109573364 6.4314188957214355\n",
            "repr, std, cov, clossl, z, norm 1.4465649127960205 0.328125 2.4296607971191406 0.7605972290039062 1.267421007156372 5.689092636108398\n",
            "repr, std, cov, clossl, z, norm 0.31609249114990234 0.32666015625 2.673163414001465 0.5810984969139099 1.2686383724212646 6.028860092163086\n",
            "repr, std, cov, clossl, z, norm 1.5517548322677612 0.327880859375 2.588346004486084 0.7904182076454163 1.3184733390808105 6.42495584487915\n",
            "repr, std, cov, clossl, z, norm 0.3449752628803253 0.3271484375 2.7618327140808105 0.5618464946746826 1.309329628944397 2.8992631435394287\n",
            "repr, std, cov, clossl, z, norm 1.2372926473617554 0.33544921875 2.363467216491699 0.7753035426139832 1.280875325202942 5.460075855255127\n",
            "repr, std, cov, clossl, z, norm 0.3731182813644409 0.352294921875 1.4038915634155273 0.5289962291717529 1.293661117553711 3.329685926437378\n",
            "repr, std, cov, clossl, z, norm 2.3379132747650146 0.347900390625 1.5618226528167725 0.834047257900238 1.2420884370803833 3.479395866394043\n",
            "repr, std, cov, clossl, z, norm 0.370186984539032 0.348388671875 1.7643870115280151 0.6392731070518494 1.2210602760314941 2.9907476902008057\n",
            "repr, std, cov, clossl, z, norm 2.075336456298828 0.35400390625 1.4909095764160156 0.7981550097465515 1.3256113529205322 5.003111839294434\n",
            "repr, std, cov, clossl, z, norm 0.3169696629047394 0.3544921875 1.2808455228805542 0.5702406167984009 1.2677369117736816 3.325995445251465\n",
            "repr, std, cov, clossl, z, norm 1.3275954723358154 0.34912109375 1.6376631259918213 0.6847137808799744 1.2654567956924438 4.013431549072266\n",
            "repr, std, cov, clossl, z, norm 0.3460646867752075 0.343994140625 1.7006908655166626 0.5441227555274963 1.3478285074234009 2.09041166305542\n",
            "repr, std, cov, clossl, z, norm 1.9035075902938843 0.34375 1.724622368812561 0.8405161499977112 1.252731442451477 4.3407368659973145\n",
            "repr, std, cov, clossl, z, norm 0.33729010820388794 0.336181640625 2.2057926654815674 0.5093119740486145 1.2750321626663208 5.3896379470825195\n",
            "repr, std, cov, clossl, z, norm 1.9413409233093262 0.33740234375 2.0445432662963867 0.6984049677848816 1.2098270654678345 3.7593088150024414\n",
            "16\n",
            "repr, std, cov, clossl, z, norm 0.2767740488052368 0.33154296875 2.552403450012207 0.550049364566803 1.2617417573928833 1.6356079578399658\n",
            "repr, std, cov, clossl, z, norm 1.7606580257415771 0.337890625 2.141852617263794 0.7668300867080688 1.3926140069961548 3.545961618423462\n",
            "repr, std, cov, clossl, z, norm 0.2900250554084778 0.33984375 1.9853259325027466 0.4926212728023529 1.2916382551193237 1.5955764055252075\n",
            "repr, std, cov, clossl, z, norm 1.485169768333435 0.345947265625 1.789241909980774 0.8215513825416565 1.326640009880066 1.0489802360534668\n",
            "repr, std, cov, clossl, z, norm 0.33133751153945923 0.354736328125 1.2565374374389648 0.5791789889335632 1.3094500303268433 2.312777042388916\n",
            "repr, std, cov, clossl, z, norm 1.686643123626709 0.34716796875 1.5723741054534912 0.7671434283256531 1.322534441947937 2.6334471702575684\n",
            "repr, std, cov, clossl, z, norm 0.400258332490921 0.337646484375 2.3625059127807617 0.5756831169128418 1.3135656118392944 2.948091506958008\n",
            "repr, std, cov, clossl, z, norm 2.218956470489502 0.34033203125 1.8552253246307373 0.7317550778388977 1.2868084907531738 2.6639723777770996\n",
            "repr, std, cov, clossl, z, norm 0.37747687101364136 0.343994140625 1.8060115575790405 0.575711727142334 1.3353976011276245 2.031099319458008\n",
            "repr, std, cov, clossl, z, norm 2.0409419536590576 0.345458984375 1.8321648836135864 0.8224360942840576 1.3000928163528442 2.809070348739624\n",
            "repr, std, cov, clossl, z, norm 0.32594698667526245 0.342041015625 2.00944185256958 0.5048321485519409 1.2779446840286255 3.7239062786102295\n",
            "repr, std, cov, clossl, z, norm 1.8150266408920288 0.34326171875 1.9475741386413574 0.8447118997573853 1.3045181035995483 2.412494421005249\n",
            "repr, std, cov, clossl, z, norm 0.2976894676685333 0.3349609375 2.167081117630005 0.501061201095581 1.278303861618042 0.8840981721878052\n",
            "repr, std, cov, clossl, z, norm 1.9489012956619263 0.339111328125 1.9054343700408936 0.745466947555542 1.312247395515442 0.767630398273468\n",
            "repr, std, cov, clossl, z, norm 0.18812903761863708 0.34326171875 1.6923952102661133 0.5180879831314087 1.299478530883789 2.5620508193969727\n",
            "repr, std, cov, clossl, z, norm 1.272740125656128 0.33642578125 2.0884740352630615 0.7154666185379028 1.3094412088394165 2.1076908111572266\n",
            "17\n",
            "repr, std, cov, clossl, z, norm 0.18898482620716095 0.32861328125 2.623227119445801 0.5148091316223145 1.2284237146377563 2.7086896896362305\n",
            "repr, std, cov, clossl, z, norm 1.2133620977401733 0.334228515625 2.218061923980713 0.8575686812400818 1.2861849069595337 3.3773293495178223\n",
            "repr, std, cov, clossl, z, norm 0.1840987354516983 0.33642578125 2.0400166511535645 0.5104717016220093 1.2715895175933838 2.467115640640259\n",
            "repr, std, cov, clossl, z, norm 1.0688050985336304 0.343017578125 1.7568029165267944 0.7252693176269531 1.3182213306427002 2.187174081802368\n",
            "repr, std, cov, clossl, z, norm 0.1944451928138733 0.34326171875 1.727135419845581 0.4851375222206116 1.324927806854248 2.818539619445801\n",
            "repr, std, cov, clossl, z, norm 0.9816182255744934 0.337646484375 1.9377689361572266 0.7509732246398926 1.2447267770767212 2.579314947128296\n",
            "repr, std, cov, clossl, z, norm 0.21642591059207916 0.3408203125 1.885732650756836 0.5405157804489136 1.3231464624404907 2.3324925899505615\n",
            "repr, std, cov, clossl, z, norm 0.9926671981811523 0.332763671875 2.3164191246032715 0.762948215007782 1.2518301010131836 2.887547016143799\n",
            "repr, std, cov, clossl, z, norm 0.2220786213874817 0.34033203125 1.945127248764038 0.48582923412323 1.2731680870056152 3.701263189315796\n",
            "repr, std, cov, clossl, z, norm 0.9077425003051758 0.33544921875 2.076420783996582 0.7827654480934143 1.2827069759368896 2.436777353286743\n",
            "repr, std, cov, clossl, z, norm 0.193556547164917 0.33056640625 2.375021457672119 0.5243142247200012 1.2604671716690063 4.034036159515381\n",
            "repr, std, cov, clossl, z, norm 0.7620304822921753 0.34375 1.6604583263397217 0.8223357200622559 1.3043547868728638 3.274930715560913\n",
            "repr, std, cov, clossl, z, norm 0.18051761388778687 0.339599609375 1.8865277767181396 0.5081327557563782 1.2542548179626465 3.3273091316223145\n",
            "repr, std, cov, clossl, z, norm 0.7556823492050171 0.334228515625 2.229978084564209 0.748546302318573 1.3061896562576294 3.176085948944092\n",
            "repr, std, cov, clossl, z, norm 0.1722167730331421 0.341796875 1.8908874988555908 0.5037819147109985 1.2588796615600586 2.2856554985046387\n",
            "repr, std, cov, clossl, z, norm 0.8289986848831177 0.339599609375 1.844238042831421 0.7840078473091125 1.3186092376708984 1.7425283193588257\n",
            "18\n",
            "repr, std, cov, clossl, z, norm 0.16262328624725342 0.331298828125 2.2136895656585693 0.5036157965660095 1.2524564266204834 3.8886871337890625\n",
            "repr, std, cov, clossl, z, norm 0.8870158195495605 0.32861328125 2.3979928493499756 0.7727075219154358 1.2684617042541504 4.160795211791992\n",
            "repr, std, cov, clossl, z, norm 0.16952960193157196 0.341796875 1.8006033897399902 0.5447990298271179 1.3308593034744263 3.7318670749664307\n",
            "repr, std, cov, clossl, z, norm 0.7506182789802551 0.34716796875 1.5817477703094482 0.7933210134506226 1.2598016262054443 3.9539601802825928\n",
            "repr, std, cov, clossl, z, norm 0.16125716269016266 0.339111328125 1.8794996738433838 0.5138534307479858 1.2804217338562012 3.922285318374634\n",
            "repr, std, cov, clossl, z, norm 0.6185674667358398 0.339599609375 1.7979235649108887 0.7884311079978943 1.3113194704055786 3.7704789638519287\n",
            "repr, std, cov, clossl, z, norm 0.1479499489068985 0.327880859375 2.42465877532959 0.5824443101882935 1.2389856576919556 2.718656539916992\n",
            "repr, std, cov, clossl, z, norm 0.7836049199104309 0.33056640625 2.332746982574463 0.7317537665367126 1.2710719108581543 4.295314788818359\n",
            "repr, std, cov, clossl, z, norm 0.14404775202274323 0.33251953125 2.273582935333252 0.5035989284515381 1.2577569484710693 3.6926281452178955\n",
            "repr, std, cov, clossl, z, norm 0.6329634785652161 0.336669921875 1.983638882637024 0.7756325006484985 1.3371387720108032 4.122750759124756\n",
            "repr, std, cov, clossl, z, norm 0.15068456530570984 0.339111328125 1.904247522354126 0.4861939251422882 1.2260593175888062 4.562632083892822\n",
            "repr, std, cov, clossl, z, norm 0.650736927986145 0.349365234375 1.4399323463439941 0.8082473874092102 1.2401210069656372 5.205366611480713\n",
            "repr, std, cov, clossl, z, norm 0.15151652693748474 0.339599609375 1.9745919704437256 0.4982822835445404 1.2468392848968506 4.566681861877441\n",
            "repr, std, cov, clossl, z, norm 0.8036333918571472 0.33447265625 2.0693721771240234 0.7593991756439209 1.1939438581466675 3.1039373874664307\n",
            "repr, std, cov, clossl, z, norm 0.17196859419345856 0.32568359375 2.5545897483825684 0.49625417590141296 1.3052012920379639 3.5396552085876465\n",
            "repr, std, cov, clossl, z, norm 1.1965945959091187 0.34033203125 1.790346622467041 0.7661439776420593 1.290245771408081 1.9775817394256592\n",
            "19\n",
            "repr, std, cov, clossl, z, norm 0.1873767375946045 0.349365234375 1.4353834390640259 0.538387656211853 1.2898921966552734 2.671654224395752\n",
            "repr, std, cov, clossl, z, norm 1.2628629207611084 0.343017578125 1.6451905965805054 0.6834420561790466 1.3458937406539917 2.9859230518341064\n",
            "repr, std, cov, clossl, z, norm 0.2098071575164795 0.3271484375 2.548837184906006 0.514401376247406 1.1965655088424683 1.8191994428634644\n",
            "repr, std, cov, clossl, z, norm 1.1022083759307861 0.322509765625 2.805431604385376 0.7319838404655457 1.2388935089111328 1.570006251335144\n",
            "repr, std, cov, clossl, z, norm 0.18692581355571747 0.334716796875 2.1903796195983887 0.5187506675720215 1.3138766288757324 3.3225038051605225\n",
            "repr, std, cov, clossl, z, norm 0.9988831281661987 0.34228515625 1.7010668516159058 0.8182611465454102 1.3139420747756958 2.614443778991699\n",
            "repr, std, cov, clossl, z, norm 0.2056092768907547 0.34423828125 1.643963098526001 0.5553898215293884 1.2839391231536865 3.176281690597534\n",
            "repr, std, cov, clossl, z, norm 0.9379494786262512 0.342041015625 1.6740901470184326 0.6815432906150818 1.255219578742981 3.5141937732696533\n",
            "repr, std, cov, clossl, z, norm 0.20673683285713196 0.32958984375 2.3249917030334473 0.5293227434158325 1.2786269187927246 3.4828341007232666\n",
            "repr, std, cov, clossl, z, norm 0.8463876247406006 0.32568359375 2.547556161880493 0.7600904703140259 1.2902904748916626 3.74808406829834\n",
            "repr, std, cov, clossl, z, norm 0.18856635689735413 0.3310546875 2.3534820079803467 0.5170514583587646 1.2470706701278687 3.1354312896728516\n",
            "repr, std, cov, clossl, z, norm 1.019983172416687 0.336669921875 1.957406997680664 0.8094021081924438 1.3163506984710693 3.2318899631500244\n",
            "repr, std, cov, clossl, z, norm 0.1855449080467224 0.35107421875 1.4080116748809814 0.5084531307220459 1.3087538480758667 2.847562313079834\n",
            "repr, std, cov, clossl, z, norm 1.010653018951416 0.344970703125 1.6650493144989014 0.7871222496032715 1.2669081687927246 2.9120683670043945\n",
            "repr, std, cov, clossl, z, norm 0.18217192590236664 0.337646484375 1.9478437900543213 0.5312550663948059 1.3343043327331543 3.525129556655884\n",
            "repr, std, cov, clossl, z, norm 0.5835705995559692 0.330810546875 2.2351067066192627 0.7449415326118469 1.3528395891189575 3.6691339015960693\n",
            "20\n",
            "repr, std, cov, clossl, z, norm 0.18407675623893738 0.3310546875 2.2326767444610596 0.5011433959007263 1.236142635345459 3.8769917488098145\n",
            "repr, std, cov, clossl, z, norm 0.7592953443527222 0.333740234375 2.0827388763427734 0.805442214012146 1.3012325763702393 3.4661977291107178\n",
            "repr, std, cov, clossl, z, norm 0.1786949634552002 0.329345703125 2.499265670776367 0.5074122548103333 1.3183320760726929 2.699669599533081\n",
            "repr, std, cov, clossl, z, norm 0.6239854097366333 0.3330078125 2.231660842895508 0.7575898766517639 1.245443344116211 3.227918863296509\n",
            "repr, std, cov, clossl, z, norm 0.15993432700634003 0.340087890625 2.0428481101989746 0.5023906230926514 1.257164716720581 3.212425470352173\n",
            "repr, std, cov, clossl, z, norm 0.45224902033805847 0.35009765625 1.4882270097732544 0.7706452012062073 1.2874906063079834 1.5877976417541504\n",
            "repr, std, cov, clossl, z, norm 0.16009783744812012 0.354736328125 1.5731570720672607 0.4970114827156067 1.3292019367218018 4.078373432159424\n",
            "repr, std, cov, clossl, z, norm 0.8528541922569275 0.3544921875 1.514765739440918 0.7402068376541138 1.3286170959472656 2.64319109916687\n",
            "repr, std, cov, clossl, z, norm 0.12350507825613022 0.35107421875 1.8724384307861328 0.5301572680473328 1.270557165145874 1.8906883001327515\n",
            "repr, std, cov, clossl, z, norm 0.8939757943153381 0.358642578125 1.2621853351593018 0.6584855914115906 1.189375638961792 3.174974203109741\n",
            "repr, std, cov, clossl, z, norm 0.1428007036447525 0.36572265625 0.9722940921783447 0.49089574813842773 1.1733007431030273 3.378042221069336\n",
            "repr, std, cov, clossl, z, norm 0.7053947448730469 0.353759765625 1.4135931730270386 0.7158244848251343 1.279780626296997 1.7481334209442139\n",
            "repr, std, cov, clossl, z, norm 0.15220744907855988 0.32958984375 2.8018319606781006 0.5111626386642456 1.2584865093231201 2.642446517944336\n",
            "repr, std, cov, clossl, z, norm 0.7449873685836792 0.32080078125 3.1637895107269287 0.7395563125610352 1.2779959440231323 3.1847500801086426\n",
            "repr, std, cov, clossl, z, norm 0.14794133603572845 0.32568359375 2.607100486755371 0.5092180967330933 1.270956039428711 2.9355762004852295\n",
            "repr, std, cov, clossl, z, norm 0.7238795161247253 0.328369140625 2.385782241821289 0.6978111863136292 1.2791064977645874 4.181939125061035\n",
            "21\n",
            "repr, std, cov, clossl, z, norm 0.18682295083999634 0.332275390625 2.292860269546509 0.505224883556366 1.281701683998108 3.4806742668151855\n",
            "repr, std, cov, clossl, z, norm 1.0504266023635864 0.333740234375 2.4282753467559814 0.7408019304275513 1.2655540704727173 2.522498369216919\n",
            "repr, std, cov, clossl, z, norm 0.15248028934001923 0.343017578125 1.8720927238464355 0.5094219446182251 1.319172978401184 3.202686309814453\n",
            "repr, std, cov, clossl, z, norm 0.778999924659729 0.34423828125 1.8145527839660645 0.7657834887504578 1.2788066864013672 2.2494606971740723\n",
            "repr, std, cov, clossl, z, norm 0.15161384642124176 0.343994140625 1.8489389419555664 0.4978671669960022 1.2578855752944946 3.2704391479492188\n",
            "repr, std, cov, clossl, z, norm 0.6864138841629028 0.344970703125 1.7680349349975586 0.8037562370300293 1.3149760961532593 3.969611883163452\n",
            "repr, std, cov, clossl, z, norm 0.1620410531759262 0.345703125 1.5280574560165405 0.47013911604881287 1.2671189308166504 3.9110543727874756\n",
            "repr, std, cov, clossl, z, norm 0.7059489488601685 0.33984375 1.6984554529190063 0.7464349269866943 1.2750636339187622 2.712885856628418\n",
            "repr, std, cov, clossl, z, norm 0.18839828670024872 0.327880859375 2.251376152038574 0.4692920744419098 1.2492871284484863 3.9009978771209717\n",
            "repr, std, cov, clossl, z, norm 1.3034765720367432 0.322265625 2.7184276580810547 0.7460089921951294 1.3538457155227661 2.461005687713623\n",
            "repr, std, cov, clossl, z, norm 0.22954408824443817 0.318603515625 3.195361614227295 0.48018941283226013 1.2776213884353638 2.804795026779175\n",
            "repr, std, cov, clossl, z, norm 1.3402401208877563 0.334228515625 2.020059585571289 0.8342005014419556 1.3484643697738647 3.6676931381225586\n",
            "repr, std, cov, clossl, z, norm 0.2256409078836441 0.3388671875 1.6546977758407593 0.6186770796775818 1.260380744934082 2.7592320442199707\n",
            "repr, std, cov, clossl, z, norm 1.3811166286468506 0.343994140625 1.4268605709075928 0.8321963548660278 1.3011143207550049 2.651685953140259\n",
            "repr, std, cov, clossl, z, norm 0.16373997926712036 0.345947265625 1.345947027206421 0.6191797256469727 1.227037787437439 2.790238857269287\n",
            "repr, std, cov, clossl, z, norm 1.1278103590011597 0.33984375 1.658270239830017 0.7266848683357239 1.2797760963439941 2.320817470550537\n",
            "22\n",
            "repr, std, cov, clossl, z, norm 0.13259780406951904 0.3408203125 1.56593918800354 0.5182379484176636 1.2788338661193848 3.959094762802124\n",
            "repr, std, cov, clossl, z, norm 1.7390382289886475 0.34228515625 1.4344167709350586 0.7520528435707092 1.287260890007019 3.1025452613830566\n",
            "repr, std, cov, clossl, z, norm 0.12685894966125488 0.324462890625 2.374019145965576 0.4855911433696747 1.3103641271591187 2.5443830490112305\n",
            "repr, std, cov, clossl, z, norm 0.7905035018920898 0.32421875 2.27410888671875 0.7527570724487305 1.2958168983459473 3.0068962574005127\n",
            "repr, std, cov, clossl, z, norm 0.11412421613931656 0.324462890625 2.160926580429077 0.48141440749168396 1.2672672271728516 4.227644443511963\n",
            "repr, std, cov, clossl, z, norm 0.5754660964012146 0.326171875 1.9989112615585327 0.805229663848877 1.3257434368133545 2.5951738357543945\n",
            "repr, std, cov, clossl, z, norm 0.12430045753717422 0.336181640625 1.6373097896575928 0.4365963935852051 1.2671364545822144 3.2573392391204834\n",
            "repr, std, cov, clossl, z, norm 0.5747473835945129 0.326416015625 1.9940285682678223 0.69159334897995 1.307227611541748 2.633103609085083\n",
            "repr, std, cov, clossl, z, norm 0.12277490645647049 0.316650390625 2.4994750022888184 0.44970202445983887 1.2831361293792725 2.8428359031677246\n",
            "repr, std, cov, clossl, z, norm 0.5884600281715393 0.314453125 2.5719175338745117 0.7549320459365845 1.185813546180725 2.746832847595215\n",
            "repr, std, cov, clossl, z, norm 0.10733886808156967 0.309814453125 2.834512233734131 0.4543481767177582 1.2823237180709839 2.1831274032592773\n",
            "repr, std, cov, clossl, z, norm 0.5040673613548279 0.3212890625 2.133594036102295 0.666316568851471 1.3098753690719604 2.4913387298583984\n",
            "repr, std, cov, clossl, z, norm 0.11476919054985046 0.319580078125 2.2413978576660156 0.4527568221092224 1.2957769632339478 3.254560708999634\n",
            "repr, std, cov, clossl, z, norm 0.7114617228507996 0.3251953125 2.0449814796447754 0.6546104550361633 1.1728614568710327 1.8024805784225464\n",
            "repr, std, cov, clossl, z, norm 0.1331760138273239 0.32763671875 1.9255293607711792 0.478182315826416 1.2612426280975342 3.5225517749786377\n",
            "repr, std, cov, clossl, z, norm 0.9474035501480103 0.326904296875 1.8877348899841309 0.6458930373191833 1.2703169584274292 3.355046272277832\n",
            "23\n",
            "repr, std, cov, clossl, z, norm 0.1314534991979599 0.32861328125 1.802333950996399 0.447689026594162 1.3056646585464478 3.1049697399139404\n",
            "repr, std, cov, clossl, z, norm 0.7500561475753784 0.32177734375 2.108480453491211 0.7564279437065125 1.2995309829711914 3.4838428497314453\n",
            "repr, std, cov, clossl, z, norm 0.11852797865867615 0.314453125 2.4759130477905273 0.44116541743278503 1.2913545370101929 4.08820104598999\n",
            "repr, std, cov, clossl, z, norm 0.5349611043930054 0.310791015625 2.756016254425049 0.6952816247940063 1.2719875574111938 2.654409408569336\n",
            "repr, std, cov, clossl, z, norm 0.13390931487083435 0.30908203125 2.7189924716949463 0.4396640956401825 1.284895420074463 3.949021339416504\n",
            "repr, std, cov, clossl, z, norm 0.5280944108963013 0.313720703125 2.4455840587615967 0.7573964595794678 1.2810934782028198 3.190537929534912\n",
            "repr, std, cov, clossl, z, norm 0.1333775669336319 0.314208984375 2.562565803527832 0.48009321093559265 1.285468339920044 2.91817307472229\n",
            "repr, std, cov, clossl, z, norm 0.49994322657585144 0.321533203125 2.0668277740478516 0.7335421442985535 1.238781213760376 2.3877885341644287\n",
            "repr, std, cov, clossl, z, norm 0.11869960278272629 0.329833984375 1.7508265972137451 0.5573899745941162 1.2724584341049194 1.9207664728164673\n",
            "repr, std, cov, clossl, z, norm 0.6467339992523193 0.33203125 1.7113430500030518 0.6515908241271973 1.2176389694213867 2.7491188049316406\n",
            "repr, std, cov, clossl, z, norm 0.10487903654575348 0.334228515625 1.6059648990631104 0.5634045600891113 1.3038990497589111 2.5171351432800293\n",
            "repr, std, cov, clossl, z, norm 0.39882445335388184 0.332763671875 1.6491469144821167 0.5786466598510742 1.2728773355484009 2.3180458545684814\n",
            "repr, std, cov, clossl, z, norm 0.08804496377706528 0.32763671875 1.9060733318328857 0.4804550111293793 1.2797054052352905 2.462132453918457\n",
            "repr, std, cov, clossl, z, norm 0.34240490198135376 0.326416015625 1.930461049079895 0.6360716819763184 1.2125893831253052 2.5037097930908203\n",
            "repr, std, cov, clossl, z, norm 0.08736903220415115 0.315673828125 2.4909579753875732 0.42958831787109375 1.2735092639923096 3.834258794784546\n",
            "repr, std, cov, clossl, z, norm 0.4457940459251404 0.314208984375 2.452658176422119 0.6857781410217285 1.2920762300491333 2.79187273979187\n",
            "24\n",
            "repr, std, cov, clossl, z, norm 0.10266198217868805 0.319580078125 2.1884565353393555 0.44514766335487366 1.2247960567474365 2.538405418395996\n",
            "repr, std, cov, clossl, z, norm 0.42279165983200073 0.31640625 2.3866488933563232 0.6287356019020081 1.3217970132827759 3.400895118713379\n",
            "repr, std, cov, clossl, z, norm 0.10448818653821945 0.313720703125 2.605560302734375 0.49259892106056213 1.3080636262893677 2.90372633934021\n",
            "repr, std, cov, clossl, z, norm 0.4165889620780945 0.321533203125 2.216299295425415 0.6852320432662964 1.2591954469680786 2.9276461601257324\n",
            "repr, std, cov, clossl, z, norm 0.08501405268907547 0.32373046875 2.076183319091797 0.46486663818359375 1.21572744846344 4.040730953216553\n",
            "repr, std, cov, clossl, z, norm 0.2920675277709961 0.325927734375 1.953161597251892 0.6717073321342468 1.2376805543899536 2.432490348815918\n",
            "repr, std, cov, clossl, z, norm 0.0786563977599144 0.323974609375 2.0103397369384766 0.45430994033813477 1.2558430433273315 2.4452929496765137\n",
            "repr, std, cov, clossl, z, norm 0.30651235580444336 0.319091796875 2.1987760066986084 0.590358555316925 1.2732292413711548 2.5805766582489014\n",
            "repr, std, cov, clossl, z, norm 0.07850909978151321 0.314208984375 2.440765380859375 0.45237621665000916 1.3019605875015259 3.636948347091675\n",
            "repr, std, cov, clossl, z, norm 0.26161354780197144 0.319091796875 2.1807141304016113 0.6560823917388916 1.2838255167007446 3.616105794906616\n",
            "repr, std, cov, clossl, z, norm 0.07919343560934067 0.3193359375 2.1743252277374268 0.45759695768356323 1.3254376649856567 2.901625394821167\n",
            "repr, std, cov, clossl, z, norm 0.26301077008247375 0.3193359375 2.1721129417419434 0.7348271608352661 1.3290032148361206 3.5845866203308105\n",
            "repr, std, cov, clossl, z, norm 0.09993677586317062 0.316650390625 2.342487096786499 0.6154738664627075 1.2668956518173218 3.064980983734131\n",
            "repr, std, cov, clossl, z, norm 0.34253832697868347 0.31640625 2.357365846633911 0.6283725500106812 1.2835768461227417 3.814016580581665\n",
            "repr, std, cov, clossl, z, norm 0.10515071451663971 0.31787109375 2.2583138942718506 0.6065898537635803 1.2821547985076904 3.2124221324920654\n",
            "repr, std, cov, clossl, z, norm 0.3140535354614258 0.3203125 2.1188437938690186 0.6664036512374878 1.258090615272522 2.56589937210083\n",
            "25\n",
            "repr, std, cov, clossl, z, norm 0.0916890874505043 0.320556640625 2.1368091106414795 0.5743114948272705 1.3071205615997314 2.9595441818237305\n",
            "repr, std, cov, clossl, z, norm 0.2724979817867279 0.319091796875 2.185816764831543 0.5930529236793518 1.3097670078277588 3.1718552112579346\n",
            "repr, std, cov, clossl, z, norm 0.10073970258235931 0.315673828125 2.347435474395752 0.4365907311439514 1.2926234006881714 3.0285849571228027\n",
            "repr, std, cov, clossl, z, norm 0.29128867387771606 0.315185546875 2.334653615951538 0.6529374718666077 1.2124396562576294 3.1471614837646484\n",
            "repr, std, cov, clossl, z, norm 0.061798885464668274 0.3173828125 2.256404399871826 0.5270556807518005 1.271539568901062 4.863674640655518\n",
            "repr, std, cov, clossl, z, norm 0.20782586932182312 0.31298828125 2.511929512023926 0.5912519097328186 1.2351199388504028 3.8384101390838623\n",
            "repr, std, cov, clossl, z, norm 0.07370156794786453 0.3154296875 2.3586173057556152 0.41270217299461365 1.2659879922866821 3.3347041606903076\n",
            "repr, std, cov, clossl, z, norm 0.30104145407676697 0.3154296875 2.327995777130127 0.6136975884437561 1.2269320487976074 2.867220878601074\n",
            "repr, std, cov, clossl, z, norm 0.06523925065994263 0.3173828125 2.218235969543457 0.42257097363471985 1.213981032371521 3.8874995708465576\n",
            "repr, std, cov, clossl, z, norm 0.23835529386997223 0.32373046875 1.9328975677490234 0.6890366673469543 1.2861180305480957 3.25498366355896\n",
            "repr, std, cov, clossl, z, norm 0.06668420881032944 0.323974609375 1.9350582361221313 0.5762856006622314 1.2955873012542725 2.3197672367095947\n",
            "repr, std, cov, clossl, z, norm 0.2322000414133072 0.318115234375 2.209188461303711 0.7408573627471924 1.1741091012954712 3.62239670753479\n",
            "repr, std, cov, clossl, z, norm 0.06441225111484528 0.3173828125 2.191364049911499 0.5491617918014526 1.3673230409622192 2.7050716876983643\n",
            "repr, std, cov, clossl, z, norm 0.2174350619316101 0.312255859375 2.4299371242523193 0.5551368594169617 1.3374170064926147 4.585233211517334\n",
            "repr, std, cov, clossl, z, norm 0.07106838375329971 0.30419921875 3.006857395172119 0.4712461233139038 1.222981572151184 2.834709405899048\n",
            "repr, std, cov, clossl, z, norm 0.2535853981971741 0.318603515625 2.13144588470459 0.6397799849510193 1.2684824466705322 2.362422466278076\n",
            "26\n",
            "repr, std, cov, clossl, z, norm 0.060824062675237656 0.328857421875 1.6865553855895996 0.4188750684261322 1.3344964981079102 3.0765326023101807\n",
            "repr, std, cov, clossl, z, norm 0.19466182589530945 0.328857421875 1.6838675737380981 0.6485593318939209 1.2662290334701538 2.835637092590332\n",
            "repr, std, cov, clossl, z, norm 0.059173066169023514 0.3232421875 1.8969569206237793 0.47858694195747375 1.2912836074829102 3.343526601791382\n",
            "repr, std, cov, clossl, z, norm 0.20542827248573303 0.31640625 2.278378486633301 0.6406819224357605 1.3249794244766235 3.4129598140716553\n",
            "repr, std, cov, clossl, z, norm 0.05585926026105881 0.311767578125 2.4289588928222656 0.4844144284725189 1.3013641834259033 2.647568702697754\n",
            "repr, std, cov, clossl, z, norm 0.17346416413784027 0.3125 2.3151395320892334 0.6565659642219543 1.2587778568267822 2.830714225769043\n",
            "repr, std, cov, clossl, z, norm 0.05767705664038658 0.309326171875 2.3898043632507324 0.43142029643058777 1.3061408996582031 3.0587925910949707\n",
            "repr, std, cov, clossl, z, norm 0.1512710601091385 0.3125 2.1431727409362793 0.5252934694290161 1.2677868604660034 3.0304644107818604\n",
            "repr, std, cov, clossl, z, norm 0.05274900421500206 0.3134765625 2.05056095123291 0.4229993522167206 1.2313261032104492 2.3607637882232666\n",
            "repr, std, cov, clossl, z, norm 0.16500815749168396 0.308349609375 2.2600808143615723 0.4817376434803009 1.2743868827819824 2.972933530807495\n",
            "repr, std, cov, clossl, z, norm 0.05904591828584671 0.30615234375 2.378073215484619 0.4317890703678131 1.2582283020019531 2.996929168701172\n",
            "repr, std, cov, clossl, z, norm 0.2081076055765152 0.30126953125 2.6677403450012207 0.452457070350647 1.2683749198913574 3.4220240116119385\n",
            "repr, std, cov, clossl, z, norm 0.06154821440577507 0.2998046875 2.7768731117248535 0.43409258127212524 1.2657018899917603 3.1598362922668457\n",
            "repr, std, cov, clossl, z, norm 0.2742151618003845 0.309326171875 2.2536325454711914 0.6576946973800659 1.2994157075881958 3.7010369300842285\n",
            "repr, std, cov, clossl, z, norm 0.07177849858999252 0.31591796875 1.9178398847579956 0.4214964509010315 1.242222785949707 2.402970552444458\n",
            "repr, std, cov, clossl, z, norm 0.29697683453559875 0.3125 2.047020435333252 0.5683398246765137 1.3251625299453735 2.4514644145965576\n",
            "27\n",
            "repr, std, cov, clossl, z, norm 0.10967320948839188 0.306884765625 2.2979094982147217 0.4106958210468292 1.2367814779281616 2.5367534160614014\n",
            "repr, std, cov, clossl, z, norm 0.37649768590927124 0.2998046875 2.676011085510254 0.6880651712417603 1.2993552684783936 2.9676272869110107\n",
            "repr, std, cov, clossl, z, norm 0.08292112499475479 0.293212890625 3.031978130340576 0.4457348883152008 1.3558003902435303 3.2332918643951416\n",
            "repr, std, cov, clossl, z, norm 0.2507300078868866 0.294921875 2.9282288551330566 0.6056758165359497 1.305672287940979 3.169658660888672\n",
            "repr, std, cov, clossl, z, norm 0.08897220343351364 0.297119140625 2.8024191856384277 0.42414647340774536 1.2554218769073486 3.3204526901245117\n",
            "repr, std, cov, clossl, z, norm 0.2541946470737457 0.30126953125 2.5671043395996094 0.5800288915634155 1.249673843383789 2.1330363750457764\n",
            "repr, std, cov, clossl, z, norm 0.08177763968706131 0.30859375 2.2296457290649414 0.4391117990016937 1.230344295501709 3.104062557220459\n",
            "repr, std, cov, clossl, z, norm 0.2125115990638733 0.306884765625 2.2990145683288574 0.5661187171936035 1.3378500938415527 3.81398606300354\n",
            "repr, std, cov, clossl, z, norm 0.09079127758741379 0.30322265625 2.448493480682373 0.4674070477485657 1.2965762615203857 2.717942476272583\n",
            "repr, std, cov, clossl, z, norm 0.1886211335659027 0.303955078125 2.4270219802856445 0.5704439282417297 1.230386734008789 2.050476312637329\n",
            "repr, std, cov, clossl, z, norm 0.07462092489004135 0.304443359375 2.4343910217285156 0.419512540102005 1.322577714920044 2.552626132965088\n",
            "repr, std, cov, clossl, z, norm 0.17255531251430511 0.307373046875 2.273756980895996 0.5663374066352844 1.2951935529708862 2.9632675647735596\n",
            "repr, std, cov, clossl, z, norm 0.07860149443149567 0.305908203125 2.33773136138916 0.43969130516052246 1.2424798011779785 1.5663037300109863\n",
            "repr, std, cov, clossl, z, norm 0.19788184762001038 0.304931640625 2.3654122352600098 0.5565763711929321 1.2008975744247437 2.9646189212799072\n",
            "repr, std, cov, clossl, z, norm 0.06962455809116364 0.3037109375 2.4285857677459717 0.4915325939655304 1.266272783279419 3.098205804824829\n",
            "repr, std, cov, clossl, z, norm 0.16428273916244507 0.30029296875 2.6161746978759766 0.5715110301971436 1.2561863660812378 2.9693922996520996\n",
            "28\n",
            "repr, std, cov, clossl, z, norm 0.06153256446123123 0.302490234375 2.511337995529175 0.44172608852386475 1.3429269790649414 2.4338719844818115\n",
            "repr, std, cov, clossl, z, norm 0.1458185315132141 0.303955078125 2.4109411239624023 0.5389944314956665 1.2783461809158325 3.0398788452148438\n",
            "repr, std, cov, clossl, z, norm 0.07001595944166183 0.30419921875 2.3821372985839844 0.41739699244499207 1.2917910814285278 1.2806423902511597\n",
            "repr, std, cov, clossl, z, norm 0.16264969110488892 0.30419921875 2.4016690254211426 0.4907425045967102 1.2688241004943848 2.307417631149292\n",
            "repr, std, cov, clossl, z, norm 0.0562373585999012 0.304931640625 2.358562469482422 0.4947253167629242 1.2240610122680664 2.0628929138183594\n",
            "repr, std, cov, clossl, z, norm 0.1442590057849884 0.3017578125 2.568471908569336 0.5376772284507751 1.2787343263626099 2.889773368835449\n",
            "repr, std, cov, clossl, z, norm 0.060602281242609024 0.299560546875 2.641456127166748 0.45866191387176514 1.3776657581329346 2.682218313217163\n",
            "repr, std, cov, clossl, z, norm 0.16525104641914368 0.302001953125 2.4963414669036865 0.6124940514564514 1.2685309648513794 2.5293972492218018\n",
            "repr, std, cov, clossl, z, norm 0.05781355872750282 0.305419921875 2.3431196212768555 0.43273842334747314 1.3431559801101685 2.655461549758911\n",
            "repr, std, cov, clossl, z, norm 0.12856361269950867 0.306640625 2.276362180709839 0.6294912099838257 1.2221418619155884 1.7552475929260254\n",
            "repr, std, cov, clossl, z, norm 0.05866618454456329 0.30126953125 2.5372190475463867 0.4354126453399658 1.2366167306900024 2.250713586807251\n",
            "repr, std, cov, clossl, z, norm 0.15415552258491516 0.30029296875 2.5903449058532715 0.5089157223701477 1.2861679792404175 3.3798320293426514\n",
            "repr, std, cov, clossl, z, norm 0.055445533245801926 0.302978515625 2.471294403076172 0.5368353724479675 1.2534605264663696 3.2850539684295654\n",
            "repr, std, cov, clossl, z, norm 0.1323121339082718 0.30322265625 2.460491895675659 0.4544287621974945 1.2647593021392822 2.8918263912200928\n",
            "repr, std, cov, clossl, z, norm 0.05794762820005417 0.302001953125 2.497913360595703 0.4300832450389862 1.2646150588989258 3.4467830657958984\n",
            "repr, std, cov, clossl, z, norm 0.11502799391746521 0.303466796875 2.4344818592071533 0.4955463111400604 1.3124741315841675 4.073096752166748\n",
            "29\n",
            "repr, std, cov, clossl, z, norm 0.06265059113502502 0.304443359375 2.384469747543335 0.431776762008667 1.2289764881134033 4.34895658493042\n",
            "repr, std, cov, clossl, z, norm 0.17264461517333984 0.306884765625 2.2644259929656982 0.5174795985221863 1.2685413360595703 3.1673147678375244\n",
            "repr, std, cov, clossl, z, norm 0.06319816410541534 0.30712890625 2.2769172191619873 0.3726496994495392 1.28708016872406 3.2029874324798584\n",
            "repr, std, cov, clossl, z, norm 0.17042717337608337 0.304443359375 2.4028987884521484 0.5299280881881714 1.2360894680023193 3.6838603019714355\n",
            "repr, std, cov, clossl, z, norm 0.05779546499252319 0.302490234375 2.487489700317383 0.484925776720047 1.2678331136703491 3.7243733406066895\n",
            "repr, std, cov, clossl, z, norm 0.1531343013048172 0.3056640625 2.3076558113098145 0.4606485068798065 1.2392091751098633 3.4429237842559814\n",
            "repr, std, cov, clossl, z, norm 0.04995002597570419 0.30712890625 2.253765344619751 0.465952068567276 1.2188524007797241 2.9581921100616455\n",
            "repr, std, cov, clossl, z, norm 0.12484525144100189 0.3017578125 2.5098633766174316 0.537774384021759 1.284188985824585 2.9295692443847656\n",
            "repr, std, cov, clossl, z, norm 0.0482017807662487 0.3046875 2.3867807388305664 0.44550102949142456 1.2500096559524536 2.5793354511260986\n",
            "repr, std, cov, clossl, z, norm 0.11902150511741638 0.308349609375 2.191030502319336 0.49807009100914 1.2991266250610352 2.866616725921631\n",
            "repr, std, cov, clossl, z, norm 0.053766533732414246 0.31298828125 2.0308172702789307 0.4123610258102417 1.2922172546386719 2.185077667236328\n",
            "repr, std, cov, clossl, z, norm 0.12246841937303543 0.3017578125 2.5290284156799316 0.5914250612258911 1.2734954357147217 2.7017126083374023\n",
            "repr, std, cov, clossl, z, norm 0.05890515074133873 0.29248046875 3.0117392539978027 0.45798414945602417 1.2556315660476685 2.2405056953430176\n",
            "repr, std, cov, clossl, z, norm 0.15235348045825958 0.30126953125 2.543914318084717 0.5165586471557617 1.3348230123519897 2.184051513671875\n",
            "repr, std, cov, clossl, z, norm 0.056809891015291214 0.31103515625 2.1117119789123535 0.4552592635154724 1.2698585987091064 2.7868456840515137\n",
            "repr, std, cov, clossl, z, norm 0.17213355004787445 0.30712890625 2.2739012241363525 0.49218374490737915 1.2748738527297974 1.8063702583312988\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "import torchvision.transforms.v2 as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "for i in range(30):\n",
        "    print(i)\n",
        "    train_data = BufferDataset(buffer, seq_len) # one line of poem is roughly 50 characters\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "    agent.train_jepa(train_loader, optim)\n",
        "\n",
        "    # checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "    # torch.save(checkpoint, folder+'agentoptimargm.pkl')\n",
        "\n",
        "    # agentsd, _ = rename_sd(agent.state_dict())\n",
        "    # all_sd = store_sd(all_sd, agentsd)\n",
        "    # torch.save(all_sd, folder+'all_sd.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "id": "Z5-_pfGZTsip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "929efbfa-35c6-49a9-a6b8-2d88a01860d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# @title procgen\n",
        "# https://github.com/openai/procgen\n",
        "import gym\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\")\n",
        "# env = gym.make(\"procgen:procgen-coinrun-v0\", start_level=0, num_levels=1)\n",
        "\n",
        "# from procgen import ProcgenGym3Env\n",
        "# env = ProcgenGym3Env(num=1, env_name=\"coinrun\")\n",
        "\n",
        "env_name=\"procgen:procgen-{}-v0\".format(\"bigfish\") # https://github.com/openai/procgen/blob/master/procgen/gym_registration.py#L29\n",
        "env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\")\n",
        "# env = gym.make(env_name, use_sequential_levels=True, render_mode=\"rgb_array\", use_backgrounds=False, restrict_themes=True, use_monochrome_assets=True)\n",
        "\n",
        "# ENV_NAMES = [\"bigfish\", \"bossfight\", \"caveflyer\", \"chaser\", \"climber\", \"coinrun\", \"dodgeball\", \"fruitbot\", \"heist\", \"jumper\", \"leaper\", \"maze\", \"miner\", \"ninja\", \"plunder\", \"starpilot\",]\n",
        "\n",
        "\n",
        "# # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "# 2  5/11 8\n",
        "# 1/10 4 7/9\n",
        "# 0  3/12 6\n",
        "\n",
        "# 13 11 14\n",
        "# 10 12 9\n",
        "\n",
        "# from gymnasium.wrappers import TimeLimit\n",
        "from gym.wrappers import TimeLimit\n",
        "env = TimeLimit(env, max_episode_steps=600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "PraFUAPB3j7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f2e675-5825-4b94-c8c1-ba1f78f9f540",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-154606bd94b8>:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-36-154606bd94b8>:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "search c ['0.00344365', '0.605907', '0.789826', '0.00860297', '0.502342', '0.59049', '0.531441', '0.433819']\n",
            "search c ['0.0028842', '0.62947', '0.801099', '0.00142951', '0.0377435', '0.59049', '0.531441', '0.478251']\n",
            "search c ['0.00368633', '0.0487897', '0.809999', '0.729', '0.6561', '0.507264', '1.40169e-06', '4.98532e-12']\n",
            "search c ['0.00396259', '0.0404572', '0.809999', '0.729', '0.6561', '0.29991', '1.177e-06', '1.62608e-11']\n",
            "search c ['0.078511', '0.0407271', '1.49744e-07', '1.48436e-09', '0.000287456', '0.59049', '0.531441', '0.477912']\n",
            "search c ['0.00495405', '0.767798', '0.632212', '0.000441167', '0.157499', '0.59049', '0.531441', '0.44955']\n",
            "search c ['0.00293964', '0.779663', '0.809966', '0.000613706', '5.68684e-07', '0.000662066', '0.531441', '0.478297']\n",
            "search c ['0.00305027', '0.756631', '0.809965', '0.000696884', '6.27086e-07', '0.000532662', '0.531441', '0.478297']\n",
            "search c ['0.00586671', '0.801971', '0.808864', '0.00642527', '0.0729651', '0.590489', '0.531441', '0.236052']\n",
            "search c ['0.00706502', '0.772513', '0.805522', '0.0223156', '0.501598', '0.59049', '0.531441', '0.354212']\n",
            "search c ['0.075616', '2.79959e-05', '1.82587e-05', '0.726235', '0.6561', '0.59049', '0.00370753', '3.36809e-07']\n",
            "search c ['0.0264626', '0.0829475', '4.25634e-05', '1.98451e-06', '0.47087', '0.59049', '0.531441', '0.47555']\n",
            "search c ['0.0069815', '0.00433922', '0.80959', '0.729', '0.6561', '0.590487', '0.0273382', '2.3993e-08']\n",
            "search c ['0.00958119', '0.00175906', '0.805629', '0.729', '0.6561', '0.590486', '0.00937612', '1.48306e-08']\n",
            "search c ['0.0599115', '0.0445698', '5.8711e-07', '3.2722e-08', '0.0341895', '0.59049', '0.531441', '0.0643698']\n",
            "search c ['0.00243909', '0.0405413', '0.809999', '0.729', '0.656099', '0.590441', '0.528693', '0.000495007']\n",
            "search c ['0.102251', '0.000866253', '6.34074e-08', '4.71317e-12', '1.05914e-09', '0.588254', '0.531441', '0.477614']\n",
            "search c ['0.101139', '0.000999256', '7.11032e-08', '3.27266e-12', '3.51009e-10', '0.579336', '0.531441', '0.477724']\n",
            "search c ['0.105027', '0.0040219', '4.11296e-05', '0.462383', '0.6561', '0.59049', '0.531439', '0.00633379']\n",
            "search c ['0.00968592', '0.704943', '0.7669', '0.107742', '0.6548', '0.59049', '0.531441', '0.389464']\n",
            "search c ['0.00301196', '0.48842', '0.802269', '0.0151927', '0.56066', '0.59049', '0.531441', '0.47565']\n",
            "search c ['0.00324865', '0.495784', '0.790134', '0.00241126', '0.237786', '0.59049', '0.531441', '0.474133']\n",
            "search c ['0.0042906', '0.0583163', '0.809999', '0.729', '0.656099', '0.171907', '5.04016e-07', '1.79767e-11']\n",
            "search c ['0.00650705', '0.71405', '0.802216', '0.00773418', '0.445342', '0.59049', '0.53144', '0.0983631']\n",
            "search c ['0.00282713', '0.667454', '0.809999', '0.146124', '8.36334e-07', '7.61575e-07', '0.493518', '0.478297']\n",
            "search c ['0.00277845', '0.594544', '0.81', '0.425343', '4.16318e-06', '4.2299e-06', '0.515558', '0.478297']\n",
            "search c ['0.00682268', '0.05269', '0.80995', '0.729', '0.6561', '0.590487', '0.0678247', '3.86832e-08']\n",
            "search c ['0.00681423', '0.0269722', '0.809972', '0.729', '0.6561', '0.590481', '0.0108668', '1.08154e-08']\n",
            "search c ['0.00537223', '0.757886', '0.809974', '0.000960701', '6.80902e-08', '1.15201e-05', '0.531046', '0.462209']\n",
            "search c ['0.0041144', '0.221803', '0.81', '0.728989', '0.00211599', '4.42206e-05', '0.437796', '0.478291']\n",
            "search c ['0.00754682', '0.0198417', '0.809892', '0.729', '0.6561', '0.590488', '0.0278352', '4.74999e-08']\n",
            "search c ['0.00420587', '0.143114', '0.809984', '0.729', '0.6561', '0.590485', '0.0811055', '4.35625e-08']\n",
            "search c ['0.00301038', '0.792225', '0.809953', '0.608036', '0.654997', '0.587324', '0.0287206', '1.07688e-05']\n",
            "search c ['0.00289139', '0.746565', '0.809997', '0.725853', '0.000121103', '2.61336e-06', '0.205078', '0.478297']\n",
            "search c ['0.00325157', '0.0987007', '0.809999', '0.729', '0.6561', '0.589764', '0.000271054', '3.38841e-10']\n",
            "search c ['0.00313168', '0.0323123', '0.809998', '0.729', '0.6561', '0.587364', '3.54359e-05', '1.43162e-10']\n",
            "search c ['0.197199', '0.0150564', '2.18252e-07', '1.1361e-10', '1.83457e-11', '0.000702333', '0.531441', '0.478297']\n",
            "search c ['0.1615', '0.0359096', '1.85558e-06', '1.62326e-10', '9.31209e-14', '8.0293e-10', '0.531191', '0.478297']\n",
            "search c ['0.00307941', '0.662746', '0.80723', '0.0175256', '0.431011', '0.59049', '0.531441', '0.390424']\n",
            "search c ['0.0109247', '0.0688071', '0.192193', '0.728778', '0.6561', '0.59049', '0.53144', '0.1172']\n",
            "search c ['0.00383227', '0.0414946', '0.809999', '0.729', '0.6561', '0.588214', '0.00763496', '4.5019e-07']\n",
            "search c ['0.00401052', '0.0272136', '0.809998', '0.729', '0.656099', '0.589968', '0.531024', '0.364483']\n",
            "search c ['0.00742174', '0.0515065', '0.809996', '0.729', '0.6561', '0.590476', '0.00580344', '5.91913e-09']\n",
            "search c ['0.00823103', '0.0416465', '0.809992', '0.729', '0.6561', '0.590475', '0.0101868', '1.13772e-08']\n",
            "search c ['0.00286227', '0.717283', '0.808475', '0.00281944', '0.0550512', '0.59049', '0.531441', '0.420625']\n",
            "search c ['0.00204402', '0.038789', '0.81', '0.729', '0.655906', '0.00593088', '5.16196e-05', '5.50864e-08']\n",
            "search c ['0.00656068', '0.569556', '0.790776', '0.00157434', '0.0837106', '0.59049', '0.531441', '0.478283']\n",
            "search c ['0.153353', '2.19784e-05', '4.26938e-06', '0.688772', '0.6561', '0.59049', '0.00206333', '2.52954e-07']\n",
            "search c ['0.112897', '0.000303943', '1.39338e-06', '0.0359414', '0.6561', '0.59049', '0.531438', '0.463659']\n",
            "search c ['0.0108525', '0.589428', '0.0778612', '2.20006e-05', '0.0204637', '0.59049', '0.531441', '0.472076']\n",
            "search c ['0.00363465', '0.0891572', '0.81', '0.729', '0.6561', '0.203209', '1.09787e-06', '1.69983e-10']\n",
            "search c ['0.00381368', '0.0925111', '0.81', '0.729', '0.656097', '0.0229523', '3.64685e-05', '0.0142908']\n",
            "search c ['0.00738831', '0.804733', '0.803134', '0.00457402', '0.18106', '0.59049', '0.531441', '0.447165']\n",
            "search c ['0.00639109', '0.794501', '0.807914', '0.00281011', '0.0346111', '0.590489', '0.531441', '0.181971']\n",
            "search c ['0.00673895', '0.0649612', '0.81', '0.729', '0.656056', '0.00363049', '7.38382e-06', '0.0102923']\n",
            "search c ['0.316837', '0.00123301', '2.49762e-08', '5.18627e-07', '0.655904', '0.59049', '0.00438381', '4.16169e-06']\n",
            "search c ['0.112801', '4.46764e-05', '1.36587e-06', '0.310622', '0.6561', '0.59049', '0.472271', '0.470757']\n",
            "search c ['0.117691', '0.00253366', '9.74612e-09', '3.15974e-09', '0.125702', '0.590488', '5.96007e-06', '1.13489e-09']\n",
            "search c ['0.00403205', '0.103463', '0.81', '0.729', '0.656099', '0.589105', '0.516646', '0.475517']\n",
            "search c ['0.00418719', '0.0516431', '0.809999', '0.729', '0.6561', '0.220364', '5.76189e-06', '4.41469e-10']\n",
            "search c ['0.221511', '0.00015098', '4.91668e-07', '0.0170519', '0.6561', '0.59049', '0.0572805', '7.54626e-06']\n",
            "search c ['0.00425556', '0.0481852', '0.809999', '0.729', '0.656098', '0.589208', '0.530423', '0.164163']\n",
            "search c ['0.0055178', '0.197241', '0.809999', '0.729', '0.6561', '0.590474', '0.0121545', '1.95362e-08']\n",
            "search c ['0.00560341', '0.374117', '0.809991', '0.729', '0.6561', '0.590479', '0.0131821', '5.25505e-08']\n",
            "search c ['0.170745', '0.0081822', '7.67677e-08', '5.67055e-10', '8.86178e-05', '0.59049', '0.531441', '0.478061']\n",
            "search c ['0.16334', '0.0123918', '2.83347e-07', '4.44253e-11', '1.96803e-08', '0.590163', '0.531441', '0.478012']\n",
            "search c ['0.00206431', '0.0133324', '0.809996', '0.729', '0.6561', '0.544876', '8.18739e-07', '2.45876e-11']\n",
            "search c ['0.00222073', '0.0101291', '0.809994', '0.729', '0.6561', '0.335311', '7.71839e-07', '2.52144e-10']\n",
            "search c ['0.00179194', '0.0610707', '0.81', '0.729', '0.656075', '0.556615', '0.498019', '0.471215']\n",
            "search c ['0.0672478', '0.000845324', '1.22347e-05', '0.433478', '0.6561', '0.59049', '0.531427', '0.00378269']\n",
            "search c ['0.0929057', '0.00173321', '7.20255e-08', '6.22509e-12', '1.88007e-09', '0.588983', '0.531441', '0.477822']\n",
            "search c ['0.0644214', '0.000277211', '4.85915e-06', '0.210679', '0.6561', '0.59049', '0.531438', '0.380182']\n",
            "search c ['0.00314437', '0.14797', '0.809998', '0.729', '0.6561', '0.590479', '0.0163832', '1.98517e-08']\n",
            "search c ['0.00285477', '0.115128', '0.809999', '0.729', '0.6561', '0.59047', '0.00315556', '5.93176e-09']\n",
            "search c ['0.00240654', '0.485455', '0.809926', '0.00070437', '2.89726e-08', '7.92568e-07', '0.531319', '0.478297']\n",
            "search c ['0.114825', '2.63137e-05', '5.3486e-07', '0.230761', '0.6561', '0.590348', '0.000442721', '2.71346e-07']\n",
            "search c ['0.261484', '0.000560945', '7.99464e-07', '0.00626618', '0.6561', '0.59049', '0.238748', '1.19749e-05']\n",
            "search c ['0.00588038', '0.302096', '0.81', '0.729', '0.6561', '0.145208', '3.2697e-07', '1.55868e-08']\n",
            "search c ['0.00294603', '0.0572642', '0.81', '0.729', '0.656029', '0.590238', '0.529917', '0.000567159']\n",
            "search c ['0.00326762', '0.0480999', '0.809999', '0.729', '0.656083', '0.590296', '0.530523', '0.00159053']\n",
            "search c ['0.00378723', '0.569036', '0.809875', '0.000496477', '4.30808e-08', '1.55893e-05', '0.531071', '0.478297']\n",
            "search c ['0.00287714', '0.0346629', '0.809998', '0.729', '0.656071', '0.0590555', '0.00136088', '0.378336']\n",
            "search c ['0.171515', '0.000185846', '4.06045e-07', '0.0069657', '0.6561', '0.59049', '0.531413', '0.000387351']\n",
            "search c ['0.00550697', '0.608921', '0.804691', '0.485418', '0.65575', '0.59049', '0.531441', '0.299318']\n",
            "search c ['0.00193742', '0.0224798', '0.809998', '0.729', '0.656093', '0.167945', '4.01586e-05', '1.25039e-08']\n",
            "search c ['0.0850968', '1.58708e-05', '1.27094e-06', '0.538628', '0.6561', '0.59049', '0.00211496', '3.82942e-07']\n",
            "search c ['0.00459023', '0.013363', '0.809993', '0.729', '0.6561', '0.589623', '0.520648', '0.473919']\n",
            "search c ['0.208386', '0.00231593', '5.81424e-09', '3.03833e-10', '0.00969138', '0.59049', '2.76645e-05', '1.38568e-10']\n",
            "search c ['0.0024134', '0.0477985', '0.81', '0.729', '0.655897', '0.00121379', '3.69803e-05', '0.00110157']\n",
            "search c ['0.00507112', '0.446379', '0.80535', '0.718101', '0.656093', '0.59049', '0.531441', '0.467987']\n",
            "search c ['0.162912', '0.131246', '7.92368e-07', '8.6499e-11', '3.48592e-08', '0.590313', '0.531441', '0.478141']\n",
            "search c ['0.00450776', '0.85054', '0.81', '0.098206', '5.81527e-06', '3.50705e-06', '7.97188e-06', '0.00204763']\n",
            "search c ['0.294231', '9.81267e-05', '4.02159e-07', '0.00668837', '0.6561', '0.59049', '0.531428', '0.46141']\n",
            "search c ['0.0118445', '0.661958', '0.807481', '2.44967e-05', '2.4875e-09', '3.25801e-07', '0.531421', '0.478297']\n",
            "search c ['0.00621059', '0.0134533', '0.809967', '0.729', '0.6561', '0.590482', '0.017054', '1.21022e-08']\n",
            "search c ['0.00547981', '0.0182012', '0.809985', '0.729', '0.6561', '0.590475', '0.0046117', '5.65026e-09']\n",
            "search c ['0.125111', '5.07709e-05', '1.79202e-06', '0.0775483', '0.6561', '0.59049', '0.528363', '0.478002']\n",
            "search c ['0.00235727', '0.0539863', '0.81', '0.729', '0.0124358', '5.71685e-08', '4.38814e-08', '0.4009']\n",
            "search c ['0.00739691', '0.800628', '0.807797', '0.00428839', '0.0582913', '0.59049', '0.531441', '0.352177']\n",
            "search c ['0.0118105', '0.802433', '0.738643', '0.000265839', '0.0213222', '0.59049', '0.531441', '0.319705']\n",
            "search c ['0.00257607', '0.0091878', '0.809995', '0.729', '0.656073', '0.00352582', '1.28377e-05', '0.0453604']\n",
            "search c ['0.147682', '0.000213613', '8.98089e-09', '4.78384e-07', '0.655781', '0.59049', '0.530372', '0.476151']\n",
            "search c ['0.0135588', '0.0568432', '0.448866', '0.728997', '0.6561', '0.59049', '0.531438', '0.0510626']\n",
            "search c ['0.00305263', '0.746235', '0.809181', '0.00534491', '0.0586303', '0.590489', '0.531441', '0.0900221']\n",
            "search c ['0.00320688', '0.0488894', '0.809997', '0.729', '0.6561', '0.585483', '8.75028e-05', '2.19337e-06']\n",
            "search c ['0.00351697', '0.432005', '0.809934', '0.728777', '0.00264193', '5.30442e-08', '2.66632e-07', '0.476897']\n",
            "search c ['0.00593313', '0.799078', '0.804323', '0.00563084', '0.282946', '0.59049', '0.531441', '0.324294']\n",
            "search c ['0.005773', '0.787364', '0.805614', '0.00569852', '0.221467', '0.59049', '0.531441', '0.322816']\n",
            "search c ['0.0036791', '0.713694', '0.809733', '0.00045003', '3.10516e-05', '0.541265', '0.531441', '0.478297']\n",
            "search c ['0.00347157', '0.0869516', '0.809999', '0.729', '0.6561', '0.586255', '0.000196162', '2.03943e-06']\n",
            "search c ['0.00711464', '0.836092', '0.809747', '0.0178936', '0.11469', '0.590488', '0.531436', '0.0664181']\n",
            "search c ['0.00634846', '0.190266', '0.81', '0.729', '0.6561', '0.590473', '0.430293', '8.72514e-07']\n",
            "search c ['0.019105', '0.847286', '0.798105', '0.00195428', '0.475929', '0.590485', '0.245327', '3.89339e-07']\n",
            "search c ['0.170301', '0.138133', '0.012081', '3.24354e-07', '3.34454e-08', '0.145725', '0.531441', '0.478297']\n",
            "search c ['0.0246333', '0.00607794', '0.808785', '0.729', '0.6561', '0.59041', '0.061626', '7.38838e-08']\n",
            "search c ['0.386921', '0.000173518', '4.29122e-07', '0.0162189', '0.6561', '0.59049', '0.531383', '0.000337838']\n",
            "search c ['0.00597028', '0.189328', '0.81', '0.729', '0.656069', '0.00672507', '7.00179e-05', '0.000120667']\n",
            "search c ['0.00896145', '0.842101', '0.809978', '0.000446071', '1.92216e-08', '5.88382e-07', '0.531288', '0.478297']\n",
            "search c ['0.315602', '0.000189668', '7.25099e-07', '0.0258073', '0.6561', '0.59049', '0.531422', '0.00189698']\n",
            "search c ['0.0197866', '0.00619615', '0.808948', '0.729', '0.6561', '0.590458', '0.382145', '2.17403e-06']\n",
            "search c ['0.00428016', '0.0232094', '0.809999', '0.729', '0.235254', '0.00335244', '0.485753', '0.478297']\n",
            "search c ['0.0754587', '2.9867e-05', '4.60327e-05', '0.727675', '0.6561', '0.590489', '0.526736', '0.477449']\n",
            "search c ['0.152963', '3.81144e-05', '9.84154e-08', '0.00390157', '0.6561', '0.59049', '0.531299', '0.000168097']\n",
            "search c ['0.143447', '5.81865e-05', '1.55881e-07', '0.00598749', '0.6561', '0.59049', '0.531308', '9.33956e-05']\n",
            "search c ['0.00220539', '0.132649', '0.809994', '0.729', '0.656095', '0.590419', '0.531427', '0.473003']\n",
            "search c ['0.00477977', '0.0113955', '0.808839', '0.729', '0.6561', '0.590477', '0.531432', '0.47326']\n",
            "search c ['0.00657025', '0.0327704', '0.809991', '0.729', '0.6561', '0.590478', '0.00517351', '5.73398e-09']\n",
            "search c ['0.00723834', '0.0202391', '0.809973', '0.729', '0.6561', '0.590474', '0.00416175', '5.95564e-09']\n",
            "search c ['0.0039528', '0.0383505', '0.81', '0.729', '0.655737', '0.00310114', '2.64258e-05', '3.226e-08']\n",
            "search c ['0.00405329', '0.0363247', '0.809999', '0.729', '0.656011', '0.0311673', '0.000418276', '6.44873e-08']\n",
            "search c ['0.156783', '7.33814e-05', '4.69616e-07', '0.0339497', '0.6561', '0.59049', '0.028528', '4.38903e-06']\n",
            "search c ['0.00447404', '0.589677', '0.792991', '0.00126174', '0.113296', '0.59049', '0.531441', '0.466684']\n",
            "search c ['0.00229473', '0.453518', '0.808097', '0.510538', '0.00012668', '1.26888e-07', '0.000330654', '0.478297']\n",
            "search c ['0.00901471', '0.000254229', '0.636679', '0.729', '0.6561', '0.59034', '0.52894', '0.477542']\n",
            "search c ['0.115951', '6.8007e-05', '5.72718e-07', '0.0555728', '0.6561', '0.59049', '0.531374', '0.000298881']\n",
            "search c ['0.062543', '0.000527356', '1.60281e-05', '0.579644', '0.6561', '0.59049', '0.531427', '0.00138329']\n",
            "search c ['0.00404278', '0.0612966', '0.809997', '0.729', '0.6561', '0.59011', '0.529727', '0.261926']\n",
            "search c ['0.186781', '0.00181414', '2.92491e-08', '5.07276e-12', '7.51964e-09', '0.590361', '0.531441', '0.476442']\n",
            "search c ['0.00358602', '0.498375', '0.803652', '0.622341', '0.656023', '0.59049', '0.531441', '0.413945']\n",
            "search c ['0.00266292', '0.666325', '0.808033', '0.00345056', '0.0824783', '0.59049', '0.531441', '0.128381']\n",
            "search c ['0.00221848', '0.0126548', '0.809997', '0.729', '0.656085', '0.00716508', '3.73264e-05', '0.0107493']\n",
            "search c ['0.0738978', '1.13317e-05', '4.63635e-06', '0.716115', '0.6561', '0.59049', '0.002029', '2.81527e-07']\n",
            "search c ['0.00397939', '0.709362', '0.80984', '0.16991', '1.67058e-07', '8.59716e-11', '1.53124e-07', '0.478294']\n",
            "search c ['0.167729', '8.45164e-05', '1.19589e-06', '0.0942151', '0.6561', '0.59049', '0.531421', '0.0348536']\n",
            "search c ['0.00280938', '0.0455034', '0.809999', '0.729', '0.6561', '0.578855', '7.91129e-06', '2.75777e-11']\n",
            "search c ['0.00285733', '0.0390163', '0.809999', '0.729', '0.6561', '0.587153', '3.41283e-05', '3.76403e-11']\n",
            "search c ['0.00184152', '0.067209', '0.81', '0.728999', '0.00957388', '0.000202538', '0.470733', '0.4782']\n",
            "search c ['0.104887', '4.00057e-05', '2.37204e-06', '0.418589', '0.6561', '0.59049', '0.530602', '0.477426']\n",
            "search c ['0.138606', '8.86869e-05', '2.81016e-06', '0.358953', '0.6561', '0.59049', '0.53142', '0.00453194']\n",
            "search c ['0.00417374', '0.0320767', '0.809997', '0.729', '0.6561', '0.588658', '0.00136207', '6.3403e-07']\n",
            "search c ['0.0045746', '0.0376422', '0.809998', '0.729', '0.6561', '0.590479', '0.024954', '1.02116e-08']\n",
            "search c ['0.00420116', '0.106131', '0.809998', '0.729', '0.656099', '0.589256', '0.522256', '0.477368']\n",
            "search c ['0.00761529', '0.151421', '0.195643', '0.712915', '0.6561', '0.59049', '0.531441', '0.222659']\n",
            "search c ['0.00263457', '0.60026', '0.807949', '0.0141393', '0.317285', '0.59049', '0.531441', '0.160327']\n",
            "search c ['0.115879', '0.0362797', '2.24889e-07', '1.08247e-10', '5.93842e-07', '0.590487', '0.531441', '0.477934']\n",
            "search c ['0.0309008', '0.332954', '0.000459898', '8.15527e-07', '0.0380506', '0.59049', '0.53057', '0.00177332']\n",
            "search c ['0.00382044', '0.460719', '0.809665', '0.000218529', '2.51249e-08', '8.75148e-06', '0.531434', '0.478296']\n",
            "search c ['0.00322452', '0.410876', '0.809858', '0.604478', '1.20399e-06', '2.65763e-10', '1.20684e-07', '0.478288']\n",
            "search c ['0.00937811', '0.606245', '0.808975', '0.728952', '0.6561', '0.59049', '0.531441', '0.25027']\n",
            "search c ['0.00639803', '0.813883', '0.809041', '0.0053052', '0.0405133', '0.590489', '0.531441', '0.227177']\n",
            "search c ['0.00290186', '0.516621', '0.809989', '0.728968', '0.00251782', '3.34224e-08', '6.14307e-08', '0.473209']\n",
            "search c ['0.0036385', '0.64907', '0.804516', '0.00320882', '0.201405', '0.59049', '0.531441', '0.316164']\n",
            "search c ['0.00192545', '0.071428', '0.809999', '0.729', '0.656099', '0.58948', '0.521374', '0.476273']\n",
            "search c ['0.00176208', '0.0710315', '0.809999', '0.729', '0.656099', '0.590237', '0.530624', '0.029498']\n",
            "search c ['0.00259394', '0.0498549', '0.809999', '0.729', '0.6561', '0.539756', '1.89439e-06', '1.50925e-11']\n",
            "search c ['0.00256368', '0.0554011', '0.81', '0.729', '0.6561', '0.32506', '3.39629e-07', '5.02487e-12']\n",
            "search c ['0.224638', '8.18385e-05', '1.32101e-06', '0.443335', '0.6561', '0.566408', '5.66413e-06', '1.54959e-07']\n",
            "search c ['0.177199', '0.000100968', '3.2118e-05', '0.726483', '0.6561', '0.457474', '0.222652', '4.42913e-06']\n",
            "search c ['0.00361218', '0.0341858', '0.809999', '0.729', '0.656083', '0.00655843', '1.04745e-05', '0.0118873']\n",
            "search c ['0.00494732', '0.657815', '0.809365', '0.183155', '8.17011e-07', '4.79711e-09', '0.000469225', '0.478294']\n",
            "search c ['0.00330517', '0.078605', '0.809999', '0.729', '0.6561', '0.590471', '0.00931123', '5.51688e-09']\n",
            "search c ['0.00371125', '0.0557374', '0.809998', '0.729', '0.6561', '0.59047', '0.00636635', '8.03267e-09']\n",
            "search c ['0.00185558', '0.0159538', '0.809999', '0.729', '0.656098', '0.590099', '0.529037', '0.00450324']\n",
            "search c ['0.00180494', '0.0167589', '0.809999', '0.729', '0.656099', '0.590367', '0.529387', '0.000190833']\n",
            "search c ['0.0745148', '0.0313195', '3.79011e-06', '7.11734e-11', '1.08427e-09', '0.541287', '0.531441', '0.478283']\n",
            "search c ['0.181568', '5.70814e-05', '7.86253e-07', '0.0685426', '0.6561', '0.59049', '0.0240599', '3.32731e-06']\n",
            "search c ['0.00558124', '0.713294', '0.81', '0.729', '0.23441', '4.41511e-07', '7.84863e-09', '0.00974145']\n",
            "search c ['0.00491621', '0.423975', '0.81', '0.729', '0.110076', '0.000504613', '0.336646', '0.478295']\n",
            "search c ['0.00453185', '0.055167', '0.809999', '0.729', '0.6561', '0.447563', '1.02805e-06', '6.53845e-12']\n",
            "search c ['0.00455356', '0.056824', '0.809999', '0.729', '0.6561', '0.397786', '2.45421e-07', '5.73519e-12']\n",
            "search c ['0.00421141', '0.130706', '0.81', '0.729', '0.592638', '2.68378e-05', '3.69573e-06', '0.308879']\n",
            "search c ['0.184', '0.000727521', '1.06509e-07', '4.59262e-05', '0.656098', '0.59049', '0.53143', '0.00164888']\n",
            "search c ['0.00676821', '0.686044', '0.806472', '0.0431168', '0.397062', '0.414416', '1.63752e-06', '3.15276e-10']\n",
            "search c ['0.0072447', '0.693156', '0.809612', '0.000187645', '6.13561e-07', '0.0023517', '0.531441', '0.478297']\n",
            "search c ['0.00635359', '0.0668337', '0.786141', '0.729', '0.6561', '0.59049', '0.531438', '0.0948192']\n",
            "search c ['0.00215425', '0.660839', '0.809276', '0.0103605', '0.118638', '0.59049', '0.531441', '0.096425']\n",
            "search c ['0.23647', '0.0138291', '2.51386e-08', '3.02388e-10', '0.0011116', '0.59049', '0.00198176', '4.85323e-06']\n",
            "search c ['0.232796', '0.0053144', '1.07823e-08', '9.71858e-10', '0.00848351', '0.59049', '0.531441', '0.000341029']\n",
            "search c ['0.00205967', '0.0652203', '0.81', '0.729', '0.65609', '0.0125535', '1.45211e-05', '0.00568441']\n",
            "search c ['0.00207923', '0.0586854', '0.81', '0.729', '0.656089', '0.00996019', '1.08931e-05', '0.00501256']\n",
            "search c ['0.206901', '0.000592273', '1.28819e-06', '0.0159444', '0.6561', '0.59049', '0.531414', '0.000687734']\n",
            "search c ['0.260156', '0.000196509', '2.90494e-07', '0.002875', '0.6561', '0.59049', '0.531394', '0.00029237']\n",
            "search c ['0.00328125', '0.70857', '0.809083', '0.0235479', '0.140975', '0.59048', '0.531441', '0.478297']\n",
            "search c ['0.00365145', '0.71236', '0.808228', '0.00650909', '0.406894', '0.583953', '0.00977812', '0.22161']\n",
            "search c ['0.00360517', '0.0663998', '0.809999', '0.729', '0.6561', '0.590481', '0.479466', '7.24582e-06']\n",
            "search c ['0.00387517', '0.112821', '0.809999', '0.729', '0.6561', '0.590471', '0.424235', '1.68708e-06']\n",
            "search c ['0.00262973', '0.11077', '0.809993', '0.729', '0.6561', '0.58792', '0.00343952', '9.22778e-08']\n",
            "search c ['0.00309583', '0.627906', '0.804167', '0.0043032', '0.152099', '0.59049', '0.531441', '0.476971']\n",
            "search c ['0.00182373', '0.0423344', '0.81', '0.729', '0.6561', '0.112548', '1.42701e-07', '4.67382e-12']\n",
            "search c ['0.00190554', '0.0507271', '0.81', '0.729', '0.656085', '0.00662188', '1.57987e-05', '0.000142492']\n",
            "search c ['0.165748', '0.000150653', '8.6576e-07', '0.04998', '0.6561', '0.59049', '0.531426', '0.0018181']\n",
            "search c ['0.00606998', '0.0124912', '0.809968', '0.729', '0.6561', '0.590066', '0.0110169', '1.7194e-08']\n",
            "search c ['0.165761', '0.00012086', '1.04e-06', '0.0166529', '0.6561', '0.59049', '0.531241', '0.477887']\n",
            "search c ['0.151003', '0.00028144', '1.00792e-06', '0.0114992', '0.6561', '0.59049', '0.531434', '0.0193496']\n",
            "search c ['0.00236615', '0.0883598', '0.81', '0.729', '0.656086', '0.0102772', '2.2216e-05', '0.00260062']\n",
            "search c ['0.00235551', '0.0750191', '0.81', '0.729', '0.656089', '0.0101587', '1.26534e-05', '0.00867211']\n",
            "search c ['0.00629884', '0.0621154', '0.805037', '0.729', '0.6561', '0.59049', '0.531428', '0.0108424']\n",
            "search c ['0.128786', '6.06197e-05', '2.8723e-07', '0.0156389', '0.6561', '0.59049', '0.531351', '0.000256425']\n",
            "search c ['0.00314266', '0.0252415', '0.809998', '0.729', '0.6561', '0.274745', '9.11243e-07', '2.54219e-08']\n",
            "search c ['0.147403', '0.00243074', '5.87644e-08', '2.70908e-12', '2.78874e-10', '0.57661', '0.531441', '0.477875']\n",
            "search c ['0.041881', '0.000104178', '4.39849e-06', '0.573364', '0.6561', '0.59049', '0.531351', '0.00018288']\n",
            "search c ['0.0451677', '6.36191e-05', '2.80998e-06', '0.488231', '0.6561', '0.59049', '0.531278', '0.00014776']\n",
            "search c ['0.00398531', '0.753312', '0.809895', '0.0006822', '3.1572e-06', '0.0140517', '0.531441', '0.478297']\n",
            "search c ['0.00475336', '0.717725', '0.809725', '0.000235499', '3.44469e-07', '0.000619704', '0.531441', '0.478297']\n",
            "search c ['0.0641878', '0.000789257', '2.202e-05', '0.617966', '0.6561', '0.59049', '0.531425', '0.00150304']\n",
            "search c ['0.0181759', '0.0115281', '0.0343583', '0.728966', '0.6561', '0.59049', '0.531438', '0.0203652']\n",
            "search c ['0.00212546', '0.0201497', '0.809998', '0.729', '0.6561', '0.589982', '0.530985', '0.350509']\n",
            "search c ['0.00198354', '0.0385709', '0.809999', '0.729', '0.656099', '0.589481', '0.530722', '0.307385']\n",
            "search c ['0.00377602', '0.0552171', '0.809999', '0.729', '0.6561', '0.590468', '0.0074815', '2.66354e-09']\n",
            "search c ['0.00376508', '0.0561716', '0.809999', '0.729', '0.6561', '0.590466', '0.00434358', '4.42361e-09']\n",
            "search c ['0.00247732', '0.0195582', '0.809999', '0.729', '0.656098', '0.590315', '0.52892', '0.00228874']\n",
            "search c ['0.00255168', '0.0153335', '0.809998', '0.729', '0.65609', '0.587458', '0.515538', '0.000391773']\n",
            "search c ['0.00205864', '0.0623387', '0.81', '0.729', '0.656091', '0.0117191', '1.83711e-05', '0.000557485']\n",
            "search c ['0.00206054', '0.0628089', '0.81', '0.729', '0.656099', '0.0378796', '1.21287e-07', '1.98451e-11']\n",
            "search c ['0.191858', '0.0356087', '1.69227e-07', '1.77172e-10', '1.9681e-06', '0.590489', '0.531441', '0.477989']\n",
            "search c ['0.0801179', '0.0168179', '0.000117944', '0.36315', '0.6561', '0.59049', '0.531441', '0.477339']\n",
            "search c ['0.020885', '0.000450631', '0.48321', '0.729', '0.6561', '0.590489', '0.0372707', '2.91869e-08']\n",
            "search c ['0.0191511', '0.000430843', '0.489557', '0.729', '0.6561', '0.590489', '0.0195872', '2.59704e-08']\n",
            "search c ['0.0017936', '0.480446', '0.81', '0.380327', '2.94936e-06', '3.43121e-06', '0.526352', '0.478297']\n",
            "search c ['0.00189885', '0.0238745', '0.809999', '0.729', '0.656081', '0.261596', '0.0101169', '8.97792e-06']\n",
            "search c ['0.00264247', '0.0293945', '0.809999', '0.729', '0.6561', '0.112266', '1.37356e-07', '5.82215e-12']\n",
            "search c ['0.00268814', '0.0742465', '0.81', '0.728999', '0.0408166', '0.00367174', '6.10004e-05', '2.91417e-05']\n",
            "search c ['0.00321126', '0.723584', '0.809987', '0.728707', '0.655999', '0.59049', '0.531441', '0.477119']\n",
            "search c ['0.00259923', '0.148838', '0.81', '0.729', '0.656099', '0.590333', '0.530484', '0.025129']\n",
            "search c ['0.00203616', '0.0189484', '0.809997', '0.729', '0.6561', '0.499704', '1.58972e-06', '1.25474e-10']\n",
            "search c ['0.00204674', '0.0320493', '0.809998', '0.729', '0.656097', '0.0233299', '0.000714629', '0.000165715']\n",
            "search c ['0.00526265', '0.0771874', '0.809998', '0.729', '0.6561', '0.590479', '0.0122247', '1.45197e-08']\n",
            "search c ['0.00554813', '0.0376794', '0.809996', '0.729', '0.6561', '0.590476', '0.00531498', '5.74891e-09']\n",
            "search c ['0.00206615', '0.116248', '0.81', '0.728997', '0.00476926', '0.000126826', '0.476576', '0.478296']\n",
            "search c ['0.00298374', '0.607868', '0.809933', '0.000609767', '2.75267e-08', '6.50518e-07', '0.531236', '0.478297']\n",
            "search c ['0.00833879', '0.0101851', '0.809803', '0.729', '0.6561', '0.590487', '0.0285428', '4.80954e-08']\n",
            "search c ['0.00700653', '0.0149619', '0.809939', '0.729', '0.6561', '0.590484', '0.0125032', '8.99566e-09']\n",
            "search c ['0.00287945', '0.0647542', '0.81', '0.729', '0.656071', '0.149739', '0.0341574', '0.141138']\n",
            "search c ['0.00304923', '0.0538273', '0.81', '0.729', '0.655748', '0.00271098', '1.61076e-05', '2.434e-08']\n",
            "search c ['0.00866193', '0.705846', '0.800118', '0.00126843', '0.0517012', '0.59049', '0.531441', '0.457959']\n",
            "search c ['0.0105315', '0.103742', '0.809842', '0.729', '0.6561', '0.590489', '0.105564', '3.78454e-06']\n",
            "search c ['0.224325', '0.00200073', '1.29171e-08', '1.29116e-11', '3.31784e-07', '0.590488', '0.531441', '0.47389']\n",
            "search c ['0.22501', '0.00196112', '7.75752e-09', '2.38604e-11', '3.43609e-06', '0.59049', '0.531441', '0.466563']\n",
            "search c ['0.003953', '0.023781', '0.809994', '0.729', '0.6561', '0.586005', '0.000356944', '8.77907e-07']\n",
            "search c ['0.0037416', '0.0218436', '0.809996', '0.729', '0.6561', '0.142059', '8.07932e-07', '6.54293e-09']\n",
            "search c ['0.00407716', '0.18701', '0.809999', '0.729', '0.6561', '0.590477', '0.0383168', '3.77114e-07']\n",
            "search c ['0.00400123', '0.097933', '0.809999', '0.729', '0.6561', '0.590468', '0.0297559', '1.88993e-08']\n",
            "search c ['0.00257186', '0.0245475', '0.809998', '0.729', '0.656099', '0.589661', '0.530237', '0.455788']\n",
            "search c ['0.00254901', '0.0314872', '0.81', '0.729', '0.120776', '0.00136978', '0.401995', '0.478297']\n",
            "search c ['0.0961611', '0.00012154', '8.41594e-07', '0.0504097', '0.6561', '0.59049', '0.531425', '0.00165165']\n",
            "search c ['0.00220813', '0.0364855', '0.809999', '0.729', '0.6561', '0.587744', '0.00344534', '8.24792e-08']\n",
            "search c ['0.109316', '0.0055428', '2.26447e-08', '7.01417e-10', '0.00407028', '0.59049', '0.0284688', '2.076e-05']\n",
            "search c ['0.0611642', '0.000264845', '0.000610495', '0.728914', '0.6561', '0.590488', '0.529713', '0.47709']\n",
            "search c ['0.00992535', '0.0137704', '0.167339', '0.728995', '0.6561', '0.59049', '0.53144', '0.143588']\n",
            "search c ['0.0270868', '0.0602205', '2.5379e-06', '1.0703e-08', '0.000638189', '0.59049', '0.531441', '0.00969668']\n",
            "search c ['0.0042929', '0.032193', '0.809997', '0.729', '0.656099', '0.590162', '0.531212', '0.415765']\n",
            "search c ['0.00469059', '0.644744', '0.80807', '0.00707036', '0.0673123', '0.590487', '0.531441', '0.478297']\n",
            "search c ['0.151318', '0.000442536', '1.32342e-06', '0.0315561', '0.6561', '0.59049', '0.531412', '0.000353825']\n",
            "search c ['0.20061', '0.000115282', '2.93146e-07', '0.00553744', '0.6561', '0.59049', '0.531386', '8.73476e-05']\n",
            "search c ['0.00225977', '0.0258725', '0.809999', '0.729', '0.6561', '0.499046', '9.508e-05', '8.61674e-09']\n",
            "search c ['0.109096', '0.00155205', '4.33312e-08', '1.27104e-11', '3.6247e-08', '0.590464', '0.531441', '0.477282']\n",
            "search c ['0.0035327', '0.346964', '0.808335', '0.728386', '0.656097', '0.59049', '0.531441', '0.348412']\n",
            "search c ['0.00293125', '0.520455', '0.808004', '0.120682', '0.62671', '0.59049', '0.531441', '0.119688']\n",
            "search c ['0.00272779', '0.178691', '0.809999', '0.729', '0.654028', '0.000153305', '1.08212e-06', '0.0114808']\n",
            "search c ['0.00257856', '0.0308531', '0.809999', '0.729', '0.6561', '0.355139', '1.85666e-05', '8.98388e-10']\n",
            "search c ['0.00405292', '0.122287', '0.81', '0.729', '0.654422', '0.000208582', '6.25342e-06', '0.000243069']\n",
            "search c ['0.157892', '0.0519407', '1.53795e-07', '8.05319e-10', '6.94806e-05', '0.59049', '0.531441', '0.0493446']\n",
            "search c ['0.00461754', '0.0652921', '0.809999', '0.729', '0.6561', '0.246493', '2.0687e-07', '5.13901e-12']\n",
            "search c ['0.00457145', '0.0706662', '0.809999', '0.729', '0.6561', '0.0676533', '1.53206e-07', '1.8918e-12']\n",
            "search c ['0.043517', '0.164233', '1.73215e-05', '3.43472e-06', '0.605124', '0.59049', '0.531441', '0.105493']\n",
            "search c ['0.00363267', '0.0536657', '0.81', '0.729', '0.656099', '0.588558', '0.529846', '0.228038']\n",
            "search c ['0.00761817', '0.00673736', '0.809826', '0.729', '0.6561', '0.590486', '0.0135714', '3.06911e-08']\n",
            "search c ['0.010734', '0.00266419', '0.807661', '0.729', '0.6561', '0.590487', '0.018301', '2.02189e-08']\n",
            "search c ['0.0067598', '0.82327', '0.809991', '0.169461', '0.040661', '0.589666', '0.531441', '0.478297']\n",
            "search c ['0.00916549', '0.854971', '0.809023', '0.0943409', '0.0317172', '5.18082e-06', '5.47359e-09', '0.000481711']\n",
            "search c ['0.00415734', '0.74267', '0.807336', '0.00654021', '0.158925', '0.59049', '0.531441', '0.474129']\n",
            "search c ['0.00477551', '0.702819', '0.803408', '0.0109462', '0.439941', '0.59049', '0.531441', '0.471317']\n",
            "search c ['0.00631961', '0.40792', '0.554103', '0.00539227', '0.647669', '0.59049', '0.53144', '0.284961']\n",
            "search c ['0.00359251', '0.0139786', '0.809994', '0.729', '0.656092', '0.0108146', '0.000295202', '0.0016579']\n",
            "search c ['0.0148431', '0.00160806', '0.774346', '0.729', '0.6561', '0.590489', '0.0186789', '1.3325e-07']\n",
            "search c ['0.00901667', '0.00328772', '0.808505', '0.729', '0.6561', '0.590486', '0.0103835', '1.06878e-08']\n",
            "search c ['0.00263022', '0.0615705', '0.81', '0.729', '0.0507044', '0.000767484', '0.505315', '0.478287']\n",
            "search c ['0.00277279', '0.0262678', '0.809999', '0.729', '0.656093', '0.588826', '0.526662', '0.000884588']\n",
            "search c ['0.136099', '6.05474e-05', '5.79149e-07', '0.0594472', '0.6561', '0.59049', '0.0322601', '0.001682']\n",
            "search c ['0.00238341', '0.03298', '0.809999', '0.729', '0.656099', '0.0570441', '1.56863e-07', '1.31961e-12']\n",
            "search c ['0.00193733', '0.0697482', '0.809999', '0.729', '0.656097', '0.589099', '0.524998', '0.477485']\n",
            "search c ['0.0210624', '0.00104596', '0.292368', '0.729', '0.6561', '0.59049', '0.531411', '0.109515']\n",
            "search c ['0.00182349', '0.0237385', '0.809999', '0.729', '0.6561', '0.091732', '1.9663e-07', '1.5829e-11']\n",
            "search c ['0.00183108', '0.0262764', '0.809999', '0.729', '0.6561', '0.0499423', '1.0793e-07', '1.35188e-11']\n",
            "search c ['0.00704161', '0.265149', '0.262632', '0.485839', '0.656099', '0.59049', '0.531441', '0.456107']\n",
            "search c ['0.106736', '0.00431684', '1.02901e-08', '1.21177e-10', '6.33753e-05', '0.59049', '0.531441', '0.472479']\n",
            "search c ['0.00686308', '0.0137963', '0.809974', '0.729', '0.6561', '0.590068', '0.0139523', '5.60562e-08']\n",
            "search c ['0.180636', '0.000119247', '9.74346e-07', '0.117444', '0.6561', '0.59049', '0.531397', '0.000796053']\n",
            "search c ['0.00364484', '0.102898', '0.81', '0.729', '0.656099', '0.196169', '1.61214e-07', '8.33872e-13']\n",
            "search c ['0.00606282', '0.801631', '0.806073', '0.00191644', '0.0400005', '0.59049', '0.531441', '0.476857']\n",
            "search c ['0.00386204', '0.0440413', '0.809999', '0.729', '0.6561', '0.188556', '4.24342e-07', '5.47681e-12']\n",
            "search c ['0.00387074', '0.0411674', '0.809999', '0.729', '0.6561', '0.122758', '5.76711e-08', '4.60629e-12']\n",
            "search c ['0.00262233', '0.200083', '0.809983', '0.729', '0.6561', '0.590485', '0.531374', '0.0781656']\n",
            "search c ['0.0647174', '0.0235874', '1.09471e-07', '1.80758e-10', '3.78977e-06', '0.59049', '0.531441', '0.477472']\n",
            "search c ['0.0029486', '0.0215713', '0.809998', '0.729', '0.6561', '0.590187', '0.000258972', '5.73856e-10']\n",
            "search c ['0.00280313', '0.0257352', '0.809998', '0.729', '0.6561', '0.581904', '1.57854e-05', '1.28343e-11']\n",
            "search c ['0.00574338', '0.00365137', '0.809941', '0.729', '0.656083', '0.0260897', '0.000310242', '1.19389e-06']\n",
            "search c ['0.00916478', '0.00153784', '0.809336', '0.729', '0.65608', '0.00677755', '0.000370256', '7.25333e-05']\n",
            "search c ['0.00323847', '0.0677806', '0.809997', '0.729', '0.6561', '0.560627', '1.68764e-06', '1.59864e-11']\n",
            "search c ['0.048709', '0.00101118', '0.00190978', '0.72895', '0.6561', '0.59049', '0.197262', '1.43075e-07']\n",
            "search c ['0.00280072', '0.011456', '0.809996', '0.729', '0.6561', '0.265411', '6.09208e-07', '2.23279e-08']\n",
            "search c ['0.00314856', '0.0119038', '0.809996', '0.729', '0.656099', '0.587141', '0.523859', '0.471454']\n",
            "search c ['0.00319298', '0.0233556', '0.809997', '0.729', '0.6561', '0.590385', '0.0010652', '2.05478e-09']\n",
            "search c ['0.00316635', '0.0311774', '0.809998', '0.729', '0.6561', '0.59016', '0.0164797', '9.48811e-09']\n",
            "search c ['0.00255939', '0.0233887', '0.809998', '0.729', '0.6561', '0.431582', '0.000463406', '0.00298972']\n",
            "search c ['0.136688', '0.0029437', '8.64513e-09', '6.64266e-10', '0.00331483', '0.59049', '0.531441', '0.476404']\n",
            "search c ['0.0122913', '0.861867', '0.785307', '0.00133937', '0.0872595', '0.59049', '0.531441', '0.314263']\n",
            "search c ['0.27249', '0.000474513', '5.96683e-07', '0.00538129', '0.6561', '0.59049', '0.531435', '0.00384274']\n",
            "search c ['0.00988073', '0.267494', '0.00334948', '3.59317e-05', '0.530984', '0.59049', '0.531441', '0.478293']\n",
            "search c ['0.00289065', '0.529564', '0.802249', '0.00189293', '0.0880904', '0.59049', '0.531441', '0.477736']\n",
            "search c ['0.00203512', '0.0197747', '0.809998', '0.729', '0.6561', '0.583993', '1.19912e-05', '3.67592e-11']\n",
            "search c ['0.00205645', '0.0195919', '0.809998', '0.729', '0.6561', '0.558315', '3.23291e-06', '5.34138e-12']\n",
            "search c ['0.155569', '0.000121501', '7.76022e-06', '0.703285', '0.6561', '0.0221659', '0.129476', '0.464767']\n",
            "search c ['0.00314977', '0.25133', '0.81', '0.729', '0.00232605', '1.30605e-08', '7.17958e-09', '0.194713']\n",
            "search c ['0.0267614', '0.00625875', '0.000573772', '0.715659', '0.6561', '0.59049', '0.53144', '0.0279019']\n",
            "search c ['0.00314459', '0.605137', '0.802394', '0.0018226', '0.0797943', '0.59049', '0.531441', '0.167945']\n",
            "search c ['0.00301838', '0.0632222', '0.809999', '0.729', '0.6561', '0.58722', '0.000414239', '2.22959e-06']\n",
            "search c ['0.00345635', '0.0731944', '0.809998', '0.729', '0.6561', '0.587379', '0.00400228', '2.44603e-07']\n",
            "search c ['0.00427662', '0.0359161', '0.809996', '0.729', '0.6561', '0.590472', '0.00503086', '1.19338e-08']\n",
            "search c ['0.00431143', '0.732217', '0.808409', '0.000160781', '2.68155e-05', '0.555188', '0.531441', '0.478297']\n",
            "search c ['0.00590249', '0.718391', '0.80542', '0.00481074', '0.134074', '0.59049', '0.531441', '0.476508']\n",
            "search c ['0.00706267', '0.395347', '0.809112', '0.728971', '0.656099', '0.59049', '0.531441', '0.197265']\n",
            "search c ['0.0979761', '0.0071624', '1.89064e-07', '1.04531e-11', '7.20965e-11', '0.212117', '0.531441', '0.478294']\n",
            "search c ['0.00243074', '0.705286', '0.809804', '0.000423095', '4.90489e-06', '0.0586129', '0.531441', '0.478297']\n",
            "search c ['0.107829', '6.98871e-05', '6.59924e-07', '0.0553626', '0.6561', '0.59049', '0.53138', '0.000368366']\n",
            "search c ['0.00462947', '0.138324', '0.794847', '0.728977', '0.6561', '0.59049', '0.531441', '0.167143']\n",
            "search c ['0.00262609', '0.0167537', '0.809994', '0.729', '0.6561', '0.586913', '0.00281688', '1.01311e-07']\n",
            "search c ['0.00223453', '0.0286104', '0.809998', '0.729', '0.6561', '0.585246', '0.0019843', '1.72769e-07']\n",
            "search c ['0.0740833', '0.00254768', '1.56953e-07', '3.01719e-11', '2.64482e-08', '0.59039', '0.531441', '0.478104']\n",
            "search c ['0.00474623', '0.00423655', '0.809793', '0.729', '0.6561', '0.590391', '0.525763', '0.47225']\n",
            "search c ['0.00288104', '0.314472', '0.81', '0.729', '0.656097', '0.589941', '0.528845', '0.0101504']\n",
            "search c ['0.00402548', '0.0444334', '0.809999', '0.729', '0.6561', '0.590464', '0.530849', '0.000999068']\n",
            "search c ['0.00652323', '0.665573', '0.809254', '0.120138', '0.493972', '0.590489', '0.531441', '0.478297']\n",
            "search c ['0.00671492', '0.69963', '0.80825', '0.0888462', '0.0667103', '0.0381746', '0.523869', '0.478297']\n",
            "search c ['0.00231536', '0.117713', '0.81', '0.729', '0.648598', '0.575635', '0.527271', '0.000457272']\n",
            "search c ['0.00277807', '0.131599', '0.809999', '0.729', '0.656099', '0.590467', '0.528682', '0.000150633']\n",
            "search c ['0.017582', '0.709865', '0.735345', '1.29688e-05', '1.23529e-05', '0.585604', '0.531441', '0.478297']\n",
            "search c ['0.00861236', '0.624697', '0.809995', '0.156273', '0.000313396', '1.06265e-06', '3.8225e-06', '0.477872']\n",
            "search c ['0.00419563', '0.00949001', '0.809984', '0.729', '0.6561', '0.590488', '0.505', '5.61274e-06']\n",
            "search c ['0.00435376', '0.0093232', '0.809983', '0.729', '0.6561', '0.590486', '0.499873', '2.54289e-06']\n",
            "search c ['0.0164366', '0.780305', '0.0319529', '2.7681e-05', '0.0756903', '0.59049', '0.531441', '0.465468']\n",
            "search c ['0.00537828', '0.846274', '0.808375', '0.0039155', '0.0327465', '0.590487', '0.531441', '0.47829']\n",
            "search c ['0.00301184', '0.14958', '0.81', '0.729', '0.6561', '0.590472', '0.00765331', '1.03255e-08']\n",
            "search c ['0.00322812', '0.0926728', '0.81', '0.729', '0.6561', '0.590464', '0.00508493', '1.09565e-08']\n",
            "search c ['0.003632', '0.0205617', '0.809999', '0.729', '0.655885', '0.0013267', '5.83082e-05', '0.000240529']\n",
            "search c ['0.0037362', '0.0299103', '0.809998', '0.729', '0.656099', '0.590435', '0.530572', '0.00418744']\n",
            "search c ['0.00305132', '0.0556596', '0.81', '0.729', '0.656092', '0.0164072', '2.59205e-05', '0.00536869']\n",
            "search c ['0.00314175', '0.0550178', '0.81', '0.729', '0.65609', '0.00869276', '1.01894e-05', '0.0020823']\n",
            "search c ['0.0702464', '0.000119313', '2.7475e-06', '0.368778', '0.6561', '0.59049', '0.531296', '0.000201846']\n",
            "search c ['0.106165', '5.25478e-05', '9.0091e-08', '0.00117859', '0.6561', '0.59049', '0.531399', '0.000171189']\n",
            "search c ['0.00189915', '0.0803449', '0.81', '0.729', '0.65604', '0.00290725', '5.3296e-06', '0.0103285']\n",
            "search c ['0.00229768', '0.447061', '0.809997', '0.729', '0.656062', '0.590481', '0.518712', '0.00364453']\n",
            "search c ['0.00332886', '0.037516', '0.809998', '0.729', '0.6561', '0.590456', '0.00480999', '2.87138e-09']\n",
            "search c ['0.00299954', '0.0769597', '0.809999', '0.729', '0.6561', '0.590404', '0.000570595', '2.80914e-09']\n",
            "search c ['0.00526553', '0.306452', '0.809761', '0.000993434', '1.08912e-07', '8.09442e-06', '0.531428', '0.478297']\n",
            "search c ['0.00185208', '0.0523101', '0.81', '0.729', '0.0309914', '1.09149e-07', '5.93352e-08', '0.369945']\n",
            "search c ['0.0428639', '0.14348', '3.75983e-05', '2.58361e-05', '0.652448', '0.59049', '0.531441', '0.0353053']\n",
            "search c ['0.00425961', '0.0541231', '0.809999', '0.729', '0.6561', '0.582704', '0.110581', '4.76498e-06']\n",
            "search c ['0.0440329', '1.75153e-05', '4.55541e-05', '0.728785', '0.6561', '0.59049', '0.00135143', '1.21569e-07']\n",
            "search c ['0.0870117', '0.000787389', '6.80127e-08', '2.59552e-12', '2.12488e-10', '0.574475', '0.531441', '0.477701']\n",
            "search c ['0.262904', '0.000373799', '1.21965e-06', '0.0262788', '0.6561', '0.59049', '0.531398', '0.000424835']\n",
            "search c ['0.0557129', '0.294345', '0.000267993', '5.38677e-06', '0.281627', '0.59049', '0.531441', '0.281763']\n",
            "search c ['0.00653396', '0.733015', '0.806946', '0.121866', '6.30503e-05', '0.00027564', '0.0069485', '0.129927']\n",
            "search c ['0.00679105', '0.715493', '0.803534', '0.00294065', '0.206985', '0.59049', '0.531441', '0.202624']\n",
            "search c ['0.00687118', '0.627055', '0.809992', '0.729', '0.656094', '0.590457', '0.531386', '0.47829']\n",
            "search c ['0.00672672', '0.438425', '0.809997', '0.729', '0.656099', '0.590286', '0.530244', '0.478116']\n",
            "search c ['0.0611298', '0.0144983', '0.000350686', '0.669724', '0.6561', '0.59049', '0.53144', '0.0131919']\n",
            "search c ['0.00646507', '0.729762', '0.805215', '0.00491031', '0.179381', '0.59049', '0.531441', '0.231011']\n",
            "search c ['0.245693', '0.0171786', '2.55942e-08', '3.28175e-10', '0.00154343', '0.59049', '0.00179524', '4.29881e-06']\n",
            "search c ['0.0053496', '0.0710178', '0.809999', '0.729', '0.6561', '0.221606', '4.7958e-06', '2.50156e-09']\n",
            "search c ['0.0191085', '0.00246763', '0.772972', '0.729', '0.6561', '0.590489', '0.447216', '0.000369945']\n",
            "search c ['0.0146778', '0.00171946', '0.795654', '0.729', '0.6561', '0.590432', '0.528573', '0.476721']\n",
            "search c ['0.00630668', '0.749387', '0.804039', '0.00154174', '0.0664741', '0.59049', '0.531441', '0.173137']\n",
            "search c ['0.00443173', '0.0541245', '0.809999', '0.729', '0.656098', '0.172161', '0.000475822', '8.53888e-08']\n",
            "search c ['0.00369723', '0.0895496', '0.81', '0.729', '0.656071', '0.00568596', '1.73178e-05', '0.00277073']\n",
            "search c ['0.00547726', '0.766277', '0.808259', '0.00161873', '0.0335863', '0.59049', '0.531441', '0.352651']\n",
            "search c ['0.00763334', '0.188328', '0.809997', '0.729', '0.656099', '0.590007', '0.527246', '0.477616']\n",
            "search c ['0.00750382', '0.146187', '0.809997', '0.729', '0.6561', '0.590213', '0.513107', '0.435996']\n",
            "search c ['0.00329356', '0.0203176', '0.809995', '0.729', '0.6561', '0.585548', '0.000125318', '1.89899e-06']\n",
            "search c ['0.102045', '0.000236789', '1.75299e-06', '0.134792', '0.6561', '0.59049', '0.531417', '0.000978727']\n",
            "search c ['0.004667', '0.0604774', '0.81', '0.729', '0.656089', '0.012905', '4.31334e-05', '0.00021785']\n",
            "search c ['0.00724012', '0.728575', '0.804248', '0.00253652', '0.0961139', '0.59049', '0.531441', '0.47507']\n",
            "search c ['0.00305371', '0.0564659', '0.809999', '0.729', '0.656094', '0.0180595', '2.33303e-05', '0.0083153']\n",
            "search c ['0.00326786', '0.0562099', '0.809999', '0.729', '0.65607', '0.0139603', '0.00010018', '0.0825949']\n",
            "search c ['0.008243', '0.817846', '0.806758', '0.0037041', '0.0898738', '0.59049', '0.531441', '0.248807']\n",
            "search c ['0.00792594', '0.820405', '0.807266', '0.0020923', '0.0332938', '0.59049', '0.531441', '0.253501']\n",
            "search c ['0.00320153', '0.707226', '0.809822', '0.0968766', '0.332664', '0.590488', '0.531441', '0.478297']\n",
            "search c ['0.00278889', '0.0842643', '0.81', '0.729', '0.656098', '0.587903', '0.515998', '0.47576']\n",
            "search c ['0.00274576', '0.733567', '0.808547', '0.00479201', '0.090568', '0.59049', '0.531441', '0.141489']\n",
            "search c ['0.010186', '0.0967209', '0.397182', '0.728932', '0.6561', '0.59049', '0.53144', '0.118377']\n",
            "search c ['0.115461', '0.00467587', '1.08901e-07', '9.50837e-12', '5.10095e-09', '0.590078', '0.531441', '0.477798']\n",
            "search c ['0.00333083', '0.684744', '0.809828', '0.000602559', '1.85921e-05', '0.330437', '0.531441', '0.478297']\n",
            "search c ['0.0678129', '0.0156385', '0.000669495', '0.708518', '0.6561', '0.59049', '0.53144', '0.0190437']\n",
            "search c ['0.009434', '0.74506', '0.773408', '0.00581899', '0.522736', '0.59049', '0.531441', '0.328841']\n",
            "search c ['0.00266878', '0.0177726', '0.809997', '0.729', '0.6561', '0.588045', '0.503149', '0.472993']\n",
            "search c ['0.00375752', '0.552828', '0.796937', '0.0010292', '0.112797', '0.59049', '0.531441', '0.431522']\n",
            "search c ['0.00642223', '0.777444', '0.809843', '0.000156214', '1.67841e-07', '0.000369156', '0.531441', '0.478297']\n",
            "search c ['0.00610339', '0.769083', '0.808678', '0.148513', '0.0716767', '3.6569e-06', '4.57371e-08', '0.0378004']\n",
            "search c ['0.0051973', '0.136978', '0.81', '0.729', '0.656099', '0.590468', '0.530303', '0.00038321']\n",
            "search c ['0.00631601', '0.0783531', '0.809999', '0.729', '0.656099', '0.590435', '0.530473', '0.00453356']\n",
            "search c ['0.0987156', '0.0115173', '5.08109e-08', '7.10003e-09', '0.055483', '0.59049', '0.531441', '0.0416102']\n",
            "search c ['0.00298087', '0.032784', '0.809999', '0.729', '0.656089', '0.00989188', '6.99462e-06', '0.000205904']\n",
            "search c ['0.00448697', '0.0155269', '0.809984', '0.729', '0.6561', '0.588296', '0.00288879', '1.08942e-07']\n",
            "search c ['0.00299528', '0.0619548', '0.809999', '0.729', '0.6561', '0.179387', '1.42412e-06', '4.58912e-08']\n",
            "search c ['0.208486', '4.3229e-05', '4.66453e-07', '0.0447589', '0.6561', '0.59049', '0.299726', '1.13881e-06']\n",
            "search c ['0.184172', '2.63029e-05', '5.5634e-07', '0.21772', '0.6561', '0.590408', '0.00380761', '1.13563e-05']\n",
            "search c ['0.0065344', '0.00346268', '0.809206', '0.729', '0.6561', '0.590406', '0.527614', '0.475517']\n",
            "search c ['0.00344527', '0.0194788', '0.809993', '0.729', '0.6561', '0.590148', '0.512703', '0.455656']\n",
            "search c ['0.143647', '0.0302975', '4.98629e-08', '3.65099e-10', '8.91624e-05', '0.59049', '0.531441', '0.000209454']\n",
            "search c ['0.0772628', '0.135406', '1.02925e-06', '4.56704e-09', '0.000315575', '0.59049', '0.531441', '0.0020675']\n",
            "search c ['0.126017', '0.0016985', '6.30515e-08', '8.46373e-12', '1.62876e-11', '0.0219328', '0.531441', '0.478297']\n",
            "search c ['0.0170833', '0.118131', '0.00186012', '6.42346e-09', '2.24061e-09', '0.0676823', '0.531441', '0.478297']\n",
            "search c ['0.131909', '0.000103185', '5.21546e-07', '0.0234437', '0.6561', '0.59049', '0.531398', '0.000322398']\n",
            "search c ['0.145999', '8.27285e-05', '2.44564e-07', '0.00442486', '0.6561', '0.59049', '0.531407', '0.000350767']\n",
            "search c ['0.00821858', '0.837827', '0.80983', '0.0011249', '6.341e-05', '0.515088', '0.531441', '0.478297']\n",
            "search c ['0.281845', '0.0249007', '3.21513e-07', '1.71385e-11', '3.21305e-10', '0.468284', '0.531441', '0.478289']\n",
            "search c ['0.0454393', '0.00907916', '0.0034434', '0.728569', '0.6561', '0.59049', '0.531439', '0.0116585']\n",
            "search c ['0.00480696', '0.713701', '0.808692', '0.0803343', '0.590078', '0.59049', '0.531441', '0.221896']\n",
            "search c ['0.00409986', '0.626164', '0.808621', '0.278399', '1.05429e-05', '2.74679e-08', '0.000285833', '0.478297']\n",
            "search c ['0.00382805', '0.612952', '0.809245', '0.42466', '1.55576e-05', '2.22051e-08', '0.00012658', '0.478297']\n",
            "search c ['0.00295023', '0.43', '0.805306', '0.627776', '0.655972', '0.59049', '0.531441', '0.281692']\n",
            "search c ['0.00298682', '0.539023', '0.788909', '0.00100241', '0.0888481', '0.59049', '0.531441', '0.164633']\n",
            "search c ['0.00536327', '0.595152', '0.76836', '2.58316e-06', '6.43579e-08', '0.0125704', '0.531441', '0.478297']\n",
            "search c ['0.0394081', '0.0759438', '8.90011e-06', '1.78767e-10', '5.08799e-09', '0.577233', '0.531441', '0.478281']\n",
            "search c ['0.00423469', '0.0608017', '0.809999', '0.729', '0.6561', '0.590468', '0.530865', '0.0013043']\n",
            "search c ['0.210526', '0.000128172', '3.54503e-07', '0.00791093', '0.6561', '0.59049', '0.531395', '0.000349289']\n",
            "search c ['0.00396452', '0.760378', '0.807239', '0.00133441', '0.0133365', '0.590487', '0.531441', '0.478291']\n",
            "search c ['0.0996', '0.0189677', '5.28219e-07', '1.85113e-11', '6.32007e-10', '0.54996', '0.531441', '0.478271']\n",
            "search c ['0.0609923', '4.81598e-05', '4.27749e-07', '0.0344288', '0.6561', '0.59049', '0.531416', '0.000477279']\n",
            "search c ['0.0578857', '4.58074e-05', '7.48272e-07', '0.0842313', '0.6561', '0.59049', '0.531383', '0.00010723']\n",
            "search c ['0.0096054', '0.678991', '0.771314', '6.68936e-06', '2.18919e-07', '0.0312772', '0.531441', '0.478297']\n",
            "search c ['0.00415059', '0.0938182', '0.809999', '0.729', '0.6561', '0.589378', '0.0221034', '1.65556e-06']\n",
            "search c ['0.0745158', '2.84878e-05', '2.03062e-05', '0.7252', '0.6561', '0.59049', '0.528223', '0.477455']\n",
            "search c ['0.0519176', '4.31823e-05', '0.000270932', '0.728947', '0.6561', '0.590489', '0.527813', '0.476839']\n",
            "search c ['0.00751628', '0.827605', '0.800445', '0.0029501', '0.14894', '0.59049', '0.531441', '0.403626']\n",
            "search c ['0.00750666', '0.0234934', '0.809991', '0.729', '0.6561', '0.590469', '0.53101', '0.00108382']\n",
            "search c ['0.0031437', '0.507855', '0.809979', '0.00235849', '4.53293e-08', '7.55045e-07', '0.530967', '0.478297']\n",
            "search c ['0.00309195', '0.0205188', '0.809997', '0.729', '0.6561', '0.540364', '2.84082e-05', '2.93851e-07']\n",
            "search c ['0.00168454', '0.00933844', '0.809995', '0.729', '0.6561', '0.590483', '0.37666', '1.61501e-06']\n",
            "search c ['0.00170204', '0.00865522', '0.809994', '0.729', '0.6561', '0.590472', '0.265392', '6.84834e-07']\n",
            "search c ['0.00284107', '0.229482', '0.809999', '0.728266', '0.000821552', '0.000192354', '0.524053', '0.478297']\n",
            "search c ['0.00310142', '0.585301', '0.809806', '0.000339367', '5.77926e-07', '0.00103656', '0.531441', '0.478297']\n",
            "search c ['0.00160613', '0.0129236', '0.809998', '0.729', '0.656099', '0.266154', '0.00120405', '3.73667e-07']\n",
            "search c ['0.0100588', '0.00856844', '0.0521111', '0.728988', '0.6561', '0.59049', '0.531438', '0.0432769']\n",
            "search c ['0.0294202', '1.47772e-05', '8.73392e-05', '0.728906', '0.6561', '0.59049', '0.00256226', '1.85292e-07']\n",
            "search c ['0.0436334', '1.21718e-05', '8.81956e-06', '0.724808', '0.6561', '0.59049', '0.00195491', '2.11032e-07']\n",
            "search c ['0.00904762', '0.834306', '0.806343', '0.00125268', '0.00983622', '0.590482', '0.531441', '0.478297']\n",
            "search c ['0.0269754', '0.74398', '0.083777', '2.24599e-05', '0.00740861', '0.59049', '0.531441', '0.478297']\n",
            "search c ['0.00224326', '0.0479587', '0.809999', '0.729', '0.6561', '0.581451', '0.000577926', '2.62161e-09']\n",
            "search c ['0.00328237', '0.708223', '0.807435', '0.00285901', '0.0542391', '0.59049', '0.531441', '0.271719']\n",
            "search c ['0.0045011', '0.656447', '0.809954', '0.725349', '0.653692', '0.590486', '0.531441', '0.478297']\n",
            "search c ['0.204322', '0.000226118', '5.27731e-07', '0.0109353', '0.6561', '0.59049', '0.0502953', '8.46795e-06']\n",
            "search c ['0.00264381', '0.0837469', '0.81', '0.728999', '0.205785', '0.110818', '0.53122', '0.478297']\n",
            "search c ['0.0172692', '0.3105', '0.0758574', '4.79832e-07', '2.87846e-12', '3.89413e-12', '0.00740606', '0.478297']\n",
            "search c ['0.0288488', '0.00550511', '0.00103025', '0.726322', '0.6561', '0.59049', '0.531439', '0.0108259']\n",
            "search c ['0.00547474', '0.157544', '0.794122', '0.728989', '0.6561', '0.59049', '0.53144', '0.199443']\n",
            "search c ['0.129362', '0.000767459', '3.34934e-08', '3.57568e-12', '2.40037e-09', '0.589981', '0.531441', '0.476896']\n",
            "search c ['0.00317919', '0.481042', '0.809916', '0.000534815', '5.40016e-07', '3.80784e-08', '0.000910849', '0.478297']\n",
            "search c ['0.020125', '0.210595', '0.00560741', '0.0351414', '0.656096', '0.59049', '0.531441', '0.449616']\n",
            "search c ['0.119158', '0.000510655', '2.47095e-06', '0.0749457', '0.6561', '0.59049', '0.53143', '0.00309068']\n",
            "search c ['0.00167967', '0.0180052', '0.809999', '0.729', '0.656084', '0.00726784', '1.45283e-05', '0.00535601']\n",
            "search c ['0.00167896', '0.0195999', '0.809999', '0.729', '0.656081', '0.005418', '1.2078e-05', '0.0208573']\n",
            "search c ['0.149212', '6.49957e-05', '2.04376e-07', '0.00648748', '0.6561', '0.59049', '0.531319', '6.19442e-05']\n",
            "search c ['0.124546', '0.000150058', '6.73887e-07', '0.0230006', '0.6561', '0.59049', '0.531324', '9.91448e-05']\n",
            "search c ['0.00280821', '0.632608', '0.809816', '0.629513', '1.28784e-05', '4.83599e-09', '2.10162e-06', '0.478296']\n",
            "search c ['0.0023361', '0.0388398', '0.809999', '0.729', '0.656056', '0.00331724', '7.56762e-06', '0.0191048']\n",
            "search c ['0.0040147', '0.787705', '0.808658', '0.00496732', '0.0658159', '0.59049', '0.531441', '0.27055']\n",
            "search c ['0.00602988', '0.450931', '0.809305', '0.728994', '0.6561', '0.59049', '0.53144', '0.183568']\n",
            "search c ['0.144849', '0.0015942', '3.52521e-08', '6.05998e-12', '1.07623e-08', '0.590397', '0.531441', '0.47698']\n",
            "search c ['0.123074', '0.00290106', '6.96771e-08', '1.46105e-11', '3.09558e-08', '0.590452', '0.531441', '0.477276']\n",
            "search c ['0.0868599', '0.00197856', '2.22803e-05', '0.466718', '0.6561', '0.59049', '0.531436', '0.00426706']\n",
            "search c ['0.0706486', '0.00591082', '2.2506e-05', '0.121588', '0.6561', '0.59049', '0.53144', '0.019961']\n",
            "search c ['0.0234095', '0.143255', '0.000803182', '2.60065e-09', '2.1113e-09', '0.184672', '0.531441', '0.478297']\n",
            "search c ['0.00291749', '0.650267', '0.80981', '0.000600211', '6.82838e-06', '0.0742263', '0.531441', '0.478297']\n",
            "search c ['0.119548', '0.000113428', '3.39132e-07', '0.00894209', '0.6561', '0.59049', '0.53142', '0.000346768']\n",
            "search c ['0.121435', '0.000122048', '2.36063e-07', '0.00276949', '0.6561', '0.59049', '0.531423', '0.000651771']\n",
            "search c ['0.00463882', '0.28223', '0.81', '0.728983', '0.00246112', '6.74181e-05', '0.297351', '0.478297']\n",
            "search c ['0.00474022', '0.0914409', '0.81', '0.729', '0.655829', '0.00181514', '2.44567e-05', '3.57761e-05']\n",
            "search c ['0.0270386', '0.000205385', '0.100227', '0.729', '0.6561', '0.59049', '0.0264953', '3.11531e-08']\n",
            "search c ['0.0220415', '0.000311222', '0.31394', '0.729', '0.6561', '0.590488', '0.0114427', '1.56164e-08']\n",
            "search c ['0.00256346', '0.102069', '0.81', '0.728999', '0.00557207', '8.49108e-05', '0.432319', '0.478285']\n",
            "search c ['0.00254762', '0.0266832', '0.809999', '0.729', '0.656', '0.00226886', '9.28829e-05', '0.000532645']\n",
            "search c ['0.0129151', '0.000268047', '0.401742', '0.729', '0.6561', '0.590489', '0.0304539', '2.97938e-08']\n",
            "search c ['0.0102951', '0.000464424', '0.686725', '0.729', '0.6561', '0.590488', '0.0114108', '8.66516e-09']\n",
            "search c ['0.262055', '0.00263369', '4.05656e-07', '1.93113e-11', '4.3466e-10', '0.539389', '0.531441', '0.478193']\n",
            "search c ['0.0109782', '0.0104805', '0.809983', '0.729', '0.656093', '0.0191639', '0.000393394', '0.00179836']\n",
            "search c ['0.00692541', '0.0826872', '0.809999', '0.729', '0.6561', '0.572104', '7.54486e-06', '9.99547e-12']\n",
            "search c ['0.00695794', '0.0899915', '0.809999', '0.729', '0.6561', '0.42041', '6.60049e-07', '1.48826e-11']\n",
            "search c ['0.188306', '0.00132376', '2.96755e-06', '0.0478667', '0.6561', '0.59049', '0.531438', '0.427519']\n",
            "search c ['0.198379', '0.000822017', '2.42428e-06', '0.0465801', '0.6561', '0.59049', '0.531431', '0.0191495']\n",
            "search c ['0.0536719', '0.0286446', '0.000853059', '0.679239', '0.6561', '0.59049', '0.531422', '0.478264']\n",
            "search c ['0.00969557', '0.725614', '0.767393', '0.00129662', '0.372459', '0.59049', '0.53144', '0.183011']\n",
            "search c ['0.0357022', '0.0794673', '6.32059e-05', '3.30819e-10', '5.18421e-10', '0.213122', '0.531441', '0.478296']\n",
            "search c ['0.00258963', '0.0448969', '0.809998', '0.729', '0.6561', '0.590466', '0.00866117', '4.48106e-09']\n",
            "search c ['0.0903863', '0.000271977', '0.00156561', '0.728969', '0.6561', '0.59049', '0.531395', '0.373169']\n",
            "search c ['0.123149', '0.000166461', '0.00010775', '0.727593', '0.6561', '0.59049', '0.530961', '0.471187']\n",
            "search c ['0.00218442', '0.0323235', '0.809999', '0.729', '0.6561', '0.587729', '2.53884e-05', '7.82046e-11']\n",
            "search c ['0.00364181', '0.0828358', '0.809988', '0.729', '0.6561', '0.590477', '0.00855519', '1.29624e-08']\n",
            "search c ['0.00396416', '0.0881711', '0.81', '0.729', '0.656098', '0.590408', '0.530105', '0.00339977']\n",
            "search c ['0.067741', '0.149545', '4.35631e-06', '5.80794e-08', '0.0120896', '0.59049', '0.531441', '0.101193']\n",
            "search c ['0.00256625', '0.0819456', '0.81', '0.729', '0.656093', '0.0228372', '4.74775e-07', '2.35128e-11']\n",
            "search c ['0.00285068', '0.150073', '0.809999', '0.729', '0.653938', '0.0848761', '0.175082', '0.478211']\n",
            "search c ['0.0561041', '0.00152384', '6.38815e-05', '0.69698', '0.6561', '0.59049', '0.531428', '0.000969543']\n",
            "search c ['0.0694172', '0.00108037', '1.65442e-05', '0.534482', '0.6561', '0.59049', '0.531425', '0.000746369']\n",
            "search c ['0.00680326', '0.500287', '0.770398', '8.75961e-06', '5.8744e-07', '0.215624', '0.531441', '0.478297']\n",
            "search c ['0.137894', '0.00159065', '4.59402e-08', '9.86089e-12', '1.44816e-08', '0.590385', '0.531441', '0.477593']\n",
            "search c ['0.172071', '6.93324e-05', '5.02746e-07', '0.0362647', '0.6561', '0.59049', '0.531383', '0.00019371']\n",
            "search c ['0.101414', '0.000361057', '1.39379e-05', '0.597866', '0.6561', '0.59049', '0.531404', '0.000228983']\n",
            "search c ['0.00321464', '0.155121', '0.81', '0.729', '0.6561', '0.589688', '0.031243', '1.89377e-06']\n",
            "search c ['0.167878', '0.000359898', '1.32143e-06', '0.032608', '0.6561', '0.59049', '0.0684453', '9.84125e-06']\n",
            "search c ['0.216826', '0.00737571', '1.49784e-08', '1.38192e-10', '5.47907e-05', '0.59049', '0.531441', '0.439263']\n",
            "search c ['0.00570053', '0.722646', '0.807356', '0.00187802', '0.0203462', '0.590486', '0.531441', '0.478297']\n",
            "search c ['0.0165273', '0.0238873', '0.154719', '0.728976', '0.6561', '0.59049', '0.531438', '0.0111861']\n",
            "search c ['0.0133833', '0.0245222', '0.572932', '0.728999', '0.6561', '0.59049', '0.531427', '0.00460207']\n",
            "search c ['0.00212257', '0.0170848', '0.809999', '0.729', '0.656082', '0.0066562', '1.24393e-05', '0.0165748']\n",
            "search c ['0.111104', '0.00320828', '1.01492e-08', '1.15174e-09', '0.0469103', '0.59049', '0.00152709', '4.07548e-06']\n",
            "search c ['0.00242671', '0.501685', '0.809994', '0.0222773', '3.1225e-07', '3.91193e-06', '0.52859', '0.478297']\n",
            "search c ['0.00397004', '0.558028', '0.790925', '0.00051433', '0.0100027', '0.590488', '0.531441', '0.478297']\n",
            "search c ['0.0036747', '0.592527', '0.804052', '0.00784446', '0.585109', '0.590474', '0.0179922', '1.10599e-07']\n",
            "search c ['0.0191732', '0.000798482', '0.550281', '0.729', '0.6561', '0.590489', '0.0378982', '3.42945e-08']\n",
            "search c ['0.00278751', '0.0695694', '0.81', '0.729', '0.656044', '0.170317', '0.00211694', '3.07613e-07']\n",
            "search c ['0.00291007', '0.0653191', '0.81', '0.729', '0.655954', '0.00902815', '4.20366e-05', '6.45615e-08']\n",
            "search c ['0.00276989', '0.0230742', '0.809999', '0.729', '0.656099', '0.0196613', '9.50969e-07', '2.104e-08']\n",
            "search c ['0.0027709', '0.0239972', '0.809999', '0.729', '0.656099', '0.041935', '1.79295e-07', '1.30294e-10']\n",
            "search c ['0.00262163', '0.103236', '0.81', '0.729', '0.656099', '0.589879', '0.528572', '0.473209']\n",
            "search c ['0.00259709', '0.0350978', '0.81', '0.729', '0.65609', '0.466682', '0.0307031', '3.06653e-06']\n",
            "search c ['0.00155636', '0.0281423', '0.809999', '0.729', '0.656099', '0.050533', '9.57522e-08', '1.61e-11']\n",
            "search c ['0.0912841', '0.00278802', '1.18462e-07', '2.41485e-11', '1.39042e-12', '1.63848e-05', '0.531441', '0.478297']\n",
            "search c ['0.0820031', '0.000611199', '3.98934e-06', '0.143799', '0.6561', '0.59049', '0.531433', '0.00258199']\n",
            "search c ['0.00269464', '0.715649', '0.808938', '0.00792834', '0.0978713', '0.59049', '0.531441', '0.253707']\n",
            "search c ['0.0292883', '0.473674', '0.0012181', '1.15812e-08', '1.01308e-07', '0.583707', '0.531441', '0.478296']\n",
            "search c ['0.00463155', '0.774435', '0.808987', '0.000449287', '0.000425342', '0.589803', '0.531441', '0.478297']\n",
            "search c ['0.0225635', '0.252668', '0.000192907', '3.97572e-06', '0.249714', '0.59049', '0.531441', '0.225267']\n",
            "search c ['0.00599216', '0.573036', '0.684328', '0.000809013', '0.215888', '0.59049', '0.531441', '0.243586']\n",
            "search c ['0.00307299', '0.556371', '0.809991', '0.729', '0.656025', '0.000234886', '1.13738e-09', '1.11788e-09']\n",
            "search c ['0.00309409', '0.363809', '0.809997', '0.729', '0.656098', '0.00947369', '2.26021e-07', '1.14396e-06']\n",
            "search c ['0.133976', '0.000518793', '4.18495e-08', '7.01028e-06', '0.656083', '0.59049', '0.531437', '0.00122904']\n",
            "search c ['0.0104772', '0.147716', '0.170353', '0.716997', '0.6561', '0.59049', '0.531441', '0.352826']\n",
            "search c ['0.0783669', '0.000129191', '0.000200659', '0.72884', '0.6561', '0.59049', '0.0114514', '6.19964e-07']\n",
            "search c ['0.00359733', '0.829481', '0.809799', '0.00663602', '0.0118152', '0.590415', '0.531441', '0.478297']\n",
            "search c ['0.128211', '0.000178031', '1.01951e-06', '0.0619914', '0.6561', '0.59049', '0.5314', '0.000205047']\n",
            "search c ['0.0114579', '0.0473256', '0.708544', '0.729', '0.6561', '0.59049', '0.531438', '0.0629568']\n",
            "search c ['0.00552984', '0.44515', '0.743525', '3.38915e-05', '0.000200539', '0.590414', '0.531441', '0.478297']\n",
            "search c ['0.0152395', '0.000271569', '0.520766', '0.729', '0.6561', '0.590454', '0.530659', '0.477533']\n",
            "search c ['0.0718245', '0.00043692', '3.52569e-06', '0.172708', '0.6561', '0.59049', '0.531432', '0.000934055']\n",
            "search c ['0.0390714', '0.00865073', '3.83923e-06', '0.0006545', '0.656099', '0.59049', '0.53144', '0.00321151']\n",
            "search c ['0.110263', '0.00286051', '8.77278e-08', '8.14527e-12', '2.83135e-09', '0.58947', '0.531441', '0.478057']\n",
            "search c ['0.00517783', '0.596362', '0.796607', '1.26784e-05', '1.84512e-07', '0.0136463', '0.531441', '0.478297']\n",
            "search c ['0.0142274', '0.773209', '0.723002', '0.124877', '0.655451', '0.59049', '0.531441', '0.453428']\n",
            "search c ['0.00761617', '0.837959', '0.809011', '0.00295846', '0.0257173', '0.590489', '0.531441', '0.148937']\n",
            "search c ['0.00239444', '0.046704', '0.809999', '0.729', '0.656035', '0.00255628', '5.74187e-06', '0.0194822']\n",
            "search c ['0.00237453', '0.0325145', '0.809999', '0.729', '0.656011', '0.00175699', '4.00659e-06', '0.0130876']\n",
            "search c ['0.00886679', '0.00282588', '0.631274', '0.729', '0.6561', '0.59049', '0.531424', '0.0288159']\n",
            "search c ['0.0646386', '8.3606e-05', '6.30373e-06', '0.628767', '0.6561', '0.59049', '0.531275', '9.78039e-05']\n",
            "search c ['0.0027711', '0.0277644', '0.809991', '0.729', '0.656093', '0.589164', '0.52929', '0.47803']\n",
            "search c ['0.00242482', '0.0345765', '0.809996', '0.729', '0.656095', '0.588935', '0.527596', '0.477752']\n",
            "search c ['0.0331691', '0.160299', '0.00359707', '0.265262', '0.6561', '0.59049', '0.531441', '0.276196']\n",
            "search c ['0.00662144', '0.772043', '0.807466', '0.00913866', '0.229324', '0.59049', '0.531441', '0.291182']\n",
            "search c ['0.00268397', '0.747442', '0.809713', '0.00349193', '0.011917', '0.590473', '0.531441', '0.478294']\n",
            "search c ['0.00303445', '0.743401', '0.809934', '0.000259776', '2.82717e-07', '0.000398449', '0.531441', '0.478297']\n",
            "search c ['0.0104651', '0.43428', '0.562573', '0.723815', '0.6561', '0.59049', '0.531441', '0.427168']\n",
            "search c ['0.00370633', '0.797784', '0.809338', '0.0050356', '0.0428229', '0.590489', '0.531441', '0.0904586']\n",
            "search c ['0.00275911', '0.399364', '0.809981', '0.00655931', '5.63389e-08', '2.01816e-07', '0.521249', '0.478297']\n",
            "search c ['0.00349181', '0.441132', '0.80027', '0.00561011', '0.168727', '0.59049', '0.531441', '0.478092']\n",
            "search c ['0.00209895', '0.0387275', '0.809999', '0.729', '0.6561', '0.40577', '6.10988e-07', '1.64556e-11']\n",
            "search c ['0.00218909', '0.0425179', '0.809999', '0.729', '0.6561', '0.231166', '4.22281e-07', '2.65477e-12']\n",
            "search c ['0.247185', '0.000122877', '7.73165e-06', '0.00020284', '0.650519', '0.59049', '0.531441', '0.478291']\n",
            "search c ['0.00931816', '0.0128229', '0.809921', '0.729', '0.6561', '0.59039', '0.527307', '0.473417']\n",
            "search c ['0.00668178', '0.709906', '0.792122', '0.000725992', '0.0291899', '0.59049', '0.531441', '0.474268']\n",
            "search c ['0.00446265', '0.0396538', '0.809999', '0.729', '0.656099', '0.22988', '0.000575176', '1.87278e-08']\n",
            "search c ['0.106992', '0.00816345', '2.72901e-08', '3.59548e-09', '0.0308783', '0.59049', '0.531441', '0.00340573']\n",
            "search c ['0.00558415', '0.67272', '0.733396', '0.000126116', '0.00645164', '0.590489', '0.531441', '0.478284']\n",
            "search c ['0.00784848', '0.161546', '0.809996', '0.729', '0.6561', '0.590485', '0.0157432', '4.17242e-08']\n",
            "search c ['0.00950307', '0.0678672', '0.809993', '0.729', '0.6561', '0.590476', '0.00381815', '1.17171e-08']\n",
            "search c ['0.209179', '0.00016009', '3.276e-06', '0.616647', '0.6561', '0.231553', '0.426181', '0.409102']\n",
            "search c ['0.209683', '0.000178721', '2.48573e-06', '0.173646', '0.6561', '0.59049', '0.531399', '0.0167688']\n",
            "search c ['0.00442486', '0.0608125', '0.809999', '0.729', '0.6561', '0.58418', '7.9643e-06', '5.20054e-11']\n",
            "search c ['0.00441587', '0.0610344', '0.809999', '0.729', '0.6561', '0.582102', '1.25181e-05', '5.9702e-11']\n",
            "search c ['0.180405', '0.000147229', '2.14623e-06', '0.216533', '0.6561', '0.59049', '0.531237', '0.476879']\n",
            "search c ['0.187908', '0.00012642', '1.71635e-06', '0.123312', '0.6561', '0.59049', '0.531391', '0.473365']\n",
            "search c ['0.0041743', '0.129186', '0.809998', '0.729', '0.6561', '0.590475', '0.0103812', '7.8055e-09']\n",
            "search c ['0.00315857', '0.044625', '0.809999', '0.729', '0.6561', '0.545145', '3.11868e-06', '2.47328e-11']\n",
            "search c ['0.295252', '0.00979344', '5.09418e-07', '7.46065e-11', '7.11132e-14', '1.84733e-09', '0.531376', '0.478297']\n",
            "search c ['0.00695371', '0.0522682', '0.809999', '0.729', '0.6561', '0.589821', '0.529519', '0.432859']\n",
            "search c ['0.0147165', '0.00249783', '0.805205', '0.729', '0.6561', '0.590487', '0.0208109', '1.83925e-08']\n",
            "search c ['0.00965931', '0.00753363', '0.80977', '0.729', '0.6561', '0.590485', '0.0125154', '1.82846e-08']\n",
            "search c ['0.00279965', '0.0926375', '0.81', '0.729', '0.655918', '0.00374519', '3.22254e-05', '7.27637e-07']\n",
            "search c ['0.00274428', '0.0994967', '0.81', '0.729', '0.656098', '0.590377', '0.53025', '0.00582897']\n",
            "search c ['0.00263818', '0.0690878', '0.81', '0.729', '0.656092', '0.0137335', '1.49955e-05', '0.00650966']\n",
            "search c ['0.00394291', '0.722258', '0.807966', '0.00912956', '0.532832', '0.590414', '0.00184855', '4.9184e-06']\n",
            "search c ['0.00184549', '0.0149825', '0.809999', '0.729', '0.655891', '0.0059901', '4.16922e-05', '2.07964e-08']\n",
            "search c ['0.00347596', '0.143264', '0.80884', '0.728998', '0.6561', '0.59049', '0.531438', '0.0404116']\n",
            "search c ['0.00320867', '0.0588294', '0.809997', '0.726637', '0.000743077', '0.00222512', '0.531414', '0.478297']\n",
            "search c ['0.0499287', '1.59466e-05', '0.000237527', '0.728982', '0.6561', '0.59049', '0.00178851', '1.33548e-07']\n",
            "search c ['0.158338', '0.000116364', '3.61132e-06', '0.339302', '0.6561', '0.59049', '0.531191', '0.477375']\n",
            "search c ['0.0122573', '0.656422', '0.108213', '4.88441e-05', '0.074214', '0.590489', '0.531441', '0.478295']\n",
            "search c ['0.00405899', '0.660411', '0.783452', '0.00995274', '0.584987', '0.59049', '0.531441', '0.46515']\n",
            "search c ['0.00320126', '0.623083', '0.807536', '0.0599346', '0.589061', '0.59049', '0.531441', '0.342753']\n",
            "search c ['0.0213841', '1.28897e-05', '0.000636707', '0.728998', '0.6561', '0.590489', '0.00195358', '1.18296e-07']\n",
            "search c ['0.0798252', '0.000272088', '2.55466e-08', '2.16197e-12', '5.16785e-10', '0.587002', '0.531441', '0.477229']\n",
            "search c ['0.0517457', '0.000106623', '4.66443e-06', '0.562652', '0.6561', '0.59049', '0.531343', '0.000352527']\n",
            "search c ['0.0742927', '4.28293e-05', '3.07931e-07', '0.0215606', '0.6561', '0.59049', '0.531364', '9.03094e-05']\n",
            "search c ['0.00295089', '0.785936', '0.809906', '0.00176652', '4.66331e-05', '0.386056', '0.531441', '0.478297']\n",
            "search c ['0.00382158', '0.784387', '0.809766', '0.000221174', '4.906e-07', '0.000742359', '0.531441', '0.478297']\n",
            "search c ['0.0155415', '0.0249953', '0.417939', '0.728999', '0.6561', '0.59049', '0.531438', '0.0511031']\n",
            "search c ['0.00282489', '0.744044', '0.809025', '0.0143476', '0.150251', '0.59049', '0.531441', '0.296957']\n",
            "search c ['0.137097', '0.00349364', '1.05894e-07', '5.61121e-11', '4.16015e-07', '0.590488', '0.531441', '0.477513']\n",
            "search c ['0.00365251', '0.0403978', '0.809998', '0.729', '0.6561', '0.438937', '3.74825e-06', '1.64345e-07']\n",
            "search c ['0.00403172', '0.720865', '0.808486', '0.00493652', '0.0500848', '0.590489', '0.531441', '0.478021']\n",
            "search c ['0.00374944', '0.283879', '0.809988', '0.729', '0.656098', '0.590489', '0.531301', '0.00197311']\n",
            "search c ['0.00182347', '0.0244728', '0.809997', '0.729', '0.6561', '0.587988', '0.0044483', '2.384e-07']\n",
            "search c ['0.00245944', '0.508839', '0.809893', '0.000413139', '2.9883e-08', '4.33987e-06', '0.531325', '0.478297']\n",
            "search c ['0.00708304', '0.0174546', '0.809984', '0.729', '0.6561', '0.590466', '0.0177582', '6.42321e-09']\n",
            "search c ['0.0185234', '0.00152568', '0.786282', '0.729', '0.6561', '0.590488', '0.0290722', '1.17736e-08']\n",
            "search c ['0.00362589', '0.0535282', '0.81', '0.729', '0.656088', '0.519697', '0.385336', '0.45051']\n",
            "search c ['0.00388889', '0.0603545', '0.81', '0.729', '0.656099', '0.590426', '0.526764', '0.000370286']\n",
            "search c ['0.00409441', '0.721346', '0.809767', '0.000140856', '1.32892e-07', '0.000155097', '0.531441', '0.478297']\n",
            "search c ['0.00369176', '0.721488', '0.809853', '0.000483443', '1.96218e-06', '0.0102307', '0.531441', '0.478297']\n",
            "search c ['0.143698', '9.6041e-05', '4.74039e-07', '0.0251113', '0.6561', '0.59049', '0.531384', '0.000148423']\n",
            "search c ['0.132465', '0.000163051', '6.42795e-07', '0.0179942', '0.6561', '0.59049', '0.531409', '0.000209397']\n",
            "search c ['0.101052', '0.00276963', '7.99594e-08', '1.62695e-11', '2.2068e-08', '0.59041', '0.531441', '0.477894']\n",
            "search c ['0.00418139', '0.584808', '0.805388', '4.64117e-05', '1.42908e-06', '0.11311', '0.531441', '0.478297']\n",
            "search c ['0.224851', '0.000205403', '5.81775e-07', '0.0112712', '0.6561', '0.59049', '0.531421', '0.000371364']\n",
            "search c ['0.110193', '0.00224298', '3.55936e-05', '0.490853', '0.6561', '0.59049', '0.531435', '0.00206028']\n",
            "search c ['0.00686731', '0.56239', '0.809966', '0.728973', '0.656031', '0.590486', '0.53144', '0.478297']\n",
            "search c ['0.00697805', '0.396207', '0.809984', '0.729', '0.656086', '0.590462', '0.531426', '0.478295']\n",
            "search c ['0.0118642', '0.300846', '0.799269', '0.728995', '0.6561', '0.59049', '0.531441', '0.295702']\n",
            "search c ['0.0156096', '0.24958', '0.625843', '0.728554', '0.6561', '0.59049', '0.531441', '0.247912']\n",
            "search c ['0.00500791', '0.787972', '0.809726', '0.000232198', '3.94767e-06', '0.0898504', '0.531441', '0.478297']\n",
            "search c ['0.00359624', '0.209141', '0.81', '0.729', '0.656099', '0.590151', '0.531259', '0.44109']\n",
            "search c ['0.00577965', '0.00785124', '0.80993', '0.729', '0.6561', '0.590485', '0.0127595', '9.60527e-09']\n",
            "search c ['0.00553241', '0.00865395', '0.809946', '0.729', '0.6561', '0.590481', '0.005486', '7.3863e-09']\n",
            "search c ['0.329091', '0.00909085', '2.15933e-08', '1.41434e-10', '2.10388e-05', '0.59049', '0.531441', '0.477029']\n",
            "search c ['0.321688', '0.0123472', '3.55815e-07', '4.36557e-11', '7.4721e-14', '5.23846e-09', '0.531419', '0.478297']\n",
            "search c ['0.00978811', '0.0331897', '0.796767', '0.729', '0.6561', '0.59049', '0.531427', '0.0145152']\n",
            "search c ['0.00552567', '0.0878994', '0.809634', '0.729', '0.6561', '0.590489', '0.531382', '0.00256778']\n",
            "search c ['0.0027484', '0.271323', '0.81', '0.729', '0.6561', '0.588873', '0.0196058', '1.64385e-06']\n",
            "search c ['0.00329927', '0.766404', '0.809896', '0.00320582', '0.000355216', '0.577571', '0.531441', '0.478297']\n",
            "search c ['0.132129', '0.000164007', '7.79985e-07', '0.0346937', '0.6561', '0.59049', '0.531398', '0.000271454']\n",
            "search c ['0.0939494', '0.000486797', '4.90669e-06', '0.223957', '0.6561', '0.59049', '0.531417', '0.000622193']\n",
            "search c ['0.00501314', '0.562156', '0.797234', '0.00128441', '0.0573663', '0.59049', '0.531441', '0.478294']\n",
            "search c ['0.173963', '0.00158533', '3.78357e-08', '2.69607e-12', '5.2883e-10', '0.585373', '0.531441', '0.477846']\n",
            "search c ['0.00951189', '0.389183', '0.806988', '0.728996', '0.6561', '0.59049', '0.531441', '0.339941']\n",
            "search c ['0.0122006', '0.496916', '0.658223', '0.636455', '0.656097', '0.59049', '0.531441', '0.334796']\n",
            "search c ['0.0027462', '0.0555226', '0.81', '0.729', '0.656099', '0.589824', '0.530729', '0.333676']\n",
            "search c ['0.00269742', '0.0896352', '0.81', '0.729', '0.656099', '0.587861', '0.503937', '0.473275']\n",
            "search c ['0.00630454', '0.454655', '0.801556', '0.689393', '0.656078', '0.59049', '0.531441', '0.258659']\n",
            "search c ['0.159048', '0.000233811', '9.00746e-07', '0.0266501', '0.6561', '0.59049', '0.53141', '0.000164326']\n",
            "search c ['0.00158947', '0.0148281', '0.809998', '0.729', '0.6561', '0.589923', '0.530152', '0.198606']\n",
            "search c ['0.00174484', '0.448358', '0.809413', '0.29465', '0.612441', '0.590488', '0.531441', '0.478297']\n",
            "search c ['0.00676158', '0.498674', '0.0354622', '2.63872e-05', '0.0900161', '0.59049', '0.531441', '0.212526']\n",
            "search c ['0.00205184', '0.0267041', '0.809998', '0.729', '0.6561', '0.584462', '0.000107388', '1.53311e-06']\n",
            "search c ['0.00744653', '0.844273', '0.800109', '0.00586191', '0.374735', '0.59049', '0.531441', '0.474475']\n",
            "search c ['0.00811428', '0.779399', '0.8062', '0.670838', '0.656023', '0.59049', '0.531441', '0.454014']\n",
            "search c ['0.00376683', '0.721171', '0.808733', '0.256325', '0.123995', '1.18426e-05', '4.1959e-10', '1.5505e-10']\n",
            "search c ['0.00554774', '0.701408', '0.808727', '3.08136e-05', '1.20194e-07', '0.00164453', '0.531441', '0.478297']\n",
            "search c ['0.156194', '0.000420398', '1.64459e-06', '0.0407287', '0.6561', '0.59049', '0.531429', '0.00161923']\n",
            "search c ['0.163952', '0.000390832', '1.07612e-06', '0.0151877', '0.6561', '0.59049', '0.531422', '0.000307422']\n",
            "search c ['0.00411249', '0.462313', '0.773265', '0.00153407', '0.229851', '0.59049', '0.531441', '0.473597']\n",
            "search c ['0.00285845', '0.0190747', '0.809994', '0.729', '0.6561', '0.585991', '0.00238184', '1.34072e-07']\n",
            "search c ['0.0832265', '3.3143e-05', '3.98415e-06', '0.632022', '0.6561', '0.59049', '0.530274', '0.477087']\n",
            "search c ['0.0142985', '0.000429639', '0.498647', '0.729', '0.6561', '0.590488', '0.00555861', '2.12019e-07']\n",
            "search c ['0.0955028', '7.40374e-05', '0.000353277', '0.728943', '0.6561', '0.59049', '0.0166903', '4.98822e-07']\n",
            "search c ['0.252795', '9.89817e-05', '1.21256e-06', '0.0910453', '0.6561', '0.59049', '0.531378', '0.000909343']\n",
            "search c ['0.314108', '0.00403601', '7.28093e-09', '2.93024e-10', '0.000761013', '0.59049', '0.531441', '0.476588']\n",
            "search c ['0.290793', '0.00510278', '9.56998e-09', '1.621e-10', '0.000105463', '0.59049', '0.531441', '0.477031']\n",
            "search c ['0.0992413', '0.00435029', '3.56888e-06', '0.00683988', '0.6561', '0.59049', '0.53144', '0.00428153']\n",
            "search c ['0.00597973', '0.497676', '0.809314', '0.728911', '0.656099', '0.59049', '0.53144', '0.136514']\n",
            "search c ['0.00550804', '0.74782', '0.809967', '0.000630759', '3.26832e-08', '2.70652e-06', '0.531399', '0.478297']\n",
            "search c ['0.00427376', '0.0539548', '0.809999', '0.729', '0.656094', '0.042152', '0.000391367', '3.06412e-06']\n",
            "search c ['0.00227169', '0.0278088', '0.809999', '0.729', '0.6561', '0.56743', '1.99319e-05', '2.51054e-11']\n",
            "search c ['0.0995581', '0.00481201', '1.42998e-08', '1.96009e-09', '0.0517885', '0.59049', '0.461293', '3.90588e-07']\n",
            "search c ['0.007021', '0.303045', '0.00379847', '9.78834e-07', '0.00174147', '0.59049', '0.531441', '0.478283']\n",
            "search c ['0.0329173', '0.0252927', '4.59498e-07', '4.29074e-10', '6.57319e-06', '0.59049', '0.531441', '0.477331']\n",
            "search c ['0.00466495', '0.0313882', '0.809997', '0.729', '0.6561', '0.588659', '0.00308285', '1.87222e-08']\n",
            "search c ['0.00643293', '0.0129492', '0.80998', '0.729', '0.6561', '0.590017', '0.0115319', '2.44771e-08']\n",
            "search c ['0.225177', '0.000359092', '7.03225e-07', '0.0126395', '0.6561', '0.59049', '0.168311', '9.99304e-06']\n",
            "search c ['0.00465462', '0.0737783', '0.81', '0.729', '0.6561', '0.0494071', '1.1548e-07', '9.09735e-09']\n",
            "search c ['0.00231987', '0.0648509', '0.809999', '0.729', '0.6561', '0.590477', '0.407777', '1.81901e-06']\n",
            "search c ['0.00240824', '0.131366', '0.809998', '0.729', '0.656099', '0.590464', '0.20639', '3.08374e-07']\n",
            "search c ['0.00239891', '0.597916', '0.809973', '0.722056', '0.000160339', '4.68716e-06', '0.159713', '0.47826']\n",
            "search c ['0.00216988', '0.144821', '0.81', '0.729', '0.611126', '1.85994e-05', '8.90515e-07', '3.54214e-05']\n",
            "search c ['0.0052507', '0.267306', '0.809999', '0.729', '0.6561', '0.590473', '0.0103146', '1.47225e-08']\n",
            "search c ['0.0051362', '0.255281', '0.81', '0.729', '0.6561', '0.590468', '0.0102604', '4.62264e-09']\n",
            "search c ['0.00497989', '0.205853', '0.81', '0.728999', '0.0977874', '0.0119852', '0.529606', '0.478297']\n",
            "search c ['0.00635687', '0.01677', '0.809994', '0.729', '0.6561', '0.562407', '2.08653e-05', '2.32193e-11']\n",
            "search c ['0.110948', '0.0102866', '3.75947e-08', '5.26947e-10', '0.000253461', '0.59049', '0.531441', '0.00892583']\n",
            "search c ['0.00214613', '0.127532', '0.81', '0.729', '0.6561', '0.50302', '0.000806685', '0.00128878']\n",
            "search c ['0.0366722', '0.152973', '0.0616295', '0.72774', '0.6561', '0.59049', '0.531441', '0.324747']\n",
            "search c ['0.00953051', '0.794324', '0.803209', '0.00547105', '0.243256', '0.59049', '0.531441', '0.351419']\n",
            "search c ['0.134071', '0.000841915', '2.64331e-08', '3.54236e-12', '3.75138e-09', '0.590192', '0.531441', '0.47723']\n",
            "search c ['0.00558747', '0.310555', '0.758687', '0.000302758', '0.00305464', '0.590485', '0.531441', '0.478297']\n",
            "search c ['0.0210838', '0.398655', '0.721168', '0.728927', '0.6561', '0.59049', '0.531441', '0.415511']\n",
            "search c ['0.00892224', '0.821039', '0.807465', '0.00533695', '0.0985882', '0.59049', '0.531441', '0.253408']\n",
            "search c ['0.00316481', '0.776019', '0.809686', '0.0148154', '0.0621429', '0.590483', '0.531441', '0.478297']\n",
            "search c ['0.00288129', '0.239739', '0.809999', '0.729', '0.656096', '0.589995', '0.531312', '0.456004']\n",
            "search c ['0.0153896', '0.00164653', '0.780369', '0.729', '0.6561', '0.590489', '0.0780449', '2.90401e-08']\n",
            "search c ['0.0120151', '0.00196701', '0.805015', '0.729', '0.6561', '0.590486', '0.0068618', '1.93646e-08']\n",
            "search c ['0.00277939', '0.0346973', '0.81', '0.729', '0.656045', '0.0233474', '0.000146803', '5.04318e-07']\n",
            "search c ['0.0959648', '8.88026e-05', '0.000218237', '0.728954', '0.6561', '0.00180006', '0.0067262', '0.473553']\n",
            "search c ['0.0114255', '0.00544345', '0.809366', '0.729', '0.6561', '0.590486', '0.0286813', '1.13351e-08']\n",
            "search c ['0.0110929', '0.00751468', '0.80943', '0.729', '0.6561', '0.590482', '0.006136', '3.17325e-09']\n",
            "search c ['0.00276429', '0.0199051', '0.81', '0.729', '0.415543', '1.28146e-05', '8.84476e-06', '0.472875']\n",
            "search c ['0.0513731', '2.13625e-05', '0.00011369', '0.728913', '0.6561', '0.590489', '0.0307075', '0.00158966']\n",
            "search c ['0.002068', '0.0216405', '0.809999', '0.729', '0.6561', '0.590427', '0.00521728', '2.30153e-09']\n",
            "search c ['0.00280053', '0.00622766', '0.809983', '0.729', '0.6561', '0.590102', '0.531125', '0.379112']\n",
            "search c ['0.00780353', '0.000945852', '0.798618', '0.729', '0.6561', '0.590485', '0.00839747', '1.68061e-08']\n",
            "search c ['0.00726774', '0.000942142', '0.801569', '0.729', '0.6561', '0.590486', '0.00973557', '8.36579e-09']\n",
            "search c ['0.00296049', '0.75745', '0.809994', '0.00389193', '8.96476e-08', '8.07939e-07', '0.530791', '0.478297']\n",
            "search c ['0.00273785', '0.679982', '0.809996', '0.0102408', '2.75118e-06', '0.000210752', '0.531439', '0.478297']\n",
            "search c ['0.0895699', '0.000151254', '1.07975e-06', '0.0690508', '0.6561', '0.59049', '0.531431', '0.00307718']\n",
            "search c ['0.00196391', '0.0341659', '0.809999', '0.729', '0.6561', '0.589217', '0.00177943', '6.80731e-10']\n",
            "search c ['0.00153203', '0.00705609', '0.809995', '0.729', '0.656099', '0.590241', '0.529791', '0.00325216']\n",
            "search c ['0.00162653', '0.00590341', '0.809996', '0.729', '0.656021', '0.01059', '0.000139215', '2.91166e-07']\n",
            "search c ['0.00167076', '0.0339141', '0.809996', '0.729', '0.6561', '0.588042', '0.000138685', '1.3053e-06']\n",
            "search c ['0.002248', '0.449219', '0.809852', '0.00104675', '2.069e-08', '2.53681e-07', '0.529465', '0.478297']\n",
            "search c ['0.0094079', '0.205847', '0.336933', '0.707338', '0.6561', '0.59049', '0.531441', '0.375288']\n",
            "search c ['0.00456948', '0.401427', '0.806277', '0.713064', '0.656071', '0.59049', '0.531441', '0.183707']\n",
            "search c ['0.00223246', '0.0285559', '0.809999', '0.729', '0.655829', '0.000908818', '9.45533e-06', '0.00613099']\n",
            "search c ['0.00321526', '0.56771', '0.806793', '0.000292514', '0.000676104', '0.590304', '0.531441', '0.478297']\n",
            "search c ['0.00243187', '0.0393599', '0.809998', '0.729', '0.6561', '0.590395', '0.514987', '2.76356e-05']\n",
            "search c ['0.0973448', '4.85217e-05', '2.08144e-06', '0.403039', '0.6561', '0.59049', '0.259907', '3.3692e-06']\n",
            "search c ['0.00388014', '0.733191', '0.807902', '0.00360831', '0.0346405', '0.590486', '0.531441', '0.478297']\n",
            "search c ['0.00431335', '0.729777', '0.809746', '0.0541888', '6.47368e-08', '3.10918e-10', '8.94533e-05', '0.478127']\n",
            "search c ['0.00183772', '0.113714', '0.809918', '0.729', '0.6561', '0.590489', '0.167094', '1.09577e-06']\n",
            "search c ['0.00293239', '0.0124404', '0.809909', '0.729', '0.6561', '0.590485', '0.00721969', '7.14888e-08']\n",
            "search c ['0.11076', '3.71728e-05', '6.07027e-07', '0.153557', '0.6561', '0.59049', '0.528001', '0.477795']\n",
            "search c ['0.123015', '1.92678e-05', '3.41145e-07', '0.0462033', '0.6561', '0.59049', '0.531378', '0.135703']\n",
            "search c ['0.0116156', '0.00367714', '0.807304', '0.729', '0.6561', '0.590488', '0.0329212', '3.58604e-08']\n",
            "search c ['0.00447573', '0.575609', '0.802745', '0.00376102', '0.522377', '0.590482', '0.014535', '6.98289e-07']\n",
            "search c ['0.140073', '7.44829e-05', '2.22621e-06', '0.459782', '0.6561', '0.59049', '0.530782', '0.473952']\n",
            "search c ['0.00559991', '0.636669', '0.777291', '0.000281737', '0.0117161', '0.590489', '0.531441', '0.478297']\n",
            "search c ['0.0175035', '0.000618562', '0.457392', '0.729', '0.6561', '0.590489', '0.0112186', '3.24094e-07']\n",
            "search c ['0.0263852', '0.000372958', '0.0608079', '0.729', '0.6561', '0.59049', '0.0492988', '1.99688e-08']\n",
            "search c ['0.00480637', '0.698304', '0.800715', '0.00365247', '0.181956', '0.59049', '0.531441', '0.478297']\n",
            "search c ['0.00217435', '0.248621', '0.81', '0.728989', '0.00287718', '8.00185e-05', '0.466957', '0.47828']\n",
            "search c ['0.00276897', '0.734747', '0.80948', '0.0112839', '0.250179', '0.590488', '0.530799', '2.06501e-05']\n",
            "search c ['0.00288452', '0.749864', '0.809369', '0.00882278', '0.348922', '0.590487', '0.509681', '1.65238e-05']\n",
            "search c ['0.175601', '0.0171931', '3.17239e-08', '1.79537e-10', '3.20674e-05', '0.59049', '0.531441', '0.440324']\n",
            "search c ['0.0101256', '0.769473', '0.809259', '4.49559e-05', '4.16481e-08', '5.58486e-05', '0.531441', '0.478297']\n",
            "search c ['0.00400857', '0.0262853', '0.809998', '0.729', '0.6561', '0.565952', '4.11385e-06', '8.83838e-12']\n",
            "search c ['0.00396816', '0.027536', '0.809998', '0.729', '0.6561', '0.585589', '3.47085e-05', '3.58683e-11']\n",
            "search c ['0.00308338', '0.62003', '0.809971', '0.00145841', '6.22533e-08', '2.83769e-06', '0.531327', '0.478297']\n",
            "search c ['0.00257726', '0.0391925', '0.81', '0.729', '0.65407', '0.000147265', '1.35647e-06', '0.0207327']\n",
            "search c ['0.0374568', '0.549781', '0.000882382', '1.99246e-06', '0.012758', '0.59049', '0.531441', '0.19634']\n",
            "search c ['0.0105206', '0.723694', '0.793842', '0.0648015', '0.646372', '0.59049', '0.531441', '0.349127']\n",
            "search c ['0.146684', '0.0215494', '1.40713e-07', '2.30999e-10', '1.06452e-05', '0.59049', '0.531441', '0.477061']\n",
            "search c ['0.0981369', '0.00176619', '5.1307e-05', '0.680248', '0.6561', '0.59049', '0.164318', '9.86035e-06']\n",
            "search c ['0.0294862', '6.89998e-05', '0.00335869', '0.729', '0.6561', '0.576324', '7.25233e-05', '6.47578e-10']\n",
            "search c ['0.00149732', '0.0512749', '0.809999', '0.729', '0.656098', '0.588167', '0.494016', '0.471981']\n",
            "search c ['0.0708144', '0.00123956', '1.70751e-05', '0.483733', '0.6561', '0.59049', '0.531425', '0.000768686']\n",
            "search c ['0.0256251', '0.0130161', '0.0111667', '0.728795', '0.6561', '0.59049', '0.531438', '0.0115925']\n",
            "search c ['0.00234293', '0.721898', '0.809763', '0.0464726', '0.14531', '0.590481', '0.531441', '0.478297']\n",
            "search c ['0.00220288', '0.533918', '0.809987', '0.728996', '0.655237', '0.000224209', '3.08811e-07', '0.00191512']\n",
            "search c ['0.00794536', '0.275742', '0.782252', '0.72899', '0.6561', '0.59049', '0.531441', '0.41433']\n",
            "search c ['0.00360162', '0.725959', '0.808816', '0.0341044', '0.406125', '0.59049', '0.531441', '0.176632']\n",
            "search c ['0.00367288', '0.647463', '0.809869', '0.0005176', '4.44105e-09', '1.21294e-08', '0.459038', '0.478297']\n",
            "search c ['0.0117645', '0.431952', '0.00643219', '1.36786e-05', '0.139683', '0.59049', '0.531441', '0.394596']\n",
            "search c ['0.0996001', '0.00101602', '7.71108e-08', '3.84843e-12', '3.51191e-10', '0.580171', '0.531441', '0.477829']\n",
            "search c ['0.0931687', '0.00135185', '9.37389e-08', '4.73196e-12', '3.61839e-10', '0.574915', '0.531441', '0.478017']\n",
            "search c ['0.11991', '6.25432e-05', '7.52422e-07', '0.0863746', '0.6561', '0.59049', '0.531331', '0.000158338']\n",
            "search c ['0.0059176', '0.0527338', '0.804123', '0.729', '0.6561', '0.59049', '0.531431', '0.0260987']\n",
            "search c ['0.00636995', '0.594511', '0.797304', '1.04982e-05', '1.92571e-07', '0.0282872', '0.531441', '0.478297']\n",
            "search c ['0.100363', '0.0107606', '4.23478e-07', '1.2105e-11', '5.26994e-10', '0.569995', '0.531441', '0.478146']\n",
            "search c ['0.004936', '0.753941', '0.807824', '0.0323095', '0.538589', '0.59049', '0.531441', '0.265449']\n",
            "search c ['0.00513472', '0.728783', '0.808281', '0.210894', '0.647768', '0.59049', '0.531441', '0.113119']\n",
            "search c ['0.0919806', '0.290349', '7.33339e-06', '2.35673e-08', '0.00553529', '0.59049', '0.301554', '5.75514e-05']\n",
            "search c ['0.00666991', '0.804035', '0.809874', '0.188138', '2.34725e-07', '2.68589e-10', '4.24298e-06', '0.478297']\n",
            "search c ['0.064236', '0.260675', '1.78569e-05', '5.61345e-07', '0.159875', '0.59049', '0.531441', '0.00417276']\n",
            "search c ['0.0061469', '0.0532176', '0.809999', '0.729', '0.656099', '0.52922', '0.00638476', '5.68759e-08']\n",
            "search c ['0.00519346', '0.747552', '0.809447', '0.000102738', '5.46074e-07', '0.00638266', '0.531441', '0.478297']\n",
            "search c ['0.00346184', '0.0527638', '0.809999', '0.729', '0.6561', '0.590037', '0.530848', '0.342148']\n",
            "search c ['0.006589', '0.142436', '0.809999', '0.729', '0.6561', '0.590472', '0.0146565', '1.98423e-08']\n",
            "search c ['0.00695371', '0.12743', '0.809998', '0.729', '0.6561', '0.59047', '0.00300592', '8.43616e-09']\n",
            "search c ['0.00939418', '0.847087', '0.809958', '0.00066659', '7.28557e-09', '1.4737e-08', '0.502907', '0.478297']\n",
            "search c ['0.0102664', '0.848224', '0.809915', '0.00031833', '9.56489e-09', '6.85248e-08', '0.528645', '0.478297']\n",
            "search c ['0.00384945', '0.0432175', '0.809998', '0.729', '0.6561', '0.590479', '0.0106002', '1.57222e-08']\n",
            "search c ['0.00434582', '0.0281365', '0.809995', '0.729', '0.6561', '0.590475', '0.00500211', '4.38537e-09']\n",
            "search c ['0.00276175', '0.594132', '0.80999', '0.00597426', '6.53229e-08', '3.18706e-07', '0.529557', '0.478297']\n",
            "search c ['0.00258889', '0.362728', '0.809999', '0.539433', '3.17479e-06', '1.37674e-06', '0.509457', '0.478297']\n",
            "search c ['0.00472531', '0.0510986', '0.809989', '0.729', '0.6561', '0.590485', '0.0128454', '2.29646e-08']\n",
            "search c ['0.00695095', '0.0156505', '0.80993', '0.729', '0.6561', '0.590486', '0.0500796', '1.63196e-08']\n",
            "search c ['0.00168996', '0.387518', '0.795533', '0.00234945', '0.262966', '0.59049', '0.531441', '0.435327']\n",
            "search c ['0.00120097', '0.00781068', '0.809997', '0.729', '0.656098', '0.590076', '0.529749', '0.000721689']\n",
            "search c ['0.00249804', '0.65448', '0.806276', '0.00437246', '0.102449', '0.59049', '0.531441', '0.477302']\n",
            "search c ['0.00183126', '0.59214', '0.809997', '0.0108163', '1.31756e-07', '9.14749e-07', '0.528359', '0.478297']\n",
            "search c ['0.0425732', '0.0729013', '1.27001e-06', '1.74738e-08', '0.00352304', '0.59049', '0.531441', '0.00122945']\n",
            "search c ['0.0664727', '0.0266652', '1.55753e-07', '1.14922e-09', '0.000174411', '0.59049', '0.531441', '0.000658572']\n",
            "search c ['0.00263736', '0.0388723', '0.809999', '0.729', '0.6561', '0.590062', '0.530631', '0.303714']\n",
            "search c ['0.00266169', '0.0418939', '0.809999', '0.729', '0.6561', '0.581266', '0.00148406', '8.9969e-08']\n",
            "search c ['0.00385151', '0.110006', '0.81', '0.729', '0.6561', '0.590446', '0.00193892', '2.31844e-08']\n",
            "search c ['0.00414469', '0.0850935', '0.81', '0.729', '0.654566', '0.496016', '0.52508', '0.478187']\n",
            "search c ['0.00216204', '0.0309351', '0.809999', '0.729', '0.6561', '0.589392', '0.00674674', '5.88113e-08']\n",
            "search c ['0.0711057', '0.000102429', '6.09211e-06', '0.61931', '0.6561', '0.59049', '0.5314', '0.00451559']\n",
            "search c ['0.0024198', '0.0498511', '0.81', '0.729', '0.6561', '0.0848696', '1.47141e-07', '7.04496e-11']\n",
            "search c ['0.00250413', '0.0490611', '0.81', '0.729', '0.6561', '0.0681125', '5.52105e-08', '4.56479e-12']\n",
            "search c ['0.00207516', '0.0628689', '0.81', '0.729', '0.656066', '0.590317', '0.529243', '0.000189707']\n",
            "search c ['0.00210992', '0.0555988', '0.81', '0.729', '0.656096', '0.590413', '0.525035', '0.000194501']\n",
            "search c ['0.00429364', '0.792226', '0.809523', '0.285143', '0.0756298', '1.48981e-06', '2.05839e-09', '0.000117399']\n",
            "search c ['0.00499194', '0.794755', '0.809884', '0.000151044', '2.66135e-07', '0.00119862', '0.531441', '0.478297']\n",
            "search c ['0.0903212', '8.11266e-05', '8.35147e-07', '0.0956824', '0.6561', '0.59049', '0.531327', '9.3141e-05']\n",
            "search c ['0.0637231', '0.000241409', '4.48703e-06', '0.352226', '0.6561', '0.59049', '0.531378', '0.000251556']\n",
            "search c ['0.0923625', '0.00329288', '6.95523e-08', '1.68407e-10', '1.32627e-05', '0.59049', '0.531441', '0.477687']\n",
            "search c ['0.0477495', '0.0274819', '5.12865e-06', '1.79877e-10', '9.1734e-09', '0.588322', '0.531441', '0.478272']\n",
            "search c ['0.00731328', '0.478496', '0.797104', '0.679586', '0.656083', '0.59049', '0.531441', '0.361364']\n",
            "search c ['0.00685632', '0.57036', '0.78757', '0.0422596', '0.64704', '0.59049', '0.531441', '0.184019']\n",
            "search c ['0.00595918', '0.774038', '0.809495', '0.00207197', '0.00285055', '0.590391', '0.531441', '0.478297']\n",
            "search c ['0.0501329', '0.27758', '0.000313368', '3.08046e-09', '3.06083e-08', '0.584351', '0.531441', '0.478294']\n",
            "search c ['0.00623351', '0.738532', '0.806268', '0.00891226', '0.33083', '0.59049', '0.531441', '0.277141']\n",
            "search c ['0.0428174', '0.0687108', '0.000469141', '0.102332', '0.6561', '0.59049', '0.531441', '0.0367307']\n",
            "search c ['0.187087', '0.0141886', '1.947e-07', '5.87739e-12', '6.07204e-10', '0.582977', '0.531441', '0.47789']\n",
            "search c ['0.00653694', '0.777664', '0.809835', '0.000158229', '3.1688e-07', '0.00131736', '0.531441', '0.478297']\n",
            "search c ['0.0947796', '0.00324628', '1.75611e-05', '0.249762', '0.6561', '0.59049', '0.531437', '0.00342594']\n",
            "search c ['0.0748662', '0.0129068', '9.27858e-06', '0.00414596', '0.6561', '0.59049', '0.53144', '0.00614687']\n",
            "search c ['0.00507035', '0.784689', '0.809801', '0.000149181', '3.4957e-07', '0.00193675', '0.531441', '0.478297']\n",
            "search c ['0.00435649', '0.785035', '0.809401', '0.154702', '0.20047', '0.00262329', '3.18179e-09', '4.33809e-12']\n",
            "search c ['0.173457', '0.00706934', '2.02852e-08', '1.62151e-09', '0.0219598', '0.590489', '4.39762e-06', '7.68826e-11']\n",
            "search c ['0.17094', '0.00915926', '3.83622e-07', '2.38764e-11', '2.08831e-13', '8.09088e-07', '0.531441', '0.478297']\n",
            "search c ['0.00702857', '0.749874', '0.805837', '0.00913842', '0.55238', '0.59037', '0.00181965', '5.00361e-08']\n",
            "search c ['0.00734801', '0.0622758', '0.809996', '0.729', '0.6561', '0.590473', '0.00387286', '8.38568e-09']\n",
            "search c ['0.0687738', '0.147694', '9.34998e-05', '3.04282e-09', '2.22879e-13', '5.32e-11', '0.512867', '0.478297']\n",
            "search c ['0.00445968', '0.0140699', '0.809994', '0.729', '0.6561', '0.57288', '3.04284e-05', '3.93119e-11']\n",
            "search c ['0.00305888', '0.0273614', '0.809999', '0.729', '0.65608', '0.309042', '0.234949', '0.319583']\n",
            "search c ['0.057995', '0.000162891', '0.0107548', '0.729', '0.6561', '0.587123', '0.51575', '8.04768e-05']\n",
            "search c ['0.144147', '8.33144e-05', '1.61781e-05', '0.712206', '0.6561', '0.59049', '0.00801206', '7.28366e-07']\n",
            "search c ['0.00663273', '0.797679', '0.80368', '0.00144788', '0.0458903', '0.59049', '0.531441', '0.477252']\n",
            "search c ['0.00423821', '0.0553091', '0.809999', '0.729', '0.656096', '0.0234813', '3.57261e-05', '0.00989857']\n",
            "search c ['0.00428974', '0.0561518', '0.809999', '0.729', '0.656098', '0.0360656', '5.1571e-07', '5.6122e-10']\n",
            "search c ['0.00536197', '0.814781', '0.809323', '0.00487222', '0.0206265', '0.590484', '0.531441', '0.477859']\n",
            "search c ['0.00593716', '0.797779', '0.809728', '0.000288961', '4.38318e-06', '0.0727633', '0.531441', '0.478297']\n",
            "search c ['0.00369597', '0.0229709', '0.809997', '0.729', '0.6561', '0.108036', '1.46161e-06', '3.31923e-09']\n",
            "search c ['0.00399548', '0.0420592', '0.809996', '0.729', '0.6561', '0.587378', '0.00173705', '1.95361e-09']\n",
            "search c ['0.00326262', '0.0572755', '0.809999', '0.729', '0.6561', '0.230302', '6.49334e-07', '1.61456e-08']\n",
            "search c ['0.00395586', '0.0207068', '0.809997', '0.729', '0.6561', '0.587338', '0.501236', '0.473205']\n",
            "search c ['0.00326901', '0.675999', '0.807863', '0.00881299', '0.247557', '0.59049', '0.531441', '0.176258']\n",
            "search c ['0.00798822', '0.0963369', '0.78402', '0.729', '0.6561', '0.59049', '0.531438', '0.10216']\n",
            "search c ['0.118684', '0.00330107', '1.34946e-08', '1.65142e-10', '0.000185999', '0.59049', '0.531441', '0.161664']\n",
            "search c ['0.125693', '0.00168741', '7.18845e-08', '3.39452e-12', '4.12543e-10', '0.582267', '0.531441', '0.477397']\n",
            "search c ['0.14116', '3.99287e-05', '1.63351e-07', '0.0098101', '0.6561', '0.59049', '0.531345', '0.000174318']\n",
            "search c ['0.114201', '9.61487e-05', '4.91757e-07', '0.0317559', '0.6561', '0.59049', '0.531351', '0.000196161']\n",
            "search c ['0.00465089', '0.820869', '0.807529', '0.00120543', '0.00768682', '0.59048', '0.531441', '0.478296']\n",
            "search c ['0.00342879', '0.790757', '0.809903', '0.0820432', '0.216662', '0.590487', '0.531441', '0.47829']\n",
            "search c ['0.00524035', '0.0131543', '0.80995', '0.729', '0.6561', '0.590485', '0.0306801', '1.77998e-08']\n",
            "search c ['0.00552279', '0.00889698', '0.809912', '0.729', '0.6561', '0.59048', '0.00493201', '1.06729e-08']\n",
            "search c ['0.186288', '0.000162261', '4.38053e-07', '0.00595256', '0.6561', '0.59049', '0.531427', '0.00469452']\n",
            "search c ['0.00351871', '0.027845', '0.809999', '0.729', '0.655908', '0.00243444', '4.73413e-05', '8.80106e-06']\n",
            "search c ['0.00492246', '0.0142436', '0.80991', '0.729', '0.6561', '0.590487', '0.021401', '2.99642e-08']\n",
            "search c ['0.00280234', '0.646279', '0.807145', '0.00424091', '0.452438', '0.590484', '0.164458', '1.36147e-07']\n",
            "search c ['0.00342774', '0.0499593', '0.809999', '0.729', '0.656099', '0.59045', '0.530615', '0.00490219']\n",
            "search c ['0.00332555', '0.0397034', '0.809999', '0.729', '0.656099', '0.590435', '0.530489', '0.00319533']\n",
            "search c ['0.00277546', '0.120167', '0.81', '0.728999', '0.194394', '7.08078e-06', '7.08835e-08', '0.00705061']\n",
            "search c ['0.00295365', '0.0296406', '0.809999', '0.729', '0.656085', '0.00684532', '1.98598e-05', '0.00318675']\n",
            "search c ['0.0569466', '0.016259', '2.10089e-07', '1.61083e-07', '0.554693', '0.59049', '0.531441', '0.000547237']\n",
            "search c ['0.0629233', '0.0026601', '1.02989e-06', '0.000525971', '0.656099', '0.59049', '0.531439', '0.00176252']\n",
            "search c ['0.00475387', '0.757397', '0.809809', '0.237524', '0.555653', '0.590489', '0.531441', '0.478294']\n",
            "search c ['0.00439853', '0.0735829', '0.809999', '0.729', '0.6561', '0.589893', '0.530822', '0.340246']\n",
            "search c ['0.0054994', '0.100359', '0.809999', '0.729', '0.6561', '0.590472', '0.00462839', '5.2961e-09']\n",
            "search c ['0.00626726', '0.0664591', '0.809998', '0.729', '0.6561', '0.59047', '0.00412511', '5.29362e-09']\n",
            "search c ['0.00625982', '0.0405502', '0.81', '0.729', '0.655801', '0.00128278', '1.25151e-05', '0.0400314']\n",
            "search c ['0.251237', '0.0109342', '4.46103e-07', '2.04089e-11', '1.37194e-09', '0.580876', '0.531441', '0.478184']\n",
            "search c ['0.00406024', '0.0167502', '0.809987', '0.729', '0.6561', '0.59017', '0.0171943', '3.8001e-08']\n",
            "search c ['0.00406348', '0.017697', '0.809991', '0.729', '0.6561', '0.589511', '0.00516252', '3.58875e-08']\n",
            "search c ['0.295088', '0.000395993', '1.25363e-05', '0.0137559', '0.6561', '0.59049', '0.531438', '0.478226']\n",
            "search c ['0.30855', '0.000924059', '4.02437e-06', '6.68996e-07', '0.0549634', '0.59049', '0.531441', '0.478286']\n",
            "search c ['0.00226478', '0.0335631', '0.809998', '0.729', '0.6561', '0.56615', '2.68387e-06', '1.27978e-11']\n",
            "search c ['0.00246627', '0.0162321', '0.809995', '0.729', '0.6561', '0.551628', '2.159e-06', '2.6104e-11']\n",
            "search c ['0.00264697', '0.0141113', '0.809998', '0.729', '0.656044', '0.0107528', '0.000116352', '5.6502e-07']\n",
            "search c ['0.132953', '0.00274248', '1.05033e-07', '1.745e-11', '1.22523e-13', '3.22956e-07', '0.531441', '0.478297']\n",
            "search c ['0.074497', '0.000360821', '0.00132404', '0.728978', '0.6561', '0.59049', '0.0356932', '1.01597e-07']\n",
            "search c ['0.0578881', '0.000128222', '0.00804544', '0.728999', '0.6561', '0.59049', '0.0162931', '1.08374e-08']\n",
            "search c ['0.00206307', '0.725112', '0.809858', '0.636507', '0.224608', '8.97459e-07', '2.01886e-10', '7.31412e-07']\n",
            "search c ['0.00176352', '0.0549444', '0.81', '0.729', '0.655816', '0.00433064', '3.47286e-05', '3.11987e-08']\n",
            "search c ['0.00459316', '0.0431416', '0.809999', '0.729', '0.656097', '0.027363', '5.79068e-05', '0.011856']\n",
            "search c ['0.0070121', '0.566431', '0.801434', '0.206547', '0.655587', '0.59049', '0.531247', '0.0128124']\n",
            "search c ['0.00187317', '0.0060681', '0.809991', '0.729', '0.6561', '0.587704', '0.000143439', '2.09972e-06']\n",
            "search c ['0.00174241', '0.011768', '0.809994', '0.729', '0.6561', '0.587461', '9.30471e-05', '1.56403e-06']\n",
            "search c ['0.085617', '0.00153501', '4.61089e-07', '0.000247807', '0.656099', '0.59049', '0.53144', '0.00252159']\n",
            "search c ['0.117898', '0.000220638', '3.27694e-07', '0.00259065', '0.6561', '0.59049', '0.531435', '0.00186143']\n",
            "search c ['0.00447202', '0.00486526', '0.809866', '0.729', '0.6561', '0.590364', '0.53131', '0.42127']\n",
            "search c ['0.00297347', '0.0101538', '0.809993', '0.729', '0.6561', '0.589609', '0.530293', '0.227659']\n",
            "search c ['0.00566745', '0.18622', '0.809999', '0.729', '0.6561', '0.590473', '0.0101346', '8.18103e-09']\n",
            "search c ['0.00598592', '0.206302', '0.809999', '0.729', '0.6561', '0.590476', '0.0106693', '3.34456e-08']\n",
            "search c ['0.00339685', '0.0209153', '0.809996', '0.729', '0.6561', '0.590453', '0.00103323', '2.8599e-08']\n",
            "search c ['0.00260666', '0.0358333', '0.809999', '0.729', '0.655968', '0.295685', '0.405994', '0.384354']\n",
            "search c ['0.00562501', '0.127095', '0.809999', '0.729', '0.6561', '0.590056', '0.000914977', '9.70278e-10']\n",
            "search c ['0.00618481', '0.253126', '0.809999', '0.729', '0.6561', '0.59025', '0.000615077', '7.35335e-10']\n",
            "search c ['0.125187', '0.182377', '6.41893e-05', '9.58575e-10', '5.05524e-13', '6.19078e-09', '0.531407', '0.478297']\n",
            "search c ['0.015212', '0.416872', '0.809985', '0.196801', '5.16028e-06', '1.71805e-05', '0.530849', '0.478297']\n",
            "search c ['0.00164361', '0.0646773', '0.809999', '0.729', '0.656098', '0.169426', '0.000560512', '5.69877e-07']\n",
            "search c ['0.0831106', '0.000130167', '5.74481e-07', '0.027833', '0.6561', '0.59049', '0.531416', '0.000678233']\n",
            "search c ['0.00275305', '0.556391', '0.809947', '0.000728992', '3.86085e-08', '2.10752e-06', '0.531031', '0.478297']\n",
            "search c ['0.073721', '1.87837e-05', '1.05383e-05', '0.722198', '0.6561', '0.59049', '0.00308568', '3.48981e-07']\n",
            "search c ['0.00244039', '0.308636', '0.81', '0.728962', '0.00220055', '8.95121e-05', '0.424601', '0.478297']\n",
            "search c ['0.00264209', '0.136065', '0.81', '0.729', '0.600071', '0.281911', '0.528926', '0.478294']\n",
            "search c ['0.00287056', '0.0303047', '0.809998', '0.729', '0.656099', '0.590427', '0.529166', '0.000227592']\n",
            "search c ['0.00332246', '0.0521704', '0.809996', '0.729', '0.6561', '0.590455', '0.530687', '0.00128067']\n",
            "search c ['0.00215165', '0.103927', '0.81', '0.729', '0.65607', '0.00491944', '8.09e-06', '0.0073772']\n",
            "search c ['0.00218222', '0.0559918', '0.81', '0.729', '0.656075', '0.0058884', '1.08276e-05', '0.00917032']\n",
            "search c ['0.00805458', '0.82448', '0.804882', '0.00533325', '0.19933', '0.59049', '0.531441', '0.332522']\n",
            "search c ['0.00680474', '0.830075', '0.808895', '0.00330786', '0.0241699', '0.590488', '0.531441', '0.136768']\n",
            "search c ['0.00128286', '0.156983', '0.809999', '0.729', '0.599757', '8.71767e-06', '2.02786e-07', '0.059912']\n",
            "search c ['0.0644085', '2.61507e-05', '9.8621e-07', '0.36464', '0.6561', '0.59049', '0.0119026', '1.64925e-06']\n",
            "search c ['0.157331', '0.00304283', '9.52783e-09', '1.42824e-09', '0.0142742', '0.59049', '0.531441', '0.000107797']\n",
            "search c ['0.173904', '0.00010231', '1.78351e-07', '0.000368034', '0.6561', '0.59049', '0.531133', '0.477789']\n",
            "search c ['0.00452869', '0.75695', '0.803741', '0.0410191', '0.617145', '0.59049', '0.531441', '0.391054']\n",
            "search c ['0.00621456', '0.748594', '0.721519', '0.000234946', '0.0237991', '0.59049', '0.531441', '0.290469']\n",
            "search c ['0.00412187', '0.707763', '0.80931', '0.376605', '0.647714', '0.538195', '1.55818e-06', '2.07681e-09']\n",
            "search c ['0.00391894', '0.725913', '0.809982', '0.00105209', '2.78598e-08', '3.36681e-07', '0.529495', '0.478297']\n",
            "search c ['0.228761', '0.000126685', '9.15582e-08', '0.000314636', '0.6561', '0.59049', '0.53143', '0.00183979']\n",
            "search c ['0.203251', '0.000269058', '1.12763e-07', '0.000196506', '0.6561', '0.59049', '0.53143', '0.000399241']\n",
            "search c ['0.00364744', '0.717982', '0.809837', '0.000702591', '8.50445e-06', '0.0947678', '0.531441', '0.478297']\n",
            "search c ['0.0044719', '0.677404', '0.809479', '0.000187027', '7.03206e-07', '0.00275937', '0.531441', '0.478297']\n",
            "search c ['0.105801', '0.000650932', '6.60181e-06', '0.248247', '0.6561', '0.59049', '0.531428', '0.00121098']\n",
            "search c ['0.00604676', '0.451335', '0.798746', '0.61964', '0.656036', '0.59049', '0.531441', '0.422622']\n",
            "search c ['0.0143895', '0.486512', '0.00871727', '4.3498e-06', '0.00585909', '0.59049', '0.531441', '0.478296']\n",
            "search c ['0.00261181', '0.0765872', '0.809999', '0.729', '0.6561', '0.588085', '0.000242834', '3.47839e-06']\n",
            "search c ['0.0752133', '0.000309757', '3.00185e-06', '0.166122', '0.6561', '0.59049', '0.531434', '0.00377002']\n",
            "search c ['0.0846511', '0.000393255', '3.56965e-07', '0.00108679', '0.6561', '0.59049', '0.531437', '0.0225517']\n",
            "search c ['0.00366237', '0.103677', '0.81', '0.729', '0.6561', '0.151294', '6.65605e-07', '5.72007e-12']\n",
            "search c ['0.00362101', '0.106705', '0.81', '0.729', '0.656099', '0.0797395', '1.26198e-07', '6.04134e-12']\n",
            "search c ['0.173299', '8.54641e-05', '6.82365e-07', '0.0513456', '0.6561', '0.59049', '0.531427', '0.0205203']\n",
            "search c ['0.0757626', '0.00466729', '1.61929e-05', '0.0354385', '0.6561', '0.59049', '0.531434', '0.00210528']\n",
            "search c ['0.00198724', '0.0305343', '0.809999', '0.729', '0.6561', '0.590058', '0.5308', '0.338457']\n",
            "search c ['0.0565646', '0.0177754', '8.29275e-07', '4.82726e-11', '7.75559e-09', '0.589494', '0.531441', '0.47816']\n",
            "search c ['0.00542711', '0.67172', '0.795434', '0.0107261', '0.548749', '0.59049', '0.531441', '0.389846']\n",
            "search c ['0.00472164', '0.696652', '0.805056', '0.00296305', '0.0912784', '0.59049', '0.531441', '0.215896']\n",
            "search c ['0.00304269', '0.550315', '0.807305', '0.000187148', '0.000162339', '0.58947', '0.531441', '0.478297']\n",
            "search c ['0.00324801', '0.516618', '0.806699', '7.00536e-05', '1.31695e-05', '0.555815', '0.531441', '0.478297']\n",
            "search c ['0.00230138', '0.0293095', '0.809998', '0.729', '0.6561', '0.588637', '0.0029532', '1.66561e-08']\n",
            "search c ['0.00236108', '0.0329219', '0.809998', '0.729', '0.6561', '0.588728', '0.00313413', '2.91079e-09']\n",
            "search c ['0.102788', '0.00496986', '1.81433e-05', '0.0660752', '0.6561', '0.59049', '0.531422', '0.478236']\n",
            "search c ['0.0140273', '0.770045', '0.210009', '1.53121e-05', '0.000512668', '0.590483', '0.531441', '0.478297']\n",
            "search c ['0.00365618', '0.0339513', '0.809998', '0.729', '0.6561', '0.590279', '0.0265131', '2.65706e-08']\n",
            "search c ['0.00357639', '0.029197', '0.809998', '0.729', '0.6561', '0.581781', '4.70165e-06', '2.25802e-11']\n",
            "search c ['0.00431937', '0.595422', '0.81', '0.722259', '1.89797e-06', '1.04253e-10', '4.40146e-09', '0.474974']\n",
            "search c ['0.00422937', '0.455624', '0.81', '0.728629', '1.27993e-05', '2.51874e-10', '2.08973e-09', '0.445865']\n",
            "search c ['0.0107588', '0.566313', '0.487564', '0.0160566', '0.653542', '0.59049', '0.531441', '0.364505']\n",
            "search c ['0.0295936', '0.139064', '0.00157443', '0.0406587', '0.656099', '0.59049', '0.531441', '0.0828344']\n",
            "search c ['0.126873', '0.000127195', '1.28413e-07', '0.000829754', '0.6561', '0.59049', '0.531394', '0.00354552']\n",
            "search c ['0.00326476', '0.010785', '0.809991', '0.729', '0.6561', '0.589979', '0.531023', '0.362002']\n",
            "search c ['0.0102065', '0.00242271', '0.807176', '0.729', '0.6561', '0.590486', '0.00610867', '1.46179e-08']\n",
            "search c ['0.0100267', '0.00244438', '0.807425', '0.729', '0.6561', '0.590486', '0.0238784', '1.126e-08']\n",
            "search c ['0.0634638', '0.292536', '2.84767e-05', '2.80426e-08', '7.27857e-05', '0.59049', '0.531441', '0.47829']\n",
            "search c ['0.00422487', '0.0824809', '0.81', '0.729', '0.656099', '0.590461', '0.530355', '0.00268557']\n",
            "search c ['0.00938569', '0.796744', '0.808284', '0.191487', '0.117221', '9.69909e-05', '5.77752e-10', '4.39105e-12']\n",
            "search c ['0.00961932', '0.788036', '0.808048', '0.115718', '0.0699217', '5.46504e-06', '2.33212e-08', '0.00749379']\n",
            "search c ['0.00503982', '0.0822793', '0.81', '0.729', '0.656093', '0.287374', '0.00102751', '3.23173e-08']\n",
            "search c ['0.00538836', '0.068937', '0.81', '0.729', '0.656051', '0.0218123', '6.22521e-05', '2.47058e-08']\n",
            "search c ['0.0084657', '0.088431', '0.809999', '0.729', '0.656095', '0.581282', '0.477541', '0.475841']\n",
            "search c ['0.0083414', '0.0584107', '0.809999', '0.729', '0.656083', '0.00680435', '3.06169e-05', '0.00165738']\n",
            "search c ['0.00583077', '0.0640682', '0.809999', '0.729', '0.6561', '0.57803', '5.57352e-06', '1.68e-11']\n",
            "search c ['0.232455', '0.000215005', '6.98246e-07', '0.0224062', '0.6561', '0.59049', '0.343889', '1.02512e-05']\n",
            "search c ['0.00565887', '0.0621729', '0.809999', '0.729', '0.6561', '0.590478', '0.114817', '2.25464e-07']\n",
            "search c ['0.00776213', '0.728685', '0.805975', '0.000504072', '0.00364918', '0.590484', '0.531441', '0.478292']\n",
            "search c ['0.00959263', '0.0101044', '0.809801', '0.729', '0.6561', '0.590485', '0.00914843', '1.18746e-08']\n",
            "search c ['0.00809363', '0.00977184', '0.809925', '0.729', '0.6561', '0.59048', '0.0100344', '3.00593e-09']\n",
            "search c ['0.202625', '0.000116734', '2.35529e-06', '0.257157', '0.6561', '0.59049', '0.531397', '0.00268908']\n",
            "search c ['0.028713', '0.00115139', '0.768312', '0.729', '0.656099', '0.0130294', '0.00178746', '0.210925']\n",
            "search c ['0.00369634', '0.435259', '0.808192', '0.724747', '0.656099', '0.59049', '0.136413', '1.28837e-07']\n",
            "search c ['0.00709873', '0.00786843', '0.809666', '0.729', '0.6561', '0.590485', '0.0053606', '7.25636e-09']\n",
            "search c ['0.00257305', '0.584087', '0.809999', '0.223248', '9.213e-07', '9.55025e-07', '0.525664', '0.478297']\n",
            "search c ['0.00301288', '0.701643', '0.809987', '0.00144311', '1.9572e-08', '2.07489e-07', '0.530966', '0.478297']\n",
            "search c ['0.00821279', '0.25259', '0.809998', '0.729', '0.6561', '0.590482', '0.0224898', '3.21985e-08']\n",
            "search c ['0.00841555', '0.181568', '0.809998', '0.729', '0.6561', '0.590471', '0.0134149', '4.61237e-09']\n",
            "search c ['0.00207914', '0.116253', '0.81', '0.729', '0.656097', '0.590372', '0.530615', '0.0031701']\n",
            "search c ['0.00201639', '0.0485304', '0.81', '0.729', '0.656074', '0.585116', '0.494862', '0.000438333']\n",
            "search c ['0.00262887', '0.0734663', '0.809998', '0.729', '0.65533', '0.270985', '0.326825', '0.478181']\n",
            "search c ['0.00236202', '0.0279109', '0.809999', '0.729', '0.656089', '0.00954639', '7.72196e-06', '0.000525556']\n",
            "search c ['0.00488128', '0.0130066', '0.809977', '0.729', '0.6561', '0.588905', '0.00538007', '4.22093e-07']\n",
            "search c ['0.122628', '8.24155e-05', '2.88671e-06', '0.461769', '0.6561', '0.59049', '0.531384', '0.000527681']\n",
            "search c ['0.00501871', '0.0450061', '0.809998', '0.729', '0.6561', '0.590484', '0.0293136', '1.18109e-06']\n",
            "search c ['0.144502', '3.67355e-05', '1.59365e-05', '0.722636', '0.6561', '0.59049', '0.00261983', '3.00797e-07']\n",
            "search c ['0.00519754', '0.504886', '0.794247', '8.46832e-06', '6.35329e-09', '1.79125e-05', '0.531441', '0.478297']\n",
            "search c ['0.110353', '2.69238e-05', '1.79384e-06', '0.618536', '0.6561', '0.589852', '0.0824843', '1.49351e-06']\n",
            "search c ['0.00431428', '0.29879', '0.809997', '0.729', '0.656099', '0.59048', '0.502768', '0.00327148']\n",
            "search c ['0.0548916', '0.000238949', '0.00939192', '0.728999', '0.6561', '0.590489', '0.531393', '0.418175']\n",
            "search c ['0.00482423', '0.00699465', '0.809932', '0.729', '0.6561', '0.59048', '0.00589526', '9.76589e-09']\n",
            "search c ['0.00519175', '0.00630212', '0.809895', '0.729', '0.6561', '0.590479', '0.00942761', '4.61367e-09']\n",
            "search c ['0.16203', '9.40239e-05', '3.74967e-06', '0.419429', '0.6561', '0.59049', '0.531416', '0.00107101']\n",
            "search c ['0.030257', '0.16004', '7.05716e-05', '1.27193e-05', '0.633263', '0.59049', '0.531441', '0.0757816']\n",
            "search c ['0.00246237', '0.0242227', '0.809999', '0.729', '0.655915', '0.00299317', '9.70671e-05', '2.03762e-05']\n",
            "search c ['0.00970385', '0.295462', '0.00914313', '5.26452e-06', '0.0335353', '0.59049', '0.531441', '0.478212']\n",
            "search c ['0.0113845', '0.00197516', '0.802014', '0.729', '0.6561', '0.590489', '0.086654', '1.02316e-07']\n",
            "search c ['0.00736724', '0.00374308', '0.809471', '0.729', '0.6561', '0.590483', '0.00857876', '1.22393e-08']\n",
            "search c ['0.00207991', '0.0117934', '0.809998', '0.729', '0.656064', '0.558152', '0.422069', '0.00024998']\n",
            "search c ['0.00209696', '0.017805', '0.809998', '0.729', '0.656098', '0.590295', '0.529784', '0.00496439']\n",
            "search c ['0.0020363', '0.060907', '0.81', '0.729', '0.656094', '0.0196268', '2.56349e-06', '2.50546e-06']\n",
            "search c ['0.00203846', '0.0622304', '0.81', '0.729', '0.656089', '0.00811763', '7.00765e-06', '0.000327488']\n",
            "search c ['0.168755', '0.000651707', '3.27122e-07', '0.000802916', '0.6561', '0.59049', '0.531434', '0.00063405']\n",
            "search c ['0.134382', '0.00138318', '9.31288e-07', '0.00276186', '0.6561', '0.59049', '0.531435', '0.00252884']\n",
            "search c ['0.00307265', '0.097311', '0.809998', '0.729', '0.656099', '0.590225', '0.531212', '0.409707']\n",
            "search c ['0.00323723', '0.153595', '0.809996', '0.729', '0.656093', '0.590391', '0.531418', '0.46495']\n",
            "search c ['0.0191169', '0.00143265', '0.774406', '0.729', '0.6561', '0.590488', '0.0247477', '1.8109e-08']\n",
            "search c ['0.0199823', '0.00106066', '0.752711', '0.729', '0.6561', '0.590489', '0.0162629', '2.78377e-08']\n",
            "search c ['0.00488128', '0.311877', '0.81', '0.728917', '5.11004e-05', '1.37292e-09', '9.90948e-09', '0.458217']\n",
            "search c ['0.00357002', '0.675347', '0.809993', '0.0150502', '1.09296e-07', '1.41005e-07', '0.505732', '0.478297']\n",
            "search c ['0.00749836', '0.819039', '0.80923', '0.0128013', '0.352156', '0.59048', '0.51296', '1.5435e-06']\n",
            "search c ['0.00772618', '0.825196', '0.809061', '0.00784762', '0.243392', '0.590474', '0.457522', '1.37019e-06']\n",
            "search c ['0.00588598', '0.720399', '0.809893', '0.000386662', '8.02089e-09', '1.02966e-07', '0.530689', '0.478297']\n",
            "search c ['0.229158', '8.95051e-05', '1.07868e-06', '0.0837301', '0.6561', '0.59049', '0.531408', '0.0128683']\n",
            "search c ['0.00283401', '0.0212343', '0.809998', '0.729', '0.6561', '0.139325', '1.4849e-07', '1.69094e-11']\n",
            "search c ['0.00291329', '0.0216145', '0.809998', '0.729', '0.6561', '0.118583', '1.03506e-07', '6.69905e-13']\n",
            "search c ['0.00266587', '0.505041', '0.809999', '0.253756', '1.01618e-06', '5.58632e-07', '0.492045', '0.478297']\n",
            "search c ['0.147133', '3.48051e-05', '5.5573e-07', '0.102072', '0.6561', '0.59049', '0.0968077', '0.00180393']\n",
            "search c ['0.130415', '0.000381371', '1.01611e-08', '4.15728e-08', '0.6341', '0.59049', '0.531441', '0.26571']\n",
            "search c ['0.103127', '2.58916e-05', '2.35705e-06', '0.580823', '0.6561', '0.59049', '0.524053', '0.477339']\n",
            "search c ['0.118751', '4.85159e-05', '2.49338e-07', '0.0185503', '0.6561', '0.59049', '0.531294', '4.49129e-05']\n",
            "search c ['0.0964392', '0.000107896', '5.64211e-07', '0.0286978', '0.6561', '0.59049', '0.531369', '0.000298069']\n",
            "search c ['0.00230933', '0.579617', '0.809405', '0.00011591', '4.2877e-07', '0.00313646', '0.531441', '0.478297']\n",
            "search c ['0.00201953', '0.561435', '0.809835', '0.000399788', '9.0159e-07', '0.00279234', '0.531441', '0.478297']\n",
            "search c ['0.0503646', '0.00682358', '0.00267487', '0.72861', '0.6561', '0.59049', '0.531436', '0.0112948']\n",
            "search c ['0.0112209', '0.20689', '0.782125', '0.728993', '0.6561', '0.59049', '0.53144', '0.250533']\n",
            "search c ['0.00387144', '0.747654', '0.809202', '0.18668', '0.150828', '0.000241001', '9.95457e-10', '3.40995e-13']\n",
            "search c ['0.003856', '0.755848', '0.809301', '0.0131655', '0.304061', '0.589826', '0.000875003', '2.88002e-08']\n",
            "search c ['0.00839213', '0.81769', '0.809954', '0.000739884', '9.40266e-09', '3.87451e-08', '0.52539', '0.478297']\n",
            "search c ['0.228502', '0.000597151', '2.42395e-05', '0.680361', '0.6561', '0.590477', '0.527025', '0.478156']\n",
            "search c ['0.0229421', '0.619317', '0.00196989', '1.9814e-06', '0.00970708', '0.59049', '0.531441', '0.103752']\n",
            "search c ['0.00406502', '0.05835', '0.809999', '0.729', '0.6561', '0.5904', '0.530532', '0.00166015']\n",
            "search c ['0.00303737', '0.0305791', '0.809999', '0.729', '0.656099', '0.050258', '4.4314e-08', '6.31745e-12']\n",
            "search c ['0.00301667', '0.0281378', '0.809999', '0.729', '0.656099', '0.0298613', '2.23481e-07', '1.21214e-11']\n",
            "search c ['0.00478266', '0.740649', '0.800479', '0.00192257', '0.106354', '0.59049', '0.531441', '0.452679']\n",
            "search c ['0.146538', '0.000292604', '2.90618e-06', '0.257969', '0.6561', '0.578976', '0.511474', '0.478289']\n",
            "search c ['0.0892532', '0.00879879', '1.67272e-07', '7.48835e-07', '0.65269', '0.59049', '0.53144', '0.000288368']\n",
            "search c ['0.00766406', '0.601886', '0.303156', '0.000169644', '0.197645', '0.59049', '0.531441', '0.29462']\n",
            "search c ['0.104456', '0.00212156', '4.85027e-08', '1.37842e-10', '2.04009e-05', '0.59049', '0.531441', '0.476779']\n",
            "search c ['0.00350145', '0.643814', '0.803377', '0.00304134', '0.178494', '0.59049', '0.531441', '0.473431']\n",
            "search c ['0.00177508', '0.0719445', '0.81', '0.729', '0.65607', '0.015179', '8.5568e-05', '0.073669']\n",
            "search c ['0.00182235', '0.0657912', '0.81', '0.729', '0.656091', '0.0118686', '1.07498e-05', '0.0025526']\n",
            "search c ['0.218089', '0.000153862', '3.61084e-07', '0.00610325', '0.6561', '0.59049', '0.531408', '0.000723019']\n",
            "search c ['0.215176', '0.000180247', '3.20357e-07', '0.00416634', '0.6561', '0.59049', '0.531404', '0.000589361']\n",
            "search c ['0.00312612', '0.110725', '0.809999', '0.729', '0.656099', '0.590386', '0.531204', '0.21936']\n",
            "search c ['0.00851043', '0.632371', '0.765538', '3.23238e-06', '4.00092e-08', '0.00354995', '0.531441', '0.478297']\n",
            "search c ['0.00678128', '0.772824', '0.79628', '0.0128571', '0.578747', '0.59049', '0.531441', '0.330351']\n",
            "search c ['0.00508808', '0.80311', '0.808883', '0.0029283', '0.0248643', '0.590489', '0.531441', '0.0821615']\n",
            "search c ['0.00376784', '0.0585539', '0.81', '0.729', '0.656021', '0.00283848', '1.8392e-05', '0.000181791']\n",
            "search c ['0.00393842', '0.0611601', '0.809998', '0.729', '0.6561', '0.585965', '0.00228604', '1.60702e-07']\n",
            "search c ['0.14411', '4.3731e-05', '2.34324e-06', '0.496851', '0.6561', '0.59049', '0.00839432', '7.28243e-07']\n",
            "search c ['0.0510528', '3.50422e-05', '0.000797809', '0.728992', '0.6561', '0.59049', '0.00506702', '1.44274e-07']\n",
            "search c ['0.0732536', '0.000108502', '0.00201638', '0.728996', '0.6561', '0.59049', '0.0283093', '7.79882e-06']\n",
            "search c ['0.2246', '0.0130966', '3.53659e-07', '3.82578e-11', '5.83664e-14', '5.78975e-09', '0.531428', '0.478297']\n",
            "search c ['0.0030893', '0.448069', '0.805048', '0.174045', '0.65185', '0.59049', '0.531441', '0.278223']\n",
            "search c ['0.00308323', '0.485221', '0.802632', '0.0266878', '0.590903', '0.59049', '0.531441', '0.20146']\n",
            "search c ['0.0750561', '6.95326e-05', '0.000279315', '0.728953', '0.6561', '0.59049', '0.00721057', '3.56589e-07']\n",
            "search c ['0.00533799', '0.73825', '0.804485', '0.00186438', '0.0767888', '0.59049', '0.531441', '0.476903']\n",
            "search c ['0.00156395', '0.0212085', '0.809999', '0.729', '0.656097', '0.0256863', '5.49147e-05', '0.0056685']\n",
            "search c ['0.00163857', '0.0204965', '0.809999', '0.729', '0.656092', '0.00856771', '1.98008e-05', '0.0209404']\n",
            "search c ['0.00821999', '0.820821', '0.80802', '0.00575317', '0.093778', '0.59049', '0.531441', '0.396699']\n",
            "search c ['0.00794302', '0.820495', '0.808192', '0.00325835', '0.0333349', '0.590489', '0.531441', '0.203458']\n",
            "search c ['0.00199988', '0.0754424', '0.81', '0.729', '0.655846', '0.0173991', '2.31299e-07', '2.57058e-08']\n",
            "search c ['0.00215267', '0.374149', '0.809999', '0.729', '0.30274', '9.90229e-07', '4.18998e-08', '0.124169']\n",
            "search c ['0.158743', '0.000213137', '6.49954e-07', '0.0139296', '0.6561', '0.59049', '0.531424', '0.000937496']\n",
            "search c ['0.00932311', '0.493472', '0.493574', '0.14797', '0.656011', '0.59049', '0.531441', '0.471288']\n",
            "search c ['0.00340992', '0.0458511', '0.809999', '0.729', '0.656092', '0.014097', '1.43992e-05', '0.000179197']\n",
            "search c ['0.00354891', '0.0412733', '0.809999', '0.729', '0.656084', '0.0066048', '1.35702e-05', '0.0182241']\n",
            "search c ['0.00493813', '0.264165', '0.8006', '0.724248', '0.656097', '0.59049', '0.531441', '0.243293']\n",
            "search c ['0.00387338', '0.442733', '0.801932', '0.100174', '0.646369', '0.59049', '0.531441', '0.238833']\n",
            "search c ['0.00278961', '0.468854', '0.809906', '0.699341', '0.641356', '0.590489', '0.531441', '0.478297']\n",
            "search c ['0.00240205', '0.0352648', '0.809999', '0.729', '0.6561', '0.458779', '6.5975e-06', '3.51186e-07']\n",
            "search c ['0.164196', '0.000296908', '1.69561e-06', '0.0829164', '0.6561', '0.59049', '0.531432', '0.00227399']\n",
            "search c ['0.172852', '0.000501883', '4.51525e-07', '0.00182887', '0.6561', '0.59049', '0.531436', '0.00396077']\n",
            "search c ['0.00285914', '0.0807448', '0.81', '0.729', '0.656075', '0.00573388', '1.00648e-05', '0.00789077']\n",
            "search c ['0.0029355', '0.0512852', '0.81', '0.729', '0.656085', '0.00761474', '1.07667e-05', '0.0083971']\n",
            "search c ['0.0139478', '0.056515', '0.677127', '0.728999', '0.6561', '0.59049', '0.531438', '0.0301039']\n",
            "search c ['0.112702', '0.0011002', '1.20222e-06', '0.00726343', '0.6561', '0.59049', '0.531433', '0.00173127']\n",
            "search c ['0.00546023', '0.429629', '0.809999', '0.728613', '0.638789', '0.0226212', '3.11119e-06', '0.000131871']\n",
            "search c ['0.00661614', '0.727723', '0.807967', '0.00609341', '0.165887', '0.59049', '0.53144', '0.26576']\n",
            "search c ['0.00297163', '0.0225476', '0.809996', '0.729', '0.6561', '0.193073', '7.75321e-07', '1.65211e-09']\n",
            "search c ['0.00356464', '0.485208', '0.804654', '0.00525393', '0.129596', '0.59049', '0.531441', '0.478297']\n",
            "search c ['0.00431273', '0.842784', '0.808791', '0.00298982', '0.0165303', '0.590477', '0.531441', '0.478297']\n",
            "search c ['0.00318835', '0.202681', '0.81', '0.729', '0.656099', '0.590439', '0.53064', '0.00300961']\n",
            "search c ['0.00559873', '0.0633215', '0.809999', '0.729', '0.656088', '0.0112884', '1.39464e-05', '0.00866002']\n",
            "search c ['0.28377', '0.00680233', '1.48745e-07', '1.01614e-11', '2.13834e-12', '0.000269162', '0.531441', '0.478297']\n",
            "search c ['0.160223', '0.0013388', '2.59515e-06', '0.034533', '0.6561', '0.59049', '0.531435', '0.00393364']\n",
            "search c ['0.00665338', '0.764658', '0.807452', '0.133752', '0.634618', '0.59049', '0.531441', '0.387']\n",
            "search c ['0.00276312', '0.200993', '0.809949', '0.729', '0.656098', '0.590487', '0.53144', '0.477779']\n",
            "search c ['0.00321171', '0.548457', '0.809374', '0.00196623', '7.81624e-06', '0.0584868', '0.531441', '0.478297']\n",
            "search c ['0.00374663', '0.0313804', '0.809998', '0.729', '0.6561', '0.587341', '2.89336e-05', '4.99067e-11']\n",
            "search c ['0.158288', '7.37858e-05', '1.00188e-06', '0.0961483', '0.6561', '0.59049', '0.478701', '2.03962e-06']\n",
            "search c ['0.00260443', '0.48677', '0.81', '0.729', '0.00508555', '2.85729e-07', '3.47286e-08', '8.84594e-07']\n",
            "search c ['0.00375957', '0.700348', '0.805514', '0.00103338', '0.0207092', '0.590488', '0.531441', '0.478297']\n",
            "search c ['0.00211186', '0.430233', '0.809605', '0.727652', '0.656098', '0.589983', '0.000217667', '1.2855e-09']\n",
            "search c ['0.0130198', '0.000639372', '0.475869', '0.729', '0.6561', '0.59049', '0.059867', '2.15159e-08']\n",
            "search c ['0.00231789', '0.0196401', '0.809999', '0.729', '0.6561', '0.495592', '3.71068e-05', '1.55776e-10']\n",
            "search c ['0.103674', '0.00798122', '2.41626e-08', '1.26931e-09', '0.0186479', '0.590489', '3.59281e-05', '4.09948e-05']\n",
            "search c ['0.194058', '0.00972515', '3.08827e-07', '2.17676e-11', '7.11915e-13', '1.37499e-05', '0.531441', '0.478297']\n",
            "search c ['0.185693', '0.00177312', '2.31691e-08', '1.37363e-07', '0.646847', '0.59049', '0.531438', '0.000362663']\n",
            "search c ['0.0645419', '0.0153659', '2.27898e-06', '4.97945e-11', '5.04078e-10', '0.468516', '0.531441', '0.478291']\n",
            "search c ['0.00353347', '0.491176', '0.806152', '0.0896769', '0.538009', '0.501844', '8.09589e-06', '6.72903e-09']\n",
            "search c ['0.186726', '6.75663e-05', '2.45643e-06', '0.597296', '0.656099', '0.00781662', '0.143749', '0.477547']\n",
            "search c ['0.00604473', '0.644687', '0.784584', '0.000741297', '0.042287', '0.59049', '0.531441', '0.454198']\n",
            "search c ['0.0621663', '0.000449208', '6.41777e-08', '5.70946e-12', '8.37055e-10', '0.586292', '0.531441', '0.477949']\n",
            "search c ['0.0625449', '0.00199255', '6.61892e-09', '1.87278e-10', '0.000364176', '0.59049', '0.531441', '0.278169']\n",
            "search c ['0.00602168', '0.00413826', '0.809701', '0.729', '0.6561', '0.590486', '0.0108052', '1.40396e-08']\n",
            "search c ['0.00574358', '0.00400868', '0.809663', '0.729', '0.6561', '0.590484', '0.00971865', '1.27269e-08']\n",
            "search c ['0.00261561', '0.0291907', '0.81', '0.729', '0.655829', '0.00452039', '3.07467e-05', '2.29401e-07']\n",
            "search c ['0.144625', '0.000191525', '6.28589e-08', '8.34262e-05', '0.656099', '0.59049', '0.531436', '0.448119']\n",
            "search c ['0.00825098', '0.00552084', '0.809682', '0.729', '0.6561', '0.590486', '0.0128228', '1.41499e-08']\n",
            "search c ['0.0066278', '0.00893353', '0.80994', '0.729', '0.6561', '0.590481', '0.00467473', '6.46374e-09']\n",
            "search c ['0.0180916', '0.467577', '0.649269', '9.82819e-06', '2.60999e-11', '2.48833e-12', '6.5547e-05', '0.478297']\n",
            "search c ['0.163568', '0.00909293', '2.38906e-06', '2.86506e-10', '1.114e-13', '1.04754e-09', '0.531304', '0.478297']\n",
            "search c ['0.00566659', '0.232909', '0.808551', '0.728999', '0.6561', '0.59049', '0.53144', '0.135123']\n",
            "search c ['0.0048698', '0.588612', '0.795147', '0.0172241', '0.591309', '0.59049', '0.531441', '0.358258']\n",
            "search c ['0.00390441', '0.714146', '0.809317', '0.00125177', '0.00194488', '0.59033', '0.531441', '0.478297']\n",
            "search c ['0.00653704', '0.113132', '0.808623', '0.728957', '0.656099', '0.59049', '0.531441', '0.477889']\n",
            "search c ['0.00922011', '0.230678', '0.809985', '0.729', '0.6561', '0.590487', '0.0488498', '1.13417e-07']\n",
            "search c ['0.00612012', '0.0687708', '0.809999', '0.729', '0.6561', '0.227442', '2.34578e-07', '3.28551e-10']\n",
            "search c ['0.131904', '0.00560175', '1.9048e-08', '4.98013e-10', '0.000624251', '0.59049', '0.531441', '0.477126']\n",
            "search c ['0.131398', '0.000119743', '1.77402e-06', '0.460956', '0.6561', '0.589401', '0.000195754', '2.6104e-09']\n",
            "search c ['0.311823', '0.0136314', '4.21083e-08', '5.3624e-09', '5.8065e-05', '0.590484', '0.531441', '0.478293']\n",
            "search c ['0.298364', '0.000302121', '2.38812e-06', '0.0854164', '0.6561', '0.59049', '0.531402', '0.477589']\n",
            "search c ['0.00508109', '0.0921091', '0.81', '0.729', '0.6561', '0.534858', '4.20621e-06', '2.31696e-12']\n",
            "search c ['0.00522775', '0.089444', '0.81', '0.729', '0.6561', '0.432202', '5.26629e-07', '1.36923e-12']\n",
            "search c ['0.00353087', '0.624885', '0.809908', '0.000535824', '7.15042e-09', '4.59409e-08', '0.527342', '0.478297']\n",
            "search c ['0.120353', '0.00547401', '9.08414e-07', '1.37269e-10', '8.64124e-14', '1.03959e-09', '0.53132', '0.478297']\n",
            "search c ['0.0452289', '0.0119016', '0.000207723', '0.590912', '0.6561', '0.59049', '0.53144', '0.0309717']\n",
            "search c ['0.00453572', '0.744278', '0.805887', '0.00157908', '0.0472641', '0.59049', '0.531441', '0.272152']\n",
            "search c ['0.00367472', '0.676883', '0.809985', '0.00128838', '2.94362e-08', '5.96601e-07', '0.531079', '0.478297']\n",
            "search c ['0.00429784', '0.673895', '0.808669', '0.000141118', '4.82707e-05', '0.584553', '0.531441', '0.478297']\n",
            "search c ['0.111882', '0.00878185', '1.72331e-08', '2.24166e-10', '0.000185364', '0.59049', '0.531441', '8.35844e-05']\n",
            "search c ['0.00330365', '0.0288865', '0.809992', '0.729', '0.6561', '0.587809', '0.115689', '0.131296']\n",
            "search c ['0.00300278', '0.754953', '0.808237', '0.00264759', '0.0669709', '0.59049', '0.531441', '0.166953']\n",
            "search c ['0.0047198', '0.715003', '0.720251', '0.00290428', '0.561933', '0.59049', '0.531441', '0.242626']\n",
            "search c ['0.00753193', '0.626058', '0.801768', '1.51427e-05', '6.30037e-07', '0.181551', '0.531441', '0.478297']\n",
            "search c ['0.171422', '0.00303362', '5.78251e-08', '4.38099e-12', '1.70502e-09', '0.589375', '0.531441', '0.477243']\n",
            "search c ['0.195293', '0.000558079', '5.70775e-07', '0.00316341', '0.6561', '0.59049', '0.531428', '0.00081507']\n",
            "search c ['0.00940635', '0.324994', '0.806816', '0.728994', '0.6561', '0.59049', '0.53144', '0.196096']\n",
            "search c ['0.0169602', '0.43265', '0.475256', '0.581535', '0.6561', '0.59049', '0.500979', '0.000136689']\n",
            "search c ['0.264488', '0.000126632', '7.59117e-07', '0.0505769', '0.6561', '0.59049', '0.0114387', '2.0837e-06']\n",
            "search c ['0.134514', '0.00317138', '3.95424e-08', '5.98401e-11', '2.28955e-06', '0.59049', '0.531441', '0.473627']\n",
            "search c ['0.131143', '0.000121412', '4.59164e-07', '0.0057066', '0.6561', '0.59049', '0.53143', '0.00152384']\n",
            "search c ['0.00989599', '0.780387', '0.809331', '0.0908654', '3.12408e-07', '3.75801e-09', '5.58255e-05', '0.478297']\n",
            "search c ['0.0105419', '0.782495', '0.802673', '0.00185831', '0.0379909', '0.590489', '0.531441', '0.478295']\n",
            "search c ['0.00494486', '0.0358365', '0.809998', '0.729', '0.6561', '0.585669', '1.8694e-05', '2.55147e-11']\n",
            "search c ['0.00518064', '0.0329892', '0.809998', '0.729', '0.6561', '0.582615', '6.75675e-06', '3.86291e-11']\n",
            "search c ['0.297119', '0.000147382', '1.64184e-06', '0.247388', '0.6561', '0.59049', '0.525831', '0.477898']\n",
            "search c ['0.302853', '0.000156872', '5.17915e-07', '0.0125443', '0.6561', '0.59049', '0.037101', '5.22527e-06']\n",
            "search c ['0.00179246', '0.0258897', '0.81', '0.729', '0.652992', '0.000105481', '1.68296e-06', '0.0639003']\n",
            "search c ['0.00171726', '0.033763', '0.81', '0.729', '0.653683', '0.000137274', '1.25601e-06', '0.0127931']\n",
            "search c ['0.0200478', '0.293883', '0.00080558', '0.000143309', '0.651215', '0.59049', '0.531441', '0.212809']\n",
            "search c ['0.00472592', '0.0124985', '0.809982', '0.729', '0.6561', '0.587972', '0.000155878', '3.70117e-06']\n",
            "search c ['0.05488', '0.00339626', '2.46728e-05', '0.467981', '0.6561', '0.59049', '0.531437', '0.00541753']\n",
            "search c ['0.0315238', '0.0458804', '3.47898e-05', '0.00213387', '0.656099', '0.59049', '0.531441', '0.042304']\n",
            "search c ['0.00345157', '0.635587', '0.80962', '0.0376843', '3.6443e-08', '2.49704e-10', '3.23915e-05', '0.478297']\n",
            "search c ['0.0577007', '0.0538574', '1.2715e-05', '1.94153e-10', '5.05911e-12', '2.83197e-05', '0.531441', '0.478297']\n",
            "search c ['0.155796', '0.000165534', '6.90966e-07', '0.0275024', '0.6561', '0.59049', '0.531394', '0.000312436']\n",
            "search c ['0.158747', '0.000176739', '3.42303e-07', '0.00381607', '0.6561', '0.59049', '0.531425', '0.00122301']\n",
            "search c ['0.0144924', '0.554971', '0.212221', '1.38194e-07', '2.78503e-09', '0.000662525', '0.531441', '0.478297']\n",
            "search c ['0.00377966', '0.2008', '0.809992', '0.729', '0.656095', '0.590469', '0.531435', '0.470211']\n",
            "search c ['0.00334137', '0.117472', '0.809995', '0.729', '0.6561', '0.590478', '0.0314781', '1.05739e-08']\n",
            "search c ['0.00388559', '0.0267055', '0.809995', '0.729', '0.6561', '0.590473', '0.00431232', '5.17799e-09']\n",
            "search c ['0.00386159', '0.0442059', '0.809999', '0.729', '0.6561', '0.262626', '6.11012e-07', '1.14072e-08']\n",
            "search c ['0.0545431', '0.200675', '0.000786661', '1.22365e-08', '3.13346e-13', '1.27148e-11', '0.348647', '0.478297']\n",
            "search c ['0.100366', '0.000764858', '4.17896e-06', '0.133155', '0.6561', '0.59049', '0.531434', '0.0018047']\n",
            "search c ['0.054919', '0.00836964', '1.44716e-05', '0.0163225', '0.6561', '0.59049', '0.53144', '0.00479789']\n",
            "search c ['0.0775787', '0.000786808', '3.38845e-09', '2.83574e-10', '0.00312985', '0.59049', '0.531441', '0.0547354']\n",
            "search c ['0.0745013', '0.000430089', '3.26301e-08', '3.49871e-12', '1.761e-09', '0.589669', '0.531441', '0.476385']\n",
            "search c ['0.154059', '0.000932459', '1.69304e-06', '0.0190863', '0.6561', '0.59049', '0.531432', '0.000775131']\n",
            "search c ['0.184906', '0.00071443', '2.72361e-07', '0.000489502', '0.6561', '0.59049', '0.531432', '0.00125379']\n",
            "search c ['0.00284575', '0.744069', '0.809494', '0.184869', '0.045478', '0.00140289', '0.135207', '0.478297']\n",
            "search c ['0.00289157', '0.731546', '0.809574', '0.260908', '0.0594557', '5.90246e-06', '2.50349e-07', '0.207904']\n",
            "search c ['0.00916507', '0.571002', '0.0530136', '0.00109087', '0.639211', '0.59049', '0.531441', '0.416566']\n",
            "search c ['0.0346652', '0.0387777', '3.93189e-05', '0.00312856', '0.656099', '0.59049', '0.531441', '0.0580789']\n",
            "search c ['0.00420291', '0.234462', '0.81', '0.729', '0.656047', '0.0703934', '4.97277e-07', '1.64306e-08']\n",
            "search c ['0.00577018', '0.818129', '0.809187', '0.000614009', '0.000740606', '0.590057', '0.531441', '0.478297']\n",
            "search c ['0.169463', '8.93274e-05', '3.20137e-07', '0.0116136', '0.6561', '0.59049', '0.531398', '0.000322083']\n",
            "search c ['0.03742', '0.0067641', '0.00136756', '0.726695', '0.6561', '0.59049', '0.531437', '0.0280287']\n",
            "search c ['0.0025265', '0.653016', '0.809827', '0.000231035', '6.23808e-07', '0.00367768', '0.531441', '0.478297']\n",
            "search c ['0.00236801', '0.655108', '0.809895', '0.000385279', '1.11871e-06', '0.00570459', '0.531441', '0.478297']\n",
            "search c ['0.175097', '0.000106912', '4.3979e-07', '0.0155347', '0.6561', '0.59049', '0.531417', '0.00100033']\n",
            "search c ['0.198673', '5.63428e-05', '1.87044e-07', '0.00404397', '0.6561', '0.59049', '0.53139', '0.000137289']\n",
            "search c ['0.06842', '0.0564913', '6.31134e-06', '2.52045e-10', '1.18588e-08', '0.587367', '0.531441', '0.478285']\n",
            "search c ['0.163159', '0.00383825', '6.84761e-08', '7.71139e-12', '3.61309e-09', '0.58977', '0.531441', '0.477967']\n",
            "search c ['0.139862', '0.0185977', '3.81085e-07', '3.68834e-07', '0.607343', '0.59049', '0.531441', '0.00338161']\n",
            "search c ['0.26688', '0.000116592', '2.22979e-07', '0.00206549', '0.6561', '0.59049', '0.531414', '0.000322011']\n",
            "search c ['0.00374749', '0.753264', '0.809521', '0.0386688', '0.223375', '0.590488', '0.531441', '0.478297']\n",
            "search c ['0.0052772', '0.747556', '0.809503', '7.88827e-05', '2.92568e-07', '0.00258951', '0.531441', '0.478297']\n",
            "search c ['0.0217607', '0.0259586', '0.0303554', '0.728907', '0.6561', '0.59049', '0.53144', '0.0517918']\n",
            "search c ['0.00342881', '0.706575', '0.808322', '0.00411981', '0.0629947', '0.59049', '0.531441', '0.0915919']\n",
            "search c ['0.00188717', '0.0187057', '0.809998', '0.729', '0.656071', '0.00487007', '0.000146006', '0.000204111']\n",
            "search c ['0.00202711', '0.0247708', '0.809998', '0.729', '0.655991', '0.00134511', '4.38459e-06', '0.0250198']\n",
            "search c ['0.00754422', '0.729594', '0.796126', '0.586605', '0.656044', '0.59049', '0.531441', '0.450043']\n",
            "search c ['0.00473351', '0.830645', '0.809184', '0.00378433', '0.0257384', '0.590488', '0.531441', '0.219586']\n",
            "search c ['0.00302185', '0.133204', '0.809997', '0.729', '0.65609', '0.589827', '0.53044', '0.478169']\n",
            "search c ['0.00293356', '0.375675', '0.809998', '0.707272', '0.130231', '0.584736', '0.294107', '0.000188572']\n",
            "search c ['0.0216453', '0.00537811', '0.0482614', '0.728997', '0.6561', '0.59049', '0.294198', '8.78874e-06']\n",
            "search c ['0.00360799', '0.498274', '0.809636', '0.24195', '1.51375e-07', '8.23854e-11', '7.305e-07', '0.478296']\n",
            "search c ['0.0626408', '0.00251981', '7.13722e-06', '0.0827652', '0.6561', '0.59049', '0.531438', '0.0116974']\n",
            "search c ['0.00499117', '0.559424', '0.77718', '0.311123', '0.655903', '0.59049', '0.531441', '0.327177']\n",
            "search c ['0.00316349', '0.0305804', '0.809999', '0.729', '0.6561', '0.215387', '6.66176e-07', '2.93174e-08']\n",
            "search c ['0.0216867', '0.00287579', '0.107996', '0.728891', '0.6561', '0.59049', '0.531426', '0.478285']\n",
            "search c ['0.00570643', '0.821451', '0.807557', '0.00312179', '0.0627838', '0.59049', '0.531441', '0.270905']\n",
            "search c ['0.00650066', '0.786869', '0.802468', '0.00602421', '0.30966', '0.59049', '0.531441', '0.262434']\n",
            "search c ['0.146369', '9.14936e-05', '4.1144e-07', '0.0213086', '0.6561', '0.59049', '0.0229546', '4.12363e-06']\n",
            "search c ['0.0525768', '4.46406e-05', '0.000269288', '0.728961', '0.6561', '0.59049', '0.00297692', '2.08714e-07']\n",
            "search c ['0.0234728', '0.395164', '0.000389752', '1.20525e-06', '0.104961', '0.59049', '0.0333285', '0.000113027']\n",
            "search c ['0.136639', '0.000111495', '2.59997e-06', '0.242107', '0.6561', '0.59049', '0.531399', '0.445333']\n",
            "search c ['0.0049176', '0.0857785', '0.809998', '0.729', '0.6561', '0.590479', '0.0143385', '5.63155e-09']\n",
            "search c ['0.00498369', '0.0494979', '0.809998', '0.729', '0.6561', '0.590474', '0.00818034', '1.41712e-08']\n",
            "search c ['0.00395063', '0.56084', '0.809999', '0.328101', '7.66425e-06', '1.31296e-05', '0.529207', '0.478297']\n",
            "search c ['0.0051921', '0.727034', '0.809905', '0.000376264', '4.09606e-09', '2.25386e-08', '0.526066', '0.478297']\n",
            "search c ['0.0192957', '0.00118811', '0.76625', '0.729', '0.6561', '0.590489', '0.045275', '4.70182e-08']\n",
            "search c ['0.0130993', '0.00194529', '0.805812', '0.729', '0.6561', '0.590487', '0.0105115', '6.79027e-09']\n",
            "search c ['0.130211', '3.2044e-05', '1.11737e-06', '0.188755', '0.6561', '0.590486', '0.510639', '0.47813']\n",
            "search c ['0.0023269', '0.0540033', '0.81', '0.729', '0.0038273', '2.0956e-08', '2.26886e-08', '0.409597']\n",
            "search c ['0.00644102', '0.750238', '0.80272', '0.0120762', '0.427177', '0.59049', '0.531441', '0.413601']\n",
            "search c ['0.00610386', '0.756488', '0.805995', '0.0190359', '0.458773', '0.59049', '0.531441', '0.299314']\n",
            "search c ['0.00399669', '0.589868', '0.808957', '0.000723786', '0.000224465', '0.587783', '0.531441', '0.478297']\n",
            "search c ['0.00272732', '0.0366075', '0.809999', '0.729', '0.655876', '0.00245049', '7.86308e-05', '1.71871e-05']\n",
            "search c ['0.00548539', '0.640872', '0.80421', '0.0803438', '0.652484', '0.589981', '0.00158118', '3.58506e-07']\n",
            "search c ['0.00535538', '0.633733', '0.803902', '0.00375255', '0.501487', '0.590488', '0.103832', '4.40889e-07']\n",
            "search c ['0.13352', '0.000159164', '0.00174468', '0.728995', '0.656099', '0.0103105', '0.00717506', '0.0124353']\n",
            "search c ['0.0191628', '0.0018615', '0.804631', '0.729', '0.6561', '0.59038', '0.529018', '0.476895']\n",
            "search c ['0.00492241', '0.0697253', '0.809999', '0.729', '0.6561', '0.590463', '0.530742', '0.00196413']\n",
            "search c ['0.0347972', '0.602856', '0.000669151', '4.76152e-07', '0.00127228', '0.59049', '0.531441', '0.220077']\n",
            "search c ['0.194793', '0.00169702', '3.91792e-08', '3.83631e-12', '2.29563e-09', '0.589923', '0.531441', '0.47718']\n",
            "search c ['0.185505', '0.0018946', '5.39717e-08', '3.20393e-12', '6.78206e-10', '0.587051', '0.531441', '0.477643']\n",
            "search c ['0.2488', '0.000119346', '2.31155e-07', '0.00308139', '0.6561', '0.59049', '0.53142', '0.000451804']\n",
            "search c ['0.236461', '0.000151005', '2.9089e-07', '0.00399433', '0.6561', '0.59049', '0.531406', '0.00036085']\n",
            "search c ['0.0112296', '0.597537', '0.431814', '1.5458e-06', '6.18224e-07', '0.485658', '0.531441', '0.478297']\n",
            "search c ['0.0602958', '0.0748995', '9.29576e-06', '2.51988e-10', '4.79213e-09', '0.572292', '0.531441', '0.478289']\n",
            "dided\n",
            "time\n",
            "[5, 11, 5, 9, 9, 9, 8, 9, 11, 1, 5, 5, 5, 4, 14, 3, 9, 2, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 9, 9, 2, 11, 9, 5, 11, 1, 2, 8, 11, 9, 2, 5, 11, 5, 5, 5, 13, 9, 1, 5, 5, 13, 6, 0, 5, 11, 8, 5, 5, 13, 6, 10, 5, 5, 8, 4, 14, 4, 14, 14, 14, 5, 11, 1, 1, 9, 5, 5, 1, 5, 9, 5, 6, 5, 1, 5, 9, 11, 11, 11, 9, 5, 11, 5, 5, 11, 5, 1, 9, 11, 5, 5, 9, 11, 9, 9, 6, 13, 8, 11, 8, 5, 5, 5, 9, 5, 5, 1, 12, 11, 4, 5, 3, 6, 14, 14, 14, 11, 5, 5, 9, 9, 5, 8, 8, 5, 5, 5, 8, 5, 8, 9, 3, 9, 11, 8, 4, 9, 8, 5, 9, 5, 6, 13, 11, 3, 13, 2, 12, 5, 5, 5, 11, 5, 5, 8, 5, 6, 13, 5, 8, 8, 4, 1, 11, 5, 9, 9, 13, 5, 11, 5, 8, 5, 5, 5, 5, 13, 3, 8, 4, 11, 5, 5, 9, 6, 13, 9, 2, 5, 1, 5, 9, 9, 14, 9, 2, 9, 1, 5, 11, 9, 4, 5, 9, 11, 11, 1, 5, 4, 4, 1, 11, 11, 5, 2, 1, 5, 6, 2, 14, 11, 5, 1, 8, 11, 5, 10, 5, 6, 11, 4, 0, 5, 4, 1, 1, 5, 11, 5, 11, 5, 5, 5, 11, 1, 5, 5, 5, 1, 11, 11, 6, 11, 5, 8, 4, 11, 1, 1, 1, 9, 1, 11, 9, 5, 5, 9, 13, 9, 11, 1, 6, 5, 11, 9, 2, 9, 4, 8, 4, 11, 5, 8, 4, 9, 8, 1, 5, 5, 2, 5, 9, 5, 11, 9, 2, 9, 11, 5, 6, 5, 1, 5, 9, 1, 5, 5, 9, 5, 11, 5, 2, 9, 5, 1, 5, 1, 9, 9, 12, 5, 9, 11, 13, 5, 11, 5, 8, 9, 11, 7, 11, 4, 8, 5, 8, 9, 9, 12, 11, 1, 8, 5, 9, 6, 6, 10, 4, 11, 4, 12, 11, 11, 1, 5, 9, 4, 9, 9, 13, 5, 4, 1, 1, 4, 11, 11, 4, 5, 2, 9, 9, 9, 1, 1, 1, 9, 11, 2, 11, 5, 5, 1, 9, 3, 1, 5, 8, 4, 5, 5, 8, 5, 14, 2, 2, 11, 11, 5, 8, 8, 1, 5, 5, 5, 11, 12, 4, 5, 4, 8, 5, 5, 5, 5, 8, 11, 1, 1, 8, 5, 9, 5, 6, 9, 2, 12, 8, 9, 5, 5, 1, 9, 12, 11, 4, 9, 5, 8, 1, 9, 5, 8, 9, 5, 5, 5, 9, 11, 4, 1, 8, 5, 5, 5, 9, 5, 10, 5, 1, 9, 4, 12, 8, 5, 5, 11, 5, 5, 2, 1, 1, 9, 1, 5, 5, 11, 9, 2, 2, 1, 5, 1, 9, 5, 12, 1, 5, 13, 2, 12, 8, 5, 5, 8, 4, 11, 5, 9, 2, 5, 11, 1, 5, 5, 9, 5, 5, 4, 5, 5, 4, 11, 5, 1, 9, 11, 11, 9, 9, 1, 5, 9, 9, 5, 14, 9, 5, 1, 5, 1, 9, 5, 1, 1, 12, 5, 5, 1, 3, 1, 1, 9, 13, 9, 2, 5, 5, 5, 5, 5, 5, 5, 5, 9, 2, 5, 9, 1, 11, 1, 5, 5, 11, 1, 11, 1, 1, 5, 1, 5, 9, 9, 5, 5, 11, 9, 11, 5, 11, 11, 5, 5, 9, 1, 5, 5, 9, 9, 0, 5, 1, 5, 9, 9, 6, 5, 1, 1, 14, 1, 5, 9, 1, 5, 9, 1, 5, 5, 2, 5, 11, 11, 5, 5, 5, 11, 1, 5, 5, 11, 11, 1, 11, 5, 1, 1, 11, 13, 6, 7, 8, 11, 1, 5, 11, 5, 11, 9, 2, 5, 4, 8, 5, 1, 5, 1, 1, 11, 4, 8, 11, 5, 1, 1, 5, 5, 9, 9, 2, 13, 14, 9, 11, 5, 4, 11, 12, 11, 2, 5, 5, 9, 4, 8, 11, 9, 9, 11, 6, 5, 5, 5, 1, 5, 5, 9, 9, 9, 4, 5, 11, 11, 14, 7, 14, 5, 11, 1, 7, 9, 5, 8, 4, 1, 4, 1, 5, 5, 11, 8, 4, 3, 6, 4, 4, 5, 5, 1, 5, 13, 9, 6, 2, 11, 5, 5, 5, 9, 1, 8, 5, 5, 4, 9, 13, 1, 1, 1, 9, 5, 5, 5, 5, 5, 5, 5, 1, 9, 1, 5, 9, 5, 6, 5, 5, 5, 8, 5, 9, 5, 5, 5, 4, 9, 11, 5, 9, 6, 6, 10, 10, 11, 1, 11, 9, 5, 1, 1, 9, 9, 9, 1, 1, 13, 6, 5, 5, 1, 5, 5, 9, 5, 5, 5, 5, 9, 9, 5, 13, 5, 8, 8, 8, 5, 5, 5, 5, 5, 5, 5, 9, 5, 11, 8, 4, 9, 9, 2, 12, 5, 3, 5, 4, 6, 4, 1, 5, 4, 8, 11, 4, 4, 4, 8, 8, 9, 1, 5, 9, 1, 5, 5, 13, 9, 1, 11, 9, 5, 4, 9, 9, 11, 4, 0, 14, 4, 4, 11, 8, 11, 5, 8, 4, 5, 4, 1, 5, 4, 11, 7, 11, 5, 5, 5, 5, 1, 11, 5, 9, 5, 5, 1, 1, 9, 4, 6, 11, 9, 1, 5, 5, 9, 13, 9, 7, 9, 11, 11, 11, 5, 5, 5, 9, 5, 5, 5, 2, 9, 11, 8, 11, 11, 1, 1, 9, 1, 3, 13, 2, 5, 5, 5, 8, 9, 13, 2, 2, 4, 5, 11, 4, 9, 9, 9, 6, 5, 11, 1, 1, 9, 9, 9, 10, 5, 5, 5, 8, 13, 4, 8, 5, 4, 8, 11, 0, 9, 4, 4, 5, 9, 9, 8, 1, 13, 4, 12, 8, 9, 5, 4, 5, 13, 3, 5, 8, 1, 5, 1, 9, 5, 5, 5, 8, 1, 1, 5, 9, 4, 4, 5, 9, 11, 11, 1, 1, 5, 11, 5, 10, 1, 5, 5, 1, 9, 9, 1, 1, 5, 5, 8, 4, 3, 4, 8, 11, 11, 5, 8, 4, 9, 5, 9, 12, 5, 11, 1, 5, 5, 5, 8, 2, 6, 6, 5, 8, 9, 4, 8, 14, 9, 5, 8, 5, 11, 5, 1, 5, 5, 2, 5, 5, 5, 9, 1, 5, 11, 9, 5, 9, 11, 1, 1, 11, 5, 11, 9, 13, 9, 9, 1, 9, 5, 4, 9, 2, 11, 5, 5, 5, 5, 11, 9, 2, 5, 5, 1, 8, 9, 11, 4, 13, 5, 5, 8, 4, 11, 4, 8, 5, 1, 1, 1, 5, 9, 13, 13, 10, 5, 1, 5, 8, 5, 6, 11, 0, 1, 2, 5, 9, 5, 4, 3, 2, 5, 1, 5, 9, 5, 9, 5, 5, 9, 5, 1, 9, 6, 13, 9, 0, 1, 5, 1, 9, 11, 2, 8, 8, 5, 4, 5, 5, 5, 11, 11, 8, 5, 9, 11, 8, 9, 5, 8, 4, 5, 11, 1, 9, 3, 6, 1, 1, 5, 5, 5, 14, 5, 4, 2, 11, 5, 9, 5, 5, 5, 11, 1, 5, 5, 9, 9, 13, 11, 9, 8, 8, 5, 1, 5, 9, 8, 5, 9, 2, 5, 5, 8, 4, 9, 4, 8, 5, 1, 1, 1, 9, 5, 5, 5, 9, 11, 4, 8, 9, 5, 4, 0, 7, 9, 1, 1, 5, 11, 9, 1, 1, 9, 4, 8, 14, 4, 5, 5, 4, 9, 4, 8, 11, 8, 5, 5, 4, 9, 4, 6, 14, 9, 5, 1, 5, 5, 4, 1, 5, 5, 8, 5, 14, 4, 13, 7, 8, 5, 8, 8, 1, 9, 4, 5, 5, 5, 5, 8, 4, 11, 3, 5, 5, 9, 5, 9, 9, 5, 5, 9, 1, 8, 5, 5, 11, 5, 9, 5, 2, 5, 5, 1, 9, 5, 5, 5, 5, 5, 11, 5, 13, 4, 5, 5, 4, 5, 1, 5, 5, 5, 5, 4, 4, 11, 8, 11, 8, 4, 2, 14, 14, 5, 11, 1, 5, 5, 5, 9, 2, 5, 5, 9, 5, 11, 11, 6, 11, 5, 11, 1, 1, 5, 5, 5, 5, 1, 8, 1, 5, 9, 14, 10, 5, 9, 4, 8, 5, 5, 5, 5, 9, 11, 1, 5, 5, 1, 5, 11, 9, 5, 11, 13, 2, 1, 1, 5, 9, 11, 11, 5, 9, 5, 5, 1, 1, 11, 9, 2, 0, 5, 5, 5, 5, 5, 5, 1, 5, 9, 9, 1, 12, 5, 5, 5, 13, 5, 4, 14, 14, 5, 8, 8, 4, 5, 11, 5, 11, 11, 4, 8, 0, 5, 5, 5, 9, 5, 11, 13, 2, 5, 5, 5, 8, 5, 9, 5, 9, 5, 8, 1, 5, 1, 1, 5, 8, 11, 2, 8, 9, 5, 11, 8, 4, 9, 5, 8, 1, 9, 6, 12, 8, 9, 1, 5, 9, 5, 11, 2, 5, 5, 5, 5, 13, 11, 6, 4, 4, 11, 5, 1, 5, 1, 1, 5, 1, 5, 9, 2, 12, 11, 5, 5, 8, 11, 11, 5, 9, 5, 5, 5, 9, 5, 11, 5, 5, 4, 11, 4, 2, 5, 8, 5, 8, 8, 4, 5, 5, 9, 9, 9, 9, 5, 9, 5, 13, 5, 5, 1, 11, 9, 4, 1, 1, 9, 11, 5, 3, 3, 4, 4, 11, 5, 8, 8, 7, 9, 9, 1, 13, 5, 1, 1, 5, 5, 1, 1, 5, 5, 11, 5, 9, 11, 4, 0, 14, 5, 1, 1, 1, 9, 9, 12, 4, 9, 5, 11, 4, 5, 1, 1, 9, 9, 6, 8, 11, 11, 11, 5, 13, 5, 5, 8, 4, 4, 14, 4, 11, 5, 5, 9, 5, 5, 4, 13, 2, 1, 11, 9, 8, 5, 11, 13, 2, 5, 5, 5, 8, 9, 5, 5, 9, 4, 8, 11, 11, 5, 5, 5, 11, 5, 4, 8, 11, 9, 9, 1, 12, 5, 5, 5, 11, 5, 5, 5, 5, 9, 4, 8, 5, 5, 11, 5, 5, 9, 2, 1, 5, 9, 11, 1, 1, 8, 5, 5, 1, 8, 5, 5, 13, 9, 5, 5, 9, 11, 5, 1, 5, 3, 9, 0, 11, 5, 1, 8, 9, 5, 11, 13, 1, 5, 5, 5, 9, 4, 11, 5, 8, 9, 5, 1, 5, 5, 9, 5, 6, 6, 6, 6, 4, 5, 9, 5, 4, 4, 5, 9, 8, 5, 5, 5, 9, 6, 2, 7, 9, 5, 5, 5, 1, 5, 9, 1, 9, 5, 5, 11, 13, 5, 9, 11, 14, 4, 0, 7, 11, 9, 1, 1, 12, 5, 4, 5, 6, 9, 5, 12, 8, 9, 4, 11, 4, 1, 9, 5, 13, 11, 5, 5, 5, 11, 4, 8, 4, 5, 11, 12, 5, 11, 8, 4, 14, 5, 4, 7, 8, 1, 5, 5, 9, 5, 11, 5, 4, 14, 11, 8, 4, 5, 9, 5, 9, 5, 11, 5, 13, 4, 4, 0, 14, 5, 4, 8, 11, 5, 5, 5, 11, 5, 3, 13, 13, 11, 5, 5, 8, 5, 5, 5, 9, 9, 5, 5, 8, 9, 1, 1, 12, 5, 5, 11, 5, 5, 4, 9, 9, 5, 4, 11, 1, 5, 9, 4, 4, 5, 5, 1, 5, 9, 6, 4, 8, 11, 11, 9, 1, 9, 5, 2, 1, 1, 5, 1, 6, 5, 4, 7, 14, 5, 1, 5, 13, 5, 5, 1, 5, 5, 13, 1, 8, 11, 1, 5, 1, 9, 11, 1, 5, 5, 5, 2, 11, 13, 13, 4, 8, 5, 5, 1, 11, 5, 11, 9, 13, 5, 8, 5, 5, 9, 11, 1, 5, 5, 5, 1, 8, 5, 1, 8, 4, 11, 5, 5, 5, 5, 11, 8, 8, 5, 11, 9, 1, 5, 5, 5, 14, 11, 5, 5, 8, 5, 11, 9, 2, 5, 5, 5, 5, 5, 1, 5, 5, 9, 11, 8, 7, 6, 7, 7, 0, 5, 4, 5, 1, 5, 5, 8, 13, 9, 4, 4, 8, 11, 1, 1, 9, 11, 5, 6, 8, 1, 9, 5, 9, 5, 11, 11, 5, 5, 5, 8, 8, 8, 5, 9, 6, 5, 4, 0, 11, 1, 5, 5, 5, 5, 2, 12, 9, 8, 8, 8, 8, 5, 8, 5, 5, 5, 5, 1, 2, 5, 5, 5, 8, 5, 1, 5, 5, 5, 1, 5, 9, 14, 6, 10, 4, 4, 9, 11, 4, 5, 8, 8, 5, 5, 5, 5, 5, 5, 11, 9, 13, 11, 5, 8, 4, 11, 5, 5, 5, 1, 1, 8, 5, 9, 5, 9, 9, 9, 9, 5, 7, 1, 1, 5, 9, 9, 9, 2, 6, 5, 5, 5, 4, 9, 11, 5, 14, 9, 1, 9, 2, 5, 5, 5, 9, 6, 5, 8, 8, 5, 6, 2, 0, 11, 9, 5, 11, 5, 11, 11, 9, 11, 5, 1, 5, 11, 4, 8, 11, 11, 6, 7, 8, 1, 1, 5, 9, 5, 4, 11, 9, 5, 11, 5, 8, 5, 4, 2, 11, 5, 11, 8, 5, 8, 5, 1, 9, 6, 13, 11, 11, 9, 4, 12, 9, 11, 5, 1, 5, 5, 5, 9, 1, 5, 9, 5, 2, 11, 5, 5, 1, 5, 5, 9, 2, 11, 11, 4, 0, 5, 5, 5, 9, 5, 11, 13, 2, 1, 5, 1, 9, 9, 1, 8, 8, 4, 11, 8, 4, 9, 5, 5, 9, 4, 6, 8, 14, 11, 1, 1, 9, 1, 4, 9, 13, 9, 9, 1, 1, 9, 9, 9, 5, 9, 6, 5, 9, 5, 1, 1, 5, 11, 5, 5, 3, 11, 5, 13, 11, 5, 5, 5, 13, 5, 0, 1, 9, 5, 5, 5, 8, 13, 6, 6, 4, 5, 5, 5, 11, 5, 5, 5, 4, 5, 11, 9, 13, 4, 5, 8, 8, 5, 8, 4, 11, 5, 8, 1, 5, 3, 10, 8, 11, 5, 5, 8, 8, 11, 5, 5, 1, 4, 5, 9, 13, 13, 3, 9, 13, 9, 5, 5, 5, 5, 8, 13, 2, 11, 1, 5, 8, 5, 4, 9, 11, 4, 8, 11, 11, 11, 4, 11, 14, 4, 8, 11, 8, 5, 9, 5, 9, 5, 5, 1, 9, 11, 13, 13, 10, 11, 5, 12, 5, 5, 5, 5, 9, 9, 9, 5, 2, 1, 5, 11, 9, 11, 9, 11, 9, 5, 11, 5, 5, 8, 5, 9, 2, 5, 5, 8, 8, 5, 11, 9, 1, 5, 9, 5, 13, 5, 5, 5, 5, 13, 13, 4, 8, 11, 1, 5, 9, 5, 11, 9, 2, 5, 1, 12, 9, 11, 9, 13, 12, 8, 11, 5, 5, 4, 9, 9, 13, 4, 1, 11, 14, 9, 4, 8]\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 4\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# buffer=[]\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    print(npimg.shape)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "# print(env.action_space) # 15\n",
        "\n",
        "def simulate(agent, buffer=[], k=4):\n",
        "    # agent.eval()\n",
        "    out=None\n",
        "    writer = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    # writer = cv2.VideoWriter('video{}.avi'.format(time.time()), cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    act=[]\n",
        "    act_list=[]\n",
        "    lstate=[]\n",
        "    # h0 = torch.randn((agent.jepa.pred.num_layers, agent.d_model), device=device)\n",
        "    while True:\n",
        "    # for i in range(400):\n",
        "    # while not done:\n",
        "        state = transform(state).unsqueeze(0).to(device)\n",
        "        # state = transform(state).unsqueeze(0).to(device)[0]\n",
        "        # with torch.no_grad():\n",
        "        #     st = agent.jepa.enc(state)\n",
        "        #     # st_ = agent.jepa.pred(st)\n",
        "        #     stt = agent.tcost(st).squeeze(-1)\n",
        "        #     imshow(state.detach().cpu().squeeze(0))\n",
        "        #     print(stt)\n",
        "            # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # if len(act)<=0: act = agent(state).cpu()[:1].tolist()\n",
        "        # if len(act)<=0: act = agent(state).cpu()[0,:4].tolist()\n",
        "        # print(act.shape, h0.shape) # [1, 6], [1, 256]\n",
        "        lstate.append(state)\n",
        "        if len(act)<=0:\n",
        "            # lact, lh0, lx, lz = agent(state, h0)\n",
        "            # act = lact.cpu()[0,:k].tolist()\n",
        "            # act = agent(state, k)\n",
        "            act = agent(lstate, k=k)\n",
        "            lstate=[]\n",
        "        action = act.pop(0)\n",
        "        state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        # print(i, 'act: ',action, 'reward: ',reward)\n",
        "        act_list.append(action)\n",
        "        writer.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            print(\"dided\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    print('time')\n",
        "    print(act_list)\n",
        "    env.close()\n",
        "    writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "_=simulate(agent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cm6KjvBrnNO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title wwwwwwwwwwww\n",
        "for i in range(30):\n",
        "    # print(\"#### simulate ####\")\n",
        "    for _ in range(5):\n",
        "        buffer = simulate(agent, buffer)\n",
        "\n",
        "    train_data = BufferDataset(buffer, seq_len)\n",
        "    # train_loader = DataLoader(train_data, shuffle = True, collate_fn=collate_fn, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "    train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # [3,batch, T]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "\n",
        "    print(i,\"#### train ####\")\n",
        "    agent.train_jepa(train_loader, optim)\n",
        "\n",
        "    # checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "    # torch.save(checkpoint, folder+'agentoptim1.pkl')\n",
        "\n",
        "    # buffer = [episode for episode in buffer if episode[-1][2]==-1]\n",
        "    # with open(folder+'buffergo.pkl', 'wb') as f: pickle.dump((buffer), f)\n",
        "\n",
        "    # agentsd, _ = rename_sd(agent.state_dict())\n",
        "    # all_sd = store_sd(all_sd, agentsd)\n",
        "    # torch.save(all_sd, folder+'all_sd.pkl')\n",
        "\n",
        "# repr, std, cov 0.009419754147529602 0.478271484375 0.005037273280322552\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksm4ha7XA-BN"
      },
      "outputs": [],
      "source": [
        "# print(optim.param_groups[0][\"lr\"])\n",
        "optim.param_groups[0][\"lr\"] = 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8zxYU9jpE8K"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -hide_banner -loglevel error -i video.avi video.mp4 -y\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4', \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"<video width=400 controls autoplay><source src=\"{data_url}\" type=\"video/mp4\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhkK_9AQm8_q"
      },
      "source": [
        "##save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uT9m-J1BUWyz",
        "outputId": "877d9b2f-8db4-4309-8176-ba1c722fcb5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py:90: SentryHubDeprecationWarning: `sentry_sdk.Hub` is deprecated and will be removed in a future major release. Please consult our 1.x to 2.x migration guide for details on how to migrate `Hub` usage to the new API: https://docs.sentry.io/platforms/python/migration/1.x-to-2.x\n",
            "  self.hub = sentry_sdk.Hub(client)\n",
            "/usr/local/lib/python3.10/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbobdole\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240923_143420-57apmii7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/procgen/runs/57apmii7' target=\"_blank\">dashing-plasma-58</a></strong> to <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/procgen' target=\"_blank\">https://wandb.ai/bobdole/procgen</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/procgen/runs/57apmii7' target=\"_blank\">https://wandb.ai/bobdole/procgen/runs/57apmii7</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(project=\"procgen\",\n",
        "    config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RCD647ZpPrGf"
      },
      "outputs": [],
      "source": [
        "# @title agent save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = get_res(d_model)\n",
        "        self.sense.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        # self.critic = GRU(\n",
        "        # self.critic = nn.Sequential(\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model, d_model),\n",
        "            # )\n",
        "        # self.actor = nn.Sequential( # -> goal sx/ssx/sssx/...\n",
        "        #     nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "        #     nn.Linear(d_model, d_model),\n",
        "        #     )\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.quantizer = FSQ(levels = [3,3,2])\n",
        "        self.conv = Conv()\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state.flatten()).unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        la, lact = self.search(sx, T=2) # 20\n",
        "        # a, act = la[0][0], lact[0][0]\n",
        "        # return act\n",
        "        return lact[0]\n",
        "\n",
        "    # def search(self, sx, T=256, bptt=32):\n",
        "    def search(self, sx, T=None, bptt=None):\n",
        "        if T==None: T = 256\n",
        "        if bptt==None: bptt = min(T,32)\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.rand((batch, T, 3),device=device)*2 -1) # FSQ 3 levels\n",
        "        optim = torch.optim.SGD([x], lr=1e5)\n",
        "        xx = torch.split(x, bptt, dim=1)\n",
        "        for _ in range(5): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "            for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "                la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "                loss, sx_ = self.rnn_pred(sx_, la)\n",
        "                loss.backward()\n",
        "                optim.step()\n",
        "                optim.zero_grad()\n",
        "                sx_ = sx_.detach()\n",
        "        print(\"search\",loss.item())\n",
        "        return la, lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        # out=[]\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        # print(\"rnn pred\",lsx[0][:5])\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            # sx = self.jepa.pred(sxaz)\n",
        "            sx = sx + self.jepa.pred(sxaz)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.1*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            tcost = self.tcost(sx)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            # out.append(sx)\n",
        "        # out=torch.cat(out)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        return cost, sx#, z\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        # print(\"get\", state.shape)\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        # current = self.sense(state.unsqueeze(-1)) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "    def train_jepa(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            _mem = Stm()\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sx_ = self.jepa.enc(world_state.flatten(start_dim=1))\n",
        "            sx_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "            # print(lst,len(Sar[0]))\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.quantizer.indices_to_codes(action)\n",
        "                    z = self.jepa.argm(sx_, a, sy)\n",
        "                    sxaz = torch.cat([sx_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(sxaz)\n",
        "                    sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # print(\"train jepa sy_\", sy_) # 11.7910 # 1.3963e-06\n",
        "                    # repr_loss = self.jepa.sim_coeff * F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = self.jepa.sim_coeff * F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = repr_loss + std_loss + cov_loss\n",
        "                    # c_ = torch.cat([c_, self.tcost(sy_).squeeze(-1)]) # [batch_size, 1] -> [batch_size]\n",
        "                    # c = torch.cat([c, self.icost(sy) + reward.to(torch.float32)])\n",
        "                    # with torch.no_grad(): c = torch.cat([c, self.icost(sy.detach()) + reward.to(torch.float32)])\n",
        "\n",
        "                    state_ = self.conv(world_state_.detach())\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    loss = loss + jloss + conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(c_)\n",
        "                    # print(c)\n",
        "                    # closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # loss = loss + 100*closs\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sx_ = sx_.detach()\n",
        "                    loss=0\n",
        "                    c,c_= torch.tensor([], device=device), torch.tensor([], device=device)\n",
        "                else:\n",
        "                    scaler.scale(jloss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "                # if batch % 100 == 0:\n",
        "                #     loss, current = loss.item(), batch * len(X)\n",
        "                #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "    # def save(self, folder, name='agent.pth'):\n",
        "    #     torch.save(self.state_dict(), folder+name)\n",
        "    #     self.mem.save(file=folder+name)\n",
        "    # def load(self, folder, name='agent.pth'):\n",
        "    #     self.load_state_dict(torch.load(folder+name), strict=False)\n",
        "    #     # self.mem.load(file=folder+name)\n",
        "\n",
        "\n",
        "# lsx, lc\n",
        "# self.tcost(sx).squeeze(-1)\n",
        "# self.icost(sx_) + reward.to(torch.float32)\n",
        "#                     closs=F.l1_loss(c_, c) # mse_loss, l1_loss\n",
        "\n",
        "\n",
        "agent = Agent().to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "optim = torch.optim.AdamW([{'params': others, 'lr': 1e-3},\n",
        "    {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xLh80kPvEzwX"
      },
      "outputs": [],
      "source": [
        "# @title agent pixel save\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 #\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.emb = torch.nn.Embedding(15, dim_a) # env.action_space # 15\n",
        "        self.deconv = Deconv(d_model)\n",
        "        self.jepa.sim_coeff=2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 20.0 # 1.0 # ν cov Covariance\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        for i in range(20): # num epochs\n",
        "            sx_ = sx.detach()\n",
        "\n",
        "            dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "\n",
        "            # loss, sx_ = self.rnn_pred(sx_, x)\n",
        "            loss, sx_ = self.rnn_pred(sx_, x_)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "    def rnn_pred(self, sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        if z is None: z=torch.zeros((batch,self.dim_z),device=device) # average case?\n",
        "        # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len): # simple single layer\n",
        "            a = la[:,t] # [1, dim_a]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                sx = self.jepa.pred(sxaz)\n",
        "                # sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0.5*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # loss=torch.tensor(0, dtype=torch.float)\n",
        "            state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    sy_ = self.jepa.pred(syaz)\n",
        "                    # sy_ = sx_ + self.jepa.pred(sxaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # # # ae loss\n",
        "                    # state_ = self.deconv(sy.detach()) # not self.deconv(sy)\n",
        "                    # conv_loss = F.mse_loss(state_, state)\n",
        "\n",
        "                    # cost loss\n",
        "                    # reward_ = self.tcost(sy).squeeze(-1) # [batch_size]\n",
        "                    # clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    stt = self.tcost(self.jepa.enc(st)).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb #+ clossl\n",
        "\n",
        "                    loss = loss + jloss + closs #+ conv_loss\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    # print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, conv\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    # print(\"repr, std, cov, conv, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), conv_loss.item(), closs.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    sy_ = sy_.detach()\n",
        "                    loss=0\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item(), \"closs\": closs.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29O1eyvhnRSD",
        "outputId": "c470e601-d5da-4b94-bb69-928dd9d823af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-77-bbc83a6aed37>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent combine\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "if not 'sim_coeff' in locals(): # locals() globals()\n",
        "    sim_coeff, std_coeff, cov_coeff = 0.08, 0.033, 1. # 0.1,0.487,0.01 # expected starting loss?\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=1, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v)\n",
        "        self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            )\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=2. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=1. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=30 # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        # 0.0083 0.06 1.0 = 1, 7, 120.5\n",
        "\n",
        "    def forward(self, state): # live run in env # np (64, 64, 3)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            self.icost.update(sx)\n",
        "        lact = self.search(sx, T=6) # 20\n",
        "        return lact\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        optim_z = torch.optim.SGD([z], lr=1e2, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.95)) #\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.data)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i,x.data, z.squeeze(), loss.item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        # print(\"search\",loss.item())\n",
        "        return lact#, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        _,T,_ = sx.shape\n",
        "        batch = 1\n",
        "        lr = 1e-1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                x[:,:self.lx.shape[1]] = self.lx[:,:T]\n",
        "                z[:,:self.lz.shape[1]] = self.lz[:,:T]\n",
        "\n",
        "        # min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(\"search\",x.data, z.squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=min, max=max)\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            # print(i,x.data, z.squeeze(), loss.item())\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        return lact, lh0, x, z # [batch_size, T]\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, gamma=0.9): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        lsx=sx\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                # sx = self.jepa.pred(sxaz)\n",
        "                sx = sx + self.jepa.pred(sxaz)\n",
        "                tcost = -self.tcost(sx)\n",
        "            lsx = torch.cat([lsx, sx], dim=0)\n",
        "            # print(lsx.requires_grad, sx.requires_grad)\n",
        "            icost = 0*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(icost.requires_grad)\n",
        "            # cost += tcost + icost\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "            print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        return cost, sx\n",
        "\n",
        "\n",
        "    def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "        # if _mem==None: _mem = self.mem\n",
        "        if world_state==None: world_state = self.world_state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        Q = self.q(current) # [batch_size, d_model]\n",
        "        # mem = _mem(Q) # _mem(current)\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "        return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            # loss=0\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    # sy_ = self.jepa.pred(syaz)\n",
        "                    sy_ = sy_ + self.jepa.pred(syaz)\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "                    # balance jepa coeffs\n",
        "                    decay=0.99\n",
        "                    sim_coeff, std_coeff, cov_coeff = sim_coeff*decay + (1-decay)*repr_loss.detach(), std_coeff*decay + (1-decay)*std_loss.detach(), cov_coeff*decay + (1-decay)*cov_loss.detach()\n",
        "                    step=0.001\n",
        "                    if repr_loss.detach()>sim_coeff: self.jepa.sim_coeff=self.jepa.sim_coeff*(1+step)\n",
        "                    if std_loss.detach()>std_coeff: self.jepa.std_coeff=self.jepa.std_coeff*(1+step)\n",
        "                    if cov_loss.detach()>cov_coeff: self.jepa.cov_coeff=self.jepa.cov_coeff*(1+step)\n",
        "                    self.jepa.sim_coeff, self.jepa.std_coeff = self.jepa.sim_coeff/self.jepa.cov_coeff, self.jepa.std_coeff/self.jepa.cov_coeff\n",
        "                    self.jepa.cov_coeff = 1.\n",
        "\n",
        "                    # cost loss\n",
        "                    reward_ = self.tcost(sy_).squeeze(-1) # [batch_size]\n",
        "                    clossl = F.mse_loss(reward_, reward)\n",
        "                    try: st, r = next(trainiter)\n",
        "                    except StopIteration:\n",
        "                        st, r = next(trainiter)\n",
        "                        trainiter = iter(c_loader)\n",
        "                    st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "                    _, st = self.get(st, world_state=world_zero)\n",
        "                    # print(\"stt\",st.shape)\n",
        "                    stt = self.tcost(self.jepa.enc(st.unsqueeze(1))).squeeze(-1)\n",
        "                    clossb = F.mse_loss(stt, r)\n",
        "                    closs =  clossb + clossl\n",
        "\n",
        "                    # loss = loss + jloss + closs\n",
        "                    loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, closs\", repr_loss.item(), std_loss.item(), cov_loss.item(), closs.item())\n",
        "                    print(\"repr, std, cov, closslb\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), clossb.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    print(self.jepa.sim_coeff, self.jepa.std_coeff, self.jepa.cov_coeff)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    # loss=0\n",
        "                else:\n",
        "                    scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"clossl\": clossl.item(), \"clossb\": clossb.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "\n",
        "# tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "# others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "# optim = torch.optim.AdamW([{'params': others, 'lr': 1e-4},\n",
        "#     {'params': tcost_params, 'lr': 1e-2}], betas=(0.9, 0.95))\n",
        "\n",
        "\n",
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 28488545\n",
        "# dreamer v3 https://arxiv.org/pdf/2301.04104 https://vitalab.github.io/article/2023/01/19/DreamerV3.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbad0ffc-e8e9-4a4d-bd3c-d6d9ddaf6e21",
        "cellView": "form",
        "id": "2mY7BITKjSKC"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2336301\n",
            "1278976\n",
            "399360\n",
            "1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-76-e20d23bca149>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "# @title agent gru\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import torchvision.transforms as transforms\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, d_model=256, dim_a=3, dim_z=3, dim_v=512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dim_a, self.dim_z, self.dim_v = dim_a, dim_z, dim_v\n",
        "        # self.sense = Conv(d_model)\n",
        "        # self.sense = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "\n",
        "        # self.mem = Mem()\n",
        "        # self.world_state = torch.zeros((d_model, d_model), device=device) # Sum i] vi kiT\n",
        "        self.jepa = JEPA(d_model**2, d_model, dim_a, dim_z, dim_v, drop=0.2)\n",
        "        # self.icost = ICost(d_model) # intrinsic cost\n",
        "        self.tcost = TCost((1+self.jepa.pred.num_layers)*d_model)\n",
        "        # self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        # self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.emb = torch.nn.Embedding(15, dim_a, max_norm=1.) # env.action_space # 15\n",
        "        self.jepa.sim_coeff=10. # 2.4 # 100.0 # 25.0 # λ repr Invariance reconstruction, ->0 slowly\n",
        "        self.jepa.std_coeff=50. # 1.0 # 1.0 # 25.0 # µ std Variance\n",
        "        self.jepa.cov_coeff=1. # 30 # 20.0 # 1.0 # ν cov Covariance\n",
        "        self.h0 = torch.zeros((self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # e = d_model**-0.5\n",
        "        # self.h0 = torch.empty((self.jepa.pred.num_layers, 1, d_model), device=device).uniform_(-e, e) # [num_layers, batch, d_model]\n",
        "        # self.h0 = torch.normal(mean=0, std=e, size=(self.jepa.pred.num_layers, 1, d_model), device=device) # [num_layers, batch, d_model]\n",
        "        # torch.nn.init.xavier_uniform_(self.h0) # xavier_uniform_, kaiming_normal_\n",
        "\n",
        "        # self.lx, self.lz = torch.empty(1,0,dim_a), torch.empty(1,0,dim_z)\n",
        "        self.lx, self.lz = None, None\n",
        "        state = torch.zeros((1, 3,64,64), device=device)\n",
        "        self.sx = self.jepa.enc(state)\n",
        "\n",
        "    # def forward(self, state, k=1): # live run in env # np (64, 64, 3)\n",
        "    def forward(self, lstate, laction=None, k=1): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        self.update_h0(lstate, laction)\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "            sx = self.jepa.enc(lstate[-1])#.unsqueeze(0)\n",
        "            # self.icost.update(sx)\n",
        "        lact, lh0, lx, lz = self.search(sx, T=6, h0=self.h0) # [T], [T, num_layers, d_model], [T, dim_a], [T, dim_z]\n",
        "        act = lact.cpu()[:k].tolist()\n",
        "        self.h0=lh0[k].unsqueeze(1) # [num_layers, 1, d_model]\n",
        "        # self.lx, self.lz = lx[:,k:], lz[:,k:] # [batch, T, dim_a], [batch, T, dim_z]\n",
        "        self.lx, self.lz = lx[k:], lz[k:] # [T, dim_a], [T, dim_z]\n",
        "        return act\n",
        "\n",
        "    def update_h0(self, lstate, laction=None): # live run in env # np (64, 64, 3)\n",
        "        # self.eval()\n",
        "        with torch.no_grad():\n",
        "            # # self.mem, self.world_state = self.get(state, self.mem, self.world_state)\n",
        "            # _, self.world_state = self.get(state, world_state=self.world_state)\n",
        "            # sx = self.jepa.enc(self.world_state[None,None,...]) # [1, d_model]\n",
        "            with torch.cuda.amp.autocast():\n",
        "\n",
        "                # sx = self.jepa.enc(state)#.unsqueeze(0)\n",
        "                # print(torch.cat(lstate, dim=0).shape)\n",
        "                # lsx = self.jepa.enc(torch.stack(lstate, dim=0))#.unsqueeze(0)\n",
        "                lsx = self.jepa.enc(torch.cat(lstate, dim=0))#.unsqueeze(0)\n",
        "                # self.icost.update(sx)\n",
        "                out_ = lsx-torch.cat([self.sx, lsx[:-1]], dim=0)\n",
        "                # batch, seq_len, _ = lstate.shape\n",
        "                # seq_len, _ = lstate.shape\n",
        "                seq_len = len(lstate)\n",
        "                if laction!=None:\n",
        "                    try: la = self.emb(self.la[:seq_len])\n",
        "                    except:\n",
        "                        print(\"err self.la\")\n",
        "                        # la = self.emb([0]*seq_len)\n",
        "                        la = self.emb(torch.zeros(seq_len, dtype=int, device=device))\n",
        "\n",
        "        # lz = nn.Parameter(torch.zeros((batch, seq_len, self.dim_z),device=device))\n",
        "        lz = nn.Parameter(torch.zeros((seq_len, self.dim_z),device=device))\n",
        "        torch.nn.init.xavier_normal_(lz) # xavier_normal_ xavier_uniform_\n",
        "        # optim_z = torch.optim.SGD([lz], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([lz], 1e0, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "\n",
        "        for i in range(20): # num epochs\n",
        "            sxaz = torch.cat([lsx, la, lz], dim=-1).unsqueeze(0) # [1, seq_len, d_model+dim_a+dim_z]\n",
        "            with torch.cuda.amp.autocast(): # with torch.amp.autocast('cuda'):\n",
        "                # print(sxaz.shape, self.h0.shape)\n",
        "                out, h0 = self.jepa.pred(sxaz, self.h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                # sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                loss = F.mse_loss(out_, out)\n",
        "            loss.backward()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            print(lz.data)\n",
        "            with torch.no_grad(): lz.clamp_(min=-1, max=1)\n",
        "        self.h0 = h0\n",
        "        self.sx = lsx[-1]\n",
        "        self.la = la[k:]\n",
        "        return h0\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 16\n",
        "        lr = 1e1 # adamw 1e-1, 3e-1\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim_x = torch.optim.SGD([x], lr=1e-1)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_normal_(z)\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e0, maximize=True) # 3e3\n",
        "        optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        if self.lx is not None:\n",
        "            with torch.no_grad():\n",
        "                # x[:,:self.lx.shape[1]], z[:,:self.lz.shape[1]] = self.lx[:,:T], self.lz[:,:T]\n",
        "                x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].repeat(batch,1,1), self.lz[:T].repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        sx = sx.detach()\n",
        "        h0 = h0.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            # print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            # print(\"loss\",loss)\n",
        "            loss.sum().backward()\n",
        "            optim_x.step(); optim_z.step()\n",
        "            optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z.clamp_(min=-1, max=1)\n",
        "            print(i, \"loss\", loss.squeeze().data)\n",
        "            # print(x.shape,torch.argmax(-dist,dim=-1).shape,z.shape,loss.shape) # [16, 6, 3], [16, 6], [16, 6, 1], [16, 1]\n",
        "            # print(i, torch.cat([x,torch.argmax(-dist,dim=-1),z],dim=-1).squeeze().data)\n",
        "            print(i, \"x act z\", torch.cat([x[0],torch.argmax(-dist,dim=-1)[0].unsqueeze(-1),z[0]],dim=-1).squeeze().data)\n",
        "            # print(i,x[0].squeeze()[0].data, z[0].squeeze().data, loss.squeeze().item())\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, t, dim_a], [batch, t, dim_z]\n",
        "\n",
        "\n",
        "    def search_optimxz(self, sx, T=6, h0=None):\n",
        "        self.eval()\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch = 4 # 16\n",
        "        lr = 1e1 # adamw 1e-1, 3e-1 ; sgd\n",
        "        ratio = 4\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "        # optim_x = torch.optim.SGD([x], lr=lr)\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "        z = nn.Parameter(torch.zeros((batch, T, self.dim_z),device=device))\n",
        "        # torch.nn.init.xavier_uniform_(z) # xavier_normal_, xavier_uniform_\n",
        "        torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "        # optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.SGD([z], lr=1e1, maximize=True) # 3e3\n",
        "        # optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "        optim_z = torch.optim.AdamW([z], 1e2, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x[:,:self.lx.shape[0]], z[:,:self.lz.shape[0]] = self.lx[:T].unsqueeze(0).repeat(batch,1,1), self.lz[:T].unsqueeze(0).repeat(batch,1,1) # [batch, seq_len, dim_az]\n",
        "\n",
        "        # print(\"search\",x[0].data, z[0].squeeze())\n",
        "        print(\"search\", z[0].squeeze())\n",
        "        sx, h0 = sx.detach(), h0.detach()\n",
        "        for i in range(10): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, lsx, lh0,c = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            # loss, sx_ = self.rnn_pred(sx, x, z) # use raw x as act emb\n",
        "            loss.sum().backward()\n",
        "            # optim_x.step(); optim_z.step()\n",
        "            # optim_x.zero_grad(); optim_z.zero_grad()\n",
        "            optim_z.step()\n",
        "            optim_z.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "            print(i, \"search loss\", loss.squeeze().data)\n",
        "            # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "            print(i, \"search z\", z[0].squeeze().data)\n",
        "            # print(torch.argmin(dist,dim=-1).int())\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "        print(\"c\",torch.stack(c)[:,idx])\n",
        "        return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, T, dim_a], [batch, T, dim_z]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # cost += tcost + icost\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0\n",
        "\n",
        "\n",
        "    def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "        self.jepa.pred.train()\n",
        "        batch, seq_len, dim_a = la.shape\n",
        "        cost = 0\n",
        "        sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "        lsx=sx.unsqueeze(1)\n",
        "        h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        # print(lsx.shape, la.shape, lz.shape)\n",
        "        c=[]\n",
        "        for t in range(seq_len):\n",
        "            a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "            # print(sx.shape, a.shape, z.shape)\n",
        "            sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "                syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                tcost = -self.tcost(syh0)\n",
        "            c.append(tcost)\n",
        "            lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "            lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "            icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "            # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "            cost += (tcost + icost)*gamma**t\n",
        "        return cost, lsx, lh0, c\n",
        "\n",
        "\n",
        "    # def get(self, state, _mem=None, world_state=None): # update world_state and mem from state\n",
        "    #     # if _mem==None: _mem = self.mem\n",
        "    #     if world_state==None: world_state = self.world_state\n",
        "    #     current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "    #     Q = self.q(current) # [batch_size, d_model]\n",
        "    #     # mem = _mem(Q) # _mem(current)\n",
        "    #     obs = current# + mem # [batch_size, d_model]\n",
        "    #     K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "    #     # self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "    #     # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "    #     K = F.normalize(K, dim=-1)\n",
        "    #     if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    #     V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "    #     world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "    #     # _mem.add(K, V) # [batch_size, 1, d_model] or [1, d_model]\n",
        "    #     return _mem, world_state#, cost\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                # with torch.amp.GradScaler('cuda'):\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "                    z = self.jepa.argm(sy_, a, sy)\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "\n",
        "                    # imshow(state[0].cpu())\n",
        "                    # print(\"norm\", torch.norm(sy[0]-sy_[0], dim=-1))\n",
        "                    # # if torch.norm(sy[0]-sy_[0], dim=-1) > 1:\n",
        "                    # print(i, reward[0])\n",
        "                    # print(sy)\n",
        "                    # print(sy_)\n",
        "                    # print(sy[0]-sy_[0])\n",
        "\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "                    # jloss = repr_loss + std_loss + cov_loss\n",
        "\n",
        "\n",
        "                    # cost loss\n",
        "                    # syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    syh0 = torch.cat([sy.flatten(1),F.dropout(h0, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                    closs = 100*clossl\n",
        "\n",
        "                    loss = loss + jloss + closs # for no retain_graph\n",
        "                    # loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    h0 = h0.detach()\n",
        "                    loss=0 # no retain_graph\n",
        "                # else:\n",
        "                #     scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "    def train_jepa(self, dataloader, c_loader, optim, bptt=25): #32\n",
        "        self.train()\n",
        "        global sim_coeff, std_coeff, cov_coeff\n",
        "        trainiter = iter(c_loader)\n",
        "        for batch, Sar in enumerate(dataloader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "            # _mem = Stm()\n",
        "            lst=list(range(0,len(Sar[0]),bptt))[1:]+[len(Sar[0])] # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            loss=0\n",
        "            # world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # world_zero = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "            # sy_ = self.jepa.enc(world_state.unsqueeze(1)) # [batch_size, 1, d_model, d_model]\n",
        "            h0 = torch.zeros((self.jepa.pred.num_layers, batch_size, self.d_model), device=device) # [num_layers, batch, d_model]\n",
        "            state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "            sy_ = self.jepa.enc(state) # [batc h_size, d_model]\n",
        "            # sx=sy_\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n",
        "                    # _mem, world_state_ = self.get(state, _mem, world_state)\n",
        "                    # _, world_state_ = self.get(state, world_state=world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    a = self.emb(action)\n",
        "\n",
        "                    # z = self.jepa.argm(sy_, a, sy)\n",
        "                    z = self.argm(sy, sy_, h0, a, reward)\n",
        "                    with torch.no_grad(): z.mul_(torch.rand_like(z)).mul_((torch.rand_like(z)>0.5).bool()) # dropout without scailing\n",
        "\n",
        "                    syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "                    out, h0 = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                    out = out[:, -1, :]\n",
        "                    sy_ = sy_ + out\n",
        "\n",
        "                    # repr_loss = F.mse_loss(sy.detach(), sy_) # s(sy, sy~) # invariance loss\n",
        "                    repr_loss = F.mse_loss(sy, sy_) # s(sy, sy~) # invariance loss\n",
        "                    std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    jloss = self.jepa.sim_coeff * repr_loss + self.jepa.std_coeff * std_loss + self.jepa.cov_coeff * cov_loss\n",
        "\n",
        "                    # cost loss\n",
        "                    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    # syh0 = torch.cat([sy.flatten(1),F.dropout(h0, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "                    clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "                    closs = self.closs_coeff * clossl\n",
        "\n",
        "                    # print(h0.requires_grad)\n",
        "                    # pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "                    # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "                    # print(\"reward, pred, clossl\", reward[mask].data, pred[mask].data, clossl.item())\n",
        "                    # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "                    # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "                    # torch.norm(sy-sx, dim=-1)\n",
        "                    # sx=sy\n",
        "\n",
        "                    loss = loss + jloss + closs # for no retain_graph\n",
        "                    # loss = jloss + closs\n",
        "\n",
        "                if i+1 in lst:\n",
        "                    # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "                    norm = torch.norm(sy, dim=-1)[0].item()\n",
        "                    z_norm = torch.norm(z)\n",
        "                    # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "                    # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "                    print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "                    # loss.backward()\n",
        "                    # optim.step()\n",
        "                    scaler.scale(loss).backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # world_state = world_state_.detach()\n",
        "                    sy_ = sy_.detach()\n",
        "                    h0 = h0.detach()\n",
        "                    loss=0 # no retain_graph\n",
        "                # else:\n",
        "                #     scaler.scale(loss).backward(retain_graph=True)\n",
        "\n",
        "                try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": clossl.item()})\n",
        "                except: pass\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "\n",
        "\n",
        "\n",
        "agent = Agent(d_model=256).to(device)\n",
        "# agent = torch.compile(Agent(d_model=256), mode='max-autotune').to(device)\n",
        "\n",
        "optim = torch.optim.AdamW(agent.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4\n",
        "\n",
        "print(sum(p.numel() for p in agent.parameters() if p.requires_grad)) # 1lyr:2727982, 2lyr:4401710\n",
        "print(sum(p.numel() for p in agent.jepa.enc.parameters() if p.requires_grad)) # 1278976\n",
        "print(sum(p.numel() for p in agent.jepa.pred.parameters() if p.requires_grad)) # 1lyr:397824, 2lyr:792576\n",
        "print(sum(p.numel() for p in agent.tcost.parameters() if p.requires_grad)) # 197633\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title z.grad.data = -z.grad.data\n",
        "\n",
        "# self.eval()\n",
        "batch = 4 # 16\n",
        "x = nn.Parameter(torch.empty((batch, T, agent.dim_a),device=device))\n",
        "torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "z = nn.Parameter(torch.zeros((batch, T, agent.dim_z),device=device))\n",
        "torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "# optim_ = torch.optim.SGD([x,z], lr=1e1) # 3e3\n",
        "optim_ = torch.optim.AdamW([x,z], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "print(\"search z\", z[0].squeeze())\n",
        "print(\"search x\", x[0].squeeze())\n",
        "sx, h0 = sx.detach(), h0.detach()\n",
        "for i in range(10): # num epochs\n",
        "    dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    x_ = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "    # print(sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [4, 1, 3], [4, 1, 8], [1, 1, 256]\n",
        "    loss, lsx, lh0,c = agent.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "    loss.sum().backward()\n",
        "    z.grad.data = -z.grad.data\n",
        "    optim_.step()\n",
        "    optim_.zero_grad()\n",
        "    with torch.no_grad():\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "    print(i, \"search loss\", loss.squeeze().data)\n",
        "    print(i, \"search z\", z[0].squeeze().data)\n",
        "    print(i, \"search x\", x[0].squeeze().data)\n",
        "dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "print(\"c\",torch.stack(c)[:,idx])\n",
        "# return lact[idx], lh0[:,:,idx,:], x[idx], z[idx] # [batch, T], [T, num_layers, batch, d_model], [batch, T, dim_a], [batch, T, dim_z]\n",
        "# print(lact[idx], lh0[:,:,idx,:], x[idx], z[idx])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cg0BI2TwY9-p",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title argm agent.rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def argm(sx, x,h0, lr=3e3): # 3e3\n",
        "    # agent.eval()\n",
        "    # batch_size, T, _ = sx.shape\n",
        "    batch = 16 # 16\n",
        "    z = nn.Parameter(torch.zeros((batch, T, agent.dim_z),device=device))\n",
        "    torch.nn.init.normal_(z, mean=0., std=1./z.shape[-1]**0.5) # norm ~1\n",
        "    optim_z = torch.optim.SGD([z], lr=1e3, maximize=True) # 3e3\n",
        "    # optim_z = torch.optim.AdamW([z], 1e-2, (0.9, 0.999), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "    # optim_z = torch.optim.AdamW([z], 1e-0, (0.9, 0.95), maximize=True) # ? 1e0 ; 3e-2 1e-1\n",
        "    # optim_z = torch.optim.LBFGS([z], max_iter=5, lr=1)\n",
        "\n",
        "    # print(\"argm\", z[0].squeeze())\n",
        "    sx, h0 = sx.detach(), h0.detach()\n",
        "    x = x.detach().repeat(batch,1,1)\n",
        "    for i in range(5): # num epochs\n",
        "        # print(sx.shape, x.shape, z.shape, h0.shape) # [1, 256], [4, 1, 3], [4, 1, 8], [1, 1, 256]\n",
        "        loss, lsx, lh0,c = agent.rnn_pred(sx, x, z, h0) # snap x to act emb\n",
        "        loss.sum().backward()\n",
        "        optim_z.step()\n",
        "        optim_z.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1) # z.clamp_(min=-1, max=1)\n",
        "        # print(i, \"argm loss\", loss.squeeze().data)\n",
        "        # print(i, \"argm z\", z[0].squeeze().data)\n",
        "    idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "    return z[idx].unsqueeze(0)\n",
        "\n",
        "\n",
        "T=1\n",
        "xx = torch.empty((1, T, agent.dim_a))\n",
        "torch.nn.init.xavier_uniform_(xx)\n",
        "x = nn.Parameter(xx.clone())#.repeat(batch,1,1))\n",
        "# print(x.shape)\n",
        "optim_x = torch.optim.SGD([x], lr=1e1) # 1e-1,1e-0,1e4 ; 1e2\n",
        "# optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "# optim_x = torch.optim.AdamW([x], 1e1, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "h0 = torch.zeros((agent.jepa.pred.num_layers, 1, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "\n",
        "state = torch.zeros((1, 3,64,64))\n",
        "with torch.no_grad():\n",
        "    sx = agent.jepa.enc(state).detach()\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "print(time.time()-start)\n",
        "\n",
        "print(\"search\",x.squeeze().data)\n",
        "for i in range(20): # 5\n",
        "    dist = torch.norm(agent.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    x_ = ste_argmax(-dist) @ agent.emb.weight.data\n",
        "    z = argm(sx, x_,h0)\n",
        "    # print(sx.shape, x_.shape, z.shape, h0.shape) # [1, 256], [1, 1, 3], [1, 1, 8], [1, 1, 256]\n",
        "    loss, lsx, lh0,c = agent.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "    loss.sum().backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "    print(i, \"search loss\", x.squeeze().data, loss.item())\n",
        "    # print(i, \"search x z\", x[0].data, z[0].squeeze().data)\n",
        "\n",
        "# z sgd 1e3\n",
        "# 9 search loss tensor([0.0142, 0.0142, 0.0142, 0.0142])\n",
        "# 9 search z tensor([-0.3381, -0.7005, -0.5877, -0.0664, -0.1439,  0.0283,  0.0541, -0.1439])\n",
        "\n",
        "# x sgd 1e2\n",
        "# 1 tensor([0.3561, 0.3059, 0.8830]) 0.014148875139653683\n",
        "# 9 tensor([0.3560, 0.3064, 0.8828]) 2.328815611463142e-07\n",
        "\n",
        "# 1e0\n",
        "# 19 tensor([-0.5768,  0.5778,  0.5774]) 6.543130552927323e-07\n",
        "# 19 tensor([0.3570, 0.6689, 0.6521]) 2.474381801675918e-07\n",
        "# 19 tensor([0.5783, 0.5765, 0.5772]) 1.519319567933053e-07\n",
        "# 19 tensor([0.3427, 0.6795, 0.6487]) 4.220427456402831e-07\n",
        "#\n",
        "\n"
      ],
      "metadata": {
        "id": "5VMebkQ1mJtD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title torch.optim.LBFGS\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# Example of a deep nonlinear model f(x)\n",
        "class DeepNonlinearModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNonlinearModel, self).__init__()\n",
        "        self.lin = nn.Sequential(\n",
        "            nn.Linear(10, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)\n",
        "\n",
        "f = DeepNonlinearModel()\n",
        "# x = torch.randn(1, 10, requires_grad=True)\n",
        "# xx = torch.randn((1,10))\n",
        "x = nn.Parameter(xx.clone())#.repeat(batch,1,1))\n",
        "\n",
        "# Define loss function (mean squared error for this example)\n",
        "target = torch.tensor([[0.0]])  # Target output\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "def closure():\n",
        "    optimizer.zero_grad()  # Zero out the gradients\n",
        "    output = f(x)          # Forward pass through the model\n",
        "    loss = loss_fn(output, target)  # Calculate the loss\n",
        "    loss.backward()         # Backpropagate\n",
        "    return loss\n",
        "\n",
        "optimizer = torch.optim.LBFGS([x], lr=1.0, max_iter=5)  # Limit to 2-3 iterations for speed\n",
        "start_time = time.time()\n",
        "for _ in range(2):  # LBFGS does multiple iterations internally\n",
        "    loss = optimizer.step(closure)  # Perform a step of optimisation\n",
        "    print(loss.item())\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimisation completed in {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Final loss: {loss.item()}\")\n",
        "print(f\"Optimised x: {x.detach().numpy()}\")\n",
        "\n",
        "start_time = time.time()\n",
        "optimizer = torch.optim.SGD([x], lr=1e1, maximize=True) # 3e3\n",
        "for _ in range(5):  # LBFGS does multiple iterations internally\n",
        "    loss = optimizer.step()  # Perform a step of optimisation\n",
        "    output = f(x)          # Forward pass through the model\n",
        "    loss = loss_fn(output, target)  # Calculate the loss\n",
        "    loss.backward()         # Backpropagate\n",
        "    optimizer.zero_grad()  # Zero out the gradients\n",
        "    print(loss.item())\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Optimisation completed in {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Final loss: {loss.item()}\")\n",
        "print(f\"Optimised x: {x.detach().numpy()}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gcvgdCB1h1_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KZeny7pRU6bG"
      },
      "outputs": [],
      "source": [
        "# @title test search, argm\n",
        "# # def search(self, sx, T=None, bptt=None):\n",
        "T=20\n",
        "bptt=None\n",
        "if T==None: T = 256\n",
        "if bptt==None: bptt = min(T,32)\n",
        "d_model=agent.d_model\n",
        "# sx=torch.randn((1, d_model), device=device)\n",
        "# batch=sx.size(dim=0)\n",
        "batch=32\n",
        "# scale = torch.sqrt(torch.tensor((d_model,), device=device))\n",
        "\n",
        "# x_ = torch.rand((batch, T, 3),device=device)*2 -1\n",
        "# *self.dim_z**(-0.5) # 1/d^(1/2)\n",
        "# x_ = torch.zeros((batch, T, 3),device=device) # dont, deterministic, stuck\n",
        "x=nn.Parameter(x_.clone())\n",
        "# optim = torch.optim.SGD([x], lr=1e3, momentum=0.9)\n",
        "optim = torch.optim.SGD([x], lr=1e2)\n",
        "# optim = torch.optim.SGD([x], lr=1e5)\n",
        "# optim = torch.optim.SGD([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=3e2)\n",
        "# optim = torch.optim.AdamW([x], lr=1e5)\n",
        "\n",
        "# xx = torch.split(x, bptt, dim=1)\n",
        "# for _ in range(10): # num epochs\n",
        "#     sx_ = sx.detach()\n",
        "#     # print(sx_[0][:10])\n",
        "#     for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "#         la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "#         print(lact)\n",
        "#         loss, sx_ = agent.rnn_pred(sx_, la)\n",
        "#         loss.backward()\n",
        "#         optim.step()\n",
        "#         optim.zero_grad()\n",
        "#         sx_ = sx_.detach()\n",
        "#         print(\"search\",loss.item())\n",
        "\n",
        "\n",
        "# argm\n",
        "# sx = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# sy = torch.rand((batch, d_model),device=device)*2 -1\n",
        "# a = torch.rand((batch, agent.dim_a),device=device)*2 -1\n",
        "# z_ = torch.rand((batch, agent.dim_z),device=device)*2 -1\n",
        "# # z_ = torch.rand((batch, agent.dim_z),device=device)\n",
        "# # z_ = z_/scale\n",
        "\n",
        "z=nn.Parameter(z_.clone()) # argm 0.38188403844833374 3.86767578125\n",
        "# torch.nn.init.zeros_(z)\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# print(z)\n",
        "# optim = torch.optim.SGD([z], lr=1e2, momentum=0.9)\n",
        "# optim = torch.optim.SGD([z], lr=1e4)\n",
        "optim = torch.optim.SGD([z], lr=3e3)\n",
        "# optim = torch.optim.SGD([z], lr=3e1)\n",
        "# optim = torch.optim.AdamW([z], lr=3e-1)\n",
        "lossfn = torch.nn.MSELoss()\n",
        "num_steps = 100\n",
        "agent.jepa.eval()\n",
        "import time\n",
        "start=time.time()\n",
        "for i in range(num_steps):\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # loss, sx = agent.rnn_pred(sx, la)s\n",
        "    sy_ = agent.jepa.pred(sxaz)\n",
        "    # print(\"y_, y\",y_.shape, y.shape)\n",
        "    loss = lossfn(sy_, sy)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(\"argm\",loss.item(), z[0].item())\n",
        "# print(time.time()-start)\n",
        "print(z.squeeze())\n",
        "\n",
        "want z around [-1,1], large lr, few steps, punish large z\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6):\n",
        "        batch=sx.size(dim=0)\n",
        "        x = nn.Parameter(torch.empty((batch, T, self.dim_a),device=device))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "        # optim = torch.optim.SGD([x], lr=1e4) #, maximize=True)\n",
        "        optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.95))\n",
        "        min, max = self.emb.weight.data.min(dim=0).values, self.emb.weight.data.max(dim=0).values\n",
        "        print(x)\n",
        "        sx = sx.detach()\n",
        "        for i in range(20): # num epochs\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            loss, sx_ = self.rnn_pred(sx, x_)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad():\n",
        "                # x.clamp_(min=-1, max=1)\n",
        "                x.clamp_(min=min, max=max)\n",
        "            print(i,x)\n",
        "        # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "        dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "        lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "        print(\"search\",loss.item())\n",
        "        return lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "    # def argm(self, sx, a, lr=3e3): # 3e3\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     z = nn.Parameter(torch.zeros((batch,self.dim_z),device=device))\n",
        "    #     optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    #     # optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    #     sx, a = sx.detach(), a.detach()\n",
        "    #     for i in range(5): # 10\n",
        "    #         sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    #         with torch.amp.autocast('cuda'):\n",
        "    #             # sx_ = self.jepa.pred(sxaz)\n",
        "    #             sx_ = sx + self.jepa.pred(sxaz)\n",
        "    #             cost = -self.tcost(sx_)\n",
        "\n",
        "    #         cost.backward()\n",
        "    #         optim.step()\n",
        "    #         # scaler.scale(cost).backward()\n",
        "    #         # scaler.step(optim)\n",
        "    #         # scaler.update()\n",
        "    #         optim.zero_grad()\n",
        "    #         with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "    #         print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    #     return z.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def argm(self, sx, lr=3e3): # 3e3\n",
        "        # batch=sx.size(dim=0)\n",
        "        batch_size, T, _ = sx.shape\n",
        "        batch = 16\n",
        "        # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "        z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "        torch.nn.init.xavier_uniform_(z)\n",
        "        # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "        optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "        sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "        # sx = sx.detach()\n",
        "        for i in range(20): # 10\n",
        "            # print(sx.shape,z.shape)\n",
        "            sxz = torch.cat([sx, z], dim=-1)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                cost = model(sxz)\n",
        "            cost.sum().backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "            with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "            # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "            # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "            # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "        # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "        # return z.detach()\n",
        "        # print(\"argm z\",z.squeeze().data)\n",
        "        # print(\"cost\",cost.squeeze())\n",
        "        idx = torch.argmax(loss)\n",
        "        # return z[idx].detach().unsqueeze(0)\n",
        "        return z[:,idx].detach()\n",
        "\n",
        "\n",
        "    def search(self, sx, T=6, h0=None):\n",
        "        batch=1\n",
        "        T=1\n",
        "        x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "        torch.nn.init.xavier_uniform_(x)\n",
        "\n",
        "        lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "        # ratio = 6e0\n",
        "        lr = 1e-1 # adamw 1e-1\n",
        "        ratio = 4\n",
        "        # optim_x = torch.optim.SGD([x], lr=lr)\n",
        "        # optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "        optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "        # print(x.shape)\n",
        "\n",
        "\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze())\n",
        "        # print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "        for i in range(50):\n",
        "            dist = torch.norm(self.emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "            print(\"act\",torch.argmax(-dist,dim=-1))\n",
        "            x_ = ste_argmax(-dist) @ self.emb.weight.data\n",
        "            z = argm(x)\n",
        "            # loss, sx_ = self.rnn_pred(sx, x_, z) # snap x to act emb\n",
        "            loss, lsx, lh0 = self.rnn_pred(sx, x_, z, h0) # snap x to act emb\n",
        "            loss.sum().backward()\n",
        "            optim_x.step()\n",
        "            optim_x.zero_grad()\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "            # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "            # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "            # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "            with torch.no_grad():\n",
        "                # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "                x.clamp_(min=-1, max=1)\n",
        "            # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "        idx = torch.argmax(loss)\n",
        "        print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F8nNzai_b-G5"
      },
      "outputs": [],
      "source": [
        "# @title test quant icost search rnn_pred\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "d_model=16\n",
        "sicost = ICost(d_model, n=4)\n",
        "stcost=nn.Sequential(nn.Linear(d_model, 1)).to(device)\n",
        "dim_z=1\n",
        "jepa_pred=nn.Sequential(nn.Linear(d_model+dim_z+3, d_model)).to(device)\n",
        "\n",
        "\n",
        "def search(sx, T=None, bptt=None):\n",
        "    if T==None: T = 256\n",
        "    if bptt==None: bptt = min(T,32)\n",
        "    batch=sx.size(dim=0)\n",
        "    # with torch.amp.autocast('cuda'):\n",
        "    x = nn.Parameter(torch.zeros((batch, T, 3),device=device))\n",
        "    torch.nn.init.xavier_uniform_(x)\n",
        "    # optim = torch.optim.SGD([x], lr=1e5, maximize=True)\n",
        "    optim = torch.optim.SGD([x], lr=1e5)\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    for _ in range(3): # num epochs\n",
        "        sx_ = sx.detach()\n",
        "        for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "            loss, sx_ = rnn_pred(sx_, la)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            print(loss)\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optim)\n",
        "            # scaler.update()\n",
        "            optim.zero_grad()\n",
        "\n",
        "            with torch.no_grad(): x = torch.clamp(x, min=-1, max=1)\n",
        "            sx_ = sx_.detach()\n",
        "            # print(loss.item(), lact)\n",
        "    # print(\"search\",loss.item())\n",
        "    # return la, lact # [batch_size, T]\n",
        "    return la, lact, x # [batch_size, T]\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.95): # [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    if z is None: z=torch.zeros((batch,dim_z),device=device) # average case?\n",
        "    # z = self.jepa.argm(sx, a, sx_) # worst case\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    # for t in range(seq_len): # simple single layer\n",
        "    t=0\n",
        "    a = la[:,t] # [1, dim_a]\n",
        "    sxaz = torch.cat([sx, a, z], dim=-1)\n",
        "    # sx = sx + jepa_pred(sxaz)\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        sx = jepa_pred(sxaz)\n",
        "    print(lsx)\n",
        "    lsx = torch.cat([lsx, sx], dim=0)\n",
        "    print(lsx)\n",
        "    # print(lsx.requires_grad, sx.requires_grad)\n",
        "    # icost = 0.5*sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    icost = sicost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "    # print(icost.requires_grad)\n",
        "    tcost = -stcost(sx.squeeze(0)).squeeze(0)\n",
        "    cost += (tcost + icost)*gamma**t\n",
        "    print(\"tcost, icost\", tcost, icost)\n",
        "    # cost=icost\n",
        "    # print(cost)\n",
        "    return cost, sx#, z\n",
        "\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "\n",
        "batch=1\n",
        "sx=torch.rand((batch,d_model), device=device)\n",
        "la, lact, x = search(sx, T=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uivwksBdwVH"
      },
      "outputs": [],
      "source": [
        "state = buffer[7][80][0]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "state = transform(state).unsqueeze(0).to(device)[0]\n",
        "sx_ = agent.jepa.enc(state.unsqueeze(0))\n",
        "out= agent.deconv(sx_).squeeze(0)\n",
        "print(out.shape)\n",
        "imshow(state.detach().cpu())\n",
        "imshow(out.detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjm2kV3H7ZVR"
      },
      "outputs": [],
      "source": [
        "# print(sum(p.numel() for p in agent.parameters() if p.requires_grad))\n",
        "for name, p in agent.named_parameters():\n",
        "    if p.requires_grad:\n",
        "        print(name, p.numel())\n",
        "\n",
        "\n",
        "# 23921665 # agent # 6872065\n",
        "# 12219840 # jepa # 3695040\n",
        "# 24M params\n",
        "# 24M * 3 * 4bytes\n",
        "# 288MB\n",
        "\n",
        "# 4 byte *3*64*64\n",
        "# 4 *3*64*64 = 49152 # 1 img 50kb\n",
        "# 64 img -> 3.2mb\n",
        "# seq len 50 -> 160mb\n",
        "\n",
        "# 64*64*3=12288\n",
        "# 256*256=65536\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e3fpbtNOiz1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title data weighted\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "data = [step for episode in buffer for step in episode]\n",
        "state, action, reward = zip(*data)\n",
        "# print(\"reward\",type(reward))\n",
        "data_targets=(torch.tensor(reward)==0).int()\n",
        "ctrain_data=list(zip(state,reward))\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "class Datasetme(Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.dataset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, torch.tensor(y, dtype=torch.float)\n",
        "        # return x, y+1\n",
        "ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "class_count=torch.tensor([x[1] for x in class_count])\n",
        "weight=1./class_count\n",
        "weights = weight[data_targets]\n",
        "\n",
        "# batch_size = 64 #\n",
        "\n",
        "train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "# train_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "c_loader = torch.utils.data.DataLoader(ctrain_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "\n",
        "\n",
        "def make_weighted(buffer):\n",
        "    data = [step for episode in buffer for step in episode]\n",
        "    state, action, reward = zip(*data)\n",
        "    # print(\"reward\",type(reward))\n",
        "    data_targets=(torch.tensor(reward)==0).int()\n",
        "    ctrain_data=list(zip(state,reward))\n",
        "    ctrain_data = Datasetme(ctrain_data)\n",
        "\n",
        "    from collections import Counter\n",
        "    class_count = torch.tensor(list(Counter(data_targets.tolist()).values()))\n",
        "    class_count = sorted(Counter(data_targets.tolist()).items())\n",
        "    class_count=torch.tensor([x[1] for x in class_count])\n",
        "    weight=1./class_count\n",
        "    weights = weight[data_targets]\n",
        "\n",
        "    # batch_size = 64 #\n",
        "    train_sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
        "    c_loader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, pin_memory=True)\n",
        "    return c_loader\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "matplotlib.rcParams['figure.dpi'] = 300\n",
        "def imshow(img): # display img from torch tensor\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    plt.axis('off')\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "trainiter = iter(c_loader)\n",
        "images, labels = next(trainiter)\n",
        "# images, labels = images.to(device), labels.to(device)\n",
        "batch=40\n",
        "images, labels = images[:batch], labels[:batch]\n",
        "# imshow(torchvision.utils.make_grid(images,nrow=10))\n",
        "# print(labels)\n",
        "for x in range(len(labels)//10):\n",
        "    print(labels[10*x:10*x+10])\n",
        "\n",
        "# # try:\n",
        "with torch.no_grad():\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).argmax(-1).cpu()\n",
        "    # pred = agent.tcost(agent.jepa.enc(images.to(device))).squeeze(-1).cpu()\n",
        "    # _, world_state = agent.get(images.to(device))\n",
        "    # pred = agent.tcost(agent.jepa.enc(world_state.unsqueeze(1))).squeeze(-1).cpu()\n",
        "    h0 = torch.zeros((agent.jepa.pred.num_layers, batch, agent.d_model), device=device)\n",
        "    # h0 = torch.empty((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device)\n",
        "    # torch.nn.init.xavier_normal_(h0)\n",
        "    sy = agent.jepa.enc(images.to(device)) # [batch_size, d_model]\n",
        "    syh0 = torch.cat([sy.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "    agent.tcost.update_loss_weight(train_data)\n",
        "    pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "\n",
        "    # print(pred)\n",
        "    for x in range(len(pred)//10):\n",
        "        print(pred[10*x:10*x+10])\n",
        "    # print((labels==pred).sum())\n",
        "# except: pass\n",
        "print(agent.tcost.loss(syh0, labels.to(device)).squeeze(-1))\n",
        "print(F.mse_loss(labels, pred))\n",
        "\n",
        "# torch.where(abs(labels- pred)>0.5,1,0)\n",
        "for x in range(len(pred)//10):\n",
        "    print(torch.where(abs(labels- pred)>0.5,1,0)[10*x:10*x+10])\n",
        "\n",
        "mask = torch.where(abs(labels- pred)>0.5,1,0).bool()\n",
        "print(\"reward, pred\", labels[mask].data, pred[mask].data)\n",
        "try: imshow(torchvision.utils.make_grid(images[mask], nrow=10))\n",
        "except ZeroDivisionError: pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mhTHWmEjI0JO"
      },
      "outputs": [],
      "source": [
        "# @title gym\n",
        "# https://gymnasium.farama.org/\n",
        "# https://github.com/Farama-Foundation/Gymnasium\n",
        "import gymnasium as gym\n",
        "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "env = gym.make(\"Pendulum-v1\") # https://gymnasium.farama.org/environments/classic_control/pendulum/\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "   action = env.action_space.sample()  # this is where you would insert your policy\n",
        "   observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "   if terminated or truncated:\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "\n",
        "quantizer = FSQ(levels = [2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fsealXK3OPQa"
      },
      "outputs": [],
      "source": [
        "# @title train test function\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def strain(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            x1, x2 = trs(x)\n",
        "            loss = model.loss(x1,x2)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        optimizer.zero_grad()\n",
        "        # model.conv_ema.update_parameters(model.conv)\n",
        "        # model.exp_ema.update_parameters(model.exp)\n",
        "\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        train_loss = loss.item()/len(y)\n",
        "        loss_list.append(loss.item())\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x1, x2 = trs(x)\n",
        "        loss = model.loss(x1,x2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return loss_list\n",
        "\n",
        "\n",
        "# def ctrain(dataloader, model, loss_fn, optimizer, scheduler=None, verbose=True):\n",
        "def ctrain(dataloader, model, loss_fn, optimizer, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            x = model(x)\n",
        "        pred = model.classify(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if batch % (size//10) == 0:\n",
        "        # if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(x)\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x = model(x)\n",
        "            pred = model.classify(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            # predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOB1Kh3jL6YV"
      },
      "outputs": [],
      "source": [
        "# @title rnn train, gen\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred,_ = model(X)\n",
        "        loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss = loss.item()/ len(X)\n",
        "\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, bptt=32):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # hid = model.init_hidden(bptt)\n",
        "        hid = model.init_hidden(X.shape[0])\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(\"X.shape:\",X.shape) # [batch_size, seq_len]\n",
        "        Xs, ys = torch.split(X, bptt, dim=1), torch.split(y, bptt, dim=1)\n",
        "        for (X, y) in zip(Xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            optimizer.zero_grad()\n",
        "            # print(\"X.shape:\",X.shape) # [batch_size, bptt]\n",
        "            pred, hid = model(X, hid)\n",
        "            loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.flatten())\n",
        "            # loss = loss_fn(pred.flatten(0,1), y.flatten())\n",
        "            # loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            hid = hid.detach()\n",
        "\n",
        "        train_loss = loss.item()/ len(X)\n",
        "        try: wandb.log({\"train loss\": train_loss})\n",
        "        except: pass\n",
        "        # if batch % 100 == 0:\n",
        "        #     loss, current = loss.item(), batch * len(X)\n",
        "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "def generate(model, context, max_steps = 64, temperature=1):\n",
        "    # x = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    x=ix = torch.tensor([train_dataset.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "    model.eval()\n",
        "    hidden=None\n",
        "    with torch.no_grad():\n",
        "        for n in range(max_steps):\n",
        "            # output, hidden = model(x, hidden)\n",
        "            output, hidden = model(ix, hidden)\n",
        "            hidden=hidden[:, -1, :].unsqueeze(1)\n",
        "            output = output[:, -1, :] # get logit for last character\n",
        "            output = output/temperature\n",
        "            output = F.softmax(output, dim = -1) # vocab_size to char\n",
        "            ix = torch.multinomial(output, num_samples = 1) # rand sample by output distribution\n",
        "            x = torch.cat((x, ix),1)\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in x.flatten()])\n",
        "        return completion\n",
        "\n",
        "# out=generate(model, \"A wi\")\n",
        "# print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HcOidvtW9KAH"
      },
      "outputs": [],
      "source": [
        "# @title from RNN2\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None): # [batch_size, seq_len, input_size]\n",
        "        if h0 is None: h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        if c0 is None: c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        out, h0 = self.rnn(x, h0)\n",
        "        # out, (h0,c0) = self.lstm(x, (h0,c0))\n",
        "        # out:(batch_size, seq_length, hidden_size) (n, 28, 128)\n",
        "        out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "d_model,dim_a,dim_z = 256,3,1\n",
        "pred = nn.Sequential(\n",
        "    nn.Linear(d_model+dim_a+dim_z, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "    nn.Linear(d_model, d_model),\n",
        "    )\n",
        "gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "\n",
        "print(sum(p.numel() for p in pred.parameters() if p.requires_grad)) # 264192\n",
        "print(sum(p.numel() for p in gru.parameters() if p.requires_grad)) # 397824\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKAELerd8MuR"
      },
      "outputs": [],
      "source": [
        "# @title simulate\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# history = []\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "buffer = []\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    state = transform(state).unsqueeze(0)\n",
        "    action = agent(state) # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    state, reward, done, info = env.step(action) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "    # print(action.item(), reward)\n",
        "    out.write(state)\n",
        "    if done:\n",
        "        buffer.append((state, action, reward-100))\n",
        "        break\n",
        "    buffer.append((state, action, reward))\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9OFjAK232GNp"
      },
      "outputs": [],
      "source": [
        "# @title mha\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class MHAme(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None): # [batch_size, seq_len, d_model]\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2) # [batch_size, n_heads, seq_len, head_dim]\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # [batch_size, n_heads, seq_len, seq_len]\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        # x = torch.matmul(self.drop(attention), V)\n",
        "        x = self.drop(attention) @ V # [batch_size, n_heads, seq_len, head_dim]\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model) # [batch_size, seq_len, d_model]\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "# @title test mha\n",
        "# import torch\n",
        "# batch_size=3\n",
        "# L=5\n",
        "# d_model=8\n",
        "# n_heads=2\n",
        "\n",
        "# trg = torch.rand(batch_size,L, d_model)\n",
        "# src = torch.rand(batch_size,L, d_model)\n",
        "\n",
        "# mha = MultiHeadAttention(d_model, n_heads)\n",
        "# x, attn = mha(trg,src,src)\n",
        "\n",
        "# head_dim = d_model // n_heads\n",
        "\n",
        "# # trg1=trg.view(batch_size, -1, n_heads, head_dim).transpose(1, 2)\n",
        "# trg=trg.view(batch_size, n_heads, -1, head_dim)\n",
        "# src=src.view(batch_size, n_heads, -1, head_dim)\n",
        "# # print(trg1)\n",
        "# # print(\"##########\")\n",
        "# # print(trg2)\n",
        "# attn = trg @ src.transpose(2, 3)\n",
        "# x=attn@trg\n",
        "# print(x.shape)\n",
        "# print(attn.shape)\n",
        "\n",
        "# # trg1=trg1.view(batch_size,L, d_model)\n",
        "# trg1=trg1.reshape(batch_size,L, d_model)\n",
        "# trg2=trg2.view(batch_size,L, d_model)\n",
        "# print(trg1)\n",
        "# print(\"##########\")\n",
        "# print(trg2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TA_rcOQQTxan"
      },
      "outputs": [],
      "source": [
        "# @title simulate save\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "\n",
        "# print(env.action_space)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "    action = agent(state)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    # print(state.shape) # 0-255 (64, 64, 3)\n",
        "    print(action, reward, done)\n",
        "    out.write(state)\n",
        "\n",
        "    # break\n",
        "    if done:\n",
        "        break\n",
        "env.close()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-_r1P15L9Um",
        "outputId": "6c79ab20-46bb-4299-c26b-0a27e138c717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2775104\n",
            "2362625\n",
            "torch.Size([4, 256])\n",
            "torch.Size([4, 1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# @title autoencoder\n",
        "\n",
        "class autoencoder(torch.nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.enc = get_res(d_model)\n",
        "        # self.enc.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 1, 1, 1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        # self.enc = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        #     nn.Conv2d(3, d_model, 3, 2, 1), nn.ReLU(), # 256\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     nn.Conv2d(d_model, d_model, 3, 2, 1), nn.ReLU(),\n",
        "        #     # nn.Conv2d(d_model, 3, 3, 1, 1), nn.ReLU(), # 32\n",
        "        # )\n",
        "        self.deconv = Deconv(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x): return self.enc(x).squeeze()\n",
        "    # def decode(self, x): return self.deconv(x.unsqueeze(-1).unsqueeze(-1))\n",
        "    def decode(self, x): return self.deconv(x)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = autoencoder(256).to(device)\n",
        "print(sum(p.numel() for p in model.enc.parameters() if p.requires_grad)) # res 2775104, convpool 2951424, stride 2957315\n",
        "print(sum(p.numel() for p in model.deconv.parameters() if p.requires_grad)) # 2957315\n",
        "\n",
        "input = torch.rand((4,3,64,64), device=device)\n",
        "out = model.encode(input)\n",
        "print(out.shape)\n",
        "i2= model.decode(out)\n",
        "print(i2.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzzjgoXCnhT7"
      },
      "outputs": [],
      "source": [
        "# @title train autoencoder\n",
        "# print(train_data.data)\n",
        "# sar=train_data.data\n",
        "# state, action, reward = zip(*sar)\n",
        "\n",
        "# loader = DataLoader(state, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-4, (0.9, 0.95)) # lr = 1e-4 #3e-4\n",
        "optim = torch.optim.AdamW(model.parameters(), 3e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "def train(dataloader, model, optimizer, scheduler=None, verbose=True):\n",
        "    size = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, state in enumerate(dataloader):\n",
        "        state = state.to(device)\n",
        "        # sx_ = agent.jepa.enc(state)\n",
        "        # state_ = agent.conv(sx_)\n",
        "        state_ = model(state)\n",
        "        loss = F.mse_loss(state_, state)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "        if batch % (size//10) == 0:\n",
        "            loss, current = loss.item(), batch\n",
        "            if verbose: print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "    print(i)\n",
        "    train(train_loader,model,optim)\n",
        "    state = buffer[7][80][0]\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    state = transform(state).unsqueeze(0).to(device)[0]\n",
        "    sx_ = model.encode(state.unsqueeze(0))\n",
        "    out= model.decode(sx_)\n",
        "    imshow(state.detach().cpu())\n",
        "    imshow(out.detach().cpu())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQf-rtGL1q1W",
        "outputId": "3586547e-37cc-4514-caab-e92d7354bd0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.039520263671875\n"
          ]
        }
      ],
      "source": [
        "# @title text E norm (d/3)^(1/2)\n",
        "# a=torch.rand(16, 1, 256)\n",
        "# b=torch.tensor([])\n",
        "# c=torch.cat((a,b),dim=1)\n",
        "\n",
        "# a=torch.rand(16, 1, 1)\n",
        "# b=torch.rand(16, 1, 256)\n",
        "# # c=torch.bmm(a,b)\n",
        "# c=a@b\n",
        "# print(c.shape)\n",
        "\n",
        "d=16\n",
        "# a=torch.rand(d)/(d/3)**(1/2)\n",
        "# a=torch.rand(d)*2-1\n",
        "# # a=torch.rand(d,d)\n",
        "# print(a)\n",
        "# print(a.norm().item())\n",
        "\n",
        "# w=torch.rand(d,d)*2-1\n",
        "# w=(torch.rand(d,d)*2-1)*(3**0.5)/d\n",
        "# print(w)\n",
        "w = F.normalize(w)\n",
        "k,v = torch.rand(1,d), torch.rand(1,d)\n",
        "k,v = k*2-1, v*2-1\n",
        "# k,v = F.normalize(k), F.normalize(v)\n",
        "# print(k)\n",
        "# print(k.T@v)\n",
        "# print(k@v.T)\n",
        "print((k.T@v).norm().item())\n",
        "# print(w.norm().item())\n",
        "# print(w[0].norm().item())\n",
        "# print(w[:,0].norm().item())\n",
        "# print((w@k.T).norm().item())\n",
        "\n",
        "# (d/3)^(1/2) # E norm of dim d vec [0-1] or [-1-1]\n",
        "# print(4/(3**0.5))\n",
        "# k@v.T d/4 [0-1], 0 [-1-1],\n",
        "# w norm: d^2 a^2 = print(16/(3**0.5))\n",
        "\n",
        "# int int ab db da = int [1/2 a b^2] da = int 1/2 a da =\n",
        "# 1/4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ErwMF9NijD17"
      },
      "outputs": [],
      "source": [
        "# @title 514\n",
        "n=100\n",
        "a=torch.linspace(n,0,n)\n",
        "i=0\n",
        "o=0\n",
        "# oo=[]\n",
        "while True:\n",
        "    m = torch.randint(0, n, (1,))\n",
        "    a[m] = i\n",
        "    o_=i-a.min()\n",
        "    oo.append(o_.item())\n",
        "    print(sum(oo)/len(oo))\n",
        "    i+=1\n",
        "# 514?\n",
        "# p=1.064422028?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUCet57LcPdf"
      },
      "outputs": [],
      "source": [
        "n=100\n",
        "tt=0\n",
        "a=1+1/(n*(n-1))\n",
        "print(a)\n",
        "for i in range(n-1):\n",
        "    a=(1+ 1/(n-i))*a\n",
        "    print(a)\n",
        "    tt+=a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hEUffQ24mkRY"
      },
      "outputs": [],
      "source": [
        "# @title augmentations\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/augmentations.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "class TrainTransform(object):\n",
        "    def __init__(self):\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=1.0),\n",
        "                # transforms.RandomSolarize(threshold=130, p=0.0)\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        self.transform_prime = transforms.Compose([\n",
        "                transforms.RandomResizedCrop(32, interpolation=InterpolationMode.BICUBIC),#224\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)), # clamp else ColorJitter will return nan https://discuss.pytorch.org/t/input-is-nan-after-transformation/125455/6\n",
        "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)], p=0.8,),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),], p=0.1),\n",
        "                # transforms.RandomSolarize(threshold=130/255, p=0.2) # og threshold=130, /255 bec after normalising\n",
        "                transforms.RandomSolarize(threshold=.9, p=0.2),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "                # transforms.RandomPerspective(distortion_scale=0.3, p=0.5), # me\n",
        "                # transforms.RandomErasing(p=0.5, scale=(0.1, 0.11), ratio=(1,1), value=0, inplace=True), # default p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False\n",
        "        # dims = len(sample.shape)\n",
        "        # if dims==3: x1 = self.transform(sample) # same transforms per minibatch\n",
        "        # elif dims==4: x1 = transforms.Lambda(lambda x: torch.stack([self.transform(x_) for x_ in x]))(sample) # diff transforms per img in minibatch\n",
        "    def __call__(self, sample):\n",
        "        x1 = self.transform(sample)\n",
        "        x2 = self.transform_prime(sample)\n",
        "        return x1, x2\n",
        "\n",
        "trs=TrainTransform()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r0mXVAUnVYX-"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        # nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        # nn.Linear(512, dim_embd, bias=None),\n",
        "        # nn.Softmax(dim=1),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# print(get_res(256).to(device))\n",
        "# model = get_res(256).to(device)\n",
        "# input = torch.rand(16,3,64,64)\n",
        "# input = torch.rand(16,1,256,256)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AodVas3L4ZS",
        "outputId": "f1940ab6-b72d-4c8d-f97d-6d876f1b92e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256])\n",
            "136960\n",
            "136960\n"
          ]
        }
      ],
      "source": [
        "# @title efficientnet\n",
        "# https://arxiv.org/pdf/2207.10318 # visualise kernal\n",
        "\n",
        "# https://pytorch.org/hub/research-models\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/shufflenetv2.py\n",
        "\n",
        "import torch\n",
        "# https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py\n",
        "from torchvision.models.efficientnet import *\n",
        "from torchvision.models import efficientnet\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # https://arxiv.org/pdf/2104.00298\n",
        "# Stage Operator Stride #Channels #Layers\n",
        "# 0 Conv3x3 2 24 1\n",
        "# 1 Fused-MBConv1, k3x3 1 24 2\n",
        "# 2 Fused-MBConv4, k3x3 2 48 4\n",
        "# 3 Fused-MBConv4, k3x3 2 64 4\n",
        "# 4 MBConv4, k3x3, SE0.25 2 128 6\n",
        "# 5 MBConv6, k3x3, SE0.25 1 160 9\n",
        "# 6 MBConv6, k3x3, SE0.25 2 256 15\n",
        "# 7 Conv1x1 & Pooling & FC - 1280 1\n",
        "\n",
        "# # elif arch.startswith(\"efficientnet_v2_s\"):\n",
        "# inverted_residual_setting = [\n",
        "#     FusedMBConvConfig(1, 3, 1, 24, 24, 2),\n",
        "#     FusedMBConvConfig(4, 3, 2, 24, 48, 4),\n",
        "#     FusedMBConvConfig(4, 3, 2, 48, 64, 4),\n",
        "#     MBConvConfig(4, 3, 2, 64, 128, 6),\n",
        "#     MBConvConfig(6, 3, 1, 128, 160, 9),\n",
        "#     MBConvConfig(6, 3, 2, 160, 256, 15),\n",
        "# ]\n",
        "# last_channel = 1280\n",
        "\n",
        "# d_list=[24, 48, 64, 128, 160, 256] #\n",
        "d_list=[16, 32, 48, 96, 108, 172] #\n",
        "inverted_residual_setting = [\n",
        "    efficientnet.FusedMBConvConfig(1, 3, 1, d_list[0], d_list[0], 2),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[0], d_list[1], 4),\n",
        "    efficientnet.FusedMBConvConfig(4, 3, 2, d_list[1], d_list[2], 4),\n",
        "    efficientnet.MBConvConfig(4, 3, 2, d_list[2], d_list[3], 6),\n",
        "    efficientnet.MBConvConfig(6, 3, 1, d_list[3], d_list[4], 9),\n",
        "    efficientnet.MBConvConfig(6, 3, 2, d_list[4], d_list[5], 15),\n",
        "]\n",
        "last_channel = 512\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "effnet = EfficientNet(inverted_residual_setting, dropout=0.1, last_channel=last_channel, num_classes=256)\n",
        "effnet.features = efficientnet.Conv2dNormActivation(1, last_channel, kernel_size=3, stride=2, norm_layer=partial(nn.BatchNorm2d, eps=1e-03), activation_layer=nn.SiLU)\n",
        "\n",
        "#   (features): Sequential(\n",
        "#     (0): Conv2dNormActivation(\n",
        "#       (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "#       (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "#       (2): SiLU(inplace=True)\n",
        "\n",
        "input = torch.rand((1,1,256,256), device=device)\n",
        "out = effnet(input)\n",
        "print(out.shape)\n",
        "# print(effnet)\n",
        "print(sum(p.numel() for p in effnet.parameters() if p.requires_grad)) #\n",
        "print(sum(p.numel() for p in effnet.parameters())) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V15LtR8myLL9",
        "outputId": "cebfa4c2-53bf-4353-9765-520fe0f561c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# @title vicreg next\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn, update_bn # https://pytorch.org/docs/stable/optim.html#putting-it-all-together-ema\n",
        "\n",
        "\n",
        "def off_diagonal(x):\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
        "\n",
        "# https://github.com/facebookresearch/vicreg/blob/main/resnet.py\n",
        "class VICReg(nn.Module):\n",
        "    def __init__(self, dim_embd=256, ema=False):\n",
        "        super().__init__()\n",
        "        self.conv = get_res(dim_embd=dim_embd)\n",
        "\n",
        "        # f=[dim_embd,1024,1024,1024]\n",
        "        # f=[dim_embd,512,512,512]\n",
        "        f=[dim_embd,256,256,256]\n",
        "        self.exp = nn.Sequential(\n",
        "            nn.Linear(f[0], f[1]), nn.BatchNorm1d(f[1]), nn.ReLU(),\n",
        "            nn.Linear(f[1], f[2]), nn.BatchNorm1d(f[2]), nn.ReLU(),\n",
        "            nn.Linear(f[-2], f[-1], bias=False)\n",
        "            )\n",
        "        self.ema = ema\n",
        "        if ema:\n",
        "            self.conv_ema = AveragedModel(self.conv, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "            self.exp_ema = AveragedModel(self.exp, multi_avg_fn=get_ema_multi_avg_fn(0.999))\n",
        "\n",
        "    # https://arxiv.org/pdf/2105.04906.pdf\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y) # s(Z, Z')\n",
        "\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001) #ϵ=0.0001\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        batch_size=x.size(dim=0)\n",
        "        num_features=32\n",
        "        sim_coeff=10.0 # 25.0 # λ\n",
        "        std_coeff=10.0 # 25.0 # µ\n",
        "        cov_coeff=1.0 # 1.0 # ν\n",
        "\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        if y.dim() == 1: y = y.unsqueeze(0)\n",
        "\n",
        "        # # covariance loss\n",
        "        cov_x = (x.T @ x) / (batch_size - 1) #C(Z)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features) #c(Z)\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        print(\"in vicreg \",(sim_coeff * repr_loss).item() , (std_coeff * std_loss).item() , (cov_coeff * cov_loss).item())\n",
        "        return loss\n",
        "\n",
        "    def loss(self, sx, sy):\n",
        "        sx = self.forward(sx)\n",
        "        sy = self.forward(sy)\n",
        "        with torch.no_grad(): # target encoder is ema\n",
        "            sy = self.conv_ema(sy)\n",
        "            vy = self.exp_ema(sy)\n",
        "        vx = self.exp(sx)\n",
        "        vy = self.exp(sy)\n",
        "        loss = self.vicreg(vx,vy)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "model = VICReg().to(device) # create an instance and move it to device (cache?)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-nT5j864BIn",
        "outputId": "ac676107-a22d-4315-a3c7-785e3c6456c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# @title simulate 512\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "def simulate(agent, buffer=[]):\n",
        "    out = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*'DIVX'), 20, (64,64))\n",
        "    state = env.reset()\n",
        "    episode=[]\n",
        "    while True:\n",
        "    # while not done:\n",
        "        # state = transform(state).unsqueeze(0).to(device)\n",
        "        # action = agent(state).cpu() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        # state, reward, done, info = env.step(action[0]) # np(64,64,3) 0.0 False {'prev_level_seed': 736820912, 'prev_level_complete': 0, 'level_seed': 736820912, 'rgb': array([[[  0, 125, 222], ...)\n",
        "        action = env.action_space.sample() # https://github.com/openai/procgen/blob/master/procgen/env.py#L155\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # print(i, 'act: ',action.item(), 'reward: ',reward)\n",
        "        out.write(state)\n",
        "        if done:\n",
        "            episode.append((state, action, -1))\n",
        "            # print(\"ded\")\n",
        "            break\n",
        "        episode.append((state, action, 0))\n",
        "    # print('time')\n",
        "    env.close()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    buffer.append(episode)\n",
        "    return buffer\n",
        "\n",
        "# buffer = simulate(agent, buffer)\n",
        "# _=simulate(agent)\n",
        "\n",
        "buffer=[]\n",
        "for i in range(512):\n",
        "    buffer = simulate(agent, buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ko5qJO7Et09L"
      },
      "outputs": [],
      "source": [
        "# @title vector quantize\n",
        "# https://github.com/lucidrains/vector-quantize-pytorch?tab=readme-ov-file#finite-scalar-quantization\n",
        "# !pip install -qq vector-quantize-pytorch\n",
        "\n",
        "import torch\n",
        "from vector_quantize_pytorch import FSQ\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "quantizer = FSQ(levels = [3,3,2]).to(device) # https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/finite_scalar_quantization.py\n",
        "\n",
        "# # x = torch.randn(1, 1024, 3) # last dim is num levels\n",
        "# x = torch.randn(4, 256, 3)*3 -1.5 # [batch_size, T, num_levels]\n",
        "# xhat, indices = quantizer(x) # [batch_size, T, num_levels], [batch_size, T]\n",
        "# # print(xhat[0])\n",
        "# # print(indices[0])\n",
        "\n",
        "# # assert torch.all(xhat == quantizer.indices_to_codes(indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LK5u500Vad2P"
      },
      "outputs": [],
      "source": [
        "# @title FSQ jax\n",
        "# https://github.com/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "import itertools\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "Codeword = jax.Array\n",
        "Indices = jax.Array\n",
        "\n",
        "def round_ste(z):\n",
        "  \"\"\"Round with straight through gradients.\"\"\"\n",
        "  zhat = jnp.round(z)\n",
        "  return z + jax.lax.stop_gradient(zhat - z)\n",
        "\n",
        "class FSQ:\n",
        "  \"\"\"Quantizer.\"\"\"\n",
        "  def __init__(self, levels: list[int], eps: float = 1e-3):\n",
        "    self._levels = levels\n",
        "    self._eps = eps\n",
        "    self._levels_np = np.asarray(levels)\n",
        "    self._basis = np.concatenate(([1], np.cumprod(self._levels_np[:-1]))).astype(np.uint32)\n",
        "    self._implicit_codebook = self.indexes_to_codes(np.arange(self.codebook_size))\n",
        "    print(\"self._basis\",self._basis)\n",
        "    print(\"self._implicit_codebook\",self._implicit_codebook)\n",
        "\n",
        "  @property\n",
        "  def num_dimensions(self):\n",
        "    \"\"\"Number of dimensions expected from inputs.\"\"\"\n",
        "    return len(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook_size(self):\n",
        "    \"\"\"Size of the codebook.\"\"\"\n",
        "    return np.prod(self._levels)\n",
        "\n",
        "  @property\n",
        "  def codebook(self):\n",
        "    \"\"\"Returns the implicit codebook. Shape (prod(levels), num_dimensions).\"\"\"\n",
        "    return self._implicit_codebook\n",
        "\n",
        "  def bound(self, z: jax.Array) -> jax.Array:\n",
        "    \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "    half_l = (self._levels_np - 1) * (1 - self._eps) / 2\n",
        "    offset = jnp.where(self._levels_np % 2 == 1, 0.0, 0.5)\n",
        "    shift = jnp.tan(offset / half_l)\n",
        "    return jnp.tanh(z + shift) * half_l - offset\n",
        "\n",
        "  def quantize(self, z: jax.Array) -> Codeword:\n",
        "    \"\"\"Quanitzes z, returns quantized zhat, same shape as z.\"\"\"\n",
        "    quantized = round_ste(self.bound(z))\n",
        "\n",
        "    # Renormalize to [-1, 1].\n",
        "    half_width = self._levels_np // 2\n",
        "    return quantized / half_width\n",
        "\n",
        "  def _scale_and_shift(self, zhat_normalized):\n",
        "    # Scale and shift to range [0, ..., L-1]\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "  def _scale_and_shift_inverse(self, zhat):\n",
        "    half_width = self._levels_np // 2\n",
        "    return (zhat - half_width) / half_width\n",
        "\n",
        "  def codes_to_indexes(self, zhat: Codeword) -> Indices:\n",
        "    \"\"\"Converts a `code` to an index in the codebook.\"\"\"\n",
        "    assert zhat.shape[-1] == self.num_dimensions\n",
        "    zhat = self._scale_and_shift(zhat)\n",
        "    return (zhat * self._basis).sum(axis=-1).astype(jnp.uint32)\n",
        "\n",
        "  def indexes_to_codes(self, indices: Indices) -> Codeword:\n",
        "    \"\"\"Inverse of `indexes_to_codes`.\"\"\"\n",
        "    indices = indices[..., jnp.newaxis]\n",
        "    print(indices, self._basis, self._levels_np)\n",
        "    print(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    codes_non_centered = np.mod(np.floor_divide(indices, self._basis), self._levels_np)\n",
        "    return self._scale_and_shift_inverse(codes_non_centered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xHxv7ptuwVHX"
      },
      "outputs": [],
      "source": [
        "# @title FSQ torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def ste_round(x): return x.round().detach() + x - x.detach()\n",
        "\n",
        "class FSQ(nn.Module): # https://colab.research.google.com/github/google-research/google-research/blob/master/fsq/fsq.ipynb\n",
        "    def __init__(self, levels, eps = 1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.levels = torch.tensor(levels, device=device)\n",
        "        self.basis = torch.cat([torch.ones(1, device=device), torch.cumprod(self.levels[:-1], dim=0)]).long()\n",
        "        self.num_dimensions = len(levels)\n",
        "        self.codebook_size = torch.prod(self.levels).item()\n",
        "        self.codebook = self.indexes_to_codes(torch.arange(self.codebook_size, device=device))\n",
        "        # self.mean = self.codebook.mean(dim=0)\n",
        "        # self.max = self.codebook.max(dim=0).values\n",
        "        # self.min = self.codebook.min(dim=0).values\n",
        "\n",
        "    def bound(self, z):\n",
        "        \"\"\"Bound `z`, an array of shape (..., d).\"\"\"\n",
        "        half_l = (self.levels - 1) * (1 - self.eps) / 2 # [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "        # half_l = (self.levels-1)/2 # me ?\n",
        "        offset = torch.where(self.levels % 2 == 1, 0.0, 0.5) # [0.0000, 0.0000, 0.5000] mean?\n",
        "        # print(\"half_l\", half_l)\n",
        "        # shift = torch.tan(offset / half_l) # [0.0000, 0.0000, 1.5608] < tan(1)\n",
        "\n",
        "        # print(\"shift\", shift)\n",
        "        # print(\"bound\", torch.tanh(z + shift) * half_l - offset)\n",
        "\n",
        "        # print(f'half_l {half_l}, shift {shift}, bound {torch.tanh(z + shift) * half_l - offset}')\n",
        "        # return torch.tanh(z + shift) * half_l - offset\n",
        "        # return torch.tanh(z - shift) * half_l + offset\n",
        "        return torch.tanh(z) * half_l + offset\n",
        "\n",
        "    def forward(self, z):\n",
        "        quantized = ste_round(self.bound(z))\n",
        "        # print(\"quantized\", quantized)\n",
        "        half_width = self.levels // 2 # Renormalize to [-1, 1]\n",
        "        return quantized / half_width\n",
        "\n",
        "    def _scale_and_shift(self, zhat_normalized): # Scale and shift to range [0, ..., L-1]\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat_normalized * half_width) + half_width\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat):\n",
        "        half_width = self.levels // 2\n",
        "        return (zhat - half_width) / half_width\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        assert zhat.shape[-1] == self.num_dimensions\n",
        "        zhat = self._scale_and_shift(zhat)\n",
        "        return (zhat * self.basis).sum(axis=-1).long()\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        indices = indices.unsqueeze(-1)\n",
        "        codes_non_centered = torch.fmod(indices // self.basis, self.levels)\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n",
        "\n",
        "fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "# print(fsq.codebook)\n",
        "\n",
        "# batch_size, seq_len = 1, 1\n",
        "# x = torch.rand((batch_size, seq_len,3),device=device)\n",
        "\n",
        "# la = fsq(x)\n",
        "# print(la)\n",
        "# lact = fsq.codes_to_indexes(la)\n",
        "# print(lact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SnfcKPses5X",
        "outputId": "7c50a3e3-281a-4375-b86f-ece58f6775c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "half_l tensor([0.9990, 0.9990, 0.4995]), shift tensor([0.0000, 0.0000, 1.5608]), bound tensor([-0.4617,  0.5365, -0.0515])\n",
            "quantized tensor([0., 1., 0.])\n",
            "tensor([0., 1., 0.])\n"
          ]
        }
      ],
      "source": [
        "# @title test fsq\n",
        "fsq = FSQ(levels = [4])\n",
        "\n",
        "# 2: 1.6 half_l tensor([0.4995]), shift tensor([1.5608]), bound tensor([-0.5195])\n",
        "# 3: 0.6 # half_l tensor([0.9990]), shift tensor([0.]), bound tensor([-0.9207])\n",
        "# 4: 0.4, 1.3 # half_l tensor([1.4985]), shift tensor([0.3466]), bound tensor([-1.7726])\n",
        "# 5: 0.5, 1 # half_l [1.9980], shift [0.], bound [-1.8415]\n",
        "x = torch.tensor([.9],device=device)\n",
        "# x = torch.tensor([-1.6],device=device)\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "\n",
        "\n",
        "x = torch.tensor([-0.6,0.6,-1.6],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,-1.5],device=device)\n",
        "# x = torch.tensor([-0.6,0.6,1.6],device=device)\n",
        "x = torch.tensor([-0.5,0.6,-0.1],device=device)\n",
        "\n",
        "la = fsq(x)\n",
        "print(la)\n",
        "\n",
        "round emb\n",
        "\n",
        "# half_l [0.9990, 0.9990, 0.4995] < 1,1,0.5\n",
        "# offset [0.0000, 0.0000, 0.5000] mean?\n",
        "# shift [0.0000, 0.0000, 1.5608] torch.tan(offset / half_l)\n",
        "# bound [-0.5365,  0.5365, -0.4696] tanh(z + shift) * half_l - offset\n",
        "\n",
        "\n",
        "\n",
        "levels = torch.tensor([3,3,2])\n",
        "eps = 1e-3\n",
        "\n",
        "half_l = (levels - 1) * (1 - eps) / 2\n",
        "offset = torch.where(levels % 2 == 1, 0.0, 0.5)\n",
        "# print(\"half_l\", half_l)\n",
        "shift = torch.tan(offset / half_l)\n",
        "# print(\"shift\", shift)\n",
        "# print(\"bound\", torch.tanh(x + shift) * half_l - offset)\n",
        "# return torch.tanh(x + shift) * half_l - offset\n",
        "out = torch.tanh(x) * half_l + offset\n",
        "print(out)\n",
        "\n",
        "shift=torch.tan(torch.tensor([1.]))\n",
        "print(shift)\n",
        "bound = torch.tanh(x - shift)\n",
        "print(bound)\n",
        "\n",
        "print(torch.tanh(torch.tensor([0.])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z_VgsenYLpM",
        "outputId": "8c7b23ae-8cdb-4846-dae3-32fd046a4d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[0.0437, 0.3097, 0.4537]]], requires_grad=True)\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n",
            "tensor([[[-0.5242, -0.2421,  0.2851]]])\n"
          ]
        }
      ],
      "source": [
        "# @title test rnn_pred symlog\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "# fsq = FSQ(levels = [3,3,2])\n",
        "\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # xx = fsq(x)\n",
        "    # xx = fsq(x.clone())\n",
        "    # print(xx)\n",
        "    # x = torch.tanh(x)\n",
        "    # loss = x.sum()\n",
        "    # loss = model(xx)\n",
        "    loss = model(x)\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    # x = torch.clamp(x, min=-1, max=1)\n",
        "    # x = torch.clamp(x.clone(), min=-1, max=1)\n",
        "    with torch.no_grad():\n",
        "        # x.clamp_(min=-1, max=1)\n",
        "        x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# # model = nn.Sequential(nn.Linear(3,1))\n",
        "# model = nn.Sequential(nn.Linear(3*2,1))\n",
        "# device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# batch_size = 1\n",
        "# seq_len = 3\n",
        "# x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# # torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "\n",
        "# def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "#     batch, seq_len, dim_a = la.shape\n",
        "#     cost = 0\n",
        "#     lsx=sx\n",
        "#     for t in range(seq_len): # simple single layer\n",
        "#         a = la[:,t] # [1, dim_a]\n",
        "#         sxaz = torch.cat([sx, a], dim=-1)\n",
        "#         # with torch.amp.autocast('cuda'):\n",
        "#         cost = cost + model(sxaz)\n",
        "#         lsx = torch.cat([lsx, sx], dim=0)\n",
        "#     return cost, sx\n",
        "\n",
        "\n",
        "# # def ste_clamp(input, min=-1, max=1):\n",
        "# #     clamped_output = torch.clamp(input, min, max)\n",
        "# #     clamp_mask = (input < min) | (input > max)\n",
        "# #     return torch.where(clamp_mask, input, clamped_output)\n",
        "\n",
        "# def ste_clamp(x, min=-1, max=1):\n",
        "#     return torch.clamp(x, min, max).detach() + x - x.detach()\n",
        "\n",
        "# def ste_abs(x): return x.sign() * x\n",
        "# def symlog(x): return torch.sign(x) * torch.log(ste_abs(x) + 1.0)\n",
        "# def symexp(x): return torch.sign(x) * torch.exp(ste_abs(x) - 1.0)\n",
        "\n",
        "\n",
        "# sx = torch.rand((batch_size,3),device=device)\n",
        "# sx_ = sx.detach()\n",
        "# for i in range(10): # num epochs\n",
        "#     # la = fsq(x.clone())\n",
        "#     la = fsq(x)\n",
        "#     print(i)\n",
        "#     print(x,x.requires_grad)\n",
        "#     print(la,la.requires_grad)\n",
        "#     loss, sx_ = rnn_pred(sx_, la)\n",
        "#     # loss.backward()\n",
        "#     loss.backward(retain_graph=True) # retain_graph bec fsq got tanh that creates new graph?\n",
        "#     optim.step()\n",
        "#     optim.zero_grad()\n",
        "#     # x = torch.tanh(x)\n",
        "#     # x = torch.clamp(x, min=-1, max=1)\n",
        "#     # x = ste_clamp(x.clone(), min=-1, max=1)\n",
        "#     # x = symlog(x.clone())\n",
        "#     # sx_ = sx_.detach()\n",
        "\n",
        "\n",
        "# # print(xx)\n",
        "# print(x)\n",
        "# # print(xhat)\n",
        "# print(la)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YWmwVYhVVh5R"
      },
      "outputs": [],
      "source": [
        "# @title test ste_argmin\n",
        "import torch\n",
        "emb = torch.nn.Embedding(15, 3) # env.action_space # 15\n",
        "x = torch.rand(1,3)\n",
        "\n",
        "# def ste_argmin(x, dim=-1):\n",
        "#     idx = torch.argmin(x, dim)\n",
        "#     # out = torch.zeros_like(x)\n",
        "#     out = torch.zeros_like(idx).unsqueeze(-1)\n",
        "#     print(idx.shape, out.shape)\n",
        "#     out.scatter_(1, idx, 1)\n",
        "#     return out\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# def softargmin(x, beta=10):\n",
        "#     # Apply softmax to the negative of the input to approximate argmin\n",
        "#     weights = F.softmax(-x * beta, dim=-1)\n",
        "#     indices = torch.arange(x.size(-1), dtype=x.dtype, device=x.device)\n",
        "#     soft_argmin = torch.sum(weights * indices, dim=-1)\n",
        "#     return soft_argmin\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def ste_argmax(x, temp=1.0):\n",
        "    x_soft = F.softmax(x / temp, dim=-1)\n",
        "    x_hard = torch.zeros_like(x)\n",
        "    x_hard.scatter_(-1, x_soft.argmax(dim=-1, keepdim=True), 1.0)\n",
        "    x_hard = (x_hard - x_soft).detach() + x_soft\n",
        "    return x_hard # one hot\n",
        "# out = differentiable_argmax(-x)\n",
        "# print(out)\n",
        "\n",
        "\n",
        "# def softargmax1d(input, beta=100): # https://github.com/david-wb/softargmax/blob/master/softargmax.py\n",
        "#     *_, n = input.shape\n",
        "#     input = nn.functional.softmin(beta * input, dim=-1)\n",
        "#     indices = torch.linspace(0, 1, n)\n",
        "#     result = torch.sum((n - 1) * input * indices, dim=-1)\n",
        "#     return result\n",
        "\n",
        "# ste_round\n",
        "\n",
        "# # dist = torch.norm(self.emb.weight.data - x, dim=-1)\n",
        "# dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "# lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "# print(lact)\n",
        "\n",
        "device='cpu'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(3,1)).to(device)\n",
        "batch_size = 1\n",
        "seq_len = 1\n",
        "x = nn.Parameter(torch.rand((batch_size, seq_len,3),device=device)) # [batch_size, seq_len, FSQ 3 levels]\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "optim = torch.optim.SGD([x], lr=1e0)\n",
        "\n",
        "pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "\n",
        "for i in range(5): # num epochs\n",
        "    print(x)\n",
        "    # dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "    dist = torch.norm(emb.weight.data[None,None,...] - x.unsqueeze(-2), dim=-1) # [1,1,act_space,emb_dim], [batch,T,1,emb_dim] -> [batch,T,act_space]\n",
        "    A=differentiable_argmax(-dist)\n",
        "    # print(A.shape)\n",
        "    print(torch.argmax(A))\n",
        "    x_=A@emb.weight.data\n",
        "    # print(\"dist\", dist.shape)\n",
        "    # lact = torch.argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = ste_argmin(dist, dim=-1) # [batch,T]\n",
        "    # lact = softargmin(dist)\n",
        "    # print(lact)\n",
        "    # x = emb.weight.data[lact]\n",
        "\n",
        "    # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "    print(\"x_\",x_)\n",
        "    # x = emb(x_)\n",
        "\n",
        "    loss = model(x_).sum()\n",
        "    loss.backward(retain_graph=True)\n",
        "    # loss.backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "x_ = torch.tensor([14])\n",
        "x = emb(x_)\n",
        "# print(x)\n",
        "# # print(emb.weight)\n",
        "# pseudo_inverse_weight = torch.pinverse(emb.weight)\n",
        "pseudo_inverse_weight = torch.linalg.pinv(emb.weight)\n",
        "# weight_inv = torch.pinverse(emb.weight.T)\n",
        "\n",
        "dist = torch.norm(emb.weight.data - x, dim=-1)\n",
        "# print(x@pseudo_inverse_weight)\n",
        "# A=differentiable_argmax(-x@pseudo_inverse_weight)\n",
        "A=differentiable_argmax(-dist)\n",
        "print(A)\n",
        "\n",
        "# print(pseudo_inverse_weight.shape, pseudo_inverse_weight)\n",
        "# # x_ = torch.matmul(x, pseudo_inverse_weight)\n",
        "# x_ = x@ pseudo_inverse_weight\n",
        "# print(\"x_\",x_)\n",
        "\n",
        "# print(emb.weight@ pseudo_inverse_weight)\n",
        "# dist=torch.dist(emb.weight@ pseudo_inverse_weight, torch.eye(15))\n",
        "# print(dist)\n",
        "# print(pseudo_inverse_weight@ emb.weight)\n",
        "\n",
        "# print(emb.weight@ weight_inv.T)\n",
        "# print(weight_inv.T@ emb.weight)\n",
        "\n",
        "# torch.linalg.lstsq(A, B).solution\n",
        "\n",
        "\n",
        "x_ = torch.tensor([4])\n",
        "embx = emb(x_) # emb.weight[x_,:]\n",
        "print(embx)\n",
        "\n",
        "Apinv = torch.linalg.pinv(A)\n",
        "x = embx@Apinv\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pGZld_gLH1RA"
      },
      "outputs": [],
      "source": [
        "# @title test bptt\n",
        "\n",
        "x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    loss=0\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        # loss = loss -stcost(xxx.clone()).sum()\n",
        "        loss = loss -stcost(xxx).sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "# RuntimeError: Output 1 of SplitBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size, T = 1,6\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device))\n",
        "# optim = torch.optim.SGD([x], lr=1e-3)\n",
        "# # xx = torch.split(x, bptt, dim=1)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    loss=0\n",
        "    # xx = torch.split(x, bptt, dim=1)\n",
        "    for xxx in xx:\n",
        "        # loss = -stcost(xxx).sum()\n",
        "        loss = loss -stcost(xxx.clone()).sum()\n",
        "        # loss = loss -stcost(xxx).sum()\n",
        "        # loss.backward()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    print(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "jepapred = nn.Sequential(nn.Linear(3*2,3))\n",
        "stcost = nn.Sequential(nn.Linear(3,1))\n",
        "\n",
        "def rnn_pred(sx, la, z=None, gamma=0.7): # 0.95 [1, d_model], [seq_len, dim_a/z]\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    lsx=sx\n",
        "    # print(\"rnn pred\",lsx[0][:5])\n",
        "    for t in range(seq_len): # simple single layer\n",
        "        # print(la.shape)\n",
        "        a = la[:,t,:].clone() # [1, dim_a]\n",
        "        # sxaz = torch.cat([sx, a], dim=-1)\n",
        "        sxaz = torch.cat([sx.clone(), a.clone()], dim=-1)\n",
        "        # sxaz = torch.cat([sx.clone(), a], dim=-1)\n",
        "        sx = jepapred(sxaz)\n",
        "        tcost = -stcost(sx).sum()\n",
        "        lsx = torch.cat([lsx, sx], dim=0)\n",
        "        # print(lsx.requires_grad, sx.requires_grad)\n",
        "        # icost = 0.5*icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(icost.requires_grad)\n",
        "        cost += tcost# + icost\n",
        "    return cost, sx#, z\n",
        "\n",
        "\n",
        "\n",
        "batch_size=4\n",
        "sx = torch.rand((batch_size,3),device=device)\n",
        "T = 6\n",
        "bptt = 3\n",
        "# x = nn.Parameter(torch.empty((batch_size, T, 3),device=device)) # FSQ 3 levels\n",
        "x = torch.empty((batch_size, T, 3),device=device) # FSQ 3 levels\n",
        "torch.nn.init.xavier_uniform_(x)\n",
        "# optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "# optim = torch.optim.SGD([x], lr=1e-3) #, maximize=True)\n",
        "# print(x.shape)\n",
        "# print(len(xx))\n",
        "# print(xx[0].shape)\n",
        "\n",
        "x = torch.rand((batch_size, T, 3),device=device)\n",
        "bptt=2\n",
        "xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "optim = torch.optim.SGD(xx, lr=1e-3)\n",
        "\n",
        "for _ in range(10): # num epochs\n",
        "    xx = torch.split(x, bptt, dim=1)\n",
        "    sx_ = sx.detach()\n",
        "    for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "\n",
        "        # xxx=x\n",
        "        # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "        la = fsq(xxx)\n",
        "        # la = xxx\n",
        "        # print(x,x.requires_grad)\n",
        "        # print(la,la.requires_grad)\n",
        "        # loss, sx_ = rnn_pred(sx_, la)\n",
        "        loss = -stcost(la).sum()\n",
        "\n",
        "        print(\"loss\",loss)\n",
        "        loss.backward()\n",
        "        # loss.backward(retain_graph=True)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # sx_ = sx_.detach()\n",
        "        # print(loss.item(), lact)\n",
        "\n",
        "    x = torch.cat(xx,dim=1)\n",
        "    x = torch.tanh(x) # clamp\n",
        "    print(x)\n",
        "    # print(x)\n",
        "print(\"search\",loss.item())\n",
        "# print(lact)\n",
        "# return la, lact # [batch_size, T]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # # def search(self, sx, T=256, bptt=32):\n",
        "    # def search(self, sx, T=None, bptt=None):\n",
        "    #     if T==None: T = 256\n",
        "    #     if bptt==None: bptt = min(T,3)\n",
        "    #     batch=sx.size(dim=0)\n",
        "    #     x = nn.Parameter(torch.empty((batch, T, 3),device=device)) # FSQ 3 levels\n",
        "    #     torch.nn.init.xavier_uniform_(x)\n",
        "    #     # optim = torch.optim.SGD([x], lr=1e5) #, maximize=True)\n",
        "    #     # xx = torch.split(x, bptt, dim=1)\n",
        "    #     xx = [nn.Parameter(xxx) for xxx in torch.split(x, bptt, dim=1)]\n",
        "    #     optim = torch.optim.SGD(xx, lr=1e7) #, maximize=True)\n",
        "\n",
        "    #     for _ in range(10): # num epochs\n",
        "    #         sx_ = sx.detach()\n",
        "    #         for xxx in xx: # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "    #             # la, lact = quantizer(x) # xhat, indices [batch_size, T, num_levels], [batch_size, T]\n",
        "    #             la = fsq(xxx.clone())\n",
        "    #             # print(x,x.requires_grad)\n",
        "    #             # print(la,la.requires_grad)\n",
        "    #             loss, sx_ = self.rnn_pred(sx_, la)\n",
        "    #             loss.backward(retain_graph=True)\n",
        "    #             optim.step()\n",
        "    #             optim.zero_grad()\n",
        "    #             sx_ = sx_.detach()\n",
        "    #             # print(loss.item(), lact)\n",
        "    #             # xx = torch.tanh(xx) # clamp\n",
        "    #         xx = [torch.tanh(xxx) for xxx in xx]\n",
        "    #         x = torch.cat(xx,dim=1)\n",
        "    #         # x = torch.tanh(x) # clamp\n",
        "    #         print(x)\n",
        "    #     print(\"search\",loss.item())\n",
        "    #     # print(lact)\n",
        "    #     return la, lact # [batch_size, T]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx0k_ndHOEMe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title visualise kernels\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import utils\n",
        "# https://stackoverflow.com/questions/55594969/how-to-visualise-filters-in-a-cnn-with-pytorch\n",
        "\n",
        "layers = [0,3,6,9]\n",
        "layers = [0,3,6,9,12]\n",
        "layer = 9\n",
        "\n",
        "def visualise(model,layer):\n",
        "    kernels = model.cnn[layer].weight.data.clone()\n",
        "    n,c,w,h = kernels.shape\n",
        "    print(kernels.shape)\n",
        "    if c not in [1,3]:\n",
        "        # kernels = kernels.mean(dim=1, keepdim=True)\n",
        "        kernels = kernels[:,2,:,:].unsqueeze(dim=1)\n",
        "    nrow=10\n",
        "    rows = np.min((kernels.shape[0]//nrow + 1, 64))\n",
        "    grid = utils.make_grid(kernels, nrow=nrow, normalize=True, padding=1)\n",
        "    plt.figure(figsize=(nrow,rows))\n",
        "\n",
        "    kernels = kernels - kernels.min()\n",
        "    kernels = kernels / kernels.max()\n",
        "    filter_img = utils.make_grid(kernels, nrow = 12)\n",
        "    # change ordering since matplotlib requires images to\n",
        "    # be (H, W, C)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0))\n",
        "\n",
        "    # plt.imshow(grid.cpu().numpy().transpose((1, 2, 0)))\n",
        "\n",
        "# visualise(agent.sense,layer)\n",
        "visualise(agent.jepa.enc,layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2TGs69fnrZo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "e7624553-e17a-4a9f-85a4-512720ed329a",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcost.1.weight torch.Size([2, 512])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1920x1440 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRU(267, 256, batch_first=True, dropout=0.2)\n"
          ]
        }
      ],
      "source": [
        "# @title visualise lin\n",
        "# https://matplotlib.org/stable/plot_types/index.html\n",
        "from torchvision import utils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for name, param in agent.jepa.pred.named_parameters(): # for param in model.parameters():\n",
        "# for name, param in agent.emb.named_parameters():\n",
        "for name, param in agent.tcost.named_parameters():\n",
        "    print(name, param.shape)\n",
        "    if len(param.shape)==1: param=param.unsqueeze(0)\n",
        "    Z=param.detach()#.numpy()\n",
        "\n",
        "    filter_img = utils.make_grid(Z, nrow = 12, normalize=True, padding=1)\n",
        "    plt.imshow(filter_img.cpu().permute(1, 2, 0)) # (H, W, C)\n",
        "\n",
        "    # fig, ax = plt.subplots()\n",
        "    # pos=ax.imshow(Z)\n",
        "    # fig.colorbar(pos)\n",
        "    plt.show()\n",
        "\n",
        "print(agent.jepa.pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(vars(agent.jepa.pred.))\n",
        "# print(vars(agent.tcost.state_dict()))\n",
        "# print(agent.jepa.pred._parameters.keys())\n",
        "# print(agent.jepa.pred._parameters['weight_ih_l0'])\n",
        "# print(agent.jepa.pred._parameters['weight_hh_l2']) # weight_hh_l0, weight_hh_l2\n",
        "# print(agent.tcost.state_dict().keys())\n",
        "print(agent.tcost.state_dict()['tcost.1.weight']) # tcost.2.bias, tcost.4.bias\n",
        "# print(agent.tcost.named_parameters())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjEWGq2WGi9a",
        "outputId": "649e3612-f156-496e-d8d5-fc576110e2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0015, -0.0132,  0.0280,  ...,  0.0297,  0.0289,  0.0152],\n",
            "        [ 0.0168,  0.0031, -0.0288,  ..., -0.0064, -0.0137, -0.0085]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A90BTTw0Lr-t",
        "outputId": "a95870e2-bc89-43ba-d40b-febea4ce2382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n",
            "690080\n"
          ]
        }
      ],
      "source": [
        "# @title ConvEnc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ConvEnc(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[5], d_list[5], 3, 2, 1), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[5], d_list[5], 2, 2, 0), nn.BatchNorm2d(d_list[5]), nn.ReLU(),\n",
        "            # # 2457024\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # 685248\n",
        "\n",
        "            # nn.Conv2d(1, d_list[0], 4, 2, 2), nn.BatchNorm2d(d_list[0]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[0], d_list[1], 4, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[1], d_list[2], 4, 2, 2), nn.BatchNorm2d(d_list[2]), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            # nn.Conv2d(d_list[2], d_list[3], 4, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # # #\n",
        "\n",
        "\n",
        "            nn.Conv2d(1, d_list[0], 4, 4, 0), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 4, 4, 0), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 4, 4, 0), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 4, 4, 0), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            #\n",
        "\n",
        "\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[5],d_model), nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "\n",
        "\n",
        "convenc = ConvEnc(256).to(device)\n",
        "input = torch.rand((4,1,256,256), device=device)\n",
        "out = convenc(input)\n",
        "print(out.shape)\n",
        "print(sum(p.numel() for p in convenc.parameters() if p.requires_grad)) #\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title transfer weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "d = 10  # size of the first dimension\n",
        "a = 5   # size of the extra nodes to omit\n",
        "m = 8   # output dimension\n",
        "\n",
        "source_layer = nn.Linear(d+a, m)\n",
        "target_layer = nn.Linear(d, m)\n",
        "# source_layer = nn.Linear(d, m)\n",
        "# target_layer = nn.Linear(d+a, m)\n",
        "\n",
        "def transfer(tgt,src):\n",
        "    with torch.no_grad():\n",
        "        tgt.weight[:, :src.weight.shape[1]].copy_(src.weight[:, :tgt.weight.shape[1]])\n",
        "        tgt.bias.copy_(src.bias)\n",
        "    return tgt,src\n",
        "\n",
        "target_layer, source_layer = transfer(target_layer, source_layer)\n",
        "\n",
        "\n",
        "src_sd = source_layer.state_dict()\n",
        "tgt_sd = target_layer.state_dict()\n",
        "\n",
        "def transfersd(tgt,src):\n",
        "    with torch.no_grad():\n",
        "        tgt['weight'][:, :src['weight'].shape[1]].copy_(src['weight'][:, :tgt['weight'].shape[1]])\n",
        "        tgt['bias'].copy_(src['bias'])\n",
        "    return tgt\n",
        "\n",
        "tgt_sd = transfersd(tgt_sd, src_sd)\n",
        "target_layer.load_state_dict(tgt_sd)\n",
        "\n",
        "\n",
        "agent_src = Agent(d_model=256, dim_a=3, dim_z=1, dim_v=512).to(device)\n",
        "\n",
        "# agent.tcost = TCost((1+agent.jepa.pred.num_layers)*agent.d_model) # replace tcost\n",
        "\n",
        "agent = Agent(d_model=256, dim_a=3, dim_z=3, dim_v=512).to(device)\n",
        "\n",
        "# agent.jepa.pred\n",
        "# target_layer, source_layer = transfer(target_layer, source_layer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(vars(agent.jepa.pred))\n",
        "# gru = agent.jepa.pred\n",
        "# gru = agent_src.jepa.pred\n",
        "# for wht_name in gru._all_weights[0]: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "#     print(wht_name, gru._parameters[wht_name].shape)\n",
        "\n",
        "# weight_ih_l0 dim_z=3: [768, 262] , dim_z=1: [768, 260]\n",
        "# weight_hh_l0 torch.Size([768, 256])\n",
        "# bias_ih_l0 torch.Size([768])\n",
        "# bias_hh_l0 torch.Size([768])\n",
        "\n",
        "# tgt_gru = agent.jepa.pred\n",
        "# src_gru = agent_src.jepa.pred\n",
        "tgt_gru = torch.nn.GRU(d_model+dim_a+dim_z+2, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "src_gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "\n",
        "tgt_gru[]\n",
        "def transfer_gru(tgt_gru, src_gru): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for i in range(min(len(tgt_gru._all_weights), len(src_gru._all_weights))):\n",
        "        # for lyr in tgt_gru._all_weights:\n",
        "            lyr = tgt_gru._all_weights[i]\n",
        "            for wht_name in lyr: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "                # print(wht_name, tgt_gru._parameters[wht_name].shape)\n",
        "                tgt_wht, src_wht = tgt_gru._parameters[wht_name], src_gru._parameters[wht_name]\n",
        "                if len(tgt_wht.shape)==2:\n",
        "                    tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "                elif len(tgt_wht.shape)==1:\n",
        "                    tgt_gru._parameters[wht_name] = src_wht\n",
        "    return tgt_gru\n",
        "tgt_gru = transfer_gru(tgt_gru, src_gru)\n",
        "\n",
        "# for wht_name in tgt_gru._all_weights[0]: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "#     print(wht_name, tgt_gru._parameters[wht_name].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "d_model=256; dim_a=3; dim_z=1; dim_v=512\n",
        "\n",
        "pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "# pred = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "print(pred._all_weights)\n",
        "for lyr in pred._all_weights:\n",
        "    for wht_name in lyr: # weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0\n",
        "        print(wht_name, pred._parameters[wht_name].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(pred.state_dict().keys())\n",
        "\n",
        "tgt_gru = torch.nn.GRU(d_model+dim_a+dim_z+2, d_model, num_layers=3, batch_first=True, dropout=0.0)\n",
        "src_gru = torch.nn.GRU(d_model+dim_a+dim_z, d_model, num_layers=1, batch_first=True, dropout=0.0)\n",
        "print(tgt_gru.state_dict()['weight_ih_l0'][0][:10])\n",
        "print(src_gru.state_dict()['weight_ih_l0'][0][:10])\n",
        "\n",
        "print(tgt_gru.state_dict()['bias_ih_l0'][:10])\n",
        "print(src_gru.state_dict()['bias_ih_l0'][:10])\n",
        "tgt_gru.state_dict().keys()\n",
        "src_gru.state_dict().keys()\n",
        "\n",
        "# tgt_gru\n",
        "# src_gru\n",
        "for wht_name in tgt_gru.state_dict().keys():\n",
        "    if not wht_name in src_gru.state_dict().keys(): continue\n",
        "    print(wht_name)\n",
        "    # print(tgt_gru.state_dict()[wht_name])\n",
        "    # tgt_gru.state_dict()[wht_name].copy_(src_gru.state_dict()[wht_name])\n",
        "\n",
        "tgt_sd = tgt_gru.state_dict()\n",
        "src_sd = src_gru.state_dict()\n",
        "def transfer_sd(tgt_sd, src_sd): # change input dim of gru\n",
        "    with torch.no_grad():\n",
        "        for wht_name in tgt_sd.keys():\n",
        "            if not wht_name in src_sd.keys(): continue\n",
        "            # print(wht_name)\n",
        "            tgt_wht, src_wht = tgt_sd[wht_name], src_sd[wht_name]\n",
        "            if len(tgt_wht.shape)==2:\n",
        "                tgt_wht[:, :src_wht.shape[1]].copy_(src_wht[:, :tgt_wht.shape[1]])\n",
        "            elif len(tgt_wht.shape)==1:\n",
        "                tgt_wht.copy_(src_wht)\n",
        "    return tgt_sd\n",
        "tgt_sd = transfer_sd(tgt_sd, src_sd)\n",
        "print(tgt_sd['weight_ih_l0'][0][:10])\n",
        "print(tgt_sd['bias_ih_l0'][:10])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "S_xnBFjXVxgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title test init norm\n",
        "print(agent.emb.state_dict()['weight'].norm(dim=-1))\n",
        "\n",
        "# x = torch.rand(16)\n",
        "x = torch.rand(8,16)\n",
        "# print(x)\n",
        "# torch.nn.init.normal_(x, mean=0.0, std=1.0)\n",
        "# torch.nn.init.xavier_normal_(x)\n",
        "import time\n",
        "start = time.time()\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# with torch.no_grad(): x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # 0.00966, 0.000602, 0.0004\n",
        "# torch.nn.init.normal_(x, mean=0.0, std=1./x.shape[-1]**0.5)\n",
        "torch.nn.init.normal_(x, mean=0., std=.3/x.shape[-1]**0.5)\n",
        "print(time.time()-start)\n",
        "# std = ((Sum (xi-mean)^2)/ N)^(1/2)\n",
        "# print(x)\n",
        "# print(((x**2).sum())**(0.5))\n",
        "print(torch.norm(x, dim=-1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CwApoQMMKzB",
        "outputId": "98f67f91-ef5b-406f-b852-5a93130f9e58",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0012018680572509766\n",
            "tensor([0.2797, 0.2218, 0.2731, 0.3268, 0.2632, 0.2914, 0.3217, 0.2845])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title wasserstein\n",
        "import torch\n",
        "\n",
        "def wasserstein(x, y, weight=1):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "    cdf_x, cdf_y = x.cumsum(dim=-1), y.cumsum(dim=-1)\n",
        "    dist = weight * torch.abs(cdf_x - cdf_y) # Wasserstein dist = L1 norm between CDFs\n",
        "    # dist = weight * (cdf_x - cdf_y)**2 # me\n",
        "    return dist.sum()\n",
        "\n",
        "def wasserstein(x, y, weight=1):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "    # cdf_x, cdf_y = x.cumsum(dim=-1), y.cumsum(dim=-1)\n",
        "    # dist = weight * torch.abs(cdf_x - cdf_y) # Wasserstein dist = L1 norm between CDFs\n",
        "    # cs = (x-y).cumsum(dim=-1)\n",
        "    cs = (x-y) @ torch.tril(torch.ones(x.shape[0], x.shape[0]))\n",
        "    # dist = weight * torch.abs(cs)\n",
        "    dist = weight * cs**2\n",
        "    # dist = weight * (cdf_x - cdf_y)**2 # me\n",
        "    return dist.sum()\n",
        "\n",
        "\n",
        "def soft_wasserstein_loss(x, y, smoothing=0.1):\n",
        "    # Normalise distributions\n",
        "    x = x / x.sum()\n",
        "    y = y / y.sum()\n",
        "    # Compute the cumulative distributions (CDFs) with a small smoothing factor\n",
        "    cdf_x = torch.cumsum(x, dim=-1) + smoothing\n",
        "    cdf_y = torch.cumsum(y, dim=-1) + smoothing\n",
        "    # Compute smooth Wasserstein distance (L2 distance between CDFs)\n",
        "    distance = torch.norm(cdf_x - cdf_y, p=2)  # L2 distance instead of L1 for smoother gradients\n",
        "    return distance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float32)\n",
        "# x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5], dtype=float))\n",
        "x = nn.Parameter(torch.tensor([-0.01, -0.0, -0.99], dtype=torch.float))\n",
        "y = torch.tensor([0.0, 0.0, -1.0], dtype=torch.float)\n",
        "\n",
        "# x = nn.Parameter(torch.rand(1024, dtype=float))\n",
        "# y = torch.rand(1024, dtype=float)\n",
        "# a = len(train_data.buffer)/len(train_data.data) # ratio dided/tt steps\n",
        "a=1/45\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "weight = torch.where(y < -0.5, 1/a, 1/(1-a))\n",
        "print(weight)\n",
        "dist = wasserstein(x, y, weight=weight)\n",
        "print(time.time() - start)\n",
        "print(dist)  # Should output 0.7\n",
        "# dist.backward()\n",
        "\n",
        "# 0.0004496574401855469\n",
        "# 0.000331878662109375\n"
      ],
      "metadata": {
        "id": "B1yvJkX89C_o",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title wasserstein sinkhorn train\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# agent.eval()\n",
        "# batch_size, T, _ = sx.shape\n",
        "x = nn.Parameter(torch.tensor([0,0,-1,0,0,0,-0.1, 0], device=device))\n",
        "optim = torch.optim.SGD([x], lr=1e-3) # 3e3\n",
        "# optim = torch.optim.AdamW([x], 1e-1, (0.9, 0.999)) # ? 1e0 ; 3e-2 1e-1\n",
        "# optim = torch.optim.AdamW([x], 1e-0, (0.9, 0.95)) # ? 1e0 ; 3e-2 1e-1\n",
        "y = torch.tensor([0,0,0,0,0,0,-1,0], dtype=torch.float)\n",
        "a=1/45\n",
        "weight = torch.where(y < -0.5, 1/a, 1/(1-a))\n",
        "# print(weight)\n",
        "\n",
        "# loss = wasserstein(x, y, weight=weight)\n",
        "# loss = wasserstein(x, y)\n",
        "# loss = sinkhorn(x, y)\n",
        "# loss.backward()\n",
        "# print(x.grad)\n",
        "\n",
        "\n",
        "for i in range(50): # num epochs\n",
        "    loss = wasserstein(x, y, weight=weight)\n",
        "    # loss = sinkhorn(x, y)\n",
        "    # loss = sinkhorn(x, y,0.05,80)\n",
        "    loss.sum().backward()\n",
        "    optim.step()\n",
        "    optim.zero_grad()\n",
        "    print(x.data, loss.item())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def sinkhorn(x, y, epsilon=0.05, max_iters=100):\n",
        "    # x, y = x / x.sum(), y / y.sum()\n",
        "\n",
        "    # Compute the cost matrix: here the cost is the squared distance between indices\n",
        "    # (|i-j|^2 for each position i, j)\n",
        "    posx = torch.arange(x.shape[-1], dtype=torch.float).unsqueeze(1)\n",
        "    posy = torch.arange(y.shape[-1], dtype=torch.float).unsqueeze(0)\n",
        "    cost_matrix = (posx - posy).pow(2)  # squared distance\n",
        "\n",
        "    # Initialize the dual variables\n",
        "    u = torch.zeros_like(x)\n",
        "    v = torch.zeros_like(y)\n",
        "\n",
        "    # Sinkhorn iterations\n",
        "    K = torch.exp(-cost_matrix / epsilon)  # Kernel matrix, regularised with epsilon\n",
        "    for _ in range(max_iters):\n",
        "        u = x / (K @ (y / (K.t() @ u + 1e-8)) + 1e-8)\n",
        "        v = y / (K.t() @ (x / (K @ v + 1e-8)) + 1e-8)\n",
        "    # print(K,u.data,v.data)\n",
        "    plan = torch.diag(u) @ K @ torch.diag(v)\n",
        "    dist = torch.sum(plan * cost_matrix)\n",
        "    return dist\n",
        "\n",
        "# Example\n",
        "x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float, requires_grad=True)\n",
        "y = torch.tensor([0.0, 0.0, 1.0], dtype=torch.float)\n",
        "# x = nn.Parameter(torch.tensor([0,0,-1,0,0,0,-0.1, 0], device=device))\n",
        "# y = torch.tensor([0,0,0,0,0,0,-1,0], dtype=float)\n",
        "\n",
        "# dist = sinkhorn(x, y)\n",
        "dist = sinkhorn(x, y, 0.05,80)\n",
        "dist.backward()  # To compute gradients with respect to x\n",
        "\n",
        "print(dist.item())\n",
        "print(x.grad)\n",
        "\n",
        "# [2.0000e+07, 3.0000e+07, 1.0000e-08]) tensor([       0.,        0., 49999996.] episodes>=80\n",
        "\n"
      ],
      "metadata": {
        "id": "3nfZRhVc9Ssp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt_UlGz6Xoq3"
      },
      "source": [
        "## plot 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VX5IExbRriwm"
      },
      "outputs": [],
      "source": [
        "# @title sklearn RBF\n",
        "# https://gist.github.com/eljost/2c4e1af652ef02b2989da341c5569af7\n",
        "# from nn_plot.ipynb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import scipy.stats as st\n",
        "\n",
        "# np.random.seed(1)\n",
        "def func(x):\n",
        "    # print(x.shape)\n",
        "    # x= np.sum(x**2, axis=-1)\n",
        "    x=np.random.rand(x.shape[0])\n",
        "    print(x.shape)\n",
        "    return x\n",
        "\n",
        "res = 50\n",
        "num_pts=15\n",
        "X=np.random.rand(num_pts,2)*res\n",
        "# Y = func(X)\n",
        "Y=np.random.rand(num_pts)\n",
        "# print(X);print(Y)\n",
        "\n",
        "lim = 1\n",
        "# lin = np.linspace(-lim, lim, res)\n",
        "lin = np.linspace(0, res, res)\n",
        "x1, x2 = np.meshgrid(lin, lin)\n",
        "xx = np.vstack((x1.flatten(), x2.flatten())).T\n",
        "\n",
        "kernel = RBF()\n",
        "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "gp.fit(X, Y)\n",
        "# print(\"Learned kernel\", gp.kernel_)\n",
        "y_mean, y_cov = gp.predict(xx, return_cov=True)\n",
        "\n",
        "posteriors = st.multivariate_normal.rvs(mean=y_mean, cov=y_cov, size=1)\n",
        "\n",
        "ax = plt.figure().add_subplot(projection='3d')\n",
        "Z=posteriors.reshape(-1, res)\n",
        "# ax.plot_surface(x1, x2, Z)\n",
        "ax.plot_surface(x1, x2, Z, cmap='rainbow', alpha=0.7)\n",
        "\n",
        "# ax.plot_surface(x1, x2, posteriors.reshape(-1, res))\n",
        "ax.contour(x1, x2, Z, zdir='z', offset=-1, cmap='coolwarm') # https://matplotlib.org/stable/gallery/mplot3d/contour3d_3.html#sphx-glr-gallery-mplot3d-contour3d-3-py\n",
        "# ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-0.4, 0.5))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "ax.set(xlim=(0, 50), ylim=(0, 50), zlim=(-1, 2))#, xlabel='X', ylabel='Y', zlabel='Z')\n",
        "\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, c=zdata, cmap='Greens');\n",
        "# ax.scatter3D(X[:, 0], X[:, 1],Y, cmap='Greens');\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SusX7gpzxFNL",
        "outputId": "9f14a9da-e188-49ba-f5f5-70192ff33134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-2.6321], grad_fn=<AddBackward0>)\n",
            "tensor([2.7358]) tensor([-4.7358])\n"
          ]
        }
      ],
      "source": [
        "# @title chatgpt RBFKernelLayer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RBFKernelLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(RBFKernelLayer, self).__init__()\n",
        "        self.centres = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        dists = torch.cdist(x, self.centres, p=2) ** 2\n",
        "        return torch.exp(-dists / (2 * self.sigma ** 2))\n",
        "\n",
        "class SaddlePointNetwork(nn.Module):\n",
        "    def __init__(self, in_features, out_features, sigma=1.0):\n",
        "        super(SaddlePointNetwork, self).__init__()\n",
        "        self.rbf_layer = RBFKernelLayer(in_features, out_features, sigma)\n",
        "        self.linear = nn.Linear(out_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rbf_output = self.rbf_layer(x)\n",
        "        # Introduce a saddle point structure\n",
        "        linear_output = self.linear(rbf_output)\n",
        "        # Example saddle function: x^2 - y^2\n",
        "        saddle_output = torch.sum(linear_output[:, :1]**2 - linear_output[:, 1:]**2, dim=1, keepdim=True)\n",
        "        return saddle_output\n",
        "\n",
        "# sin(ax)sin(bx)\n",
        "# (x^2 - y^2)\n",
        "import torch\n",
        "\n",
        "def rbf_saddle(x, y, gamma=1.0, a=1.0, b=1.0):\n",
        "    # RBF-like term\n",
        "    rbf_term = torch.exp(-gamma * torch.norm(x - y, p=2)**2)\n",
        "    # Saddle point term\n",
        "    saddle_term = (a * x)**2 - (b * y)**2\n",
        "    return rbf_term + saddle_term\n",
        "\n",
        "# Example usage\n",
        "x = torch.tensor([1.0], requires_grad=True)\n",
        "y = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "output = rbf_saddle(x, y)\n",
        "print(output)\n",
        "\n",
        "# Compute gradients\n",
        "output.backward()\n",
        "print(x.grad, y.grad)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rTmCo7pm0NxL"
      },
      "outputs": [],
      "source": [
        "# @title plot 3d\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "x = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "y = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n",
        "X, Y = torch.meshgrid(x, y)\n",
        "Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7fWZaQTDFg1",
        "outputId": "4c5ced88-54f1-436e-89f9-66f1c8396373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000, -0.7231,  0.3792,  0.0000]]) tensor([0.3362])\n",
            "tensor(0.0035, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# @title shape NN\n",
        "num_pts=1\n",
        "\n",
        "# X=torch.rand(num_pts,4)*2-1\n",
        "# X=torch.cat([torch.tensor([0,0]).unsqueeze(0),torch.rand(num_pts,2)*2-1], dim=-1)\n",
        "X=torch.cat([torch.zeros(1,1),torch.rand(num_pts,2)*2-1,torch.zeros(1,1)], dim=-1)\n",
        "Y=torch.rand(num_pts)\n",
        "print(X,Y)\n",
        "optim = torch.optim.SGD(model.parameters(), 1e-1)\n",
        "\n",
        "# model.train()\n",
        "pred = model(X)\n",
        "# print(Y.shape,pred.shape)\n",
        "# loss = loss_fn(pred.reshape(-1,pred.shape[-1]), y.reshape(-1))\n",
        "loss = F.mse_loss(Y, pred.squeeze(-1))\n",
        "loss.backward()\n",
        "optim.step()\n",
        "optim.zero_grad()\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "boDd__PE2sGy"
      },
      "outputs": [],
      "source": [
        "# @title plot NN\n",
        "\n",
        "xx = torch.linspace(-1, 1, 100)\n",
        "yy = torch.linspace(-1, 1, 100)\n",
        "X, Y = torch.meshgrid(xx, yy) # [100,100]\n",
        "xy = torch.cat([X.unsqueeze(-1), torch.zeros(X.shape+(2,)), Y.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "with torch.no_grad(): Z = model(xy).squeeze(-1)\n",
        "# print(Z)\n",
        "# print(Z.shape)\n",
        "\n",
        "# Z = rbf_saddle(X, Y)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X.numpy(), Y.numpy(), Z.numpy(), cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW6BYoXsX57o",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title test optim saddle same time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def f(x, y):\n",
        "    return x ** 2 - y ** 2 + x * y\n",
        "# (x-y)(x+y)+xy\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "dim_x, dim_z = 3, 8\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "xx = torch.empty((1, T, dim_x))\n",
        "torch.nn.init.xavier_uniform_(xx)\n",
        "# x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "\n",
        "# tensor([[0.6478, 0.0531, 0.0861]]) tensor([[-1.,  1.]]) 0.2974517047405243\n",
        "# tensor([-0.9419, -1.0000,  0.4416, -1.0000,  1.0000,  0.2963])\n",
        "\n",
        "# x = nn.Parameter(torch.tensor([[0.6478, 0.0531, 0.0861]]))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "\n",
        "z = nn.Parameter(torch.empty((batch, T, dim_z)))\n",
        "# z = torch.empty((1, T, 1))\n",
        "torch.nn.init.xavier_uniform_(z)\n",
        "# z = nn.Parameter(z.repeat(batch,1,1))\n",
        "# z = nn.Parameter(torch.tensor([[-1.,  1.]]))\n",
        "# optim_z = torch.optim.SGD([z], lr=ratio*lr, maximize=True) # 3e3\n",
        "# optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.95), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "optim_z = torch.optim.AdamW([z], ratio*lr, (0.9, 0.999), maximize=True) # 1e0 ; 3e-2 1e-1\n",
        "# .95,1e-1,3e-1\n",
        "# .99,\n",
        "\n",
        "d_model = 4\n",
        "# model = nn.Sequential(\n",
        "#     nn.Linear(x.shape[-1]+z.shape[-1],d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model,1), nn.LeakyReLU(),\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(10): # num epochs\n",
        "    # loss = f(x,z)\n",
        "    # loss = f(x.sum(-1),z)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step(); optim_z.step()\n",
        "    optim_x.zero_grad(); optim_z.zero_grad()\n",
        "    # print(i,x.squeeze(), z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    print(i,x.squeeze()[0].data, z[0].squeeze().data, loss[0].squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "        z.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "print(loss.squeeze())\n",
        "idx = torch.argmin(loss) # choose best x even with greatest adv z\n",
        "# idx = torch.argmax(loss)\n",
        "# print(x[idx],z[idx],loss[idx])\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx].data,loss[idx].item())\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJdFpDr2wIMT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title test optim saddle argm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def argm(sx, lr=3e3): # 3e3\n",
        "    # batch=sx.size(dim=0)\n",
        "    batch_size, T, _ = sx.shape\n",
        "    batch = 16\n",
        "    # z = nn.Parameter(torch.zeros((batch,1),device=device))\n",
        "    # z = nn.Parameter(torch.empty((1,batch, T, dim_z)))\n",
        "    z = nn.Parameter(torch.empty((batch_size,batch, T, dim_z)))\n",
        "    torch.nn.init.xavier_uniform_(z)\n",
        "    # optim = torch.optim.SGD([z], lr=lr, maximize=True)\n",
        "    optim = torch.optim.AdamW([z], 1e1, (0.9, 0.95), maximize=True)\n",
        "    sx = sx.detach().unsqueeze(1).repeat(1,batch,1,1)\n",
        "    # sx = sx.detach()\n",
        "    for i in range(20): # 10\n",
        "        # print(sx.shape,z.shape)\n",
        "        sxz = torch.cat([sx, z], dim=-1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            cost = model(sxz)\n",
        "        cost.sum().backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "        # print(\"argm cost z\",i,cost.item(), z.detach().item())\n",
        "        # print(\"argm cost z\",i,cost.squeeze(), z.detach().squeeze())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    # return z.detach()\n",
        "    print(cost.squeeze().data)\n",
        "    idx = torch.argmax(cost.squeeze(), dim=1)\n",
        "    return z[torch.arange(z.shape[0]),idx].detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch=16\n",
        "T=1\n",
        "# x = nn.Parameter(torch.empty((batch, T, dim_x)))\n",
        "# torch.nn.init.xavier_uniform_(x)\n",
        "# x = nn.Parameter(xx.clone())\n",
        "x = nn.Parameter(xx.clone().repeat(batch,1,1))\n",
        "\n",
        "lr = 3e-2 # sgd 1e-1,1e-0,1e4 ; adamw 1e-1\n",
        "# ratio = 6e0\n",
        "lr = 1e-1 # adamw 1e-1\n",
        "ratio = 4\n",
        "# optim_x = torch.optim.SGD([x], lr=lr)\n",
        "# optim_x = torch.optim.AdamW([x], lr, (0.9, 0.95)) # 1e-1 ; 1e-2 3e-2\n",
        "optim_x = torch.optim.AdamW([x], lr, (0.9, 0.999)) # 1e-1 ; 1e-2 3e-2\n",
        "# print(x.shape)\n",
        "\n",
        "\n",
        "# print(\"search\",x.squeeze().data, z.squeeze())\n",
        "# print(\"search\",x.squeeze().data, z.squeeze().item())\n",
        "for i in range(50):\n",
        "    z = argm(x)\n",
        "    # print(x.shape,z.shape)\n",
        "    xz = torch.cat([x,z], dim=-1)\n",
        "    loss = model(xz)\n",
        "    loss.sum().backward()\n",
        "    optim_x.step()\n",
        "    optim_x.zero_grad()\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.sum().item())\n",
        "    # print(i,x.squeeze().data, z.squeeze().data, loss.squeeze().item())\n",
        "    # print(i,x.squeeze()[0], z.squeeze(), loss.squeeze().item())\n",
        "    # print(i,x[0].squeeze().data, z[0].squeeze().data, loss.squeeze().item())\n",
        "    with torch.no_grad():\n",
        "        # x /= torch.norm(x, dim=-1).unsqueeze(-1).clamp_(min=1) # need x in place\n",
        "        x.clamp_(min=-1, max=1)\n",
        "    # print(i,x.squeeze().data, z.squeeze().item(), loss.squeeze().item())\n",
        "\n",
        "# xz = torch.cat([x,z], dim=-1)\n",
        "# loss = model(xz)\n",
        "# print(\"z\",z)\n",
        "# print(loss.squeeze())\n",
        "idx = torch.argmin(loss)\n",
        "print(x[idx].data,z[idx],loss[idx].item())\n",
        "\n",
        "print(torch.cat([x,z,loss],dim=-1).squeeze().data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvjIJP6RlEv2",
        "outputId": "447fdefd-452b-437d-c228-1847492b36f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 10])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x = torch.randn(16, 16)\n",
        "# print((b==torch.max(b)).nonzero())\n",
        "x = torch.randn(10, 3)\n",
        "idx = torch.randint(3,(10,))\n",
        "# print(x[:,idx].shape)\n",
        "print(x[torch.arange(x.shape[0]),idx].shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhKd009Qvk3"
      },
      "source": [
        "## trash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZDtHEU4tCo5z"
      },
      "outputs": [],
      "source": [
        "# @title torch gru\n",
        "# text_generation.ipynb https://colab.research.google.com/drive/1SguQZQYZBaalRuElJcxGdgF3YxhiwkAM\n",
        "# RNNs.ipynb https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR\n",
        "\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class gru(nn.Module):\n",
        "    def __init__(self, emb_dim, rnn_units, num_layers):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(emb_dim, rnn_units, num_layers=num_layers, dropout=0.0, batch_first=True)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "        self.rnn_units = rnn_units\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.rnn_units, device=device) # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        if hidden is None: hidden = self.init_hidden(x.shape[0])\n",
        "        # print('fwd',x.shape, hidden.shape) # fwd [batch_size, bptt, emb_dim], [num_layers, batch_size, rnn_units]\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.dense(output)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "emb_dim = 256#256\n",
        "rnn_units = 1024#1024\n",
        "num_layers = 1\n",
        "# model = gru(emb_dim, rnn_units, num_layers).to(device)\n",
        "# model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5pscE7mtaPAq"
      },
      "outputs": [],
      "source": [
        "# @title ltm\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "class Ltm():\n",
        "    def __init__(self, ltmk=None, ltmv=None):\n",
        "        self.index = None\n",
        "        if ltmk is None:\n",
        "            self.ltmk = torch.tensor([])\n",
        "            self.ltmv = torch.tensor([])\n",
        "        else:\n",
        "            self.ltmk = ltmk # [len_ltm, d_model]\n",
        "            self.ltmv = ltmv\n",
        "        if len(self.ltmk)>=100:\n",
        "            self.index = makefaissindex(ltmk)\n",
        "\n",
        "    # def add(self, k, v):\n",
        "    def add(self, k, v, mask=None):\n",
        "        # self.ltmk.append(k)\n",
        "        # self.ltmv.append(v)\n",
        "        if k==None: return\n",
        "        if k.ndim==1: k, v = k.unsqueeze(0), v.unsqueeze(0)\n",
        "        if mask==None:\n",
        "            self.ltmk = torch.cat([self.ltmk, k], dim=1)\n",
        "            self.ltmv = torch.cat([self.ltmv, v], dim=1)\n",
        "        else:\n",
        "            self.ltmk[mask] = torch.cat([self.ltmk[mask], k], dim=1)\n",
        "            self.ltmv[mask] = torch.cat([self.ltmv[mask], v], dim=1)\n",
        "        if self.index:\n",
        "            self.index.add(k)\n",
        "            if torch.rand(1)<0.1:\n",
        "                self.index.train(self.ltmk)\n",
        "\n",
        "    def makefaissindex(self, vert_store):\n",
        "        d = vert_store.shape[-1]\n",
        "        nlist = 100\n",
        "        index = faiss.IndexFlatL2(d) # no need train # 1-Flat.py\n",
        "        index = faiss.IndexIVFFlat(index, d, nlist, faiss.METRIC_L2) # 2-IVFFlat.py\n",
        "        if not index.is_trained: index.train(vert_store)\n",
        "        index.add(vert_store)\n",
        "        return index\n",
        "\n",
        "    def vecsearch(self, query, k=5, treshold=36): # k nearest neighbors\n",
        "        # index.nprobe = 5 # 1\n",
        "        D, I = self.index.search(query, k) # dist, idx\n",
        "        D, I = D[0], I[0]\n",
        "        mask = I[D<treshold]\n",
        "        return mask\n",
        "\n",
        "    def __call__(self, query, k=5, treshold=36): # [batch_size, d_model]\n",
        "        if self.index!=None and len(self.ltmk)>=100:\n",
        "            mask = self.vecsearch(query, k, treshold)\n",
        "            rag = self.ltmk[mask] # [len_rag, d_model]\n",
        "        else:\n",
        "            rag = self.ltmk\n",
        "        if len(rag)==0: return 0\n",
        "        # print(\"ltm call\", query.shape, rag.shape)\n",
        "        # attn = query @ rag.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ rag.transpose(-1,-2) # [batch_size, 1, d_model] @ [batch_size, d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.ltmv\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def remove_ids(self, removing): # torch.tensor indexes\n",
        "        mask = torch.ones(len(self.ltmk), dtype=torch.bool)\n",
        "        mask[removing] = False\n",
        "        self.ltmk, self.ltmv = self.ltmk[mask], self.ltmv[mask]\n",
        "        if self.index: self.index = makefaissindex(ltmk)\n",
        "\n",
        "    def save(file='ltm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.ltmk, self.ltmv), f)\n",
        "\n",
        "    def load(file='ltm.pkl'):\n",
        "        with open(file, 'rb') as f: self.ltmk, self.ltmv = pickle.load(f)\n",
        "\n",
        "ltm = Ltm()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2V6qDLPrOlBU"
      },
      "outputs": [],
      "source": [
        "# @title stm\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "class Stm():\n",
        "    def __init__(self, stmk=None, stmv=None, meta=None):\n",
        "        self.stmk = stmk # [len_ltm, d_model]\n",
        "        self.stmv = stmv\n",
        "        self.meta = meta\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # if len(rag)==0: return 0\n",
        "        # print(\"stm call\", query.shape, self.stmk.shape)\n",
        "        # attn = query @ self.stmk.T # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attn = query.unsqueeze(1) @ self.stmk.transpose(-1,-2) # [batch_size, d_model] @ [d_model, len_ltm] = [batch_size, len_ltm]\n",
        "        attention = torch.softmax(attn, dim=-1) # [batch_size, len_ltm]\n",
        "        x = attention @ self.stmv\n",
        "        self.meta = self.meta + attn.squeeze() # attention\n",
        "        return x # [batch_size, d_model]\n",
        "\n",
        "    def add(self, k, v):\n",
        "        if k.ndim==1:\n",
        "            k=k.unsqueeze(0)\n",
        "            v=v.unsqueeze(0)\n",
        "        self.stmk = torch.cat([self.stmk, k], dim=1)\n",
        "        self.stmv = torch.cat([self.stmv, v], dim=1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1,1)], dim=-1)\n",
        "        self.meta = torch.cat([self.meta, torch.ones(self.meta.shape[0],1)], dim=-1)\n",
        "        # self.meta = torch.cat([self.meta, torch.ones(1)])\n",
        "\n",
        "    def decay(self, g=0.9, k=256):\n",
        "        self.meta = g*self.meta # decay\n",
        "        mask = self.meta>0.001 # forget not retrieved\n",
        "        self.stmk, self.stmv = self.stmk[mask], self.stmv[mask]\n",
        "        self.meta = self.meta[mask]\n",
        "\n",
        "        topk = torch.topk(self.meta, k)#, dim=None, largest=True, sorted=True\n",
        "        self.meta = topk.values # cap stm size\n",
        "        self.stmk, self.stmv = self.stmk[topk.indices], self.stmv[topk.indices]\n",
        "\n",
        "    def pop(self, t=5):\n",
        "        # if important long term, if\n",
        "        mask = self.meta>t # to pop to ltm\n",
        "        popk, popv = self.stmk[mask], self.stmv[mask]\n",
        "        self.stmk, self.stmv = self.stmk[~mask], self.stmv[~mask]\n",
        "        self.meta = self.meta[~mask]\n",
        "        return popk, popv, mask.any(dim=-1)\n",
        "\n",
        "    def save(file='stm.pkl'):\n",
        "        with open(file, 'wb') as f: pickle.dump((self.stmk, self.stmv, self.meta), f)\n",
        "\n",
        "    def load(file='stm.pkl'):\n",
        "        with open(file, 'rb') as f: self.stmk, self.stmv, self.meta = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3EGwfW9HxOMj"
      },
      "outputs": [],
      "source": [
        "# @title split params to train\n",
        "# qkv for useful for critic predicting cost?\n",
        "\n",
        "# train after each step: jepa(pred)(using SL)?\n",
        "\n",
        "# train after each episode: critic, jepa()\n",
        "\n",
        "\n",
        "# jepa is batch of same length episodes, take from history\n",
        "# cost is single full episode buffer\n",
        "\n",
        "# or combine string of episode buffers, batchify like rnn training\n",
        "\n",
        "\n",
        "# batch_size = 64\n",
        "# weights = torch.ones(len(buffer))#.expand(batch_size, -1)\n",
        "# index = torch.multinomial(weights, num_samples=batch_size, replacement=False)\n",
        "# buffer[index]\n",
        "\n",
        "\n",
        "for name, p in agent.named_parameters():\n",
        "    print(name, 'tcost' in name)\n",
        "# https://pytorch.org/docs/stable/optim.html#per-parameter-options4\n",
        "# optim.SGD([\n",
        "#                 {'params': others},\n",
        "#                 {'params': bias_params, 'weight_decay': 0}\n",
        "#             ], weight_decay=1e-2, lr=1e-2)\n",
        "\n",
        "tcost_params = [p for name, p in agent.named_parameters() if 'tcost' in name]\n",
        "others = [p for name, p in agent.named_parameters() if 'tcost' not in name]\n",
        "\n",
        "# # joptim = torch.optim.AdamW(agent.jepa.parameters(), lr=1e-3)\n",
        "# joptim = torch.optim.AdamW([agent.jepa.parameters(),agent.q.parameters(), agent.k.parameters(), agent.v.parameters()], lr=1e-3)\n",
        "# coptim = torch.optim.AdamW(agent.tcost.parameters(), lr=1e-3)\n",
        "joptim = torch.optim.AdamW(tcost_params, lr=1e-3)\n",
        "coptim = torch.optim.AdamW(others, lr=1e-3)\n",
        "agent.train(buffer, joptim, coptim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IcEM4HCwCKbl"
      },
      "outputs": [],
      "source": [
        "# @title assorted\n",
        "# print(type(buffer[0][0]))\n",
        "# print(buffer[0][0])\n",
        "# print(buffer[0][0].dtype)\n",
        "import numpy as np\n",
        "\n",
        "# b=np.random.randint(low=0, high=256, size=(1000, 64, 64, 3), dtype='uint8')\n",
        "b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(1000)]\n",
        "# print(b.shape)\n",
        "# print(b[0])\n",
        "def custom_collate(original_batch):\n",
        "    return original_batch\n",
        "\n",
        "train_data = BufferDataset(b, seq_len) # one line of poem is roughly 50 characters\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = False, batch_size = batch_size, collate_fn=custom_collate) # num_workers = 4\n",
        "# train_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    # def plan(self, ): # mpc\n",
        "    #     # xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "    #     xs, us = locuslab_mpc(x_init, goal_state, self.jepa)\n",
        "\n",
        "# def train_cost(self, dataloader, buffer, optim):\n",
        "\n",
        "#         c = c + self.icost(world_state_) + reward\n",
        "#         c_ = c_ + cost\n",
        "#     closs = nn.MSELoss()(c,c_) # L1Loss MSELoss ; Sum reward\n",
        "#     closs.backward()\n",
        "#     optim.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j058IfyZKWUj",
        "outputId": "afb580da-32c1-4fa3-c5eb-9af659a24945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "16\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 3, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# @title custom transforms ToTensorme\n",
        "import torchvision.transforms as transforms\n",
        "# 100,3\n",
        "# seq_len,\n",
        "# for batch, Sar in enumerate(train_data):\n",
        "for batch, Sar in enumerate(train_loader):\n",
        "# for batch, (State, Action, Reward) in enumerate(train_loader):\n",
        "# for batch, (Sar,_) in enumerate(train_loader):\n",
        "    # print(len(Sar[0]))\n",
        "    # print(Sar[0][0].shape)\n",
        "    # State, Action, Reward = zip(*Sar)\n",
        "    # State=Sar\n",
        "    break\n",
        "for s,a,r in zip(*Sar):\n",
        "    state=s\n",
        "    break\n",
        "print(len(State))\n",
        "print(len(State[0]))\n",
        "print(type(State[0]))\n",
        "\n",
        "\n",
        "# transforms.Lambda(lambda x : torch.clamp(x, 0., 1.)),\n",
        "\n",
        "# def ToTensorme(x):\n",
        "#     print(\"ToTensorme\",type(x))\n",
        "#     # if type(x) == np.ndarray: return x.astype(np.float32)\n",
        "#     # if type(x) == np.ndarray: return torch.from_numpy(x).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.permute(2,0,1).to(torch.float32)\n",
        "#     if type(x) == torch.Tensor: return x.permute(0,3,1,2).to(torch.float32)\n",
        "#     # if type(x) == torch.Tensor: return x.to(torch.float32)\n",
        "\n",
        "# # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform = transforms.Compose([transforms.Lambda(ToTensorme), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# # transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), transforms.Lambda(ToTensorme)])\n",
        "# # https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
        "\n",
        "print(State[0].shape)\n",
        "# out=transform(State[0][0])\n",
        "# out=transform(State[0])\n",
        "# out=transform(list(State[0]))\n",
        "# print(out)\n",
        "\n",
        "# State = torch.tensor(State)\n",
        "# print(State.shape)\n",
        "\n",
        "# State[:,,]\n",
        "# l=99\n",
        "# lst=list(range(0,l,7))[1:]+[l]\n",
        "# print(lst)\n",
        "\n",
        "\n",
        "# b=[np.random.randint(low=0, high=256, size=(64, 64, 3), dtype='uint8') for _ in range(10)]\n",
        "# for state in b:\n",
        "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#     transform(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Jp3Bx_W_TqZ3"
      },
      "outputs": [],
      "source": [
        "# @title batch pop to ltm\n",
        "import torch\n",
        "batch_size=8\n",
        "d_model=4\n",
        "# stmk=torch.rand(batch_size, 5, d_model)\n",
        "# stmv=torch.rand(batch_size, 5, d_model)\n",
        "# ltmk=torch.rand(batch_size, 5, d_model)\n",
        "# ltmv=torch.rand(batch_size, 5, d_model)\n",
        "# meta=torch.rand(batch_size, 5)*7\n",
        "# mask = meta>5 # to pop to ltm\n",
        "# popk, popv = stmk[mask], stmv[mask]\n",
        "# print(popk.shape, popv.shape)\n",
        "# stmk, stmv = stmk[~mask], stmv[~mask]\n",
        "# meta = meta[~mask]\n",
        "# # return popk, popv\n",
        "\n",
        "\n",
        "# out=torch.rand(batch_size, 1, d_model)\n",
        "out=[torch.rand(1, d_model) for _ in range(batch_size)]\n",
        "lst=torch.rand(batch_size, 5, d_model)\n",
        "mask=torch.rand(batch_size, 5) > 0.5\n",
        "# out = torch.cat([out,lst[mask]], dim=1)\n",
        "# batch, row = torch.where(mask)\n",
        "# print(batch, row)\n",
        "# out = torch.cat([out,lst[torch.where(mask)]], dim=1)\n",
        "# print(out[batch].shape,lst[batch, row,:].shape)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:]], dim=1)\n",
        "# out[batch] = torch.cat([out[batch],lst[batch, row,:].unsqueeze(1)], dim=1)\n",
        "\n",
        "for b, m in enumerate(mask):\n",
        "    # out[b] = torch.cat([out[b],lst[b][m]], dim=1)\n",
        "    out[b] = torch.cat([out[b],lst[b][m]])\n",
        "\n",
        "\n",
        "\n",
        "# num_masked = mask.sum(dim=1, keepdim=True)\n",
        "# masked_elements = lst[torch.arange(lst.size(0))[:, None], mask]\n",
        "# zeros = torch.zeros(batch_size, num_masked.max(), d_model)\n",
        "# output = zeros.scatter(dim=1, index=masked_elements.nonzero(as_tuple=True)[1], src=masked_elements)\n",
        "# torch.cat([out, output], dim=1)\n",
        "\n",
        "# empty_mask = ~mask.any(dim=1)  # Find rows where all mask values are False\n",
        "# padded_lst = torch.zeros(batch_size, 1, d_model)  # Create a zero tensor for padding\n",
        "# padded_lst[~empty_mask] = lst[mask][~empty_mask]  # Fill non-empty masks with selected values\n",
        "# out = torch.cat([out, padded_lst], dim=1)\n",
        "\n",
        "\n",
        "# print(mask)\n",
        "# print(mask[:, None])\n",
        "# print(mask[:, None].expand(-1, lst.size(1), -1))\n",
        "\n",
        "# out = torch.cat([out, lst[mask[:, None].expand(-1, lst.size(1), -1)]], dim=1)\n",
        "# out = torch.cat([out, lst[mask[:, None]]], dim=1)\n",
        "\n",
        "# print(out.shape)\n",
        "print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4vBx6CBgoTG"
      },
      "outputs": [],
      "source": [
        "# @title straight through estimator\n",
        "class STEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        return (input > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return F.hardtanh(grad_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gJ3X_hQelW2x"
      },
      "outputs": [],
      "source": [
        "# @title train\n",
        "\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(1, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            # nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[3],d_model), nn.ReLU(),\n",
        "            # nn.Linear(d_model,10),\n",
        "        )\n",
        "\n",
        "        mul=4\n",
        "        self.tcost = nn.Sequential( # trained cost\n",
        "            # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "            nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "            nn.Dropout(), nn.Linear(mul*d_model, 10),\n",
        "            )\n",
        "    # def forward(self, x): return self.cnn(x)\n",
        "\n",
        "model = Agent(d_model=256).to(device)\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.no_grad(): sx = model.cnn(image)\n",
        "        # print(sx.shape, r.shape)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(model.tcost(sx), r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        # try: wandb.log({\"loss\": loss.item()})\n",
        "        # except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wi4ODp-XlZoU"
      },
      "outputs": [],
      "source": [
        "# @title mnist data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),)\n",
        "# test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 64 # 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QYbOgNoZn6JL"
      },
      "outputs": [],
      "source": [
        "# @title resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models # https://pytorch.org/vision/0.12/models.html#id10\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def get_res(dim_embd):\n",
        "    model = models.resnet18(weights='DEFAULT') # 18 34 50 101 152\n",
        "    num_ftrs = model.fc.in_features # 1000\n",
        "    # model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    # model.layer4 = nn.Sequential()\n",
        "    model.fc = nn.Sequential( # og\n",
        "        nn.Linear(num_ftrs, dim_embd, bias=None),\n",
        "        )\n",
        "    return model\n",
        "\n",
        "# model = get_res(10).to(device)\n",
        "# model = get_res(2).to(device)\n",
        "\n",
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, d_model = 256):\n",
        "        super().__init__()\n",
        "        # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "        d_list=[32, 64, 128, 256, 256, 256] #\n",
        "        # d_list = [min(d, d_model) for d in d_list]\n",
        "        self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "            nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "            # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "            nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "            nn.Linear(d_model,1),\n",
        "        )\n",
        "    def forward(self, x): return self.cnn(x)\n",
        "model=Agent().to(device)\n",
        "\n",
        "\n",
        "# loss_function = torch.nn.CrossEntropyLoss()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 1e-3, (0.9, 0.95)) # lr = 1e-3\n",
        "\n",
        "\n",
        "def train(model, train_loader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    for image, r in train_loader:\n",
        "        image, r = image.to(device), r.to(device)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            pred = model(image).squeeze(-1) # squeeze impt for regression!!!\n",
        "            # print(pred.shape, r.shape)\n",
        "            loss = loss_function(pred, r)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, train_loader, loss_function, optimizer)\n",
        "\n",
        "    images,r = next(iter(train_loader))\n",
        "    with torch.no_grad():\n",
        "        # pred = model(images.to(device)).argmax(-1).cpu()\n",
        "        pred = model(images.to(device)).squeeze(-1).cpu()\n",
        "        print(r)\n",
        "        print(pred)\n",
        "        print((r==pred).sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_kcajtpjr7Io"
      },
      "outputs": [],
      "source": [
        "# @title bin clss\n",
        "# def train(model, train_loader, loss_function, optimizer):\n",
        "#     model.train()\n",
        "#     for image, _ in train_loader:\n",
        "#         image = image.to(device)#.reshape(-1, 28*28)\n",
        "#         reconstructed = model(image)\n",
        "#         loss = loss_function(reconstructed, image)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "\n",
        "# class Agent(torch.nn.Module):\n",
        "#     def __init__(self, d_model = 256):\n",
        "#         super().__init__()\n",
        "#         # d_list=[32, 64, 128, 256, 512, 1024] #\n",
        "#         d_list=[32, 64, 128, 256, 256, 256] #\n",
        "#         # d_list = [min(d, d_model) for d in d_list]\n",
        "#         self.cnn = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "#             nn.Conv2d(3, d_list[0], 7, 2, 3), nn.BatchNorm2d(d_list[0]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[0], d_list[1], 5, 2, 2), nn.BatchNorm2d(d_list[1]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[1], d_list[2], 3, 2, 1), nn.BatchNorm2d(d_list[2]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[2], d_list[3], 3, 2, 1), nn.BatchNorm2d(d_list[3]), nn.ReLU(),\n",
        "#             nn.Conv2d(d_list[3], d_list[4], 3, 2, 1), nn.BatchNorm2d(d_list[4]), nn.ReLU(),\n",
        "#             # nn.Conv2d(d_list[4], d_list[5], 3, 2, 1), nn.ReLU(),\n",
        "#             nn.Flatten(start_dim=1),\n",
        "#             # nn.Linear(4*d_list[4],d_list[5]), nn.ReLU(),\n",
        "#             nn.Linear(4*d_list[4],d_model), nn.ReLU(),\n",
        "#             nn.Linear(d_model,1),\n",
        "#         )\n",
        "#     def forward(self, x): return self.cnn(x)\n",
        "\n",
        "d_model = 256\n",
        "# tcost = nn.Sequential( # trained cost\n",
        "#     # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
        "#     nn.Linear(d_model, 1),\n",
        "#     ).to(device)\n",
        "mul=4\n",
        "tcost = nn.Sequential( # trained cost\n",
        "    # nn.Linear(d_model+dim_a, d_model), nn.ReLU(),\n",
        "    nn.Dropout(), nn.Linear(d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, mul*d_model), nn.LeakyReLU(),\n",
        "    nn.Dropout(), nn.Linear(mul*d_model, 2),\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# agent = Agent(d_model=256).to(device)\n",
        "# optim = torch.optim.AdamW(agent.parameters(), 1e-2, (0.9, 0.95))\n",
        "optim = torch.optim.AdamW(tcost.parameters(), 1e-3, (0.9, 0.95))\n",
        "# optim.param_groups[0][\"lr\"] = 1e-1\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# loss_function = torch.nn.MSELoss()\n",
        "# loss_function = torch.nn.L1Loss()\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "def train_cost(model, dataloader, optim, loss_function=loss_function):\n",
        "    model.train()\n",
        "    tcost.train()\n",
        "    for batch, (st, r) in enumerate(dataloader):\n",
        "        st, r = st.to(device), r.to(device)#.squeeze(-1)\n",
        "        # st.requires_grad=True; r.requires_grad=True\n",
        "        # print(st.requires_grad, r.requires_grad)\n",
        "        # loss = F.mse_loss(model.tcost(model.jepa.enc(st)), r)\n",
        "        # print(model.jepa.enc(st))\n",
        "        # loss = loss_function(model.tcost(model.jepa.enc(st)), r)\n",
        "        with torch.no_grad(): sx = model.jepa.enc(st)\n",
        "        with torch.amp.autocast('cuda'): loss = loss_function(tcost(sx), r)\n",
        "        # print(tcost(sx).squeeze(-1))\n",
        "        # loss = loss_function(model(st), r)\n",
        "        # print(next(model.tcost[0].parameters()).grad)\n",
        "        # print(next(model.jepa.enc.parameters()).grad)\n",
        "        # print(model.tcost.parameters()[0].grad)\n",
        "        # print(loss)\n",
        "        # loss.backward()\n",
        "        # optim.step()\n",
        "        # optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        optim.zero_grad()\n",
        "        print(loss.item())\n",
        "        try: wandb.log({\"closs\": loss.item()})\n",
        "        except: pass\n",
        "\n",
        "\n",
        "# for i in range(30):\n",
        "#     train_cost(agent, c_loader, optim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Su8Op3bw0OIT"
      },
      "outputs": [],
      "source": [
        "# @title train_ae\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'): # automatic mixed percision\n",
        "                    # print(\"train jepa world_state_\", world_state_) # 8.2697 # 2.0750e-11\n",
        "                    sy = self.jepa.enc(state) # [batch_size, d_model]\n",
        "                    # sy = self.jepa.enc_ema(world_state_.flatten(start_dim=1)) # [batch_size, d_model]\n",
        "\n",
        "                    # std_loss, cov_loss = self.jepa.v_creg(self.jepa.exp(sy))\n",
        "                    # jloss = std_loss + cov_loss\n",
        "\n",
        "                    # state_ = self.deconv(sy.detach()) # pure jepa\n",
        "                    state_ = self.deconv(sy) # ae\n",
        "                    # tsmall = torch.nn.Sequential(transforms.Resize((32,32)), transforms.Grayscale(1))\n",
        "\n",
        "                    conv_loss = F.mse_loss(state_, state)\n",
        "                    # conv_loss = F.mse_loss(state_, tsmall(state))\n",
        "                    # loss = jloss + conv_loss\n",
        "                    loss = conv_loss\n",
        "\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n",
        "    def get_down(self, state, world_state=None): # update world_state and mem from state\n",
        "        current = self.sense(state) # [batch_size, d_model] or [1, d_model]\n",
        "        obs = current# + mem # [batch_size, d_model]\n",
        "        K, V = self.k(obs), self.v(obs) # [batch_size, d_model]\n",
        "        # K, V = F.normalize(K, dim=-1), F.normalize(V, dim=-1)\n",
        "        K = F.normalize(K, dim=-1)\n",
        "        if V.shape[0]>1 and V.ndim==2: K, V = K.unsqueeze(1), V.unsqueeze(1) # [batch_size, 1, d_model]\n",
        "        V_ = world_state @ K.transpose(-1,-2) # [batch_size, d_model, d_model] @ [batch_size, d_model, 1] = [batch_size, d_model, 1]\n",
        "        world_state = world_state + (V.transpose(-1,-2) - V_) @ K#.T # -V_.K^T, + V.K^T # update world state\n",
        "        return world_state\n",
        "\n",
        "    def train_ae(self, dataloader, optim, bptt=32):\n",
        "        self.train()\n",
        "        for batch, Sar in enumerate(dataloader):\n",
        "            # state = torch.rand((batch_size, 3,64,64), device=device)\n",
        "            # sx_ = self.jepa.enc(state) # [batch_size, d_model]\n",
        "            world_state = torch.zeros((batch_size, self.d_model, self.d_model), device=device) # Sum i] vi kiT\n",
        "\n",
        "            for i, (state, action, reward) in enumerate(zip(*Sar)):\n",
        "                state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    world_state = self.get_down(state, world_state)\n",
        "                    # sy = self.jepa.enc(world_state_.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1)) # [batch_size, d_model]\n",
        "                    # sy = self.convenc(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "                    sy = self.effnet(world_state.unsqueeze(1).detach()) # [batch_size, d_model]\n",
        "\n",
        "                    world_state_ = self.deconvenc(sy).squeeze(1) # ae\n",
        "                    # loss = F.mse_loss(world_state_, world_state)\n",
        "                    loss = F.mse_loss(world_state_, world_state.detach())\n",
        "                    # print(\"std, cov, conv\", std_loss.item(), cov_loss.item(), conv_loss.item())\n",
        "                    print(\"loss\", loss.item())\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optim)\n",
        "                    scaler.update()\n",
        "                    optim.zero_grad()\n",
        "                    # sx_ = sx_.detach()\n",
        "\n",
        "                # try: wandb.log({\"repr\": repr_loss.item(), \"std\": std_loss.item(), \"cov\": cov_loss.item(), \"closs\": closs.item()})\n",
        "                try: wandb.log({\"std\": std_loss.item(), \"cov\": cov_loss.item(), \"conv\": conv_loss.item()})\n",
        "                except: pass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0u9XYJvdIf6p"
      },
      "outputs": [],
      "source": [
        "# @title dataloader from transformer\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "# import faiss\n",
        "import random\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer, seq_len):\n",
        "        # self.data = buffer\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        state, action, reward = zip(*sar)\n",
        "        state = [self.transform(s) for s in state]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return state, action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, buffer, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.data = [step for episode in buffer for step in episode]\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "        seq_len = data.size(0) // batch_size\n",
        "        data = data[:seq_len * batch_size]\n",
        "        # data = data.view(bsz, seq_len).t().contiguous()\n",
        "        data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "        # self.bptt = 35\n",
        "        # self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i = self.ind[index]\n",
        "        seq_len = min(self.bptt, len(self.data) - i)\n",
        "        data = self.data[i:i+seq_len]\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sar = self.data[idx*self.seq_len : (idx+1)*self.seq_len]\n",
        "        # state, action, reward = zip(*sar)\n",
        "        # state = [self.transform(s) for s in state]\n",
        "        state, action, reward = self.data[idx]\n",
        "        # print(state.shape, action.shape, reward.shape)\n",
        "        return self.transform(state), action, torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "\n",
        "def collate_fn(sar):\n",
        "    # x,y=zip(*data)\n",
        "    state, action, reward = zip(*sar)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    # x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "    state=torch.stack(list(state), dim=0)\n",
        "    action=torch.stack(list(action), dim=0)\n",
        "    reward=torch.stack(list(reward), dim=0)\n",
        "    # y=torch.stack(list(y)).T.flatten()\n",
        "    return state, action, reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title Datasetme\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data, batch_size):\n",
        "#         data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.batch_size = batch_size\n",
        "\n",
        "#         seq_len = data.size(0) // batch_size\n",
        "#         data = data[:seq_len * batch_size]\n",
        "#         # data = data.view(bsz, seq_len).t().contiguous()\n",
        "#         data = data.view(batch_size, seq_len).T.contiguous()\n",
        "\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0), step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return self.data.size(0) // self.batch_size\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         i = self.ind[index]\n",
        "#         seq_len = min(self.bptt, len(self.data) - i)\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         return data\n",
        "\n",
        "\n",
        "\n",
        "# class Datasetme(torch.utils.data.Dataset):\n",
        "#     def __init__(self, raw_data):\n",
        "#         self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "#         self.bptt = 35\n",
        "#         self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data) // self.bptt\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         i=idx*self.bptt\n",
        "#         seq_len = self.bptt\n",
        "#         data = self.data[i:i+seq_len]\n",
        "#         target = self.data[i+1:i+1+seq_len].reshape(-1)\n",
        "#         return data, target\n",
        "\n",
        "# train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "# batch_size=128\n",
        "# train_iter = Datasetme(train_iter)\n",
        "# # train_loader = Datasetme(train_iter, batch_size)\n",
        "\n",
        "\n",
        "# def collate_fn(data):\n",
        "#     x,y=zip(*data)\n",
        "#     # print(\"collate\",len(x),len(y))\n",
        "#     x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "#     y=torch.stack(list(y)).T.flatten()\n",
        "#     return x, y\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# seq_len = 50 # 50\n",
        "batch_size = 64 #512\n",
        "train_data = BufferDataset(buffer, batch_size)\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "# train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "train_loader = DataLoader(train_data, shuffle = True,collate_fn=torch.utils.data._utils.collate.default_convert, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n",
        "# train_data.data = train_data.data + episode\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title transfer_optim bad?\n",
        "\n",
        "import torch\n",
        "\n",
        "def transfer_optim(src_optim, tgt_optim, param_mapping):\n",
        "    src_sd = src_optim.state_dict()\n",
        "    tgt_sd = tgt_optim.state_dict()\n",
        "\n",
        "    # Iterate over each parameter in the target optimizer\n",
        "    for (tgt_idx, target_param) in enumerate(tgt_optim.param_groups[0]['params']):\n",
        "        target_id = id(target_param)\n",
        "\n",
        "        # Find the corresponding source parameter using param_mapping\n",
        "        if target_id in param_mapping:\n",
        "            source_param = param_mapping[target_id]\n",
        "            source_id = id(source_param)\n",
        "\n",
        "            # If there's an existing state for the source parameter, transfer it\n",
        "            if source_id in src_sd['state']:\n",
        "                source_state = src_sd['state'][source_id]\n",
        "                target_state = {}\n",
        "\n",
        "                # Handle momentum/first and second moments (e.g., `exp_avg`, `exp_avg_sq` in Adam)\n",
        "                for key in source_state.keys():\n",
        "                    if source_state[key].shape == target_param.shape: target_state[key] = source_state[key].clone()\n",
        "                    # If size doesn't match, either copy what you can or initialise new values\n",
        "                    elif key in ['exp_avg', 'exp_avg_sq']:  # Momentums (specific to Adam-like optimizers)\n",
        "                        target_state[key] = torch.zeros_like(target_param)\n",
        "                        target_state[key][:source_param.numel()] = source_state[key].flatten()[:target_param.numel()]\n",
        "                    else: target_state[key] = torch.zeros_like(target_param) # init\n",
        "                tgt_sd['state'][target_id] = target_state\n",
        "\n",
        "    # Load the updated state dict back into the target optimizer\n",
        "    tgt_optim.load_state_dict(tgt_sd)\n",
        "    return tgt_optim\n",
        "# {'state': {0: {'step': tensor(1.), 'exp_avg': tensor, 'exp_avg_sq': tensor}, 1: }}\n",
        "\n",
        "\n",
        "\n",
        "model_src = torch.nn.Linear(10, 5)  # Example source model\n",
        "model_tgt = torch.nn.Linear(20, 5)  # Example target model (with more input dimensions)\n",
        "\n",
        "# model_src = nn.Sequential( # trained cost\n",
        "#     nn.Linear(10, 5, bias=False), nn.Softmax(),\n",
        "#     )\n",
        "# d_model=4\n",
        "# model_tgt = nn.Sequential( # trained cost\n",
        "#     nn.Linear(20, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, d_model), nn.ReLU(),\n",
        "#     nn.Linear(d_model, 5), nn.Softmax(),\n",
        "#     )\n",
        "\n",
        "source_optimizer = optim.AdamW(model_src.parameters())\n",
        "target_optimizer = optim.AdamW(model_tgt.parameters())\n",
        "\n",
        "dummy_input = torch.randn(3, 10)\n",
        "dummy_target = torch.randn(3, 5)\n",
        "criterion = torch.nn.MSELoss()\n",
        "output = model_src(dummy_input)\n",
        "loss = criterion(output, dummy_target)\n",
        "loss.backward()\n",
        "source_optimizer.step()\n",
        "\n",
        "param_mapping = {id(tgt_param): src_param for src_param, tgt_param in zip(model_src.parameters(), model_tgt.parameters())}\n",
        "target_optimizer = transfer_optim(source_optimizer, target_optimizer, param_mapping)\n",
        "\n",
        "print(source_optimizer.state_dict())\n",
        "print(target_optimizer.state_dict())\n",
        "\n",
        "\n",
        "# checkpoint = {'model': agent.state_dict(), 'optimizer': optim.state_dict(),}\n",
        "# torch.save(checkpoint, folder+'agentoptim.pkl')\n",
        "\n",
        "# modelsd, optimsd = torch.load('agentoptim.pkl').values()\n",
        "\n",
        "# optim.load_state_dict(optimsd)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title transfer_optim bad? 2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def transfer_optimizer_state(source_layer, target_layer, optimizer):\n",
        "    opt_state_dict = optimizer.state_dict()\n",
        "    for group in opt_state_dict['param_groups']:\n",
        "        # For each parameter index (p in param group refers to the layer parameters)\n",
        "        for param_idx, p in enumerate(group['params']):\n",
        "            print(p,source_layer.weight)\n",
        "            if p == source_layer.weight:\n",
        "                # Find the corresponding target layer parameter (in this case, target_layer.weight)\n",
        "                target_param = target_layer.weight\n",
        "                source_state = optimizer.state[p]  # Get the state for the source parameter\n",
        "\n",
        "                # If the parameter is found in the optimizer's state dict\n",
        "                if 'exp_avg' in source_state and 'exp_avg_sq' in source_state:\n",
        "                    exp_avg = source_state['exp_avg']  # First moment (momentum)\n",
        "                    exp_avg_sq = source_state['exp_avg_sq']  # Second moment (variance)\n",
        "\n",
        "                    # Handle input dimension mismatch (copy/truncate or pad)\n",
        "                    source_in_dim = source_layer.weight.shape[1]\n",
        "                    target_in_dim = target_layer.weight.shape[1]\n",
        "\n",
        "                    # Copy optimizer state (exp_avg and exp_avg_sq) accordingly\n",
        "                    with torch.no_grad():\n",
        "                        # Copy the available part and initialize new dimensions to zero\n",
        "                        new_exp_avg = torch.zeros_like(target_param)\n",
        "                        new_exp_avg_sq = torch.zeros_like(target_param)\n",
        "                        # new_exp_avg[:, source_in_dim:] = 0  # Initialize extra dimensions\n",
        "                        # new_exp_avg_sq[:, source_in_dim:] = 0  # Initialize extra dimensions\n",
        "                        new_exp_avg[:, :source_in_dim] = exp_avg[:, :target_in_dim]\n",
        "                        new_exp_avg_sq[:, :source_in_dim] = exp_avg_sq[:, :target_in_dim]\n",
        "\n",
        "                    # Update the target layer's optimizer state\n",
        "                    optimizer.state[target_param] = {\n",
        "                        'exp_avg': new_exp_avg,\n",
        "                        'exp_avg_sq': new_exp_avg_sq,\n",
        "                        'step': source_state['step']  # Keep the same step count\n",
        "                    }\n",
        "\n",
        "                # Handle the bias (if it exists)\n",
        "                if hasattr(source_layer, 'bias') and hasattr(target_layer, 'bias'):\n",
        "                    source_bias = optimizer.state[source_layer.bias]\n",
        "                    target_bias = target_layer.bias\n",
        "\n",
        "                    optimizer.state[target_bias] = source_bias\n",
        "    return optimizer\n",
        "\n",
        "# Example usage:\n",
        "d = 10  # Input dimension of the source layer\n",
        "a = 5   # Extra nodes to be omitted or added in the target layer\n",
        "m = 8   # Output dimension (same for both)\n",
        "\n",
        "# Source layer (input dimension d+a)\n",
        "source_layer = nn.Linear(d+a, m)\n",
        "\n",
        "# Target layer (input dimension d, or d+a, or arbitrary)\n",
        "target_layer = nn.Linear(d, m)\n",
        "\n",
        "# Optimizer (using AdamW in this case)\n",
        "optimizer = torch.optim.AdamW(source_layer.parameters())\n",
        "\n",
        "# Perform weight transfer (from d+a to d or vice versa) here (assumed done already)\n",
        "\n",
        "print(optimizer.state_dict())\n",
        "# Transfer optimizer states\n",
        "optimizer = transfer_optimizer_state(source_layer, target_layer, optimizer)\n",
        "print(optimizer.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_optimizer_state(source_layer, target_layer, optimizer):\n",
        "    state_dict = optimizer.state_dict()\n",
        "    for old_param, new_param in zip(source_layer.parameters(), target_layer.parameters()):\n",
        "        # If old_param exists in optimizer state\n",
        "        if old_param in state_dict['state']:\n",
        "            # Get the state for the old parameter\n",
        "            old_state = state_dict['state'][old_param]\n",
        "            new_state = {}\n",
        "\n",
        "            for key, value in old_state.items():\n",
        "                if key in ['exp_avg', 'exp_avg_sq']:  # for Adam or AdamW momentum estimates\n",
        "                    # Handle the shape adjustment (copy, shrink, or randomly initialise the extra nodes)\n",
        "                    new_state[key] = torch.zeros_like(new_param)  # Initialise with zeros\n",
        "                    new_state[key][:old_param.shape[0]] = value[:new_param.shape[0]]  # Copy old values\n",
        "                    # else:\n",
        "                    #     new_state[key] = value.clone()  # Copy directly if shapes match\n",
        "                else:\n",
        "                    new_state[key] = value  # Copy other states directly if they exist\n",
        "\n",
        "            # Set the new parameter in optimizer state\n",
        "            state_dict['state'][new_param] = new_state\n",
        "            # Remove the old parameter from the optimizer state\n",
        "            del state_dict['state'][old_param]\n",
        "\n",
        "    # Load the updated state dict into the optimizer\n",
        "    optimizer.load_state_dict(state_dict)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def transfer_optim(src_model, tgt_model, src_optim, tgt_optim):\n",
        "    src_sd = src_optim.state_dict()\n",
        "    for src_param, tgt_param in zip(src_model.parameters(), tgt_model.parameters()):\n",
        "        # If src_param exists in optimizer state\n",
        "        if src_param in src_sd['state']:\n",
        "            # Get the state for the old parameter\n",
        "            old_state = src_sd['state'][src_param]\n",
        "            new_state = {}\n",
        "\n",
        "            for key, value in old_state.items():\n",
        "                new_state[key] = torch.zeros_like(tgt_param)  # Initialise with zeros\n",
        "                new_state[key][:src_param.shape[0]] = value[:tgt_param.shape[0]]  # Copy old values\n",
        "\n",
        "            src_sd['state'][tgt_param] = new_state\n",
        "            del src_sd['state'][src_param]\n",
        "\n",
        "    # Load the updated state dict into the optimizer\n",
        "    optimizer.load_state_dict(src_sd)\n",
        "    return optimizer\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cX71EprCMSNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title rename wht_name\n",
        "# wht_name='jepa.enc.cnn.0.weight'\n",
        "wht_name='jepa.pred.weight_ih_l0'\n",
        "# wht_name='emb.weight'\n",
        "# print(o.isnumeric())\n",
        "# mask = [x.isnumeric() for x in o]\n",
        "# print(o[mask])\n",
        "na_=''\n",
        "# j=0\n",
        "\n",
        "for wht_name in agent.state_dict().keys():\n",
        "    o=wht_name.split('.')\n",
        "    # print(o)\n",
        "    name=wht_name\n",
        "    print(\"####\", wht_name)\n",
        "    for i in range(len(o)):\n",
        "        c = o[i]\n",
        "        if c.isnumeric():\n",
        "            na = '.'.join(o[:i])\n",
        "            me = '.'.join(o[i+1:])\n",
        "            # print(c_,c, c_<c, )\n",
        "            c=int(c)\n",
        "            if na!=na_: # param name diff\n",
        "                j=0 # reset num\n",
        "                c_=c # track wht_name num\n",
        "                na_=na # track param name\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('1', name)\n",
        "            elif c_<c: # same param name, diff num\n",
        "                j+=1\n",
        "                c_=c\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('2', name)\n",
        "            else: # same param name, same num\n",
        "                name = f'{na}.{j}.{me}'\n",
        "                print('3', name)\n",
        "    print('4', name)\n"
      ],
      "metadata": {
        "id": "LKUSzmYLLuRh",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "07ca4b89-257b-4205-c5c8-6a96474ae82a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-186620617543>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# j=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mwht_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwht_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# print(o)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title geomloss, Python Optimal Transport\n",
        "# !pip install geomloss[full]\n",
        "\n",
        "import torch\n",
        "from geomloss import SamplesLoss  # See also ImagesLoss, VolumesLoss\n",
        "\n",
        "# # Create some large point clouds in 3D\n",
        "# x = torch.randn(100000, 3, requires_grad=True).cuda()\n",
        "# y = torch.randn(200000, 3).cuda()\n",
        "\n",
        "# x = torch.rand(1000, 1)\n",
        "# y = torch.rand(1000, 1)\n",
        "x = torch.tensor([0, 0, 1]).float().unsqueeze(-1)\n",
        "y = torch.tensor([0, 1, 0]).float().unsqueeze(-1)\n",
        "# k=1.\n",
        "# y = torch.tensor([k, k, k]).float().unsqueeze(-1)\n",
        "\n",
        "\n",
        "\n",
        "# loss_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.01) # 0.05, quadratic, Wasserstein-2. low blur => closer to true Wasserstein dist but slower compute\n",
        "\n",
        "loss = loss_fn(x, y)  # By default, use constant weights = 1/number of samples\n",
        "print(loss)\n",
        "# g_x, = torch.autograd.grad(L, [x])\n",
        "\n",
        "# [0, 1, 0]: 2.4253e-12, 2.4253e-12\n",
        "# [0, 0, 0.1]: 0.1350; [0, 0, 0.5]: 0.0417; [0, 0, 1]: 0\n",
        "# k=0.: 0.1666; k=0.1: 0.1383; k=0.333: 0.1111; k=0.5: 0.1250; k=1.: 0.3333\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from geomloss import SamplesLoss\n",
        "\n",
        "# Define x and y as n-dimensional tensors representing mass distributions\n",
        "# x = torch.tensor([0.2, 0.3, 0.5], dtype=torch.float32, requires_grad=True).cuda()\n",
        "# y = torch.tensor([0, 0, 1], dtype=torch.float32, requires_grad=True).cuda()\n",
        "# x = torch.tensor([0.2, 0.3, 0.5]).float().unsqueeze(-1)\n",
        "# x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5]).float().unsqueeze(-1))\n",
        "x = nn.Parameter(torch.tensor([0,1.5,0]).float().unsqueeze(-1))\n",
        "y = torch.tensor([0, 0, 1]).float().unsqueeze(-1)\n",
        "\n",
        "# Create a position tensor representing the index of each element\n",
        "positions_x = torch.arange(x.shape[0], dtype=float).unsqueeze(1)\n",
        "positions_y = torch.arange(y.shape[0], dtype=float).unsqueeze(1)\n",
        "\n",
        "# Sinkhorn loss using GeomLoss\n",
        "loss_fn = SamplesLoss(\"sinkhorn\", p=1, blur=0.05)  # p=1 for Wasserstein-1\n",
        "# loss_fn = SamplesLoss(loss=\"sinkhorn\", p=1, blur=0.05, scaling=0.9, debias=True)\n",
        "\n",
        "transport_cost = loss_fn(positions_x, x, positions_y, y)\n",
        "\n",
        "print(transport_cost.item())\n",
        "# 1.298424361328248\n",
        "\n",
        "transport_cost.backward()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pip install POT\n",
        "\n",
        "import ot\n",
        "import numpy as np\n",
        "\n",
        "def sinkhorn_distance(x, y, reg=0.01):\n",
        "    # x = x / np.sum(x)\n",
        "    # y = y / np.sum(y)\n",
        "    # Create the cost matrix (1D example, Euclidean distances between positions)\n",
        "    n = len(x)\n",
        "    cost_matrix = np.abs(np.arange(n)[:, None] - np.arange(n)[None, :])\n",
        "    # print(cost_matrix)\n",
        "    # # Compute Sinkhorn distance using POT's Sinkhorn algorithm\n",
        "    print(x, y, cost_matrix, reg)\n",
        "    transport_plan = ot.sinkhorn(x, y, cost_matrix, reg)\n",
        "    print(transport_plan)\n",
        "    distance = np.sum(transport_plan * cost_matrix)\n",
        "    return distance\n",
        "\n",
        "x = np.array([0.2, 0.3, 0.5])\n",
        "y = np.array([0, 0, 1])\n",
        "distance = sinkhorn_distance(x, y)\n",
        "print(f'Sinkhorn distance: {distance}')\n",
        "# distance.backward()\n",
        "\n",
        "def sinkhorn_distance(x, y, reg=0.01):\n",
        "    # x = x / torch.sum(x)\n",
        "    # y = y / torch.sum(y)\n",
        "    # Create the cost matrix (1D example, Euclidean distances between positions)\n",
        "    n = len(x)\n",
        "    cost_matrix = torch.abs(torch.arange(n)[:, None] - torch.arange(n)[None, :])\n",
        "    # print(cost_matrix)\n",
        "    # Compute Sinkhorn distance using POT's Sinkhorn algorithm\n",
        "    print(x, y, cost_matrix, reg)\n",
        "    transport_plan = ot.sinkhorn(x, y, cost_matrix, reg)\n",
        "    print(transport_plan)\n",
        "    distance = torch.sum(transport_plan * cost_matrix)\n",
        "    return distance\n",
        "\n",
        "# x = np.array([0.2, 0.3, 0.5])\n",
        "# y = np.array([0, 0, 1])\n",
        "x = nn.Parameter(torch.tensor([0.2, 0.3, 0.5]).float())#.unsqueeze(-1))\n",
        "y = torch.tensor([0, 0, 1]).float()#.unsqueeze(-1)\n",
        "\n",
        "distance = sinkhorn_distance(x, y)\n",
        "print(f'Sinkhorn distance: {distance}')\n",
        "distance.backward()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "4CACQCCaxA_Y",
        "outputId": "b5d127cd-18ce-49e5-b1e2-d883cb34125a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1746836772511624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title batchify argm train\n",
        "\n",
        "def rnn_pred(self, sx, la, lz=None, h0=None, gamma=0.9): # 0.95 [1, d_model], [batch, seq_len, dim_a/z], [num_layers, d_model]\n",
        "    self.jepa.pred.train()\n",
        "    batch, seq_len, dim_a = la.shape\n",
        "    cost = 0\n",
        "    sx=sx.repeat(batch, 1) # [batch, d_model]\n",
        "    lsx=sx.unsqueeze(1)\n",
        "    h0=h0.repeat(1, batch, 1) # [num_layers, batch, d_model]\n",
        "    lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "    # print(lsx.shape, la.shape, lz.shape)\n",
        "    c=[]\n",
        "    for t in range(seq_len):\n",
        "        a, z = la[:,t], lz[:,t] # [1, dim_a], [1, dim_z]\n",
        "        # print(sx.shape, a.shape, z.shape)\n",
        "        sxaz = torch.cat([sx, a, z], dim=-1).unsqueeze(1)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            out, h0 = self.jepa.pred(sxaz, h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "            sx = sx + out.squeeze(1) # [batch,seq_len,d_model]\n",
        "            syh0 = torch.cat([sx.flatten(1),h0.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            tcost = -self.tcost(syh0)\n",
        "        c.append(tcost)\n",
        "        lsx = torch.cat([lsx, sx.unsqueeze(1)], dim=1) # [batch, T, d_model]\n",
        "        lh0 = torch.cat([lh0, h0.unsqueeze(0)], dim=0) # [seq_len, num_layers, batch, d_model]\n",
        "        icost = 0#*0.0005*self.icost.boredom(lsx, linsx=None) # + self.icost(sx)\n",
        "        # print(\"tcost icost\", tcost.item(), icost.item())\n",
        "        cost += (tcost + icost)*gamma**t\n",
        "    return cost, lsx, lh0, c\n",
        "\n",
        "\n",
        "\n",
        "def argm(self, sy, sy_, h0, a, reward, lr=3e3): # 3e3\n",
        "    self.tcost.eval()\n",
        "    batch_size = sy.shape[0] # [batch_size, d_model]\n",
        "    z = nn.Parameter(torch.zeros((batch_size, self.dim_z), device=device))\n",
        "    # torch.nn.init.xavier_uniform_(z)\n",
        "    torch.nn.init.normal_(z, mean=0., std=.3/z.shape[-1]**0.5)\n",
        "    # optim = torch.optim.SGD([z], lr=1e-2) # 1e-2\n",
        "    # optim = torch.optim.AdamW([z], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "    optim = torch.optim.AdamW([z], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "    sy, sy_ = sy.detach(), sy_.detach()\n",
        "    out = sy - sy_\n",
        "    h0, a, reward = h0.detach(), a.detach(), reward.detach()\n",
        "    for i in range(10): # 10\n",
        "        with torch.amp.autocast('cuda'):\n",
        "\n",
        "\n",
        "\n",
        "            syaz = torch.cat([sy_, a, z], dim=-1)\n",
        "            out_, h0_ = self.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "            repr_loss = F.mse_loss(out, out_[:, -1, :])\n",
        "            # syh0 = torch.cat([sy.flatten(1),F.dropout(h0_, p=0.5).permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            syh0 = torch.cat([sy.flatten(1),h0_.permute(1,0,2).flatten(1)], dim=-1) # [batch_size,seq_len,d_model], [num_layers,batch_size,d_model]\n",
        "            clossl = self.tcost.loss(syh0, reward).squeeze(-1)\n",
        "            z_loss = torch.abs(z).sum() # z_loss = torch.norm(z)\n",
        "            print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = self.jepa.sim_coeff * repr_loss + self.closs_coeff * clossl + self.zloss_coeff * z_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        cost.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        # with torch.no_grad(): z.clamp_(min=-1, max=1)\n",
        "        with torch.no_grad(): z /= torch.norm(z, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    self.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    return z.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def argm(lsy, sy, h0, la, rwd):\n",
        "    # lz = agent.argm(out, h0, la, reward)\n",
        "    agent.tcost.eval()\n",
        "    batch_size, bptt, _ = lsy.shape # [batch_size, bptt, d_model]\n",
        "    lz = nn.Parameter(torch.zeros((batch_size, bptt, agent.dim_z), device=device))\n",
        "    # torch.nn.init.xavier_uniform_(lz)\n",
        "    torch.nn.init.normal_(lz, mean=0., std=.3/lz.shape[-1]**0.5)\n",
        "    # optim = torch.optim.SGD([lz], lr=1e-2) # 1e-2\n",
        "    # optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.95)) # 1e-1\n",
        "    optim = torch.optim.AdamW([lz], 1e-1, (0.9, 0.999)) # 1e-1\n",
        "    lsy, la, rwd = lsy.detach(), la.detach(), rwd.detach()\n",
        "    for i in range(3): # 10\n",
        "        sy_, h0_ = sy.detach(), h0.detach()\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        lsy_ = torch.empty((batch_size, 0, agent.d_model), device=device) # [batch_size, T, d_model]\n",
        "        with torch.cuda.amp.autocast():\n",
        "            for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                out_, h0_ = agent.jepa.pred(syaz.unsqueeze(1), h0_) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                lh0 = torch.cat((lh0, h0_.unsqueeze(0)), dim=0)\n",
        "            repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "            clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "            z_loss = torch.abs(lz).sum() # z_loss = torch.norm(z)\n",
        "            # print(\"z_loss\", i, z[0].data, z_loss)\n",
        "            cost = agent.jepa.sim_coeff * repr_loss + agent.closs_coeff * clossl + agent.zloss_coeff * z_loss\n",
        "        cost.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        with torch.no_grad(): lz /= torch.norm(lz, dim=-1).unsqueeze(-1).clamp_(min=1)\n",
        "        # print(i, \"repr c z loss, z\", torch.cat([torch.tensor([repr_loss, clossl, z_loss]), z[0].cpu()],dim=-1).squeeze().data)\n",
        "        # print(\"worst case\",i,cost.item(), torch.mean(abs(z.detach())).item())\n",
        "    # if loss.item()>0.9: print(\"argm\",loss.item(), torch.mean(abs(z.detach())).item())\n",
        "    agent.tcost.train() # https://pytorch.org/docs/stable/_modules/torch/autograd/grad_mode.html#no_grad # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module.eval\n",
        "    return lz.detach()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# closs_fn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.01)\n",
        "bptt = 25\n",
        "for batch, Sar in enumerate(train_loader): # [seq_len, batch_size, ] -> [batch_size, ]\n",
        "    h0 = torch.zeros((agent.jepa.pred.num_layers, batch_size, agent.d_model), device=device) # [num_layers, batch, d_model]\n",
        "    state = torch.zeros((batch_size, 3,64,64), device=device)\n",
        "    sy_ = agent.jepa.enc(state).unsqueeze(1) # [batch_size, 1, d_model]\n",
        "    # sx=sy_\n",
        "    state, action, reward = Sar # collate: [seq_len, batch_length], default: [batch_size, seq_len]\n",
        "    state, action, reward = state.to(device), action.to(device), reward.to(device)\n",
        "\n",
        "    for st, act, rwd in zip(torch.split(state, bptt, dim=1), torch.split(action, bptt, dim=1), torch.split(reward, bptt, dim=1)):\n",
        "        lh0 = torch.empty((0,)+h0.shape, device=device)\n",
        "        lsy_ = torch.empty((batch_size, 0, agent.d_model), device=device) # [batch_size, T, d_model]\n",
        "\n",
        "        with torch.cuda.amp.autocast(): # with torch.amp.GradScaler('cuda'):\n",
        "            lsy = agent.jepa.enc(st.flatten(end_dim=1)).unflatten(0, (batch_size, -1)) # [batch_size, bptt, d_model]\n",
        "            la = agent.emb(act) # [batch_size, bptt, dim_a]\n",
        "            out = lsy - torch.cat([sy_, lsy[:,:-1]], dim=1)\n",
        "            # lz = agent.argm(out, h0, la, reward)\n",
        "            lz = argm(lsy, sy_, h0, la, rwd)\n",
        "            # lz = torch.zeros((batch_size, bptt, agent.dim_z), device=device)\n",
        "\n",
        "            for i, (a, z) in enumerate(zip(la.permute(1,0,2), lz.permute(1,0,2))):\n",
        "                syaz = torch.cat([sy_.squeeze(1), a, z], dim=-1) # [batch_size, 1, d_model], [batch_size, dim_az]\n",
        "                out_, h0 = agent.jepa.pred(syaz.unsqueeze(1), h0) # [batch,seq_len,d_model], [num_layers,batch,d_model]\n",
        "                sy_ = sy_ + out_[:, -1, :].unsqueeze(1)\n",
        "                lsy_ = torch.cat((lsy_, sy_), dim=1)\n",
        "                lh0 = torch.cat((lh0, h0.unsqueeze(0)), dim=0)\n",
        "\n",
        "            repr_loss = F.mse_loss(lsy, lsy_)\n",
        "            std_loss, cov_loss = agent.jepa.v_creg(agent.jepa.exp(lsy.flatten(end_dim=1)))\n",
        "            jloss = agent.jepa.sim_coeff * repr_loss + agent.jepa.std_coeff * std_loss + agent.jepa.cov_coeff * cov_loss\n",
        "\n",
        "            syh0 = torch.cat([lsy_, lh0.permute(2,0,1,3).flatten(2)], dim=-1).flatten(end_dim=1) # [batch_size,bptt,d_model], [bptt,num_layers,batch_size,d_model] -> [batch_size*bptt, (1+num_layers)*d_model]\n",
        "            # print(\"syh0, rwd\",syh0.shape,rwd.shape)\n",
        "            clossl = agent.tcost.loss(syh0, rwd.flatten())\n",
        "            # reward_ = agent.tcost(syh0)\n",
        "            # clossl = wasserstein(rwd, reward_)#.squeeze(-1)\n",
        "            closs = agent.closs_coeff * clossl\n",
        "\n",
        "            # print(h0.requires_grad)\n",
        "            # pred = agent.tcost(syh0).squeeze(-1).cpu()\n",
        "            # mask = torch.where(abs(reward- pred)>0.5,1,0).bool()\n",
        "            # print(\"reward, pred, clossl\", reward[mask].data, pred[mask].data, clossl.item())\n",
        "            # try: imshow(torchvision.utils.make_grid(state[mask], nrow=10))\n",
        "            # except ZeroDivisionError: pass\n",
        "\n",
        "\n",
        "            loss = jloss + closs\n",
        "\n",
        "            # print(\"norm\", torch.norm(sy, dim=-1)[0].item())\n",
        "            norm = torch.norm(lsy, dim=-1)[0][0].item()\n",
        "            z_norm = torch.norm(z)\n",
        "            # print(\"repr, std, cov\", repr_loss.item(), std_loss.item(), cov_loss.item())\n",
        "            # print(\"repr, std, cov, clossl\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item())\n",
        "            print(\"repr, std, cov, clossl, z, norm\", repr_loss.item(), std_loss.item(), cov_loss.item(), clossl.item(), z_norm.item(), norm)\n",
        "            scaler.scale(loss).backward()\n",
        "            # torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
        "            scaler.step(optim)\n",
        "            scaler.update()\n",
        "            optim.zero_grad()\n",
        "            sy_, h0 = sy_.detach(), h0.detach()\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "id": "MqBL9hljvW-5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mhkK_9AQm8_q",
        "Jt_UlGz6Xoq3",
        "wUhKd009Qvk3"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}