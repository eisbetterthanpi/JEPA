{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JEPA_text.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4w5m_JAyK1wW",
        "BLW1TAw6K7s5",
        "75SRBabzK-6P",
        "oEr6soVkqbQU",
        "2gLKjKZej5L_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/JEPA/blob/main/JEPA_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### setup"
      ],
      "metadata": {
        "id": "4w5m_JAyK1wW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://openreview.net/pdf?id=BZ5a1r-kVsf\n",
        "# import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim\n",
        "import collections\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "!pip install optuna\n",
        "import optuna\n"
      ],
      "metadata": {
        "id": "btQjTpnUd95O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### data"
      ],
      "metadata": {
        "id": "RCu-4JfUn-wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data = open('input.txt', 'r').read()\n",
        "# data = list(data)\n",
        "\n",
        "# https://edisciplinas.usp.br/pluginfile.php/3403095/mod_resource/content/1/56ViktorFrankl_Mans%20Search.pdf\n",
        "text='''\n",
        "Only slowly could these men be guided back to the commonplace truth that no one has the right to do wrong, not\n",
        "even if wrong has been done to them. We had to strive to\n",
        "lead them back to this truth, or the consequences would\n",
        "have been much worse than the loss of a few thousand stalks\n",
        "of oats. I can still see the prisoner who rolled up his shirt\n",
        "sleeves, thrust his right hand under my nose and shouted,\n",
        "\"May this hand be cut off if I don't stain it with blood\n",
        "on the day when I get home!\" I want to emphasize that the\n",
        "man who said these words was not a bad fellow. He had\n",
        "been the best of comrades in camp and afterwards.\n",
        "Apart from the moral deformity resulting from the sudden release of mental pressure, there were two other\n",
        "fundamental experiences which threatened to damage the\n",
        "character of the liberated prisoner: bitterness and disillusionment when he returned to his former life.\n",
        "Bitterness was caused by a number of things he came up\n",
        "against in his former home town. When, on his return, a\n",
        "man found that in many places he was met only with a\n",
        "shrug of the shoulders and with hackneyed phrases, he\n",
        "tended to become bitter and to ask himself why he had\n",
        "gone through all that he had. When he heard the same\n",
        "phrases nearly everywhereâ€”\"We did not know about it,\"\n",
        "and \"We, too, have suffered,\" then he asked himself, have\n",
        "they really nothing better to say to me?\n",
        "The experience of disillusionment is different. Here it\n",
        "was not one's fellow man (whose superficiality and lack of\n",
        "feeling was so disgusting that one finally felt like creeping\n",
        "into a hole and neither hearing nor seeing human beings\n",
        "any more) but fate itself which seemed so cruel. A man who\n",
        "Experiences in a Concentration Camp 99\n",
        "for years had thought he had reached the absolute limit of\n",
        "all possible suffering now found that suffering has no limits,\n",
        "and that he could suffer still more, and still more intensely.\n",
        "When we spoke about attempts to give a man in camp\n",
        "mental courage, we said that he had to be shown something\n",
        "to look forward to in the future. He had to be reminded\n",
        "that life still waited for him, that a human being waited for\n",
        "his return. But after liberation? There were some men who\n",
        "found that no one awaited them. Woe to him who found\n",
        "that the person whose memory alone had given him courage\n",
        "in camp did not exist any more! Woe to him who, when the\n",
        "day of his dreams finally came, found it so different from all\n",
        "he had longed for! Perhaps he boarded a trolley, traveled\n",
        "out to the home which he had seen for years in his mind,\n",
        "and only in his mind, and pressed the bell, just as he has\n",
        "longed to do in thousands of dreams, only to find that the\n",
        "person who should open the door was not there, and would\n",
        "never be there again.\n",
        "'''\n",
        "\n",
        "# # make dataset\n",
        "# text=text.replace('\\n','')\n",
        "# data=list(text)\n",
        "# # print(data)\n",
        "# # data=sorted([ord(x) for x in set(data)])\n",
        "# dataset=[ord(x)-31 for x in data]\n",
        "# # dataset = map(lambda x:x.strip(\"8\"), lst)\n",
        "# # [f(x) if condition else g(x) for x in sequence]\n",
        "# dataset=[0 if x>91 else x for x in dataset] # 0 unk, 1-91\n",
        "# vocab_size=92\n",
        "# print(dataset)\n",
        "# 32space;A65-Z90;a97-z122\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "class Datasetme(Dataset): #https://www.kaggle.com/code/pinocookie/pytorch-dataset-and-dataloader/notebook\n",
        "    def __init__(self, text, embed_size=4):\n",
        "        super().__init__()\n",
        "        text=text.replace('\\n','')\n",
        "        data=list(text)\n",
        "        self.dataset=[self.encode(x) for x in data] # 0 unk, 1-91\n",
        "        self.vocab_size=vocab_size=92\n",
        "        self.embed_size=embed_size\n",
        "        self.batch_size = 1\n",
        "        self.embed=nn.Embedding(vocab_size, embed_size)\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)-1\n",
        "    def __getitem__(self, index):\n",
        "        # return self.dataset[index], self.dataset[index+1]\n",
        "        return self.get_embed(index), self.get_embed(index+1)\n",
        "    def embeder(self, char):\n",
        "        return self.embed(torch.tensor(encode(char)))\n",
        "    def encode(self, char):\n",
        "        x = ord(char)-31\n",
        "        if x>91: x=0\n",
        "        return x\n",
        "    def decode(self, x): return chr(x+31)\n",
        "dataset=Datasetme(text)\n"
      ],
      "metadata": {
        "id": "0Hz09uxCn-Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### functions"
      ],
      "metadata": {
        "id": "BLW1TAw6K7s5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def off_diagonal(x):\n",
        "    # print(\"off_diagonal\",x.shape)\n",
        "    n, m = x.shape\n",
        "    assert n == m\n",
        "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n"
      ],
      "metadata": {
        "id": "QHmNYpcUK95w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### jepa"
      ],
      "metadata": {
        "id": "75SRBabzK-6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_Xjga__gdeY"
      },
      "outputs": [],
      "source": [
        "\n",
        "class JEPA(nn.Module):\n",
        "    # def __init__(self, xin_channels, dim_sx, dim_sy, dim_z, dim_v, n_actions, space_dims, hidden_dims):\n",
        "    def __init__(self, in_dimx, in_dimy, dim_sx, dim_sy, dim_z, dim_v):\n",
        "    # def __init__(self, vocab_size, dim_sx, dim_sy, dim_z, dim_v):\n",
        "        super(JEPA, self).__init__()\n",
        "        self.enc_x = nn.Sequential(\n",
        "            # nn.Embedding(vocab_size, dim_sx),\n",
        "            # nn.Flatten(start_dim=1),\n",
        "            nn.Linear(in_dimx, dim_sx),\n",
        "            )\n",
        "        self.enc_y = nn.Sequential(\n",
        "            # nn.Embedding(vocab_size, dim_sy),\n",
        "            # nn.Flatten(start_dim=1),\n",
        "            nn.Linear(in_dimy, dim_sx),\n",
        "            )\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(dim_sx + dim_z, dim_sy),\n",
        "            nn.ReLU(True),\n",
        "            )\n",
        "        self.exp_x = nn.Sequential(\n",
        "            nn.Linear(dim_sx, dim_v),\n",
        "            nn.ReLU(True),\n",
        "            )\n",
        "        self.exp_y = nn.Sequential(\n",
        "            nn.Linear(dim_sy, dim_v),\n",
        "            nn.ReLU(True),\n",
        "            )    \n",
        "\n",
        "    def vicreg(self, x, y): # https://github.com/facebookresearch/vicreg/blob/main/main_vicreg.py\n",
        "        # invariance loss\n",
        "        repr_loss = F.mse_loss(x, y)\n",
        "        x = x - x.mean(dim=0)\n",
        "        y = y - y.mean(dim=0)\n",
        "\n",
        "        # variance loss\n",
        "        std_x = torch.sqrt(x.var(dim=0) + 0.0001)\n",
        "        std_y = torch.sqrt(y.var(dim=0) + 0.0001)\n",
        "        std_loss = torch.mean(F.relu(1 - std_x)) / 2 + torch.mean(F.relu(1 - std_y)) / 2\n",
        "\n",
        "        # # covariance loss\n",
        "        # cov_x = (x.T @ x) / (self.args.batch_size - 1)\n",
        "        # cov_y = (y.T @ y) / (self.args.batch_size - 1)\n",
        "        # cov_loss = off_diagonal(cov_x).pow_(2).sum().div(self.num_features)\\\n",
        "        #  + off_diagonal(cov_y).pow_(2).sum().div(self.num_features)\n",
        "\n",
        "        # loss = (self.args.sim_coeff * repr_loss + self.args.std_coeff * std_loss + self.args.cov_coeff * cov_loss)\n",
        "\n",
        "        batch_size=3\n",
        "        num_features=3\n",
        "        sim_coeff=1\n",
        "        std_coeff=1\n",
        "        cov_coeff=1\n",
        "\n",
        "        # print(\"x.dim()\",x.dim())\n",
        "        if x.dim() == 1:\n",
        "            x = x.view(-1, 1)\n",
        "        \n",
        "        if y.dim() == 1:\n",
        "            y = y.view(-1, 1)\n",
        "        x=x.T\n",
        "        y=y.T\n",
        "        # print(\"x\",x.shape)\n",
        "        cov_x = (x.T @ x) / (batch_size - 1)\n",
        "        cov_y = (y.T @ y) / (batch_size - 1)\n",
        "        # print(\"cov_x\",cov_x.shape)\n",
        "        cov_loss = off_diagonal(cov_x).pow_(2).sum().div(num_features)\\\n",
        "         + off_diagonal(cov_y).pow_(2).sum().div(num_features)\n",
        "\n",
        "        loss = (sim_coeff * repr_loss + std_coeff * std_loss + cov_coeff * cov_loss)\n",
        "        return loss\n",
        "\n",
        "    # def argm(self, sx, sy):\n",
        "    def argm(self, SX, SY):\n",
        "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "        sampler = optuna.samplers.NSGAIISampler()\n",
        "        # sampler = optuna.samplers.MOTPESampler()\n",
        "        pruner = optuna.pruners.MedianPruner()\n",
        "        batch_size=1\n",
        "        # if sx.dim() == 2: batch_size,_=sx.shape\n",
        "        if SX.dim() == 2: batch_size,_=SX.shape\n",
        "        s=[]\n",
        "        for i in range(batch_size):\n",
        "            sx, sy = SX[i],SY[i]\n",
        "            study = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
        "            # study = optuna.create_study()\n",
        "            def objective(trial):\n",
        "                z = trial.suggest_uniform('z', -1, 1)\n",
        "                # print(\"z trail\",sx,z)\n",
        "                # z=torch.tensor(z).to(device)\n",
        "                z=torch.tensor(z).view(1).to(device)\n",
        "                # print(\"sx, z\",sx.shape, z.shape) #[500, 20] [1]\n",
        "                sxz = torch.cat([sx, z], dim=-1)\n",
        "                sy_ = self.pred(sxz)\n",
        "                mseloss = nn.MSELoss()(sy, sy_)\n",
        "                return mseloss\n",
        "            study.optimize(objective, n_trials=10)\n",
        "            st=study.best_params\n",
        "            # print(\"st\",st['z'])\n",
        "            st=torch.tensor([st['z']])\n",
        "            s.append(st)\n",
        "        return torch.tensor(s)\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        if x.dim()==2: batch_size,_=x.shape\n",
        "        # print(\"loss\",x,x.shape,x.dtype)\n",
        "        sx = self.enc_x(x)\n",
        "        sy = self.enc_y(y)\n",
        "        sx=sx.flatten(start_dim=1)\n",
        "        sy=sy.flatten(start_dim=1)\n",
        "        # print(\"sx, sy\",sx.shape, sy.shape) #10000\n",
        "        z = self.argm(sx, sy).to(device)\n",
        "        # z=np.array(z)\n",
        "        z=torch.tensor(z).view(-1,1)\n",
        "        # print(\"sx, z\",sx.device, z.device) #[500, 20] [1]\n",
        "        sxz = torch.cat([sx, z], dim=-1)\n",
        "        sy_ = self.pred(sxz)\n",
        "        # loss(sy, sy_)\n",
        "        mseloss = nn.MSELoss()(sy, sy_)\n",
        "\n",
        "        vx = self.exp_x(sx)\n",
        "        vy = self.exp_y(sy)\n",
        "        # print(\"vx\",vx.shape) #[40]\n",
        "        vicloss = self.vicreg(vx, vy)\n",
        "        return mseloss + vicloss\n",
        "\n",
        "    def forward(self, sx, a):\n",
        "        # sx = self.enc_x(x)\n",
        "        # sx=sx.flatten()\n",
        "        sxz = torch.cat([sx, a], dim=-1)\n",
        "        # print(\"sxz\",sxz)\n",
        "        sy_ = self.pred(sxz)\n",
        "        return sy_\n",
        "    def encode(self, x): return self.enc_x(x)\n",
        "\n",
        "vocab_size=dataset.vocab_size\n",
        "embed_size=dataset.embed_size\n",
        "# embed_size=4\n",
        "in_dimx=embed_size # embedding size\n",
        "in_dimy=embed_size\n",
        "dim_sx=2\n",
        "dim_sy=2\n",
        "dim_z=1\n",
        "dim_v=5\n",
        "\n",
        "model = JEPA(in_dimx, in_dimy, dim_sx, dim_sy, dim_z, dim_v).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train eval"
      ],
      "metadata": {
        "id": "oEr6soVkqbQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "grad_clip_norm=1\n",
        "lr=1e-4\n",
        "betas=(0.9, 0.95)\n",
        "batch_size=1\n",
        "def train(loader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    pbar = tqdm(enumerate(loader), total = len(loader))\n",
        "    for it, (x, y) in pbar:\n",
        "        # print(\"x,y\",x.dtype,y.dtype) #torch.float32 [1,4]\n",
        "        # print(\"x,y\",x.shape,y.shape)\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # print(\"x\",x)\n",
        "        # with torch.set_grad_enabled(True):\n",
        "\n",
        "        # logits = model(x)\n",
        "        # # loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        # loss = loss_fn(logits, y)\n",
        "        loss = model.loss(x,y)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
        "        optimizer.step()\n",
        "        # lr = lr\n",
        "        # pbar.set_description(f\"epoch {epoch + 1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
        "        pbar.set_description(f\"epoch {epoch + 1} iter {it}: train loss {loss.item():.5f}\")\n",
        "\n",
        "\n",
        "def eval(loader, model, loss_fn):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    pbar = enumerate(loader)\n",
        "    for it, (x, y) in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # with torch.set_grad_enabled(False):\n",
        "        with torch.no_grad():\n",
        "            # logits = model(x)\n",
        "            # # loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "            # loss = loss_fn(logits, y)\n",
        "            loss = model.loss(x,y)\n",
        "            losses.append(loss.item())\n",
        "    test_loss = float(np.mean(losses))\n",
        "    logger.info(\"test loss: %f\", test_loss)\n",
        "    return test_loss\n"
      ],
      "metadata": {
        "id": "V_EUHNNBgxuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### wwwwwwwwwww"
      ],
      "metadata": {
        "id": "nsKk_OKSqfTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = lr, betas = betas)\n",
        "\n",
        "def loss_fn(logits, y):\n",
        "    return nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "\n"
      ],
      "metadata": {
        "id": "Yu4L88zkjA79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "# train_loader = DataLoader(train_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 4)\n",
        "# test_loader = DataLoader(test_dataset, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "loader = DataLoader(dataset, shuffle = False, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "epochs=1\n",
        "for epoch in range(epochs):\n",
        "    # run_epoch(train_loader)\n",
        "    train(loader, model, loss_fn, optimizer)\n",
        "    test_loss = eval(loader, model, loss_fn)\n",
        "    print('Test Loss:', test_loss)\n"
      ],
      "metadata": {
        "id": "WNxuQST8jGeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### inference"
      ],
      "metadata": {
        "id": "2gLKjKZej5L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context = \"This is what \"\n",
        "\n",
        "def encode(char):\n",
        "    x = ord(char)-31\n",
        "    if x>91: x=0\n",
        "    return x\n",
        "def decode(x): return chr(x+31)\n",
        "\n",
        "out=[]\n",
        "for _ in range(20):\n",
        "    a=torch.zeros(1,1).to(device)\n",
        "    # x=encode(\"w\")\n",
        "    x=dataset.embeder(\"w\")\n",
        "    # print(x)\n",
        "    x=torch.tensor(x).view(1,-1).to(device)\n",
        "    sx=model.encode(x)\n",
        "    # print(sx.shape, a.shape)\n",
        "    sy_ = model(sx, a)\n",
        "    out.append(sy_)\n",
        "print(out)\n",
        "print(''.join(out))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0H2ER22wNahP",
        "outputId": "ffcef79c-11ad-4847-fcb4-12082b959cb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>), tensor([[0.0130, 0.0379]], device='cuda:0', grad_fn=<ReluBackward0>)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  app.launch_new_instance()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-9fde50f579f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msy_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, Tensor found"
          ]
        }
      ]
    }
  ]
}